ID;Type;CreationDate;Title;Body;Tags;ViewCount;FavoriteCount
5;1;2014-05-13T23:58:30.457;How can I do simple machine learning without hard-coding behavior?;"I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple ""Hello World"" example - how can I avoid hard-coding behavior?For example, if I wanted to ""teach"" a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.Obviously, randomly generating code would be impractical, so how could I do this?";[education, open-source];240;1
7;1;2014-05-14T00:11:06.457;What open-source books (or other materials) provide a relatively thorough overview of data science?;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.;[education, open-source];245;1
9;2;2014-05-14T00:36:31.077;;"Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are ""fixed"" and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.";;;
10;2;2014-05-14T00:53:43.273;;"One book that's freely available is ""The Elements of Statistical Learning"" by Hastie, Tibshirani, and Friedman (published by Springer): see Tibshirani's website.Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.";;;
14;1;2014-05-14T01:25:59.677;Is Data Science the Same as Data Mining?;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?;[education, open-source];619;2
15;1;2014-05-14T01:41:23.110;What are the advantages and disadvantages of SQL versus NoSQL in data science?;In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?;[education, open-source];110;
16;1;2014-05-14T01:57:56.880;Use liblinear on big data for semantic analysis;I use Libsvm to train data and predict classification on semantic analysis problem. But it has a performance issue on large-scale data, because semantic analysis concerns n-dimension problem.Last year, Liblinear was release, and it can solve performance bottleneck.But it cost too much memory. Is MapReduce the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on Liblinear?;[education, open-source];186;
19;1;2014-05-14T03:56:20.963;How big is big data?;Lots of people use the term big data in a rather commercial way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, big data always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a big data problem?Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled big data if the dataset was big enough? Also, how big is big enough (if this is possible to define)?;[education, open-source];1253;5
20;1;2014-05-14T05:37:46.780;the data on our relational DBMS is getting big, is it the time to move to NoSQL?;we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it's a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  ;[education, open-source];189;1
21;2;2014-05-14T05:44:29.340;;"As you rightly note, these days ""big data"" is something everyone wants to say they've got, which entails a certain looseness in how people define the term.  Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop.How big your data has to actually be for that to be the case is debatable.  Here's a (somewhat provocative) blog post that claims that it's not really the case for less than 5 TB of data.  (To be clear, it doesn't claim ""Less than 5 TB isn't big data"", but just ""Less than 5 TB isn't big enough that you need Hadoop"".)But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally.Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably ""big"" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies.So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.  How you're using your data is more relevant to the question than what you're using it for (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.";;;
22;1;2014-05-14T05:58:21.927;K-Means clustering for mixed numeric and categorical data;My data set contains a number of numeric attributes and one categorical.Say, NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr, where CategoricalAttr takes one of three possible values: CategoricalAttrValue1, CategoricalAttrValue2 or CategoricalAttrValue3.I'm using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.It works with numeric data only.So my question: is it correct to split the categorical attribute CategoricalAttr into three numeric (binary) variables, like IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3 ?;[education, open-source];7716;14
23;2;2014-05-14T06:06:13.603;;Data Science specialization from Johns Hopkins University at Coursera would be a great start.https://www.coursera.org/specialization/jhudatascience/1;;;
24;2;2014-05-14T06:26:27.163;;"The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, ""The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs."" (from here)There's a variation of k-means known as k-modes, introduced in this paper by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed here (PDF), for instance.Huang's paper (linked above) also has a section on ""k-protoypes"" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.A Google search for ""k-means mix of categorical data"" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.";;;
25;2;2014-05-14T07:26:04.390;;Big Data is defined by the volume of data, that's right, but not only. The particularity of big data is that you need to store a lots of various and sometimes unstructured stuffs all the times and from a tons of sensors, usually for years or decade.Furthermore you need something scalable, so that it doesn't take you half a year to find a data back.So here's come Big Data, where traditional method won't work anymore. SQL is not scalable. And SQL works with very structured and linked data (with all those Primary and foreign key mess, innerjoin, imbricated request...). Basically, because storage becomes cheaper and cheaper and data becomes more and more valuable, big manager ask engineer to records everything. Add to this tons of new sensors with all those mobile, social network, embeded stuff ...etc. So as classic methods won't work, they have to find new technologies (storing everything in files, in json format, with big index, what we call noSQL). So Big Data may be very big but can be not so big but complexe unstructured or various data which has to be store quickly and on-the-run in a raw format. We focus and storing at first, and then we look at how to link everything together.;;;
26;2;2014-05-14T07:38:31.103;;"A few gigabytes is not very ""big"". It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).Most professionals working in a big data environment consider > ~5TB as the beginning of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.";;;
27;2;2014-05-14T07:53:02.560;;To answer this question you have to answer which kind of compromise you can afford. RDBMs implements ACID. This is expensive in terms of resources. There are no NoSQL solutions which are ACID. See CAP theorem to dive deep into these ideas. So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.;;;
28;2;2014-05-14T07:55:40.133;;"There is free ebook ""Introduction to Data Science"" based on r language";;;
29;2;2014-05-14T07:56:34.437;;"@statsRus starts to lay the groundwork for your answer in another question What characterises the difference between data science and statistics?:  Data collection: web scraping and online surveys Data manipulation: recoding messy data and extracting meaning from linguistic and social network data Data scale: working with extremely large data sets Data mining: finding patterns in large, complex data sets, with an emphasis on algorithmic techniques Data communication: helping turn ""machine-readable"" data into ""human-readable"" information via visualization Definitiondata-mining can be seen as one item (or set of skills and applications) in the toolkit of the data scientist.  I like how he separates the definition of mining from collection in a sort of trade-specific jargon.However, I think that data-mining would be synonymous with data-collection in a US-English colloquial definition.As to where to go to become proficient?  I think that question is too broad as it is currently stated and would receive answers that are primarily opinion based.  Perhaps if you could refine your question, it might be easier to see what you are asking.";;;
30;2;2014-05-14T08:03:28.117;;"Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 (source) and with a doubling time of 40 months. Can't get bigger than that :)As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 (source)  Their current warehouse is >300 petabytes.Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).In addition to size, some of the features that make it ""big"" are:it is actively analyzed, not just stored  (quote ""If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data"" Jay Parikh @ Facebook)building and running a data warehouse is a major infrastructure projectit is growing at a significant rateit is unstructured or has irregular structureGartner definition: ""Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing"" (The 3Vs)  So they also think ""bigness"" isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.";;;
31;1;2014-05-14T08:38:07.007;Clustering customer data stored in ElasticSearch;I have a bunch of customer profiles stored in a elasticsearch cluster. These profiles are now used for creation of target groups for our email subscriptions. Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).How could I search for interesting groups automatically - using data science, machine learning, clustering or something else?r programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.;[education, open-source];321;
33;2;2014-05-14T09:34:15.477;;"Depends on 2 things: the nature/structure of your data; and your performance.SQL databases excel when your data is well structured (e.g. when it can be modeled as a table or Excel spreadsheet). A set of rows with a fixed # of columns. Also, when you need to do a lot of table joins (which it sounds like you do).NoSQL databases excel when the data is UN-structured beyond key-value pairs.Performance wise, you gotta ask yourself one question: is your current SQL solution slow? If it isn't, go with ""IIABDFI"" principle. ";;;
35;1;2014-05-14T09:51:54.753;How to scale up algorithm development?;In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. How can the algorithm development itself be sped up and made more scalable?Some specific challenges: How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)How can algorithm parameters be tuned faster?How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc);[education, open-source];175;3
37;2;2014-05-14T10:41:23.823;;"To me (coming from a relational database background), ""Big Data"" is not primarily about the data size (which is the bulk of what the other answers are so far).""Big Data"" and ""Bad Data"" are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require ""Great Data""  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality.""Big Data"" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, and when the missing data is proportionally small enough to be negligible. When your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have ""Big Data"".""Big Data"" is not really about the volume, it is about the characteristics of the data.";;;
38;1;2014-05-14T10:44:58.933;What is the difference between Hadoop and noSQL;I heard about many tools / frameworks for helping people to process their data (big data environment). One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? Are they complementary?;[education, open-source];920;1
40;2;2014-05-14T11:12:03.880;;"Big Data is actually not so about the ""how big it is"". First, few gigabytes is not big at all, it's almost nothing. So don't bother yourself, your system will continu to work efficiently for some time I think.Then you have to think of how do you use your data. SQL approach: Every data is precious, well collected and selected, and the focus is put on storing high valuable and well structured data. This can be costly, everything is interlink, and it's good for well stuctured system and functionnal data.Big Data approach: In big data you basically store almost everything, regardless of the value it has, and then do a active analytics process. Things are not linked, they are copied. For example let's say I have a blog entry. In Big Data there will not be a link to its author, but the author will be embedded inside the blog entry. Way more scalable, but require a different and more complex approach.If your storing ""functionnal"" data use by your application, I will suggest you to remain on SQL. If your storing data in order to search on them later or to do reporting, and if this amount of data may increase quickly, I will suggest big data.In my opinion, big data is useful when you are dealing with real data that have to be collect and analyzed continuously.";;;
41;1;2014-05-14T11:15:40.907;Is the R language suitable for Big Data;"R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, ""Machine Learning with R"".I've seen a guideline of 5TB for a dataset to be considered as Big Data.My question is: Is R suitable for the amount of Data typically seen in Big Data problems? Are there strategies to be employed when using R with this size of dataset?";[education, open-source];1581;10
42;2;2014-05-14T11:21:31.500;;NoSQL is a way to store data that does not require there to be some sort of relation. The simplicity of its design and horizontal scale-ability, one way they store data is the key : value pair design. This lends itself to processing that is similar to Hadoop. The use of a NoSQL db really depends on the type of problem that one is after.Here is a good wikipedia link NoSQLHadoop is a system that is meant to store and process huge chunks of data. It is a distributed file system dfs. The reason it does this is that central to its design it makes the assumption that hardware failures are common, thus making multiple copies of the same piece of information and spreading it across multiple machines and racks, so if one goes down, no problem, we have two more copies. Here is a great link for Hadoop from wikipedia as well, you will see that it is, in my opinion more than just storage, but also processing: Hadoop;;;
43;2;2014-05-14T11:23:25.913;;Hadoop is not a database, hadoop is an entire ecosystem.Most people will refer to mapreduce jobs while talking about hadoop. A mapreduce job splits big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.Let's assume you load into hadoop a set of <String, Integer> with the population of some neighborhoods within a city and you want to get the average population over the whole neighborhoods of each city(figure 1).figure 1    [new york, 40394]    [new york, 134]    [la, 44]    [la, 647]    ...Now hadoop will first map each value by using the keys (figure 2)figure 2[new york, [40394,134]][la, [44,647]]...After the mapping it will reduce the values of each key to a new value (in this example the average over the value set of each key)(figure 3)figure 3[new york, [20264]][la, [346]]...now hadoop would be done with everything. You can now load the result into the HDFS (hadoop distributed file system) or into any DBMS or file.Thats just one very basic and simple example of what hadoop can do. You can run much more complicated tasks in hadoop.As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.;;;
44;2;2014-05-14T11:24:39.530;;Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you.A mature project for R and Hadoop is RHadoopRHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck (wiki).;;;
45;2;2014-05-14T11:26:40.580;;"First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.So, answering each question:characterization is definitely a means of reducing the number of iterations while trying to select proper algorithms for specific data;if you have a discrete set of criterias on which your data varies, it becomes much easier to scale up solutions, as will know what information you'd gain/lose if simpler/specific solutions were applied;after a characterization, you should be also easier to select parameters, since you'd know what kind of specific data you'd be dealing with;finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:clustering algorithms, to reduce the dimensionality of data;classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;and other possible strategies and analyses.And here is a list of some criterias on which to analyse data, which you may find helpful.";;;
46;2;2014-05-14T12:32:29.503;;Note that there is an early version of LIBLINEAR ported to Apache Spark. See mailing list comments for some early details, and the project site.;;;
47;2;2014-05-14T12:39:41.197;;The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with large datasets in R.RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being manipulated is restricted to your RAM. The overall data set can go much larger.The ff package allows using larger than RAM data sets by utilising memory-mapped pages.BigLM: It builds generalized linear models on big data. It loads data into memory in chunks.bigmemory : An R package which allows powerful and memory-efficient parallel analyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them. ;;;
50;1;2014-05-14T14:26:54.313;Running an R script programmatically;I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I'm on Windows, but I could easily put this script on my Linux machine if that would simplify the process. ;[education, open-source];128;
51;2;2014-05-14T14:48:32.180;; How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:crontab -eAnd add:00 4 * * * r source(/home/FilePath/.../myRscript.R)Source: Stack Overflow;;;
52;1;2014-05-14T15:25:21.700;Organized processes to clean data;From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?;[education, open-source];721;7
53;2;2014-05-14T15:42:02.393;;For windows, use the task scheduler to set the task to run for example daily at 4:00 AMIt gives you many other options regarding frequency etc.http://en.wikipedia.org/wiki/Windows_Task_Scheduler;;;
57;2;2014-05-14T16:29:39.927;;"From my point of view, this question is suitable for a two-step answer. The first part, let us call it soft preprocessing, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses. Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot.The second part, the hard preprocessing, actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed. To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts.For example, for people who deal with data coming from social websites (twitter, facebook, ...), the data retrieval usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so. For these cases, a simple awk script could clean up the data, producing a valid input file for later processing. From the magic set, one may also point out grep, sed, cut, join, paste, sort, and a whole multitude of other tools.In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data. In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl. This allows for building up API's to select specific data in a very straightforward and reusable way. Such API's are sometimes made public by their writers, such as IMDbPY, Stack Exchange API, and many others.So, answering the question: are there any best practices? It usually depends on your task. If you will always deal with the same data format, it's commonly best to write an organized script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job much faster than a Python script, or so. Since the clean up depends both on the dataset and on your purposes, it's hard to have everything already done. Yet, there are lots of API's that puts you halfway through with the problem.";;;
58;2;2014-05-14T17:06:33.337;;You can check out vowpal wabbit. It is quite popular for large-scale learning and includes parallel provisions.From their website: VW is the essence of speed in machine learning, able to learn from terafeature datasets with ease. Via parallel learning, it can exceed the throughput of any single machine network interface when doing linear learning, a first amongst learning algorithms.;;;
59;1;2014-05-14T17:48:21.240;What are R's memory constraints?;"In reviewing “Applied Predictive Modeling"" a reviewer states: One critique I have of statistical learning (SL) pedagogy is the  absence of computation performance considerations in the evaluation of  different modeling techniques. With its emphases on bootstrapping and  cross-validation to tune/test models, SL is quite compute-intensive.  Add to that the re-sampling that's embedded in techniques like bagging  and boosting, and you have the specter of computation hell for  supervised learning of large data sets. In fact, R's memory  constraints impose pretty severe limits on the size of models that can  be fit by top-performing methods like random forests. Though SL does a  good job calibrating model performance against small data sets, it'd  sure be nice to understand performance versus computational cost for  larger data.What are R's memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like random forests?";[education, open-source];481;
60;2;2014-05-14T17:58:48.297;;R performs all computation in-memory so you can't perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata processing like Hadoop.;;;
61;1;2014-05-14T18:09:01.940;Why Is Overfitting Bad in Machine Learning?;"Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?Related: Followup to the question above, ""When is a Model Underfitted?""";[education, open-source];1223;6
62;2;2014-05-14T18:27:56.043;;"Overfitting is empirically bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.  One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.P.S. On the ""ability to generalize"" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.";;;
64;2;2014-05-14T18:37:52.333;;Overfitting, in a nutshell, means take into account too much information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you'll always take into account those characteristics, and will produce very fine-grained results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the fine-grain on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.Edit: This might as well be of some use, perhaps adding dynamicity to the above explanation :D;;;
69;1;2014-05-14T20:03:15.233;Is it possible to automate generating reproducibility documentation?;"First, think it's worth me stating what I mean by replication & reproducibility:Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.The closet comparison I'm able to think of is documentation generators that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the ""reproducibility documentation generators"" the documentation should match.Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.Lastly, whole intent of this is to understand if it's possible to ""bake-in"" reproducibility documentation as you build out a stack, not after the stack is built.So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?UPDATE: Please note that this is the second draft of this question and that Christopher Louden was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!";[education, open-source];58;
70;2;2014-05-14T22:03:50.597;;To be reproducible without being just a replication, you would need to redo the experiment with new data, following the same technique as before.  The work flow is not as important as the techniques used.  Sample data in the same way, use the same type of models.  It doesn't matter if you switch from one language to another, so long as the models and the data manipulations are the same.This type of replication will show that the results you got in the first experiment are less likely to be a fluke than they were earlier.;;;
71;1;2014-05-14T22:12:37.203;When are p-values deceptive?;What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?;[education, open-source];264;2
72;2;2014-05-14T22:40:40.363;;"One algorithm that can be used for this is the k-means clustering algorithm.Basically:Randomly choose k datapoints from your set, m_1, ..., m_k.""Until convergence"":Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current meansReplace each m_i by the mean of all points assigned to cluster i.It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though.";;;
73;2;2014-05-14T22:43:23.587;;You shouldn't consider the p-value out of context.One rather basic point (as illustrated by xkcd) is that you need to consider how many tests you're actually doing.  Obviously, you shouldn't be shocked to see p < 0.05 for one out of 20 tests, even if the null hypothesis is true every time.  A more subtle example of this occurs in high-energy physics, and is known as the look-elsewhere effect.  The larger the parameter space you search for a signal that might represent a new particle, the more likely you are to see an apparent signal that's really just due to random fluctuations. ;;;
74;2;2014-05-14T22:58:11.583;;"One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper ""Too Big to Fail: Large Samples and the p-Value Problem"" covers some of the issues. ";;;
75;1;2014-05-15T00:26:11.387;Is there a replacement for small p-values in big data?;If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?;[education, open-source];95;1
76;1;2014-05-15T00:39:33.433;Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?;(Note: Pulled this question from the list of questions in Area51, but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.) Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?;[education, open-source];104;1
77;1;2014-05-15T01:22:35.167;Is this Neo4j comparison to RDBMS execution time correct?;"Background: Following is from the book Graph Databases, which covers a performance test mentioned in the book Neo4j in Action: Relationships in a graph naturally form paths. Querying, or  traversing, the graph involves following paths. Because of the  fundamentally path-oriented nature of the datamodel, the majority of  path-based graph database operations are highly aligned with the way  in which the data is laid out, making them extremely efficient. In  their book Neo4j in Action, Partner and Vukotic perform an experiment  using a relational store and Neo4j. The comparison shows that the graph database is substantially quicker  for connected data than a relational store.Partner and Vukotic’s  experiment seeks to find friends-of-friends in a social network, to a  maximum depth of five. Given any two persons chosen at random, is  there a path that connects them which is at most five relationships  long? For a social network containing 1,000,000 people, each with  approximately 50 friends, the results strongly suggest that graph  databases are the best choice for connected data, as we see in Table  2-1. Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4jDepth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned2       0.016                       0.01                         ~2500    3       30.267                      0.168                        ~110,000 4       1543.505                    1.359                        ~600,000 5       Unfinished                  2.132                        ~800,000 At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, it’s clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4j’s response time remains relatively flat: just a fraction of a second to perform the query—definitely quick enough for an online system. At depth four the relational database exhibits crippling latency,  making it practically useless for an online system. Neo4j’s timings  have deteriorated a little too, but the latency here is at the  periphery of being acceptable for a responsive online system. Finally,  at depth five, the relational database simply takes too long to  complete the query. Neo4j, in contrast, returns a result in around two  seconds. At depth five, it transpires almost the entire network is our  friend: for many real-world use cases, we’d likely trim the results,  and the timings.Questions are: Is this a reasonable test to emulate what one might except to find in a social network? (Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the ""rich get richer"" model would be more natural for social networks, though might be wrong.)Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible? ";[education, open-source];166;
78;2;2014-05-15T01:46:28.467;;There is no replacement in the strict sense of the word.  Instead you should look at other measures.The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also consider the magnitude of the effect size.  It may be highly statistically significant but in practice meaningless.  It is also helpful to report the confidence interval of the effect size.I would consider this paper as mentoned in DanC's answer to this question.;;;
81;1;2014-05-15T04:59:54.317;Parallel and distributed computing;What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is parallel AND distributed?;[education, open-source];127;
82;2;2014-05-15T05:19:34.757;;Simply set, 'parallel' means running concurrently on distinct resources (CPUs), while 'distributed' means running across distinct computers, involving issues related to networks.Parallel computing using for instance OpenMP is not distributed, while parallel computing with Message Passing is often distributed.Being in a 'distributed but not parallel' setting would mean under-using resources so it is seldom encountered but it is conceptually possible.;;;
83;2;2014-05-15T07:47:44.710;;I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:  Motivations for using relational database / ORM or document database / ODMSummary:for small stuff, go with whatever tools you are familiar witha few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single MySQL Cluster with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100's of TB data and a few thousand transactions per second).if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.once you have data larger than a few TB and faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.Cassandra :);;;
84;2;2014-05-15T08:19:40.577;;You are asking about Data Dredging, which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.  In particular, check out Multiple hypothesis hazard, and Testing hypotheses suggested by the data.The solution is to use some kind of correction for False discovery rate or Familywise error rate, such as Scheffé's method or the (very old-school) Bonferroni correction.In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result.  If the 99% confidence interval for the odds ratio is 10-12, then the OR is <= 1 with some extremely small probability, especially if the sample size is also large.  If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.  ;;;
85;2;2014-05-15T08:44:47.327;;See also When are p-values deceptive?When there are a lot of variables that can be tested for pair-wise correlation (for example), the replacement is to use any of the corrections for False discovery rate (to limit probability that any given discovery is false) or Familywise error rate (to limit probability of one or more false discoveries).  For example, you might use the Holm–Bonferroni method.In the case of a large sample rather than a lot of variables, something else is needed.  As Christopher said, magnitude of effect a way to treat this.  Combining these two ideas, you might use a confidence interval around your magnitude of effect, and apply a false discovery rate correction to the p-value of the confidence interval.  The effects for which even the lowest bound of the corrected confidence interval is high are likely to be strong effects, regardless of huge data set size.  I am not aware of any published paper that combines confidence intervals with false discovery rate correction in this way, but it seems like a straightforward and intuitively understandable approach.To make this even better, use a non-parametric way to estimate confidence intervals.  Assuming a distribution is likely to give very optimistic estimates here, and even fitting a distribution to the data is likely to be inaccurate.  Since the information about the shape of the distribution past the edges of the confidence interval comes from a relatively small subsample of the data, this is where it really pays to be careful.  You can use bootstrapping to get a non-parametric confidence interval.;;;
86;1;2014-05-15T09:04:09.710;Clustering unique visitors by useragent, ip, session_id;Given website access data in the form session_id, ip, user_agent, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?session_id: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymoreIP can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.User_agent is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.Data might look as the fiddle here:http://sqlfiddle.com/#!2/c4de40/1Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_idfrom     (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr    from        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a    join        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b        order by 1    )dinner join    (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5    union all select 6 union all select 7 union all select 8 union all select 9 )e    on d.nr>=e.nr;[education, open-source];242;1
87;2;2014-05-15T09:30:36.460;;Looking at this document called Anatomy of Facebook I note that the median is 100. Looking at the cumulative function plot I can bet that the average is higher, near 200. So 50 seems to not be the best number here. However I think that this is not the main issue here. The main issue is the lack of information on how the database was used.It seems reasonable that a data storage designed specially for graph structures to be more efficient than traditional RDBMs. However, even if the RDBMs are not in the latest trends as a data storage of choice, these systems evolved continuously in a race with the data set dimensions. There are various types of possible designs, various ways of indexing data, improvements related with concurrency and so on. To conclude I think that regarding reproducibility, the study lack a proper description of how the database schema was designed. I do not expect that a database to dominate on such king of interrogations, however I would expect that with a well-tuned design the differences to not be such massive.;;;
89;1;2014-05-15T11:22:27.293;How does a query into a huge database return with negligible latency?;For example, when searching something in Google, results return nigh-instantly.I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.Does MapReduce help solve this problem?EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?;[education, open-source];186;
90;2;2014-05-15T11:46:38.170;;"The terms ""parallel computing"" and ""distributed computing"" certainly have a large overlap, but can be differentiated further. Actually, you already did this in your question, by later asking about ""parallel processing"" and ""distributed processing"". One could consider ""distributed computing"" as the more general term that involves ""distributed processing"" as well as, for example, ""distributed storage"". The common term, ""distributed"", usually refers to some sort of Message Passing over a network, between machines that are physically separated.The term ""parallel computing"" is also in the process of being further defined, e.g. by explicitly differentiating between the terms ""parallel"" and ""concurrent"", where - roughly - the first one refers data parallelism and the latter to task parallelism, although there are hardly really strict and binding defintions. So one could say that ""distributed processing"" usually (although not necessarily) means that it also is ""parallel processing""""distributed computing"" is more general, and also covers aspects that are not related to parallelismand obviously, ""parallel computing""/""parallel processing"" does not imply that it is ""distributed""";;;
91;2;2014-05-15T11:56:43.607;;"Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it feasible to have such low latency on queries from all these TBs of data in different machines:distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.Considering that, lets try to address your questions: but I imagine it infeasible for the results of every single possible query to be indexedYes, it would be, and actually is infeasible to have results for every single possible query. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these n -> inf terms (2^n). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached. wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDsNowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing ruling market is money, and the investors are not interested in wasting it. So what is done?The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of performance. But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.Memory cards are expensive for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending specific geographical regions, which allows for more specialized data caching, and even better response times. Does MapReduce help solve this problem?Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely not Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time. Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?The graph below presents a curve of how the kinds of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called obscure queries, which usually consist of non-experienced users' queries, are not a negligible part of the queries.And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have relevant results. If you type in something much too obscure in a Google search, it shan't take longer to return a list of results, but will most probably show you something it inferred you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most popular results. And all these solutions can be tailored and tweaked to respect feasible waiting times of, say, less then a second? :D";;;
92;2;2014-05-15T13:18:38.693;;"MapReduce has nothing to do with real-time anything. It is a batch-oriented processing framework suitable for some offline tasks, like ETL and index building. Google has moved off of MapReduce for most jobs now, and even the Hadoop ecosystem is doing the same.The answer to low latency is generally to keep precomputed indices in memory. Anything that touches disk is hard to make fast and scale. This is how newer-generation Hadoop-based SQL engines like Impala get so much speed compared to MapReduce-based infrastructure like Hive, for example.Search infrastructure can't cache the results of every single query. But it sure can cache intermediate results, or, more complete results for top queries. With a little caching you can serve results for a significant minority of all queries.Search is also split across servers. So one machine can delegate to 100 to each get a part of the result and then combine them.You can also get away with some degree of approximation. Google does not literally form a thousand pages of search results; it just has to get the first page about right.Keep in mind that Google has millions of computers around the globe. Your queries are going to a data center geographically near to you and that is only serving your geography. This cuts out most of the latency, which is network and not processing time in the data center.";;;
93;2;2014-05-15T13:30:04.270;;There's not much you can do with just this data, but what little you can do does not rely on machine learning. Yes, sessions from the same IP but different User-Agents are almost certainly distinct users. Sessions with the same IP and User-Agent are usually the same user, except in the case of proxies / wi-fi access points. Those you might identify by looking at the distribution of session count per IP to identify likely 'aggregate' IPs. Sessions from the same IP / User-Agent that overlap in time are almost surely distinct.To further distinguish users you would need more info. For example, the sites or IP addresses that the user is connecting to would be a very strong basis for differentiating sessions. Then you could get into more sophisticated learning to figure out when sessions are the same or different users.;;;
94;1;2014-05-15T14:41:24.020;Does click frequency account for relevance?;While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?;[education, open-source];101;1
95;2;2014-05-15T15:06:24.600;;Is it valid to use click frequency, then yes. Is it valid to use only the click frequency, then probably no.Search relevance is much more complicated than just one metric. There are entire books on the subject. Extending this answer beyond a simple yes/no would likely make the answer far too broad (and opinionated);;;
96;2;2014-05-15T15:10:30.243;;"For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions.i.e.: We have historical data from 1 year over 2 products (Snowboots[], Sandalettes[])[Snowboots[1024,1253,652,123,50,12,8,4,50,148,345,896]][Sandalettes[23,50,73,100,534,701,1053,1503,1125,453,213,25]]where [0] = JanuaryAs you can see, snowboots are much more searched in January than sandalettes, so you should suggest snowboots to someone searching shoes on your site or /we on january.You can also see if something is ""fresh"" at this time, like when people often click a unknown product it could be an insight for a new comming trend or something.That are just some samples where you could use click frequency as a insight. I think there are no rules for what you can use or not in recommendations, as long as it makes sense.";;;
97;2;2014-05-15T17:14:36.817;;"Depends on the user's intent, for starters. Users normally only view the first set of links, which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a click and attention distribution heat-map for  Google search results:Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see ""YouTube's head of engineering speaking about clicks vs engagement"" for more information, though note that the size itself of the content is a factor too.Might be worth noting that historically Google was known for PageRank algorithm though it's possible your intent is only to review click-streams, so I won't delve Google ranking factors; if you are interested in the Google's approach, you might find a review of Google's Search Quality Rating Guidelines.";;;
101;2;2014-05-15T21:41:22.703;;"One possibility here (and this is really an extension of what Sean Owen posted) is to define a ""stable user.""For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):uid = MD5Hash(ip + UA.device + UA.model)Then you flag these ids with ""stable"" or ""unstable"" based on usage heuristics you observe for your users.  This can be a threshold of # of visits in a given time window, length of time their cookies persist, some end action on your site (I realize this wasn't stated in your original log), etc...The idea here is to separate the users that don't drop cookies from those that do.From here you can attribute session_ids to stable uids from your logs.  You will then have ""left over"" session_ids for unstable users that you are relatively unsure about.  You may be over or under counting sessions, attributing behavior to multiple people when there is only one, etc...  But this is at least limited to the users you are now ""less certain"" about.You then perform analytics on your stable group and project that to the unstable group.  Take a user count for example, you know the total # of sessions, but you are unsure of how many users generated those sessions.  You can find the # sessions / unique stable user and use this to project the ""estimated"" number of unique users in the unstable group since you know the number of sessions attributed to that group.projected_num_unstable_users = num_sess_unstable / num_sess_per_stable_uidThis doesn't help with per user level investigation on unstable users but you can at least get some mileage out of a cohort of stable users that persist for some time.  You can, by various methods, project behavior and counts into the unstable group.  The above is a simple example of something you might want to know.  The general idea is again to define a set of users you are confident persist, measure what you want to measure, and use certain ground truths (num searches, visits, clicks, etc...) to project into the unknown user space and estimate counts for them.This is a longstanding problem in unique user counting, logging, etc... for services that don't require log in.";;;
102;1;2014-05-16T05:09:33.557;What is the Best NoSQL backend for a mobile game;What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior.;[education, open-source];151;
103;1;2014-05-16T14:26:12.270;Clustering based on similarity scores;Assume that we have a set of elements E and a similarity (not distance) function sim(ei, ej) between two elements ei,ej ∈ E. How could we (efficiently) cluster the elements of E, using sim?k-means, for example, requires a given k, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?Note, that sim is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of E).;[education, open-source];326;2
106;2;2014-05-16T16:25:58.250;;"There are many overlaps between data mining and datascience. I would say that people with the role of datamining are concerned with data collection and the extraction of features from unfiltered, unorganised and mostly raw/wild datasets. Some very important data may be difficult to extract, not do to the implementation issues but because it may have foreign artifacts. Eg. if I needed someone to look at financial data from written tax returns in the 70s which were scanned and machine read to find out if people saved more on car insurance; a dataminer would be the person to get. If I needed someone to examine the influence Nike's Twitter profile in the tweets of Brazil and identify key positive features from the profile, I would look for a datascientist.";;;
107;1;2014-05-16T20:07:50.983;Opensource tools for help in mining stream of leader board scores;"Consider a stream containing tuples (user, new_score) representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. I would like to have some standing queries like: Which players posted more than x scores in a sliding window of one hourWhich players gained x% score in a sliding window of one hourMy question is which open source tools can I employ to jumpstart this project? I am considering Esper at the moment. Note: I have just completed reading ""Mining Data Streams"" (chapter 4 of Mining of Massive Datasets) and I am quite new to mining data streams.";[education, open-source];67;
111;2;2014-05-17T03:07:59.707;;Some factors you might consider:Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time.Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore. Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you'd be technically ok with anything, which is why I'm mainly covering other factors.;;;
112;2;2014-05-17T04:18:10.020;;This isn't a full solution, but you may want to look into OrientDB as part of your stack. Orient is a Graph-Document database server written entirely in Java. In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.Consider:An introduction to orient: http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/Complex, real-time queries: http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/A discussion about streaming options with java and orient: https://github.com/orientechnologies/orientdb/issues/1227;;;
113;1;2014-05-17T04:53:03.913;When a relational database has better performance than a no relational;When a relational database like mySQL has better performance than a no relational, like mongo?I saw a question on Quora other day, about why Quora still uses mySQL as their backend. And how their performance is still good.;[education, open-source];97;
115;1;2014-05-17T08:45:08.420;Is there any APIs for crawling abstract of paper?;"If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?The paper names are like ""Assessment of Utility in Web Mining for the Domain of Public Health"".Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.";[education, open-source];116;
116;1;2014-05-17T09:16:18.823;Machine learning techniques for estimating users' age based on Facebook sites they like;"I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.There are three crucial characteristics of my database:the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).there's many more features than samples.So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?I mainly use Python, so Python-specific hints would be greatly appreciated.";[education, open-source];990;8
120;2;2014-05-17T18:15:11.937;;arXiv has an API and bulk download but if you want something for paid journals it will be hard to come by without paying an indexer like pubmed or elsevier or the like.;;;
121;2;2014-05-17T18:53:30.123;;One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space.So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment.If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group.In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric.I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best.One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s.;;;
122;2;2014-05-17T20:56:15.577;;It depends on your data and what you're doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implementing it yourself on top of NoSQL databases which don't support it natively. ;;;
125;1;2014-05-17T21:52:34.563;How to learn noSQL databases and how to know when SQL or noSQL is better;I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I'm asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Also, your opinions on NoSQL databases, and any recommendations for learning about this topic are welcome.;[education, open-source];78;1
126;2;2014-05-17T23:53:42.700;;Please have a look at my answer here: Motivations for using relational database / ORM or document database / ODMShort version:Use NoSQL is data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), or at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is highly dependent on the usage pattern). Traditional SQL scales up to that point just fine.NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use there even if scaling doesn't force you to.Developer familiarity with tools and ops ease of deployment are major factors, don't overlook those.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve.As to how to learn it: fire up a MongoDB image on AWS, or DynamoDB, and have fun! http://docs.mongodb.org/ecosystem/platforms/amazon-ec2/http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStartedDynamoDB.html;;;
128;1;2014-05-18T06:10:52.543;Latent Dirichlet Allocation vs Hierarchical Dirichlet Process;Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Process (HDP) are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn't. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?;[education, open-source];1105;3
129;1;2014-05-18T06:17:37.587;What is generative and discriminative model? How are they used in Natural Language Processing?;This question asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? How are generative and discriminative models used in NLP?;[education, open-source];68;
130;1;2014-05-18T06:26:15.673;What is dimensionality reduction? What is the difference between feature selection and extraction?;From wikipedia,  dimensionality reduction or dimension reduction is the process of  reducing the number of random variables under consideration, and  can be divided into feature selection and feature extraction.What is the difference between feature selection and feature extraction?What is an example of dimensionality reduction in a Natural Language Processing task?;[education, open-source];1679;2
131;2;2014-05-18T06:54:08.560;;Look it up on:Google Scholar linkCiteseer linkIf you get a single exact title match then you have probably found the right article, and can fill in the rest of the info from there.  Both give you download links and bibtex-style output.  What you would likely want to do though to get perfect metadata is download and parse the pdf (if any) and look for DOI-style identifier.Please be nice and rate-limit your requests if you do this.;;;
132;2;2014-05-18T07:53:58.203;;"Simply put: feature selection: you select a subset of the original feature set; whilefeature extraction: you build a new set of features from the original feature set. Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc.Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction.";;;
133;2;2014-05-18T09:09:47.780;;"I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you'd have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim > 1/epsilon rather than the other way around.  Same story with k-means and related algorithms.Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) < sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim values means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?  What about min( 1/sim(ei, ek) + 1/sim(ek, ej) ) for all k? (that last is guaranteed to be a metric, btw)An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -> xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you'd have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You'd also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly.On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn't change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.";;;
134;1;2014-05-18T12:03:21.650;Can map-reduce algorithms written for MongoDB be ported to Hadoop later?;In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses:One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it.Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB's map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months.The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop?;[education, open-source];328;1
135;2;2014-05-18T12:30:06.853;;Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team.In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: Python Displacing R and Rich Scientific Data Structures in Python that may soon fill the gap of available libraries for R.Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity's online course) is Python.Edit:Recently DARPA has also invested $3 million to help fund Python's data processing and visualization capabilities for big data jobs, which is clearly a sign of Python's future in Big Data. (details);;;
138;1;2014-05-18T14:02:51.350;Why is it hard to grant efficiency while using libraries?;Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, people tend to reach out for C/C++/low-level languages. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).Of course such set of benefits would not come without a cost: writing the code, and sometimes even reinventing the wheel, can be quite expensive/tiresome. Although there are lots of libraries available, people are inclined to write the code by themselves whenever they need to grant performance. What disables performance assertions from using libraries while processing large databases?For example, consider an entreprise that continuously crawls webpages and parses the data collected. For each sliding-window, different data mining algorithms are run upon the data extracted. Why would the developers ditch off using available libraries/frameworks (be it for crawling, text processing, and data mining)? Using stuff already implemented would not only ease the burden of coding the whole process, but also would save a lot of time.In a single shot:what makes writing the code by oneself a guarantee of performance?why is it risky to rely on a frameworks/libraries when you must assure high performance?;[education, open-source];152;
139;2;2014-05-18T14:04:37.870;;I'd suggest Apache Kafka as message store and any stream processing solution of your choice like Apache Camel or Twitter Storm;;;
140;2;2014-05-18T14:30:10.553;;I've read very good article recently that suggests using Twitter storm for a task that looks pretty similar to yours.;;;
143;1;2014-05-18T14:37:20.477;What is the most efficient data indexing technique;As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.;[education, open-source];144;
153;2;2014-05-18T17:38:01.383;;"The answers presented so far are very nice, but I was also expecting an emphasis on a particular difference between parallel and distributed processing: the code executed. Considering parallel processes, the code executed is the same, regardless of the level of parallelism (instruction, data, task). You write a single code, and it will be executed by different threads/processors, e.g., while computing matrices products, or generating permutations.On the other hand, distributed computing involves the execution of different algorithms/programs at the same time in different processors (from one or more machines). Such computations are later merged into a intermediate/final results by using the available means of data communication/synchronization (shared memory, network). Further, distributed computing is very appealing for BigData processing, as it allows for exploiting disk parallelism (usually the bottleneck for large databases).Finally, for the level of parallelism, it may be taken rather as a constraint on the synchronization. For example, in GPGPU, which is single-instruction multiple-data (SIMD), the parallelism occurs by having different inputs for a single instruction, each pair (data_i, instruction) being executed by a different thread. Such is the restraint that, in case of divergent branches, it is necessary to discard lots of unnecessary computations, until the threads reconverge. For CPU threads, though, they commonly diverge; yet, one may use synchronization structures to grant concurrent execution of specific sections of the code.";;;
154;2;2014-05-18T17:53:37.750;;"Check Martin Fowler's Personal website He write good, and specially answer of your question One of his book : ""NoSQL Distilled"" ";;;
155;1;2014-05-18T18:45:38.957;Publicly available datasets;One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metrics from various sources for making higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somewhat repetitive work. For example analyzing tweets, facebook posts, wikipedia articles etc. is a part of lot of big data problems.Some of these data sets are accessible using public APIs provided by the provider site, but usually some valuable information or metrices are missing from these APIs and everyone has to do the same analyses again and again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API, nor available publicly in independent data sets.Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other big data problems? I mean something like GitHub (or a group of sites/public data sets or at least a comprehensive listing) for the data science. If not, what are the reasons of not having such a platform for data science? Commercial value of data, need to frequently update data sets, ...? Can we not have an open-source model for sharing data sets devised for data scientists?;[education, open-source];3088;70
156;2;2014-05-18T19:19:44.240;;Freebase is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.;;;
157;2;2014-05-18T19:22:05.160;;"R is great for ""big data""! However, you need a workflow since R is limited (with some simplification) by the amount of RAM in the operating system. The approach I take is to interact with a relational database (see the RSQLite package for creating and interacting with a SQLite databse), run SQL-style queries to understand the structure of the data, and then extract particular subsets of the data for computationally-intensive statistical analysis.This just one approach, however: there are packages that allow you to interact with other databases (e.g., Monet) or run analyses in R with fewer memory limitations (e.g., see pbdR).";;;
158;2;2014-05-18T19:29:53.530;;"There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources. Here are some of them:Public Datasets on Amazon WebServices;Frequent Itemset Mining Implementation Repository;UCI Machine Learning Repository;KDnuggets -- big list of lots of public repositories.Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can't be made publicly available, either for involving privacy restraints (as for some social network informations), or for concerning government information (like health system databases).Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some nice organization of the datasets by purpose. The taxonomy should involve social graph analysis, itemset mining, classification, and lots of other reasearch areas there may be.";;;
159;1;2014-05-18T19:46:44.653;Is Data Science just a trend or is a long term concept?;I see a lot of courses in Data Science emerging in the last 2 years. Even big universities like Stanford and Columbia offers MS specifically in Data Science. But as long as I see, it looks like data science is just a mix of computer science and statistics techniques.So I always think about this. If it is just a trend and if in 10 years from now, someone will still mention Data Science as an entire field or just a subject/topic inside CS or stats.What do you think?;[education, open-source];295;
160;2;2014-05-18T21:05:28.990;;"The one thing that you can say for sure is: Nobody can say this for sure. And it might indeed be opinion-based to some extent. The introduction of terms like ""Big Data"" that some people consider as ""hypes"" or ""buzzwords"" don't make it easier to flesh out an appropriate answer here. But I'll try. In general, interdisciplinary fields often seem to have the problem of not being taken serious by either of the fields they are spanning. However, the more research is invested into a particular field, the greater is the urge to split this field into several sub-topics. And these sub-topics sonner of later have to be re-combined in new ways, in order to prevent an overspecialization, and to increase and broaden the applicability of techniques that are developed by the (over?)specialized experts in the different fields. And I consider ""Data Science"" as such an approach to combine the expertise and findings from different fields. You described it as ...a mix of computer science and statistics techniquesAnd indeed, several questions here aim at the differentiation between data science and statistics. But a pure statistician will most likely not be able to set up a Hadoop cluster and show the results of his analysis in an interactive HTML5 dashboard. And someone who can implement a nice HTML5 dashboard might not be so familiar with the mathematical background of a Chi-Squared-Test. It is reasonable to assume that giving students enough knowledge to apply the most important techniques from the different fields that are covered by data science will lead to new applications of these techniques, and be beneficial - also for the ""purists"" in these fields. The combination of these techniques is not straightforward in many cases, and can justify an own branch of research. You also asked whether in 10 years, data science will be considered as ""just a topic inside computer science"". Again: Nobody can say for sure. But I wonder at which point people stopped asking the question whether ""Computer Science"" will one day only be considered only as a mix of (or a subject of) Electrical Engineering and Mathematics...";;;
161;2;2014-05-18T21:25:29.040;;"I don't think that everyone reaches for C/C++ when performance is an issue.The advantage to writing low-level code is using fewer CPU cycles, or sometimes, less memory. But I'd note that higher-level languages can call down to lower-level languages, and do, to get some of this value. Python and JVM languages can do this.The data scientist using, for example, scikit-learn on her desktop is already calling heavily optimized native routines to do the number crunching. There is no point in writing new code for speed.In the distributed ""big data"" context, you are more typically bottleneck on data movement: network transfer and I/O. Native code does not help. What helps is not writing the same code to run faster, but writing smarter code.Higher-level languages are going to let you implement more sophisticated distributed algorithms in a given amount of developer time than C/C++. At scale, the smarter algorithm with better data movement will beat dumb native code.It's also usually true that developer time, and bugs, cost loads more than new hardware. A year of a senior developer's time might be $200K fully loaded; over a year that also rents hundreds of servers worth of computation time. It may just not make sense in most cases to bother optimizing over throwing more hardware at it.I don't understand the follow up about ""grant"" and ""disable"" and ""assert""?";;;
162;2;2014-05-18T22:16:19.300;;There are many openly available data sets, one many people often overlook is data.gov. As mentioned previously Freebase is great, so are all the examples posted by @Rubens;;;
163;2;2014-05-18T23:21:07.220;;"As all we know, in Digital world there are many ways to do the same work / get expected results..And responsibilities / risks which comes from the code are on developers' shoulders..This is small but i guess a very useful example from .NET world..So Many .NET developers use the built-in BinaryReader - BinaryWriter on their data serialization for performance / get control over the process..This is CSharp source code of the FrameWork's built in BinaryWriter class' one of the overloaded Write Methods :// Writes a boolean to this stream. A single byte is written to the stream// with the value 0 representing false or the value 1 representing true.// public virtual void Write(bool value) {     //_buffer is a byte array which declared in ctor / init codes of the class    _buffer = ((byte) (value? 1:0));    //OutStream is the stream instance which BinaryWriter Writes the value(s) into it.    OutStream.WriteByte(_buffer[0]);}As you see, this method could written without the extra assigning to _buffer variable:public virtual void Write(bool value) {    OutStream.WriteByte((byte) (value ? 1 : 0));}Without assigning we could gain few milliseconds..This few milliseconds can accept as ""almost nothing"" but what if there are multi-thousands of writing (i.e. in a server process)?Lets suppose that ""few"" is 2 (milliseconds) and multi-Thousands instances are only 2.000..This means 4 seconds more process time..4 seconds later returning..If we continue to subject from .NET and if you can check the source codes of BCL - .NET Base Class Library- from MSDN you can see a lot of performance losts from the developer decides..Any of the point from BCL source It's normal that you see developer decided to use while() or foreach() loops which could implement a faster for() loop in their code.This small gains give us the total performance..And if we return to the BinaryWriter.Write() Method..Actually extra assigning to a _buffer implementation is not a developer fault..This is exactly decide to ""stay in safe"" !Suppose that we decide to not use _buffer and decided to implement the second method..If we try to send multi-thousands bytes over a wire (i.e. upload / download a BLOB or CLOB data) with the second method, it can fail commonly because of connection lost..Cause we try to send all data without any checks and controlling mechanism.When connection lost, Both the server and Client never know the sent data completed or not.If the developer decides ""stay in safe"" then normally it means performance costs depends to implemented ""stay in safe"" mechanism(s).But if the developer decides ""get risky, gain performance"" this is not a fault also..Till there are some discussions about ""risky"" coding.And as a small note : Commercial library developers always try to stay in safe because they can't know where their code will use.";;;
164;2;2014-05-19T00:09:17.900;;"What @Clayton posted seems about right to me, for those terms, and for ""data mining"" being one tool of the data scientist. However, I haven't really used the term ""data collection,"" and it doesn't strike me as synonymous with ""data mining.""My own answer to your question: no, the terms aren't the same. Definitions may be loose in this field, but I haven't seen those terms used interchangeably. In my work, we sometimes use them to differentiate between goals, or methodologies. For us, data-science is more about testing a hypothesis, and typically the data have been collected just for that purpose. data-mining is more about sifting through existing data, looking for structure, and perhaps generating hypotheses. Data mining can start with a hypothesis, but it's often very weak or general, and can be difficult to resolve with confidence. (Dig long enough and you'll find something, though it may turn out to be pyrite.)However, we also have used ""data science"" as a wider term, to include ""data mining."" We also talk about ""data modeling,"" which for us is about finding a model for a system of interest, based on data as well as other knowledge and objectives. Sometimes that means trying to find the math that explains the real system, and sometimes it means finding a predictive model that is good enough for a purpose.";;;
165;2;2014-05-19T08:46:47.683;;I think you messed up some things in your question. Lucene (I know nothing about Lucene,NET, but I suppose is the same) is a library used to analyze, split in tokens, and store documents in order to be able to query and retrieve them later. Lucene has a pretty old but effective model, it uses inverted trees to find and retrieve documents. Without further details, all documents are split in tokens (terms), and for each term is maintained a data structure, which stores all the documents which contains the given term. As a data structure could be used a BTree, a hash table and in the latest major revisions you can even plug in your own data structures.A BTree (see Wikipedia page for further details), is a kind of a tree data structure, which is appropriate for working with big chunks of data and is often used for storing tree-like ordered structures on disk. For in-memory other trees performs better.Murmur hash (see Wikipedia page for further details), is a family of hash functions used in hash table. The implementation of the hash table is not important, it could be a standard chained implementation or more advanced open hash addressing scheme. The idea is that the hash tables allows one to get fast a key, from an unordered set of keys, and can answer to tasks like: is this key part of this set of keys? which is the value associated with this key? Now back to your main problem. You have one library (Lucene) and to data structures, both data structures are used in Lucene. Now you see that it is not possible to answer your question in these terms since they are not comparable.However, regarding you footprint and performance part of the question. First of all you have to know which kind of operations you need to implement. Do you need only get value for key, or do you need to find all elements in a range? In other words do you need order or not? If you do, than a tree can help. If you do not, than a hash table, which is faster could be used instead. Do you have a lot of data which does not fit the memory? If yes than a disk-based solution would help (like BTree). If your data fit the memory, than use the fastest in-memory solution and use disk only as a storage (with a different structure, much simpler).;;;
166;2;2014-05-19T11:13:48.067;;"Let's say you are predicting the topic of a document given its words.A generative model describes how likely each topic is, and how likely words are given the topic. This is how it says documents are actually ""generated"" by the world -- a topic arises according to some distribution, words arise because of the topic, you have a document. Classifying documents of words W into topic T is a matter of maximizing the joint likelihood: P(T,W) = P(W|T)P(T)A discriminative model operates by only describing how likely a topic is given the words. It says nothing about how likely the words or topic are by themselves. The task is to model P(T|W) directly and find the T that maximizes this. These approaches do not care about P(T) or P(W) directly.";;;
169;1;2014-05-19T15:55:24.983;How to specify important attributes?;"Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. ""nationality"" vs ""bornIn""). My goal is to find some ""important"" attributes that somehow ""define"" the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).For example, the attribute ""lastName"" is more discriminative than the attribute ""nationality"". How could I (statistically) find such attributes that are more important than others?A naive solution would be to take the average IDF of the values of each attribute and make this the ""importance"" factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.I have seen the term feature, or attribute selection in machine learning, but I don't want to discard the remaining attributes, I just want to put higher weights to the most important ones.";[education, open-source];228;3
170;2;2014-05-19T18:08:32.327;;A possible solution is to calculate the information gain associated to each attribute:Initially you have the whole dataset, and compute the information gain of each item. The item with the best information gain is the one you should use to partition the dataset (considering the item's values). Then, perform the same computations for each item (but the ones selected), and always choose the one which best describes/differentiates the entries from your dataset.There are implementations available for such computations. Decision trees usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these important items.;;;
171;2;2014-05-19T19:44:48.500;;"Two things you might find useful:meta-learning to speedup the search for the right model and the optimal parameters.Meta learning consists in applying machine learning tools to the problem of finding the right machine learning tool/parameters for the problem at hand. This for instance this paper for a practical example;gpucomputing to speedup the algorithm on larger datasets. For instance, OpenCV can use GPUs, which are very effective at processing images/videos and can bring 10 to 100 speedups with respect to CPUs. As your computer most probably has a gpucomputing-able GPU, you could gain lots of time using it.";;;
172;1;2014-05-19T23:59:58.070;Is there a straightforward way to run pandas.DataFrame.isin in parallel?;"I have a modeling and scoring program that makes heavy use of the DataFrame.isin function of pandas, searching through lists of facebook ""like"" records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.";[education, open-source];1580;4
173;2;2014-05-20T03:56:43.147;;"Guessing it's likely you've seen this YouTube demo and the related Google Tech Talk, which is related to these papers: P-N Learning: Bootstrapping Binary Classifiers by Structural ConstraintsTracking-Learning-DetectionAnd this set of code on GitHub for OpenTLD. If you check the ""read me"" on GitHub here, you'll notice that author's email (Zdenek Kalal) is listed, so it might be worth sending him an email about your questions, or even inviting him to reply to this question too.";;;
174;2;2014-05-20T09:24:30.697;;Another suggestion is to test the logistic regression. As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.  Sklearn offers the sklearn.linear_model.LogisticRegression package that is designed to handle sparse data as well.As mentionned in the comments, in the present case, with more input variables than samples, you need to regularize the model (with sklearn.linear_model.LogisticRegression use the penalty='l1' argument).;;;
175;1;2014-05-20T22:14:02.927;Can metadata be used to adapt parsing for an unescaped in field use of the delimiter?;I have data coming from a source system that is pipe delimited. Pipe was selected over comma since it was believed no pipes appeared in field, while it was known that commas do occur. After ingesting this data into Hive however it has been discovered that rarely a field does in fact contain a pipe character.Due to a constraint we are unable to regenerate from source to escape the delimiter or change delimiters in the usual way. However we have the metadata used to create the Hive table. Could we use knowledge of the fields around the problem field to reprocess the file on our side to escape it or to change the file delimiter prior to reloading the data into Hive?;[education, open-source];34;
176;1;2014-05-21T05:29:36.787;How to animate growth of a social network?;"I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.One of the existing solutions is SoNIA: Social Network Image Animator. It let's you make movies like this one. SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?Right after posting this question I'll dig into sigma.js, so please consider this library covered.In general, my input data would be something like this:time_elapsed; node1; node21; A; B2; A; C3; B; CSo, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.";[education, open-source];453;2
179;2;2014-05-21T07:09:20.093;;"My first guess is to visualize social network in Tableau.And particularly: building network graphs in Tableau.What you need is to add time dimension to the ""Pages"" section to be able to see network change dynamics.This is screen from the link above.";;;
182;2;2014-05-21T07:19:32.297;;So, a few of your rows will have too many columns by one or more as a result. That's easy to detect, but harder to infer where the error was -- which two columns are actually one? which delimiter is not a delimiter?In some cases, you can use the metadata, because it helps you know when an interpretation of the columns can't be right. For example, if just the one column can have a text value, and all the others must be numeric, it's unambiguous where the error is. Any additional columns created by this error occur right after the text column.If they're all text, this doesn't work of course.You might be able to leverage more than the metadata's column type. For example you may know that some fields are from an enumerated set of values, and use that to determine when a column assignment is wrong.;;;
183;2;2014-05-21T10:53:26.870;;Fancy animations are coolI was very impressed when I saw this animation of the discourse git repository. They used Gourse which is specifically for git. But it may give ideas about how to represent the dynamics of growth.You can create animations with matplotlibThis stackoverflow answer seems to point at a python/networkx/matplotlib solution.But D3.js provides interactionIf you're looking for a web-based solution then d3.js is excellent. See this, this and this for example.See also this stackoverflow question, the accepted answer points to D3.js again.ConclusionI would be drawn towards the python/networkx options for network analysis (possibly to add attributes to your raw data file for example). Then, for visualisation and dissemination D3.js is perfect. You might be surprised how easy it can be to write d3.js once you get into it. I believe it even works within an ipython notebook!;;;
184;1;2014-05-21T11:22:34.657;Google prediction API: What training/prediction methods Google Prediction API employs?;The details of the Google Prediction API are on this page, but I am not able to find any details about the prediction algorithms running behind the API. So far I have gathered that they let you provide your preprocessing steps in PMML format.;[education, open-source];316;
185;2;2014-05-21T14:14:38.797;;If you take a look over the specifications of PMML which you can find here you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on).;;;
186;1;2014-05-21T15:12:18.980;Using SVM as a binary classifier, is the label for a data point chosen by consensus?;I'm learning Support Vector Machines, and I'm unable to understand how a class label is chosen for a data point in a binary classifier. Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?;[education, open-source];51;
187;2;2014-05-21T15:39:54.830;;The term consensus, as far as I'm concerned, is used rather for cases when you have more a than one source of metric/measure/choice from which to make a decision. And, in order to choose a possible result, you perform some average evaluation/consensus over the values available.This is not the case for SVM. The algorithm is based on a quadratic optimization, that maximizes the distance from the closest documents of two different classes, using a hyperplane to make the split.So, the only consensus here is the resulting hyperplane, computed from the closest documents of each class. In other words, the classes are attributed to each point by calculating the distance from the point to the hyperplane derived. If the distance is positive, it belongs to a certain class, otherwise, it belongs to the other one.;;;
188;2;2014-05-21T16:13:25.590;;You can use map reduce algorithms in Hadoop without programming them in Java. It is called streaming and works like Linux piping. If you believe that you can port your functions to read and write to terminal, it should work nicely. Here is example blog post which shows how to use map reduce functions written in Python in Hadoop.;;;
189;1;2014-05-21T19:41:19.857;Open source solver for large mixed integer programming task?;I'm currently using General Algebraic Modeling System (GAMS), and more specifically CPLEX within GAMS, to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores), and it finds an optimal solution in a relatively short amount of time.Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I'm open to suggestions of all kinds, and other users may be interested in different solutions.;[education, open-source];152;
190;2;2014-05-22T12:14:49.727;;"It turned out that this task was quite easy to accomplish using vis.js. This was the best example code which I have found.The example of what I have built upon this is here (scroll to the bottom of this post). This graph represents the growth of a subnetwork of Facebook friends. Green dots are females, blue ones are males. The darker the colour, the older the user. By clicking ""Dodaj węzły"" you can add more nodes and edges to the graph.Anyway, I am still interested in other ways to accomplish this task, so I won't accept any answer as for now.Thanks for your contributions!";;;
191;1;2014-05-22T13:36:24.120;Multi layer back propagation Neural network for classification;Can someone explain me, how to classify a data like MNIST with MLBP-Neural network if I make more than one output (e.g 8), I mean if I just use one output I can easily classify the data, but if I use more than one, which output should I choose ?;[education, open-source];144;1
192;1;2014-05-22T15:15:41.133;What are the most popular data science application use cases for consumer web companies;The most popular use case seem to be recommender systems of different kinds (such as recommending shopping items, users in social networks etc.).But what are other typical data science applications, which may be used in a different verticals?For example: customer churn prediction with machine learning, evaluating customer lifetime value, sales forecasting.;[education, open-source];127;1
193;2;2014-05-22T15:43:57.160;;It depends, of course, on the focus of the company: commerce, service, etc.  In adition to the use cases you suggested, some other use cases would be:Funnel analysis: Analyzing the way in which consumers use a website and complete a sale may include data science techniques, especially if the company operates at a large scale.Advertising: Companies that place ads use a lot of machine learning techniques to analyze and predict which ads would be most effective or most remunerative give the user's demographics that would view them.;;;
194;2;2014-05-22T19:20:14.130;;Suppose that you need to classify something in K classes, where K > 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance.This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details on Wikipedia. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities. Finally, with or without softmax you can use the output as a discriminant function to select the proper category.Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed.;;;
195;2;2014-05-22T20:48:55.297;;Satisfaction is a huge one that I run into a lot.  Huge referring to importance/difficulty/complexity.The bottom line is that for very large services (search engines, facebook, linkedin, etc...) your users are simply a collection of log lines.  You have little ability to solicit feed back from them (not a hard and fast rule necessarily).  So you have to infer their positive or negative feedback most of the time.This means finding ways, even outside of predictive modelling, to truly tell, from a collection of log lines, whether or not someone actually liked something they experienced.  This simple act is even more fundamental (in my biased opinion) than a/b testing since you're talking about metrics you will eventually track on a test scorecard.Once you have a handle on good SAT metrics then you can start making predictive models and experimenting.  But even deciding what piece of log instrumentation can tell you about SAT is non-trivial (and often changes).;;;
196;1;2014-05-22T21:47:26.980;Algorithm for generating classification rules;"So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a ""bucket"" that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:Item A { 4-door, small, steel } => { sedan }Item B { 2-door, big,   steel } => { truck }Item C { 2-door, small, steel } => { coupe }I just want the rules that say ""if it's big and a 2-door, it's a truck,"" not the rules that say ""if it's a 4-door it's also small."" One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?";[education, open-source];375;1
197;2;2014-05-22T21:54:05.660;;C45 made by Quinlan is able to produce rule for prediction. Check this Wikipedia page. I know that in Weka its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.Later editAlso you might be interested in algorithms for directly inferring rules for classification. RIPPER is one, which again in Weka it received a different name JRip. See the original paper for RIPPER: Fast Effective Rule Induction, W.W. Cohen 1995 ;;;
198;2;2014-05-23T03:05:57.990;;"Also, there seem to be a very comprehensive list of data science use cases by function and by vertical on Kaggle - ""Data Science Use Cases""";;;
199;1;2014-05-23T06:25:50.480;What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?;LDA has two hyperparameters, tuning them changes the induced topics. What does the alpha and beta hyperparameters contribute to LDA? How does the topic change if one or the other hyperparameters increase or decrease? Why are they hyperparamters and not just parameters?;[education, open-source];317;2
200;2;2014-05-23T08:34:38.900;;You also can create a MongoDB-Hadoop connection.;;;
201;2;2014-05-23T09:09:44.490;;In addition to the listed sources.Some social network data sets:Stanford University large network dataset collection (SNAP)A huge twitter dataset that includes followers + large collection of twitter datasets hereLastFM data setThere are plenty of sources listed at Stats SE:Locating freely available data samplesData APIs/feeds available as packages in RFree data set for very high dimensional classification;;;
202;2;2014-05-23T13:47:54.603;;"The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.Now the LDA uses some constructs like:- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model thisThe previous two are distributions which you do not really see from data, this is why is called latent, or hidden.Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let's say you have data x and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes' rule with p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha). In plain words is posterior probability = likelihood x prior probability / marginal likelihood. Note that here comes an alpha. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.The parameters of the prior are called hyperparameters. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters. Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters alpha_k have the same value, the pdf is symmetric in the simplex defined by the x values, which is the minimum or maximum for pdf is at the center. If all the alpha_k have values lower than unit the maximum is found at cornersor can if all values alpha_k are the same and greater than 1 the maximum will be found in center like It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values. Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts. Hope it helped.PS: It's a hell to write something without LaTeX notation. I hope moderators/administrators will do something.";;;
203;2;2014-05-23T14:28:41.563;;"Never done stuff on that scale, but as no-one else has jumped in yet have you seen these two papers that discuss non-commercial solutions?  Symphony and COIN-OR seem to be the dominant suggestions.Linderoth, Jeffrey T., and Andrea Lodi. ""MILP software."" Wiley encyclopedia of operations research and management science (2010). PDF versionLinderoth, Jeffrey T., and Ted K. Ralphs. ""Noncommercial software for mixed-integer linear programming."" Integer programming: theory and practice 3 (2005): 253-303. Compares performance";;;
204;2;2014-05-23T19:28:01.903;;I recently did a similar project in Python (predicting opinions using FB like data), and had good results with the following basic process:Read in the training set (n = N) by iterating over comma-delimited like records line-by-line and use a counter to identify the most popular pagesFor each of the K most popular pages (I used about 5000, but you can play around with different values), use pandas.DataFrame.isin to test whether each individual in the training set likes each page, then make a N x K dataframe of the results (I'll call it xdata_train)Create a series (I'll call it ydata_train) containing all of the outcome variables (in my case opinions, in yours age) with the same index as xdata_trainSet up a random forest classifier through scikit-learn to predictydata_train based on xdata_trainUse scikit-learn's cross-validation testing to tweak parameters andrefine accuracy (tweaking number of popular pages, number of trees,min leaf size, etc.)Output random forest classifier and list of most popular pages with pickle (or keep in memory if you are doing everything at once)Load in the rest of your data, load the list of popular pages (if necessary), and repeat step 2 to produce xdata_newLoad the random forest classifier (if necessary) and use it to predict values for the xdata_new dataOutput the predicted scores to a new CSV or other output format of your choosingIn your case, you'd need to swap out the classifier for a regressor (so see here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) but otherwise the same process should work without much trouble. Also, you should be aware of the most amazing feature of random forests in Python: instant parallelization! Those of us who started out doing this in R and then moved over are always amazed, especially when you get to work on a machine with a few dozen cores (see here: http://blog.yhathq.com/posts/comparing-random-forests-in-python-and-r.html).Finally, note that this would be a perfect application for network analysis if you have the data on friends as well as the individuals themselves. If you can analyze the ages of a user's friends, the age of the user will almost certainly be within a year or two of the median among his or her friends, particularly if the users are young enough to have built their friend networks while still in school (since most will be classmates). That prediction would likely trump any you would get from modeling---this is a textbook example of a problem where the right data > the right model every time. Good luck!;;;
205;1;2014-05-23T19:45:54.283;Instances vs. cores when using EC2;"Working on what could often be called ""medium data"" projects, I've been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I'm looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance? Bandwidth and RAM are non-trivial concerns in my projects, but it's easy to spot when those are the bottlenecks and readjust. It's much harder, I'd imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I've just failed to google this one properly, feel free to point me to the right answer somewhere else!";[education, open-source];216;
206;2;2014-05-23T21:01:18.630;;All things considered equal (cost, CPU perf, etc.) you could choose the smallest instance that can hold all of my dataset in memory and scale out. That way you make sure not to induce unnecessary latencies due to network communications, andyou tend to maximize the overall available memory bandwidth for your processes.Assuming you are running some sort of cross-validation scheme to optimize some meta parameter of your model, assign each core a value to test and choose an many instances as needed to cover all the parameter space in as few rounds as you see fit.If your data does not fit in the memory of one system, of course you'll need to distribute across instances. Then it is a matter of balancing memory latency (better with many instances) with network latency (better with fewer instances) but given the nature of EC2 I'd bet you'll often prefer to work with few fat instances.;;;
207;2;2014-05-24T10:36:58.987;;"A general rule of thumb is to not distribute until you have to. It's usually more efficient to have N servers of a certain capacity than 2N servers of half that capacity. More of the data access will be local, and therefore fast in memory versus slow across the network.At a certain point, scaling up one machine becomes uneconomical because the cost of additional resource scales more than linearly. However this point is still amazingly high.On Amazon in particular though, the economics of each instance type can vary a lot if you are using spot market instances. The default pricing more or less means that the same amount of resource costs about the same regardless of the instance type, that can vary a lot; large instances can be cheaper than small ones, or N small instances can be much cheaper than one large machine with equivalent resources.One massive consideration here is that the computation paradigm can change quite a lot when you move from one machine to multiple machines. The tradeoffs that the communication overhead induce may force you to, for example, adopt a data-parallel paradigm to scale. That means a different choice of tools and algorithm. For example, SGD looks quite different in-memory and in Python than on MapReduce. So you would have to consider this before parallelizing.You may choose to distribute work across a cluster, even if a single node and non-distributed paradigms work for you, for reliability. If a single node fails, you lose all of the computation; a distributed computation can potentially recover and complete just the part of the computation that was lost.";;;
208;2;2014-05-24T11:18:26.497;;"When using IPython, you very nearly don't have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.Addressing some of your specific questions:""how to reconcile distributing work across cores on an instance vs. instances on a cluster"" - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.""Is it even practical to parallelize across instances as well as across cores on each instance?"" - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.""If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?"" - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run significantly slower on N quadruple c3 than on 2N double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.";;;
209;1;2014-05-25T13:57:52.657;How should one deal with implicit data in recommendation;"A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It's likeuser_id item_id result1       4       11       7       -15       19      15       80      1where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation. Question: If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).Edit 1Let us think about it in the context of a matrix factorization approach. If we treat -1 and 1 as ratings, there will be some problem. For example, user 1 likes movie A which scores high in one factor (e.g. having glorious background music) in the latent factor space. The system recommends movie B which also scores high in ""glorious background music"", but for some reason user 1 is too busy to look into the recommendation, and we have a -1 rating movie B. If we just treat 1 or -1 equally, then the system might be discouraged to recommend movie with glorious BGM to user 1 while user 1 still loves movie with glorious BGM. I think this situation is to be avoided.";[education, open-source];101;
210;2;2014-05-26T04:07:32.390;;Assuming symmetric Dirichlet distributions (for simplicity), a low alpha value places more weight on having each document composed of only a few dominant topics (whereas a high value will return many more relatively dominant topics). Similarly, a low beta value places more weight on having each topic composed of only a few dominant words.;;;
211;1;2014-05-27T10:41:33.220;Human activity recognition using smartphone data set problem;I'm new to this community and hopefully my question will well fit in here.As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I'm concern this topic relates to Machine Learning and Support Vector Machines. I'm not well familiar with this technologies yet so I will need some help. I have decided to follow this project idea http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html (first project on the top)The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject's waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.All the data set is given in one folder with some description and feature labels. The data is divided for 'test' and 'train' files in which data is represented in this format:  2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001And that's only a very small sample of what the file contain. I don't really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use? Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?Any hints/tips would be much appreciated.;[education, open-source];752;1
212;2;2014-05-27T10:58:00.620;;Your system isn't just trained on items that are recommended right? if so you have a big feedback loop here. You want to learn from all clicks/views, I hope.You suggest that not-looking at an item is a negative signal. I strongly suggest you do not treat it that way. Not interacting with something is almost always best treated as no information. If you have an explicit signal that indicates a dislike, like a down vote (or, maybe watched 10 seconds of a video and stopped), maybe that's valid.I would not construe this input as rating-like data. (Although in your case, you may get away with it.) Instead think of them as weights, which is exactly the treatment in the Hu Koren Volinsky paper on ALS that @Trey mentions in a comment. This lets you record relative strength of positive/negative interactions.Finally I would note that this paper, while is very likely to be what you're looking for, does not provide for negative weights. It is simple to extend in this way. If you get that far I can point you to the easy extension, which exists already in two implementations that I know of, in Spark and Oryx.;;;
213;2;2014-05-27T12:07:45.920;;"The data set definitions are on the page here:Attribute Information at the bottomor you can see inside the ZIP folder the file named activity_labels, that has your column headings inside of it, make sure you read the README carefully, it has some good info in it. You can easily bring in a .csv file in R using the read.csv command.For example if you name you file samsungdata you can open R and run this command:data <- read.csv(""directory/where/file/is/located/samsungdata.csv"", header = TRUE)Or if you are already inside of the working directory in R you can just run the followingdata <- read.csv(""samsungdata.csv"", header = TRUE)Where the name data can be changed to whatever you want to call your data set.";;;
214;2;2014-05-27T16:05:02.883;;Public Data Setshttps://www.opensciencedatacloud.org/publicdata/Google Public Data Setshttp://www.google.com/publicdata/directoryAmazon Web Serviceshttps://aws.amazon.com/publicdatasets/Finding Data on the Internethttp://www.inside-r.org/howto/finding-data-internet;;;
215;1;2014-05-27T21:07:48.973;Where in the workflow should we deal with missing data?;"I'm building a workflow for creating machine learning models (in my case, using Python's pandas and sklearn packages) from data pulled from a very large database (here, Vertica by way of SQL and pyodbc), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I'm curious where best to locate this step in a multi-platform workflow.It's simple enough to do this in Python, either with the sklearn.preprocessing.Imputer class, using the pandas.DataFrame.fillna method, or by hand (depending upon the complexity of the imputation method used). But since I'm going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there's a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building ""complete"" versions of tables, so we don't need to fill in a new set of missing values from scratch every time we want to run a model.I haven't been able to find much guidance about this, but I imagine that we could:create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete columnjoin the substitute value table with the original table to assign a substitute value for each row and incomplete columnuse a series of case statements to take the original value if available and the substitute value otherwiseIs this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!";[education, open-source];338;3
216;2;2014-05-28T07:08:05.393;;My strong opinion regarding automated tasks like imputation (but, here I can include also scaling, centering, feature selection, etc) is to avoid in any way do such things without carefully inspecting your data. Of course, after deciding what kind of imputation to apply it can be automated (under the assumption that the new data has the same shape/problems).So, before anything, take a wise decision. I often wasted time trying to automate this things, destroying my data. I will give you some examples:- a marketplace encoded as N/A, which I missed and considered to be North/America- numbers like -999.0, because the data producer could not find a better replacement for missing data- number like 0 for blood pressure or body temperature, instead of missing data (it is hard to imagine a living human with 0 blood pressure)- multiple placeholders for missing data, due to the fact that the data was collected from various sourcesAfter that you need to understand what kind of imputation would resemble better the information from your data for a given task. This is often much harder to do it right than it seems.After all those things, my advice is to delay your imputation task to an upper layer where you have tools to reproduce on new data and to inspect if the assumptions for the new data are not violated (if it is possible).;;;
217;2;2014-05-28T09:43:54.197;;It looks like this (or very similar data set) is used for Coursera courses. Cleaning this dataset is task for Getting and Cleaning Data, but it is also used for case study for Exploratory Data analysis. Video from this case study is available in videos for week 4 of EDA course-ware. It might help you with starting with this data.;;;
218;1;2014-05-29T13:08:09.060;built-in cov in pandas DataFrame results ValueError array is too big;"So, I have a dataset with 39.949 variables and 180 rows. dataset is successfully saved in DataFrame but when I try to find cov() it result an error.here is the code  import pandas as pd  cov_data=pd.DataFrame(dataset).cov()Here is the errorFile ""/home/syahdeini/Desktop/FP/pca_2.py"", line 44, in find_eagencov_data=pd.DataFrame(data_mat).cov()File ""/usr/lib/python2.7/dist-packages/pandas/core/frame.py"", line 3716, in covbaseCov = np.cov(mat.T)File ""/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py"", line 1766, in covreturn (dot(X, X.T.conj()) / fact).squeeze()ValueError: array is too big.";[education, open-source];282;
219;2;2014-05-29T13:51:48.820;;Since you have 39,949 variables, the covariance matrix would have about 1.6 billion elements (39,949 * 39,949 = 1,595,922,601).  That is likely why you are getting that error.;;;
220;2;2014-05-29T14:13:00.317;;"Christopher is right about the size of the array. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you're trying to create an array of about 26 GB. Even if you have the RAM for that, I'd imagine that it's probably going to overload something else along the way. (Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.)But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it's easier to see how it might be useful.In any case, you're probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you'll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it'll make your life much easier if you start running things in parallel, and will help ensure you don't waste resources on unnecessary calculations.";;;
221;2;2014-05-29T14:30:21.357;;It's actually even simpler than that, from what you describe---you're just looking for a basic classification tree algorithm (so no need for slightly more complex variants like C4.5 which are optimized for prediction accuracy). The canonical text is:http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418This is readily implemented in R: http://cran.r-project.org/web/packages/tree/tree.pdfand Python:http://scikit-learn.org/stable/modules/tree.html;;;
222;2;2014-05-29T19:02:13.210;;Enigma is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.I used it for academic research and it saved me a lot of time.Another interesting source of data is the @unitedstates project, containing data and tools to collect them, about the United States (members of Congress, geographic shapes…).;;;
223;1;2014-05-29T20:11:16.327;How to annotate text documents with meta-data?;"Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:I saw the company's manager last day.To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.<Person name=""John"">I</Person> saw the <Organization name=""ACME"">company</Organization>'smanager <Time value=""2014-5-29"">last day</Time>.";[education, open-source];185;2
224;1;2014-05-31T14:28:42.317;How to get phrase tables from word alignments?;The output of my word alignment file looks as such:I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it . In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen .   0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it .    Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen .   0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33How can I produce the phrase tables that are used by MOSES from this output?;[education, open-source];194;1
226;2;2014-05-31T19:59:15.563;;I've found this link in Data Science Central with a list of free datasets: Big data sets available for free;;;
227;1;2014-06-01T10:25:51.163;Tradeoffs between Storm and Hadoop (MapReduce);Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer.Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. ;[education, open-source];2337;
228;1;2014-06-01T12:51:25.040;Cascaded Error in Apache Storm;Going through the presentation and material of Summingbird by Twitter, one of the reasons that is mentioned for using Storm and Hadoop clusters together in Summingbird is that processing through Storm results in cascading of error. In order to avoid this cascading of error and accumulation of it, Hadoop cluster is used to batch process the data and discard the Storm results after the same data is processed by Hadoop. What is the reasons for generation of this accumulation of error? and why is it not present in Hadoop? Since I have not worked with Storm, I do not know the reasons for it. Is it because Storm uses some approximate algorithm to process the data in order to process them in real time? or is the cause something else?;[education, open-source];44;
229;2;2014-06-01T19:48:41.693;;This is kind of like asking about the tradeoffs between frying pan and your drawer of silverware. They are not two things you compare, really. You might use them together as part of a larger project. Hadoop itself is not one thing, but a name for a federation of services, like HDFS, Hive, HBase, MapReduce, etc. Storm is something you use with some of these services, like HDFS or HBase. It is a stream-processing framework. There are others within the extended Hadoop ecosystem, like Spark Streaming.When would you choose a stream-processing framework? when you need to react to new data in near-real-time. If you need this kind of tool, you deploy this kind of tool, too.;;;
230;2;2014-06-02T15:03:35.940;;In general, you don't want to use XML tags to tag documents in this way because tags may overlap.UIMA, GATE and similar NLP frameworks denote the tags separate from the text.  Each tag, such as Person, ACME, John etc. is stored as the position that the tag begins and the position that it ends.  So, for the tag ACME, it would be stored as starting a position 11 and ending at position 17. ;;;
231;1;2014-06-05T09:00:27.950;How to get an aggregate confusion matrix from n different classifications;I want to test the accuracy of a methodology. I ran it ~400 times, and I got a different classification for each run. I also have the ground truth, i.e., the real classification to test against.For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?May I sum all confusion matrices in order to obtain the overall one?;[education, open-source];222;
232;2;2014-06-06T14:55:18.867;;"There are a few ways to achieve your ""master confusion matrix"".Sum all the confusion matrices together:  Like you suggested, summing this results in a confusion matrix.  The problem with this is you can not interpret totals.Average the entries.  This method is the same as number one, but you divide each entry by the number of trials (~400 in your case).  This would be my preferred method because then you can actually translate each category to a (mean) +- (an error measurement) and actually see which categories are the most volatile or stable.  Careful with interpreting this 'error measurement' though.Report a problem specific measurement of the confusion numbers.  For example, if your numbers have outliers, medians would preferred over means.There are other statistics that are possible to report as well.  You can redo the method to keep track of individual classifications.  Then we can say other important stats like '% of classifications that stay the same and are accurate', etc...";;;
233;2;2014-06-08T07:03:43.710;;To complete Damien's answer, an example of dimensionality reduction in NLP is a topic model, where you represent the document by a vector indicating the weights of its constituent topics.;;;
234;1;2014-06-09T04:43:03.497;Data science Ph.D. program, what do you think?;As Yann LeCun mentioned, a number of PhD programs in data science will be popping up in the next few years.NYU already have one, where Prof.LeCun is at right now.A statistics or cs PhD in machine learning is probably more rigorous than a data science one.  Is data science PhD for the less mathy people like myself? Are these cash cow programs?There is a huge industry demand for big data, but what is the academic value of these programs, as you probably can't be a professor or publish any paper.;[education, open-source];260;
235;1;2014-06-09T08:34:29.337;Are there any python based data visualization toolkit?;Data visualization is an important sub-field in data science and python programmers would need to have available toolkits for them.Is there a Python API to Tableau?Are there any Python-based data visualization toolkits?;[education, open-source];479;2
236;2;2014-06-09T18:02:00.613;;"No-one knows since no-one's completed one of these PhD programs yet! However, I would look at the syllabus and the teachers to base my decision. It all depends on what you want to do; industry or academia?";;;
237;2;2014-06-09T19:52:41.847;;There is a Tablaeu API and you can use Python to use it, but maybe not in the sense that you think. There is a Data Extract API that you could use to import your data into Python and do your visualizations there, so I do not know if this is going to answer your question entirely.As in the first comment you can use Matplotlib from Matplotlib website, or you could install Canopy from Enthought which has it available, there is also Pandas, which you could also use for data analysis and some visualizations. There is also a package called ggplot which is used in R alot, but is also made for Python, which you can find here ggplot for python.The Tableau data extract API and some information about it can be found at this link. There are a few web sources that I found concerning it using duckduckgo at this link.Here are some samples:Link 1Link 2Link 3As far as an API like matplotlib, I cannot say for certain that one exists. Hopefully this gives some sort of reference to help answer your question.Also to help avoid closure flags and downvotes you should try and show some of what you have tried to do or find, this makes for a better question and helps to illicit responses.;;;
238;2;2014-06-09T21:36:44.297;;I think this question assumes a false premise. As a student at NYU, I only know of a Masters in Data Science. You linked to a page that confirms this.It's hard to gauge the benefit of a program that doesn't exist yet.;;;
241;2;2014-06-09T21:51:53.793;;"It seems to me that the premise of a PhD is to expand knowledge in some little slice of the world. Since a ""data scientist"" is by nature is somewhat of a jack-of-all-trades it does seem a little odd to me. A masters program seems much more appropriate.What do you hope to gain from a PhD? If the rigor scares (or bores) you, then what about a more applied area? Signal processing, robotics, applied physics, operations research, etc.";;;
242;2;2014-06-09T21:57:30.240;;MapReduce: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.Storm: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.If you want to know more...If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way here are slides for a talk I gave on building real-time recommendation engines on HBase. An excellent paper that marries real-time counting and persistence in an interesting way is Google News Personalization: Scalable Online Collaborative Filtering Another interesting marriage of MR and Storm is SummingBird. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.;;;
243;2;2014-06-10T01:36:27.520;;"A variety of methods are available to the user.   The support documentation gives walkthroughs and tips for when one or another model is most appropriate. This page shows the following learning methods:""AssociationModel""""ClusteringModel"" ""GeneralRegressionModel""""MiningModel"" ""NaiveBayesModel"" ""NeuralNetwork"" ""RegressionModel""""RuleSetModel"" ""SequenceModel"" ""SupportVectorMachineModel""""TextModel"" ""TimeSeriesModel"" ""TreeModel""EDIT: I don't see any specific information about the algorithms, though.  For example, does the tree model use information gain or gini index for splits? ";;;
244;2;2014-06-10T01:40:23.263;;"Coming from a programmers perspective, frameworks rarely target performance as the highest priority.  If your library is going to be widely leveraged the things people are likely to value most are ease of use, flexibility, and reliability.Performance is generally valued in secondary competitive libraries.  ""X library is better because it's faster.""  Even then very frequently those libraries will trade off the most optimal solution for one that can be widely leveraged.  By using any framework you are inherently taking a risk that a faster solution exists.  I might go so far as to say that a faster solution almost always exists.Writing something yourself is not a guarantee of performance, but if you know what you are doing and have a fairly limited set of requirements it can help.An example might be JSON parsing.  There are a hundred libraries out there for a variety of languages that will turn JSON into a referable object and vice versa.  I know of one implementation that does it all in CPU registers.  It's measurably faster than all other parsers, but it is also very limited and that limitation will vary based on what CPU you are working with.Is the task of building a high-performant environment specific JSON parser a good idea?  I would leverage a respected library 99 times out of 100.  In that one separate instance a few extra CPU cycles multiplied by a million iterations would make the development time worth it.";;;
245;2;2014-06-10T01:54:40.647;;Computer Science is itself a multi-disciplinary field which has varying requirements among universities.  For example, Stockholm University does not require any math above algebra for its CS programs (some courses may have higher requirements, but not often).  I am not sure what you mean by a machine learning program being more rigorous.  They are just two different programs.  Data Science would likely take a broader view and focus on application and management (business courses are maybe on offer?).  The research could be rigorous in its own right, but it definitely won't be tailored to someone who wants to optimize new algorithms or solve the low-level problems of machine learning.  I don't see the Ph.D program listed yet in the link you provided.  Will you please follow up here if you get more specific information?;;;
246;2;2014-06-10T02:42:02.050;;There will definitely be a translation task at the end if you prototype using just mongo.When you run a MapReduce task on mongodb, it has the data source and structure built in.  When you eventually convert to hadoop, your data structures might not look the same.  You could leverage the mongodb-hadoop connector to access mongo data directly from within hadoop, but that won't be quite as straightforward as you might think.  The time to figure out how exactly to do the conversion most optimally will be easier to justify once you have a prototype in place, IMO.While you will need to translate mapreduce functions, the basic pseudocode should apply well to both systems.  You won't find anything that can be done in MongoDB that can't be done using Java or that is significantly more complex to do with Java.;;;
247;2;2014-06-10T02:50:51.153;;Bokeh is an excellent data visualization library for python.NodeBox is another that comes to mind.;;;
248;2;2014-06-10T03:21:56.473;;A cash cow program?  No.  PhD programs are never cash cows.I don't know why you couldn't be a professor with a PhD in data science.  Rarely does a professor of a given course have to have a specific degree in order to teach it.As far as publishing goes, there are any number of related journals that would accept papers from somebody on topics that would be covered by the topic of Data Science.When I went to college, MIS, Computer Engineering, and Computer Science were new subjects.  Most of the people in my graduating class for Computer Science couldn't program anything significant at graduation.  Within a few years, CS programs around the country matured significantly.When you are part of a new program, sometimes it's possible to help define what it is that's required for graduation.  Being a part of that puts you in rare company for that field.As far as mathematical rigor is concerned, I would expect Data Science to leverage a heavy dose of mathematically based material.  I wouldn't expect anything particularly new - statistics, calculus, etc. should have been covered in undergrad.  Masters and PhD programs should be more about applying that knowledge and not so much about learning it.;;;
249;2;2014-06-10T03:42:51.637;;Twitter uses Storm for real-time processing of data.  Problems can happen with real-time data.  Systems might go down.  Data might be inadvertently processed twice.  Network connections can be lost.  A lot can happen in a real-time system.  They use hadoop to reliably process historical data.  I don't know specifics, but for instance, getting solid information from aggregated logs is probably more reliable than attaching to the stream.If they simply relied on Storm for everything - Storm would have problems due to the nature of providing real-time information at scale.  If they relied on hadoop for everything, there's a good deal of latency involved.  Combining the two with Summingbird is the next logical step.;;;
250;2;2014-06-10T04:47:13.040;;Google does not publish the models they use, but they specifically do not support models from the PMML specification.If you look closely at the documentation on this page, you will notice that the model selection within the schema is greyed out indicating that it is an unsupported feature of the schema.The documentation does spell out that by default it will use a regression model for training data that has numeric answers, and an unspecified categorization model for training data that results in text based answers.The Google Prediction API also supports hosted models (although only a few demo models are currently available), and models specified with a PMML transform.  The documentation does contain an example of a model defined by a PMML transform.  (There is also a note on that page stating that PMML ...Model elements are not supported).  The PMML standard that google partially supports is version 4.0.1.;;;
251;2;2014-06-10T05:57:13.897;;Having done the rewriting game over and over myself (and still doing it), my immediate reaction was adaptability.While frameworks and libraries have a huge arsenal of (possibly intertwinable) routines for standard tasks, their framework property often (always?) disallows shortcuts.  In fact, most frameworks have some sort of core infrastructure around which a core layer of basic functionality is implemented.  More specific functionality makes use of the basic layer and is placed in a second layer around the core.Now by shortcuts I mean going straight from a second layer routine to another second layer routine without using the core.  Typical example (from my domain) would be timestamps: You have a timestamped data source of some kind.  Thus far the job is simply to read the data off the wire and pass it to the core so your other code can feast on it.Now your industry changes the default timestamp format for a very good reason (in my case they went from unix time to GPS time).  Unless your framework is industry-specific it is very unlikely that they're willing to change the core representation of time, so you end up using a framework that almost does what you want.  Every time you access your data you have to convert it to industry-time-format first, and every time you want it modified you have to convert it back to whatever the core deems appropriate.  There is no way that you can hand over data straight from the source to a sink without double conversion.This is where your hand-crafted frameworks will shine, it's just a minor change and you're back modelling the real world whereas all other (non-industry-specific) frameworks will now have a performance disadvantage.Over time, the discrepancy between the real world and the model will add up.  With an off-the-shelf framework you'd soon be facing questions like: How can I represent this in that or how do make routine X accept/produce Y.So far this wasn't about C/C++.  But if, for some reason, you can't change the framework, i.e. you do have to put up with double conversion of data to go from one end to another, then you'd typically employ something that minimises the additional overhead.  In my case, a TAI->UTC or UTC->TAI converter is best left to raw C (or an FPGA).  There is no elegance possible, no profound smart data structure that makes the problem trivial.  It's just a boring switch statement, and why not use a language whose compilers are good at optimising exactly that?;;;
252;1;2014-06-10T06:19:46.510;HBase connector - Thrift or REST;There seem to be at least 2 ways to connect to HBase from external application, with language other then Java (i.e. Python):HBase Thrift APIHBase Stargate (REST API) Does anyone know which one should be used in which circumstances?I.e. what are their main differences, and pros/cons?;[education, open-source];222;
253;1;2014-06-10T06:20:20.817;Do I need to learn Hadoop to be a Data Scientist?;An aspiring data scientist here. I don't know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? ;[education, open-source];3614;8
254;2;2014-06-10T07:10:45.057;;Actually there are more than one question to answer here:How to work on schemaless/loose/missing dataHow to label a person (from what I understand unsupervised) and create an identifierHow to train your system so that it can tell you which attributes you should use in order to identify the person As Rubens mentioned, you can use decision tree methods, specifically Random Forests for calculating the most important attributes based on information gain if you have already found a way to identify how to label a person.However, if you do not have any label information maybe you can use some expert view for preliminary attribute selection. After that you make unsupervised classification in order to retrieve your labels. Lastly, you can select the most important fields using Random Forest or other methods like Bayesian Belief Networks.In order to achieve all that, you also need complete data set. If your data set is loose you have to manually or heuristically find a way to couple attributes indicating same thing with different names. What is more, you can use imputation techniques such as Expectation Maximization method and complete your data set. Or you can also work with Bayesian Networks and can leave missing fields as they are.;;;
255;2;2014-06-10T07:28:49.127;;You should learn Hadoop if you want to be work as data scientist, but maybe before starting with Hadoop you should read something about ETL or Big Data... this book could be a good starting point: http://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343Hope it helps and good luck!;;;
256;2;2014-06-10T07:30:53.510;;"Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.  It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know.As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be ""data science"" in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn't necessarily make someone a data scientist.  And not taking that final step doesn't necessarily disqualify someone from being a data scientist.";;;
257;2;2014-06-10T07:42:57.470;;Yes, you should learn a platform that is capable of dissecting your problem as a data parallel problem. Hadoop is one. For your simple needs (design patterns like counting, aggregation, filtering etc.) you need Hadoop and for more complex Machine Learning stuff like doing some Bayesian, SVM you need Mahout which in turn needs Hadoop (Now Apache Spark) to solve your problem using a data-parallel approach.So Hadoop is a good platform to learn and really important for your batch processing needs. Not only Hadoop but you also need to know Spark (Mahout runs it's algorithms utilizing Spark) & Twitter Storm (for your real time analytics needs). This list will continue and evolve so if you are good with the building blocks (Distributed Computing, Data-Parallel Problems and so on) and know how one such platform (say Hadoop) operates you will fairly quickly be up to speed on others.;;;
258;2;2014-06-10T07:56:45.587;;Thrift is generally faster because the data being exchanged is smaller.  Stargate offers a web service which is an integration method that is widely supported, which is a concern when you are working with commercial products with limited integration possibilities.In a closed environment where everything is controlled, I would prefer Thrift.  If I'm exposing data to external teams or systems I would prefer Stargate.;;;
259;2;2014-06-10T08:04:20.400;;I would like to point to The Open Data Census. It is an initiative of the Open Knowledge Foundation based on contributions from open data advocates and experts around the world. The value of Open data Census is open, community driven, and systematic effort to collect and update the database of open datasets globally on country and, in some cases, like U.S., on city level. Also, it presents an opportunity to compare different countries and cities on in selected areas of interest.;;;
260;2;2014-06-10T08:38:27.093;;The algorithm that is used in this case is called one-vs-all classifier or multiclass classifier. In your case you have to take one class, e. g. number 1 , mark it as positive and combine the rest seven classes in one negative class. The neural network will output the probability of this case being class number 1 vs the rest of the classes. Afterwords, you have to assign as positive another class, e.g. number 2, assign all other classes as one big negative class and get the predicted probability from the network again.After repeating this procedure for all eight classes, assign each case to the the class that had the maximum probability from all the classes outputted from the neural network. ;;;
261;2;2014-06-10T09:21:14.013;;You could take a look at CN2 rule learner in Orange http://orange.biolab.si/docs/latest/widgets/rst/classify/cn2/;;;
262;1;2014-06-10T09:26:06.593;Benefits from using Hierarchical Data Format;What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?;[education, open-source];236;1
263;2;2014-06-10T09:41:34.697;;"Unfortunately, parallelization is not yet implemented in pandas. You can join this github issue if you want to participate in the development of this feature.I don't know any ""magic unicorn package"" for this purposes, so the best thing will be write your own solution. But if you still don't want to spend time on that and want to learn something new – you can try the two methods built into MongoDB (map reduce and agg framework).  See mongodb_agg_framework.";;;
264;2;2014-06-10T10:48:58.457;;You can also give the Expectation Maximization clustering algorithm a try.  It can work on categorical data and will give you a statistical likelihood of which categorical value (or values) a cluster is most likely to take on.;;;
265;1;2014-06-10T10:58:58.447;Can machine learning algorithms predict sports scores or plays?;I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet.Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back theoretically should have a tendency to run the ball in that situation.Scoring might be more difficult to predict, but the winning team might be.My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic.;[education, open-source];5848;5
266;1;2014-06-10T11:05:47.273;What are some easy to learn machine-learning applications?;Being new to machine-learning in general, I'd like to start playing around and see what the possibilities are.I'm curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result.Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated.;[education, open-source];1299;8
268;2;2014-06-10T11:36:19.287;;I think Weka is a good starting point. You can do a bunch of stuff like supervised learning or clustering and easily compare a large set of algorithms na methodologies.Weka's manual is actually a book on machine learning and data mining that can be used as introductory material. ;;;
269;2;2014-06-10T11:37:28.293;;"Definitely they can.I can target you to a nice paper. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.From paper's abstract: a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend's soccer matches.Keywords: Dynamic Models, Generalized Linear Models, Graphical Models, Markov  Chain Monte Carlo Methods, Prediction of Soccer MatchesCitation: Rue, Havard, and Oyvind Salvesen. ""Prediction and retrospective analysis of soccer matches in a league."" Journal of the Royal Statistical Society: Series D (The Statistician) 49.3 (2000): 399-418.";;;
270;2;2014-06-10T11:39:19.603;;Machine learning and statistical techniques can improve the forecast, but nobody can predict the real result.There was a kaggle competition a few month ago about predicting the 2014 NCAA Tournament. You can read the Competition Forum to get a better idea on what people did and what results did they achieve.;;;
271;2;2014-06-10T11:49:23.777;;"It has been shown before that machine learning techniques can be applied for predicting sport results. Simple google search should give you a bunch of results.However, it has also been showed (for NFL btw) that very complex predictive models, simple predictive models, questioning people, or crowd knowledge by utilising betting info, they all perform more or less the same. Source: ""Everything is obvious once you know the answer - How common sense Fails"", Chapter 7, by Duncan Watts. ";;;
272;2;2014-06-10T11:53:07.737;;I would recommend to start with some MOOC on machine learning. For example Andrew Ng's course at coursera.You should also take a look at Orange application. It has a graphical interface and probably it is easier to understand some ML techniques using it. ;;;
273;2;2014-06-10T12:10:28.713;;You can apply data science techniques to data on one machine so the answer to the question as the OP phrased it, is no.;;;
274;2;2014-06-10T12:57:04.307;;One benefit is wide support - C, Java, Perl, Python, and R all have HDF5 bindings.Another benefit is speed.  I haven't ever seen it benchmarked, but HDF is supposed to be faster than SQL databases.I understand that it is very good when used with both large sets of scientific data and time series data - network monitoring, usage tracking, etc.I don't believe there is a size limitation for HDF files (although OS limits would still apply.;;;
275;2;2014-06-10T13:17:48.433;;For time series data in particular, Quandl is an excellent resource -- an easily browsable directory of (mostly) clean time series.One of their coolest features is open-data stock prices -- i.e. financial data that can be edited wiki-style, and isn't encumbered by licensing.;;;
276;2;2014-06-10T13:38:31.207;;Not all government data is listed on data.gov - Sunlight Foundation put together a set of spreadsheets back in February describing sets of available data.;;;
277;2;2014-06-10T14:25:41.903;;To be honest, I think that doing some projects will teach you much more than doing a full course. One reason is that doing a project is more motivating and open-ended than doing assignments.A course, if you have the time AND motivation (real motivation), is better than doing a project. The other commentators have made good platform recommendations on tech. I think, from a fun project standpoint, you should ask a question and get a computer to learn to answer it. Some good classic questions that have good examples are:Neural Networks for recognizing hand written digitsSpam email classification using logistic regressionClassification of objects using Gaussian Mixture modelsSome use of linear regression, perhaps forecasting of grocery prices given neighborhoodsThese projects have the math done, code done, and can be found with Google readily.Other cool subjects can be done by you!Lastly, I research robotics, so for me the most FUN applications are behavioral.Examples can include (if you can play with an arduino)Create a application, that uses logistic regression perhaps, that learns when to turn the fan off and on given the inner temperature, and the status of the light in the room.Create an application that teaches a robot to move an actuator, perhaps a wheel, based on sensor input (perhaps a button press), using Gaussian Mixture Models (learning from demonstration).Anyway, those are pretty advanced. The point I'm making is that if you pick a project that you (really really) like, and spend a few week on it, you will learn a massive amount, and understand so much more than you will get doing a few assignments.;;;
278;2;2014-06-10T14:30:33.667;;Assuming you're familiar with programming I would recommend looking at scikit-learn. It has especially nice help pages that can serve as mini-tutorials/a quick tour through machine learning. Pick an area you find interesting and work through the examples. ;;;
279;2;2014-06-10T14:57:47.810;;There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs' accounts, Inflation and GDP details of UK, Grammy awards data etc.The datasets are available at http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-indexSome more resources. Some of the datasets are in R format or R commads exist for directly importing data to R.http://www.inside-r.org/howto/finding-data-internet;;;
280;1;2014-06-10T15:08:54.073;Feature selection for tracking user activity within an application;"I am developing a system that is intended to capture the ""context"" of user activity within an application; it is a framework that web applications can use to tag user activity based on requests made to the system.  It is hoped that this data can then power ML features such as context aware information retrieval.I'm having trouble deciding on what features to select in addition to these user tags - the URL being requested, approximate time spent with any given resource, estimating the current ""activity"" within the system.I am interested to know if there are good examples of this kind of technology or any prior research on the subject - a cursory search of the ACM DL revealed some related papers but nothing really spot-on.";[education, open-source];96;
282;2;2014-06-10T16:25:24.223;;Yes. Why not?!With so much of data being recorded in each sport in each game, smart use of data could lead us in obtaining important insights regarding player performance.Some examples:Baseball: In the movie Moneyball (which is an adaptation of the MoneyBall book), Brad Pitt plays a character who analyses player statistics to come up with a team that performs tremendously well! It was a depiction of the real life story of Oakland Athletics baseball team. For more info, http://www.theatlantic.com/entertainment/archive/2013/09/forget-2002-this-years-oakland-as-are-the-real-em-moneyball-em-team/279927/Cricket: SAP Labs has come up with an auction analytics tool that has given insights about impact players to buy in the 2014 Indian Premier League auction for the Kolkata Knight Riders team, which eventually went on to win the 2014 IPL Championship. For more info, http://scn.sap.com/community/hana-in-memory/blog/2014/06/10/sap-hana-academy-cricket-demo--how-sap-hana-powered-the-kolkata-knight-riders-to-ipl-championshipSo, yes, statistical analysis of the player records can give us insights about which players are more likely to perform but not which players will perform. So, machine learning, a close cousin of statistical analysis will be proving to be a game changer.;;;
284;2;2014-06-10T17:06:54.950;;"Well, this may not answer the question thoroughly, but since you're dealing with information retrieval, it may be of some use. This page mantains a set of features and associated correlations with page-ranking methods of search engines. As a disclaimer from the webpage itself: Note that these factors are not ""proof"" of what search engines use to rank websites, but simply show the characteristics of web pages that tend to rank higher.The list pointed may give you some insights on which features would be nice to select. For example, considering the second most correlated feature, # of google +1's, it may be possible to add some probability of a user making use of such service if he/she accesses many pages with high # of google +1 (infer ""user context""). Thus, you could try to ""guess"" some other relations that may shed light on interesting features for your tracking app.";;;
285;2;2014-06-10T17:15:52.953;;There are a lot of good questions about Football (and sports, in general) that would be awesome to throw to an algorithm and see what comes out. The tricky part is to know what to throw to the algorithm.A team with a good RB could just pass on 3rd-and-short just because the opponents would probably expect run, for instance. So, in order to actually produce some worthy results, I'd break the problem in smaller pieces and analyse them statistically while throwing them to the machines!There are a few (good) websites that try to do the same, you should check'em out and use whatever they found to help you out:Football OutsidersAdvanced Football AnalyticsAnd if you truly want to explore Sports Data Analysis, you should definitely check the Sloan Sports Conference videos. There's a lot of them spread on Youtube.;;;
286;2;2014-06-10T17:27:31.080;;"It depends on your employer. Many stipulate that you know it, especially if the job involves ""big data"", while others will let you learn on the job or not care. Take a look at the job boards and see for yourself!";;;
287;2;2014-06-10T17:32:19.120;;I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share. When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix. One can argue that this would give similar results like the previous method. However I think that this might be the case sometimes, often when the model has low variance, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality. Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance. I do not implemented those things but I plan to study further because I believe might worth spending some time. ;;;
288;2;2014-06-10T17:36:11.580;;The goal determines the features, so I would initially take as many as possible, then use cross validation to select the optimal subset.My educated guess is that a Markov model would work. If you discretize the action space (e.g., select this menu item, press that button, etc.), you can predict the next action based on the past ones. It's a sequence or structured prediction problem.For commercial offerings, search app analytics.;;;
289;1;2014-06-10T17:56:34.847;Qualifications for PhD Programs;"Yann LeCun mentioned in his AMA that he considers having a PhD very important in order to get a job at a top company.I have a masters in statistics and my undergrad was in economics(math intensive), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications.";[education, open-source];230;
290;2;2014-06-10T18:22:32.610;;I found the pluralsight course Introduction to machine learning encog a great resource so start with. It uses the Encog library to quickly explore different ml techniques. ;;;
291;2;2014-06-10T18:55:39.010;;"If I were you I would take a MOOC or two (e.g., Algorithms, Part I, Algorithms, Part II, Functional Programming Principles in Scala), a good book on data structures and algorithms, then just code as much as possible. You could implement some statistics or ML algorithms, for example; that would be good practice for you and useful to the community.For a PhD program, however, I would also make sure I were familiar with the type of maths they use. If you want to see what it's like at the deep end, browse the papers at the JMLR. That will let you calibrate yourself in regards to theory; can you sort of follow the maths?Oh, and you don't need a PhD to work at top companies, unless you want to join research departments like his. But then you'll spend more time doing development, and you'll need good coding skills...";;;
292;2;2014-06-10T19:43:11.860;;Your time would probably be better spent on Kaggle than in a PhD program. When you read the stories by winners (Kaggle blog) you'll see that it takes a large amount of practice and the winners are not just experts of one single method.On the other hand, being active and having a plan in a PhD program can get you connections that you otherwise would probably not get.I guess the real question is for you - what are the reasons for wanting a job at a top company?;;;
293;2;2014-06-10T20:28:54.613;;"Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats?  The main alternatives are, I think: a database, text files, or another packed/binary format.The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite.  The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly.  The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed.  Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store.  The text file format options are XML/JSON/CSV.  They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :).  If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF).  The main disadvantage here is again speed: parsing text is much, much slower than HDF.The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations.  They typically do not offer a compelling advantage.HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once.  If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option.  If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful.To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast.P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 hour to write to database.  Certainly the database write could be sped up, but you'd better have a good DBA!  This is how it works out of the box.";;;
294;2;2014-06-10T20:40:25.623;;"As a former Hadoop engineer, it is not needed but it helps. Hadoop is just one system - the most common system, based on Java, and a ecosystem of products, which apply a particular technique ""Map/Reduce"" to obtain results in a timely manner. Hadoop is not used at Google, though I assure you they use big data analytics. Google uses their own systems, developed in C++. In fact, Hadoop was created as a result of Google publishing their Map/Reduce and BigTable (HBase in Hadoop) white papers.Data scientists will interface with hadoop engineers, though at smaller places you may be required to wear both hats. If you are strictly a data scientist, then whatever you use for your analytics, R, Excel, Tableau, etc, will operate only on a small subset, then will need to be converted to run against the full data set involving hadoop. ";;;
295;2;2014-06-10T20:43:28.533;;I am glad you also found Yann LeCun's AMA page, it's very useful.Here are my opinionsQ: Should I take some intro software engineering courses at my local University to make myself a stronger candidate?A: No, you need to take more math courses.  It's not the applied stuff that's hard, it's the theory stuff.  I don't know what your school offers.  Take theoretical math courses, along with some computer science courses.  Q:What other advice you have for someone applying to PhD programs from outside the CS field?A:  How closely related are you looking for.  Without a specific question, it's hard to give a specific answer.  ;;;
296;2;2014-06-10T21:50:51.347;;"HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of ""topics"" in document-modeling terms) is not known a priori.  So that's the reason why there's a difference.Using LDA for document modeling, one treats each ""topic"" as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable.";;;
297;2;2014-06-10T22:26:53.623;;As in @damienfrancois answer feature selection is about selecting a subset of features. So in NLP it would be selecting a set of specific words (the typical in NLP is that each word represents a feature with value equal to the frequency of the word or some other weight based on TF/IDF or similar).Dimensionality reduction is the introduction of new feature space where the original features are represented. The new space is of lower dimension that the original space. In case of text an example would be the hashing trick where a piece of text is reduced to a vector of few bits (say 16 or 32) or bytes. The amazing thing is that the geometry of the space in reserved (given enough bits), so relative distances between documents remain the same as in the original space, so you can deploy standard machine learning techniques without having to deal with unbound (and huge number of) dimensions found in text.;;;
298;2;2014-06-10T22:29:52.873;;"You already have a Masters in Statistics, which is great! In general, I'd suggest to people to take as much statistics as they can, especially Bayesian Data Analysis. Depending on what you want to do with your PhD, you would benefit from foundational courses in the discipline(s) in your application area.  You already have Economics but if you want to do Data Science on social behavior, then courses in Sociology would be valuable.  If you want to work in fraud prevention, then a courses in banking and financial transactions would be good.  If you want to work in information security, then taking a few security courses would be good.There are people who argue that it's not valuable for Data Scientists to spend time on courses in sociology or other disciplines.  But consider the recent case of the Google Flu Trends project.  In this article their methods were strongly criticized for making avoidable mistakes.  The critics call it ""Big Data hubris"".There's another reason for building strength in social science disciplines: personal competitive advantage.  With the rush of academic degree programs, certificate programs, and MOOCs, there is a mad rush of students into the Data Science field.  Most will come out with capabilities for core Machine Learning methods and tools.  PhD graduates will have more depth and more theoretical knowledge, but they are all competing for the same sorts of jobs, delivering the same sorts of value.  With this flood of graduates, I expect that they won't be able to command premium salaries.But if you can differentiate yourself with a combination of formal education and practical experience in a particular domain and application area, then you should be able to set yourself apart from the crowd.(Context: I'm in a PhD program in Computational Social Science, which has a heavy focus on modeling, evolutionary computation, and social science disciplines, and less emphasis on ML and other empirical data analysis topics).";;;
300;2;2014-06-10T23:20:16.670;;Topological Data Analysis is a method explicitly designed for the setting you describe. Rather than a global distance metric, it relies only on a local metric of proximity or neighborhood. See: Topology and data and Extracting insights from the shape of complex data using topology. You can find additional resources at the website for Ayasdi.;;;
301;2;2014-06-11T00:32:54.887;;"One reason that data cleaning is rarely fully automated is that there is so much judgment required to define what ""clean"" means given your particular problem, methods, and goals.It may be as simple as imputing values for any missing data, or it might be as complex as diagnosing data entry errors or data transformation errors from previous automated processes (e.g. coding, censoring, transforming).  In these last two cases, the data looks good by outward appearance but it's really erroneous.  Such diagnosis often requires manual analysis and inspection, and also out-of-band information such as information about the data sources and methods they used.Also, some data analysis methods work better when erroneous or missing data is left blank (or N/A) rather than imputed or given a default value.  This is true when there is explicit representations of uncertainty and ignorance, such as Dempster-Shafer Belief functions.Finally, it's useful to have specific diagnostics and metrics for the cleaning process.  Are missing or erroneous values randomly distributed or are they concentrated in any way that might affect the outcome of the analysis.  It's useful to test the effects of alternative cleaning strategies or algorithms to see if they affect the final results.Given these concerns, I'm very suspicious of any method or process that treats data cleaning in a superficial, cavalier or full-automated fashion.  There are many devils hiding in those details and it pays to give them serious attention.";;;
302;2;2014-06-11T00:44:20.517;;I've seen a few similar systems over the years.  I remember a company called ClickTrax which if I'm not mistaken got bought by Google and some of their features are now part of Google Analytics.Their purpose was marketing, but the same concept can be applied to user experience analytics.  The beauty of their system was that what was tracked was defined by the webmaster - in your case the application developer.I can imagine as an application developer I would want to be able to see statistical data on two things - task accomplishment, and general feature usage.As an example of task accomplishment, I might have 3 ways to print a page - Ctrl+P, File->Print, and a toolbar button.  I would want to be able to compare usage to see if the screenspace utilized by the toolbar button was actually worth it.As an example of general feature usage, I would want to define a set of features within my application and focus my development efforts on expanding the features used most by my end users.  Some features that take maybe 5 clicks and are popular, I might want to provide a hotkey for, or slim down the number of clicks to activate that feature.  There is also event timing.  Depending on the application, I might want to know the average amount of time spent on a particular feature.  Another thing I would want to look at are click streams.  How are people getting from point A to point B in my application?  What are the most popular point B's?  What are the most popular starting points?  ;;;
303;2;2014-06-11T01:09:06.100;;"This isn't my area of specialty and I'm not familiar with Moses, but I found this after some searching.  I think you are looking for GIZA++.  You'll see GIZA++ listed in the ""Training"" section (left menu) on the Moses home page, as the second step. GIZA++ is briefly described in tutorial fashion here. Here are a few tutorial PowerPoint slides: http://www.tc.umn.edu/~bthomson/wordalignment/GIZA.ppt";;;
305;1;2014-06-11T04:24:04.183;Does Amazon RedShift replace Hadoop for ~1XTB data?;There is plenty of hype surrounding Hadoop and its eco-system.  However, in practice, where many data sets are in the terabyte range, is it not more reasonable to use Amazon RedShift for querying large data sets, rather than spending time and effort building a Hadoop cluster? Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?;[education, open-source];1480;3
306;2;2014-06-11T05:17:12.253;;"Personally, I don't think it's all that difficult to set up a hadoop cluster, but I know that it is sometimes painful when you are getting started.HDFS size limitations well exceed a TB (or did you mean exabyte?).  If I'm not mistaken it scales to yottabytes or some other measurement that I don't even know the word for.  Whatever it is, it's really big.Tools like Redshift have their place, but I always worry about vendor specific solutions.  My main concern is always ""what do I do when I am dissatisfied with their service?"" - I can go to google and shift my analysis work into their paradigm or I can go to hadoop and shift that same work into that system.  Either way, I'm going to have to learn something new and do a lot of work translating things.That being said, it's nice to be able to upload a dataset and get to work quickly - especially if what I'm doing has a short lifecycle.  Amazon has done a good job of answering the data security problem.If you want to avoid hadoop, there will always be an alternative.  But it's not all that difficult to work with once you get going with it.";;;
307;1;2014-06-11T06:07:45.767;Big data case study or use case example;I have read lot of blogs\article on how different type of industries are using Big Data Analytic. But most of these article fails to mentionWhat kinda data these companies used. What was the size of the dataWhat kinda of tools technologies they used to process the dataWhat was the problem they were facing and how the insight they got the data helped them to resolve the issue.How they selected the tool\technology to suit their need.What kinda pattern they identified from the data & what kind of patterns they were looking from the data.I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example. It would be great if someone share how finance industry is making use of Big Data Analytic.;[education, open-source];850;2
308;2;2014-06-11T06:49:04.070;;"News outlets tend to use ""Big Data"" pretty loosely.  Vendors usually provide case studies surrounding their specific products.  There aren't a lot out there for open source implementations, but they do get mentioned.  For instance, Apache isn't going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will.Here's an example case study from Cloudera in the finance sector.Quoting the study: One major global financial services conglomerate uses Cloudera and Datameer to help  identify rogue trading activity. Teams within the firm’s asset management group are  performing ad hoc analysis on daily feeds of price, position, and order information. Having  ad hoc analysis to all of the detailed data allows the group to detect anomalies across  certain asset classes and identify suspicious behavior. Users previously relied solely on  desktop spreadsheet tools. Now, with Datameer and Cloudera, users have a powerful  platform that allows them to sift through more data more quickly and avert potential  losses before they begin.. A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality  as required by the Dodd-Frank Act and other regulations. Integrating loan and branch data  as well as wealth management data, the bank’s data quality initiative is responsible for  ensuring that every record is accurate. The process includes subjectingthe data to over  50 data sanity and quality checks. The results of those checks are trended over time to  ensure that the tolerances for data corruption and data domains aren’t changing adversely  and that the risk profiles being reported to investors and regulatory agencies are prudent  and in compliance with regulatory requirements. The results are reported through a data  quality dashboard to the Chief Risk Officer and Chief Financial Officer, who are ultimately  responsible for ensuring the accuracy of regulatory compliance reporting as well as  earnings forecasts to investorsI didn't see any other finance related studies at Cloudera, but I didn't search very hard.  You can have a look at their library here.Also, Hortonworks has a case study on Trading Strategies where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R.These don't answer all of your questions.  I'm pretty sure both of these studies covered most of them. I don't see anything about tool selection specifically.  I imagine sales reps had a lot to do with getting the overall product in the door, but the data scientists themselves leveraged the tools they were most comfortable with.  I don't have a lot of insight into that area in the big data space.";;;
309;2;2014-06-11T06:51:19.143;;"tl;dr: They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  -FunctionYou can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.-Performance ProfileQuery execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term indexing very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.-Cost ProfileWhich solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift's Query Engine is based on PostgreSQL and is very mature, compared to Hive's. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It's a great tool to have, but it won't replace Hadoop.  EDIT:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift's operation aren't that mature yet. For example, Redshift can't handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. ";;;
310;1;2014-06-11T10:11:59.397;One-Class discriminatory classification with imbalanced, heterogenous Negative background?;"I'm working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.There are about 1,150 known ""positives"", against a background of about 13 million protein sequences (""Unknown/poorly annotated background""), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly ""negative"" way). My previous implementation looked at this as a binary classification problem: Positive set = Proteins marked as Neuropeptides.Negative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution. That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it's at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).My thoughts were to:1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set. (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) . 2) One class learning - Would be nice, but as I understand it, it's meant just for anomaly detection, and has poorer performance than discriminatory approaches.*) I've heard of P-U learning, which sounds neat, but I'm a programming N00b, and I don't know of any existing implementations for it. (In Python/sci-kit learn).So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the ""negative"" proteins, but they're all very very different from each other, so I don't know how well the classifier would handle them as one big , unbalanced mix).Thanks!";[education, open-source];272;3
311;2;2014-06-11T11:24:19.963;;The way I would attack the problem, in general, is to leverage statistical analysis like Principal Component Analysis or Ordinary Least Squares to help determine what attributes within these protein sequences are best suited to classify proteins as Neuropeptide hormone precursors.  In order to do that, you'll have to convert the protein sequences into numeric data, but I believe some work has already been done in that regard using formulas leveraged in Amino Acid PCA.See these two links:http://www.ncbi.nlm.nih.gov/pubmed/24496727http://www.ncbi.nlm.nih.gov/pubmed/16615809Once that work has been done, I would attempt to classify using the entire dataset and a reinforcement learning algorithm, like Naive Bayes while slimming down the data into that which PCA has identified as important.  The reason I would try to use Bayes is because it has proven to be one of the best methods for determining spam vs. regular email, which has a similarly skewed dataset.Having said all of that...Slimming down the number or type of negative classifications might skew your results a few points one way or the other, but I don't think you'll see the long term effectiveness change substantially until you do the leg work of determining how to best remove the fuzziness from your training data.  That will either require a field expert or statistical analysis.I could be completely off base.  I am interested in seeing some other answers, but that is my 2 cents.;;;
312;2;2014-06-11T11:32:13.433;;Therriault, really happy to hear you are using Vertica! Full disclosure, I am the chief data scientist there :) . The workflow you describe is exactly what I encounter quite frequently and I am a true believer in preprocessing those very large datasets in the database prior to any pyODBC and pandas work. I'd suggest creating a view or table via a file based query just to ensure reproducible work. Good Luck;;;
313;1;2014-06-11T13:28:35.980;"Books about the ""Science"" in Data Science?";"What are the books about the science and mathematics behind data science? It feels like so many ""data science"" books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?Examples: Agresti's Categorical Data Analysis, Linear Mixed Models for Longitudinal Data, etc... etc...";[education, open-source];901;17
314;2;2014-06-11T13:49:08.970;;If I could only recomend one to you, it would be: The Elements of Statistical Learning and Prediction by Hastie, Tibshirani and Friedman.  It provides the math/statistics behind a lot of commonly used techniques in data science.For Bayesian Techniques, Bayesian Data Analysis by Gelman, Carlin, Stern, Dunson, Vehtari and Rubin is excellent.Statistical Inference by Casella and Berger is a good graduate-level textbook on the theoretical foundation of statistics.  This book does require a pretty high level of comfort with math (probability theory is based on measure theory, which is not trivial to understand).With respect to data generating processes, I don't have a recommendation for a book.  What I can say is that a good understanding of the assumptions of the techniques used and ensuring that the data was collected or generated in a manner that does not violate those assumptions goes a long way towards a good analysis.;;;
316;2;2014-06-11T16:25:34.747;;As Konstantin has pointed, R performs all its computation in the system's memory i.e. RAM. Hence, RAM capacity is a very important constraint for computation intensive operations in R. Overcoming this constraint, data is being stored these days in HDFS systems, where data isn't loaded onto memory and program is run instead, program goes to the data and performs the operations, thus overcoming the memory constraints.  RHadoop (https://github.com/RevolutionAnalytics/RHadoop/wiki) is the connector you are looking for.Coming to the impact on algorithms which are computation intensive, Random Forests/Decision Trees/Ensemble methods on a considerable amount of data (minimum 50,000 observations in my experience) take up a lot of memory and are considerably slow. To speed up the process, parallelization is the way to go and parallelization is inherently available in Hadoop! That's where, Hadoop is really efficient.So, if you are going for ensemble methods which are compute intensive and are slow, you would want to try out on the HDFS system which gives a considerable performance improvement.;;;
317;2;2014-06-11T16:56:59.660;;If you already know R Studio, then the caret package is a good place to start. Here are some tutorials: https://class.coursera.org/predmachlearn-002http://caret.r-forge.r-project.org/index.htmlWith R and caret you can easily load and splice data sets, feature reduction, principal component analysis, and train and predict using various algorithms.;;;
318;2;2014-06-11T17:24:53.610;;If you can reproduce the 6x3 grid of graphs from the banner of the http://scikit-learn.org/ page then you will have learnt some ML and some Python. You didn't mention a language. Python is easy enough to learn very quickly, and scikit-learn has a wide range of algorithms implemented.Then try on your own data!;;;
319;1;2014-06-11T18:22:36.267;Debugging Neural Networks;I've built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.I've implemented gradient checking, double checked everything etc and I'm pretty certain it's working correctly.I've run it a few times and it reaches 'Optimization terminated successfully' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.I'd be interested to understand what's going on here, or if I've implemented neural net incorrectly?;[education, open-source];408;1
320;2;2014-06-11T20:34:51.873;;There are so many ways to go wrong with a neural net that it's going to be difficult to debug. Also, to address your intuition, each additional hidden layer makes learning much harder. With that said, here are some possibilities:You have added weight decay. Adding more layers adds more weights which increases your regularization cost.The problem is simple enough that a model with a single hidden layer is sufficient. Adding more hidden layers makes it harder for the network to learn (harder optimization problem).The optimization method is not doing a great job (I prefer climin to scipy.optimize).You are using the sigmoid/tanh activation function. The sigmoid function causes the vanishing gradient problem which makes learning hard with more layers. Try using the ReLu function.Training neural nets takes a lot of practice, luck, and patience. Good luck.;;;
323;1;2014-06-11T23:52:37.823;How can we calculate AUC for a simple decision tree?;The setup is simple: binary classification using a simple decision tree, each node of the tree has a single threshold applied on a single feature. In general, building a ROC curve requires moving a decision threshold over different values and computing the effect of that change on the true positive rate and the false positives rate of predictions. What's that decision threshold in the case of a simple fixed decision tree? ;[education, open-source];412;
324;1;2014-06-12T03:11:00.033;How can I extract news about a particular company from various websites using RODBC package in R? And perform sentiment analysis on the data?;I want to extract news about a company from online news by using RODBC package in R. And I want to use the extracted data for sentiment analysis. I want to accomplish this in such a way that the positive news is assigned a value of +1, the negative news is assigned a value of -1 and the neutral news is assigned a value of 0.;[education, open-source];163;
325;2;2014-06-12T05:17:46.453;;This isn't a question with a simple answer, so all I can really do is point you in the right direction.The RODBC package isn't meant to extract data online, it's meant to pull data from a database.  If you will be leveraging that package, it will be after you pull data down from the web.Jeffrey Bean put together a slideshow tutorial for doing sentiment analysis with Twitter data a few years back.  He used the Twitter stream as well as some data pulled in from web scraping.  It's a good starting point.There's also this site that discusses a few different approaches to this problem in detail, including Bean's, the sentiment package, and ViralHeat (which is a commercial sentiment analysis service who's data you can pull into R).  Sentiment has since been removed (archived versions here), but the qdap package is available and is designed for use in transcript analysis.;;;
326;1;2014-06-12T06:04:48.243;Python vs R for machine learning;I'm just starting to develop a machine learning application for academic purposes. I'm currently using R and training myself in it. However, in a lot of places, I saw people using Python.What are people using in academia and industry, and what is recommend?;[education, open-source];9203;13
327;2;2014-06-12T07:05:05.653;;"There is nothing like ""python is better"" or ""R is much better than x"". The only fact I know is that in the industry allots of people stick to python because that is what they learned at the university. The python community is really active and have a few great frameworks for ML and data mining etc. But to be honest, if you get a good c programmer he can do the same as people do in python or r, if you got a good java programmer he can also do (near to) everything in java. So just stick with the language you are comfortable with.";;;
328;2;2014-06-12T08:30:49.757;;"There is no ""better"" language. I have tried both of them and I am comfortable with Python so I work with Python only. Though I am still learning stuff, but I haven't encounter any roadblock with Python till now. The good thing about Python is community is too good and you can get a lot of help on the Internet easily. Other than that, I would say go with the language you like not the one people recommend. ";;;
330;2;2014-06-12T09:57:39.890;;You can also checkout the seaborn package for statistical charts.;;;
331;2;2014-06-12T10:09:23.887;;Some additional thoughts.The programming language 'per se' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.   As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments.The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company's infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much.I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates.For the second task, you have to think before at what you really want. If you want robust production ready tools, then C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to start a flame war, it's my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages).Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement myself everything from scratch. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, as a programming language R, for me at least is a nightmare. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else.If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called The Humble Programmer;;;
332;2;2014-06-12T10:27:14.480;;"In order to build the ROC curve and AUC (Area under curve) you have to have a binary classifier which provides you at classification time, the distribution (or at least a score), not the classification label. To give you an example, suppose you have a binary classification model, with classes c1 and c2. For a given instance, your classifier would have to return a score for c1 and another for c2. If this score is a probability-like (preferrable), than something like p(c1), p(c2) would work. In plain English is translated like ""I (the model) classify this instance as c1 with probability p(c1), and as c2 with probability  p(c2)=1-p(c1)"". This applies for all type of classifiers, not only for decision trees. Having these scores you can than compute ROC or AUC by varying a threshold on p(c1) values, from the smallest to the greatest value.Now, if you have an implementation of a decision tree and you want to change that implementation to return scores instead of labels you have to compute those values. The most used way for decision trees is to use the proportion of classes from the leaf nodes. So, for example you have built a decision tree and when you predict the class for an instance you arrive at a leaf node which have (stored from the learning phase) 10 instances of class c1 and 15 instances of class c2, you can use the ratios as the scores. So, in our example, you would return p(c1) = 10 / (10+15) = 0.4 probability of class c1 and p(c2) = 15/(10+15)=0.6 probability of being class c2.For further reading on the ROC curves, the best and inspiring source of information I found to be the Tom Fawcett paper called An Introduction to ROC Analysis, it's solid gold on this topic.";;;
334;1;2014-06-12T10:52:03.410;What do you think of Data Science certifications?;"I've now seen two data science certification programs - the John Hopkins one available at Coursera and the Cloudera one.I'm sure there are others out there.The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:R Programmingcleaning and obtaining dataData AnalysisReproducible ResearchStatistical InferenceRegression ModelsMachine LearningDeveloping Data ProductsAnd what looks to be a Project based completion task similar to Cloudera's Data Science ChallengeThe Cloudera program looks thin on the surface, but looks to answer the two important questions - ""Do you know the tools"", ""Can you apply the tools in the real world"".  Their program consists of:Introduction to Data ScienceData Science Essentials ExamData Science Challenge (a real world data science project scenario)I am not looking for a recommendation on a program or a quality comparison.I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.EDIT: These are all great answers.  I'm choosing the correct answer by votes.";[education, open-source];3425;5
335;2;2014-06-12T11:11:35.600;;The certification programs you mentioned are really entry level courses. Personally, I think these certificates show only person's persistence and they can be only useful to those who is applying for internships, not the real data science jobs.;;;
336;2;2014-06-12T11:30:20.943;;There isn't a silver bullet language that can be used to solve each and every data related problem. The language choice depends on the context of the problem, size of data and if you are working at a workplace you have to stick to what they use.Personally I use R more often than Python due to its visualization libraries and interactive style. But if I need more performance or structured code I definitely use Python since it has some of the best libraries as SciKit-Learn, numpy, scipy etc. I use both R and Python in my projects interchangeably. So if you are starting on data science work I suggest you to learn both and it's not difficult since Python also provides a similar interface to R with Pandas. If you have to deal with much larger datasets, you can't escape eco-systems built with Java(Hadoop, Pig, Hbase etc).;;;
337;2;2014-06-12T11:54:59.140;;I would add to what others have said till now. There is no single answer that one language is better than other.Having said that, R has a better community for data exploration and learning. It has extensive visualization capabilities. Python, on the other hand, has become better at data handling since introduction of pandas. Learning and development time is very less in Python, as compared to R (R being a low level language).I think it ultimately boils down to the eco-system you are in and personal preferences. For more details, you can look at this comparison here.;;;
338;2;2014-06-12T12:13:26.940;;"I did the first 2 courses and I'm planning to do all the others too.  If you don't know R, it's a really good program. There are assignments and quizzes every week. Many people find some courses very difficult. You are going to have hard time if you don't have any programming experience (even if they say it's not required). Just remember.. it's not because you can drive a car that you are a F1 pilot ;) ";;;
339;2;2014-06-12T12:59:00.663;;Some real important differences to consider when you are choosing R or Python over one another:Machine Learning has 2 phases. Model Building and Prediction phase. Typically, model building is performed as a batch process and predictions are done realtime. The model building process is a compute intensive process while the prediction happens in a jiffy. Therefore, performance of an algorithm in Python or R doesn't really affect the turn-around time of the user. Python 1, R 1.Production: The real difference between Python and R comes in being production ready. Python, as such is a full fledged programming language and many organisations use it in their production systems. R is a statistical programming software favoured by many academia and due to the rise in data science and availability of libraries and being open source, the industry has started using R. Many of these organisations have their production systems either in Java, C++, C#, Python etc. So, ideally they would like to have the prediction system in the same language to reduce the latency and maintenance issues.Python 2, R 1.Libraries: Both the languages have enormous and reliable libraries. R has over 5000 libraries catering to many domains while Python has some incredible packages like Pandas, NumPy, SciPy, Scikit Learn, Matplotlib. Python 3, R 2.Development: Both the language are interpreted languages. Many say that python has a good learning curve, it's almost like reading english (to put it on a lighter note) but R has a reputation of having a steeper learning curve. Also, both of them have good IDEs (Spyder etc for Python and RStudio for R). Python 4, R 2.Speed: R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow. Python 4, R 3.Visualizations: In data science, we frequently tend to plot data to showcase patterns to users. Therefore, visualisations become an important criteria in choosing a software and R completely kills Python in this regard. Thanks to Hadley Wickham for an incredible ggplot2 package. R wins hands down. Python 4, R 4.Dealing with Big Data: One of the constraints of R is it stores the data in system memory (RAM). So, RAM capacity becomes a constraint when you are handling Big Data. Python does well, but I would say, as both R and Python have HDFS connectors, leveraging Hadoop infrastructure would give substantial performance improvement. So, Python 5, R 5.So, both the languages are equally good. Therefore, depending upon your domain and the place you work, you have to smartly choose the right language. The technology world usually prefers using a single language. Business users (marketing analytics, retail analytics) usually go with statistical programming languages like R, since they frequently do quick prototyping and build visualisations (which is faster done in R than Python).;;;
340;2;2014-06-12T13:38:10.877;;There's plenty. If you've ever used ggplot2 in R and want to do that in python:https://pypi.python.org/pypi/ggplot/0.5.9If you want to use a similar visualisation grammar (Vega) and go via D3 then:https://github.com/wrobstory/vincentOr if you want the full-on 3d shizzle:http://docs.enthought.com/mayavi/mayavi/;;;
341;2;2014-06-12T13:42:05.383;;"You have to first make it clear what do you mean by ""learn Hadoop"". If you mean using Hadoop, such as learning to program in MapReduce, then most probably it is a good idea. But fundamental knowledge (database, machine learning, statistics) may play a bigger role as time goes on.";;;
343;2;2014-06-12T15:22:16.247;;Increasing the number of hidden layers for a standard neural network actually won't improve results in a majority of cases. Changing the size of the hidden layer will.This fact (that the number of hidden layers does very little) has actually was noted historically and is the motivation behind the field of deep learning. Deep learning is effectively clever ways of training multilayer neural networks by, for example, isolating subsets of features when training different layers.Good introductory video on this topic on YouTube;;;
345;2;2014-06-12T16:52:46.557;;Introductory:Machine Learning: The Art and Science of Algorithms that Make Sense of Data (Flach)Learning From Data (Abu-Mostafa et al.)Introduction to Statistical Learning (James et al.)Digging deeper:Elements of Statistical Learning (Hastie et al.)Pattern Recognition and Machine Learning (Bishop)Some special interest examples:Convex Optimization (Boyd)Bayesian Reasoning and Machine Learning (Barber)Probabilistic Graphical Models (Koller)Neural Networks for Pattern Recognition (Bishop)Broader reference works on machine learning (not really what you asked for, but for completeness):Machine Learning: A Probabilistic Perspective (Murphy)Artificial Intelligence: A Modern Approach (Russell & Norvig)Bonus paper:Statistical Modeling: The Two Cultures (Breiman);;;
346;2;2014-06-12T17:03:15.733;;In addition to the courses and tutorials posted, I would suggest something a bit more 'hands on': Kaggle has some introductory competitions that might pique your interest (most people start with the Titanic competition). And there's a large variety of subjects to explore and compete in when you want to get more experience.;;;
347;2;2014-06-12T17:58:21.467;;As mentioned in above answers grasp the basics of ML by following MOOCs by Prof.Andrew Ng and 'Learning From Data' by Prof. Yaser Abu-Mostafa.  R is the clear winner as the most used tool in Kaggle competitions. (Don't forget to check the resources on Kaggle wiki and forums)Learn basic R and Python. Coursera 'Data Science' track has an introductory R course. Almost all the algorithms can be found in Python and R libraries. Feel free to use the algorithms you learned in few kaggle competitions. As a starting point compare the performance of several algorithms on Titanic dataset and Digit recognizer dataset on kaggle.And do continue practising on various datasets!;;;
348;2;2014-06-12T18:33:21.540;;"Not sure about the cloud era one, but one of my friends joined the John Hopkins one and in his words it's ""brilliant to get you started"". It has also been recommended by a lot of people. I am planning to join it in few weeks. As far as seriousness is concerned, I don't think these certifications are gonna help you land a job, but they sure will help you learn.  ";;;
349;2;2014-06-12T20:51:59.930;;As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it's infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west. While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience to know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience.;;;
350;1;2014-06-12T22:11:46.607;Example tasks of a data scientist and the necessary knowledge;Could you give some examples of typical tasks that a data scientist does in his daily job, and the must-know minimum for each of the levels (like junior, senior, etc. if there are any)? If possible, something like a Programmer competency matrix.;[education, open-source];288;
351;2;2014-06-13T03:50:24.610;;There are multiple certifications going on, but they have different focus area and style of teaching.I prefer The Analytics Edge on eDX lot more over John Hopkins specialization, as it is more intensive and hands on. The expectation in John Hopkins specialization is to put in 3 - 4 hours a week vs. 11 - 12 hours a week on Analytics Edge.From an industry perspective, I take these certifications as a sign of interest and not level of knowledge a person possesses. There are too many dropouts in these MOOCs. I value other experience (like participating in Kaggle competitions) lot more than undergoing XYZ certification on MOOC.;;;
352;1;2014-06-13T05:40:39.360;How do I figure out if subsets can be plotted in a normal Euler diagram?;In some cases, it may be impossible to draw Euler diagrams with overlapping circles to represent all the overlapping subsets in the correct proportions. This type of data then requires using polygons or other figures to represent each set. When dealing with data that describes overlapping subsets, how can I figure out whether a simple Euler diagram is possible?;[education, open-source];93;
354;1;2014-06-13T07:28:37.763;Data Science as a Social Scientist?;as I am very interested in programming and statistics, Data Science seems like a great career path to me - I like both fields and would like to combine them. Unfortunately, I have studied political science with a non-statistical sounding Master. I focused on statistics in this Master, visiting optional courses and writing a statistical thesis on a rather large dataset.Since almost all job adds are requiring  a degree in informatics, physics or some other techy-field, I am wondering if there is a chance to become a data scientist or if I should drop that idea.I am lacking knowledge in machine learning, sql and hadoop, while having a rather strong informatics and statistics background. So can somebody tell me how feasible my goal of becoming a data scientist is? ;[education, open-source];148;
355;2;2014-06-13T10:08:14.087;;The downvotes are because of the topic, but I'll attempt to answer your question as best I can since it's here.Data science is a term that is thrown around as loosely as Big Data.  Everyone has a rough idea of what they mean by the term, but when you look at the actual work tasks, a data scientist's responsibilities will vary greatly from company to company.Statistical analysis could encompass the entirety of the workload in one job, and not even be a consideration for another.I wouldn't chase after a job title per se.  If you are interested in the field, network (like you are doing now) and find a good fit.  If you are perusing job ads, just look for the ones that stress statistical and informatics backgrounds.  Hadoop and SQL are both easy to become familiar with given the time and motivation, but I would stick with the areas you are strongest in and go from there.;;;
356;1;2014-06-13T10:57:10.623;How to best accomplish high speed comparison of like data?;I attack this problem frequently with inefficiency because it's always pretty low on the priority list and my clients are resistant to change until things break.  I would like some input on how to speed things up.  I have multiple datasets of information in a SQL database.  The database is vendor-designed, so I have little control over the structure.  It's a sql representation of a class-based structure.  It looks a little bit like this:Main-class table -sub-class table 1 -sub-class table 2  -sub-sub-class table ... -sub-class table nEach table contains fields for each attribute of the class.  A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class' table, joined by a unique identifier.There are hundreds of classes. which means thousands of views and tens of thousands of columns.Beyond that, there are multiple datasets, indicated by a field value in the Main-class table.  There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources.  Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again.  The rules are in place because we might trust one source of data more than another for a particular value of a particular class.The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row.  The common changes are limited to a handful of fields in each row, but since anything can change we compare each value.There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours.  We resolve that problem generally, by throwing more hardware at it, but this isn't a hadoop environment currently so there's a pretty finite limit to what can be done in that regard.How would you go about scaling a solution to this problem such that there were no limitations?  And how would you go about accomplishing the most efficient data-merge?  (currently it is field by field comparisons... painfully slow).;[education, open-source];89;
357;2;2014-06-13T12:18:53.830;;"I lead data science teams for a major Internet company and I have screened hundreds of profiles and interviewed dozens for our teams around the world. Many candidates have passed the aforementioned courses and programs or bring similar credentials. Personally, I have also taken the courses, some are good, others are disappointing but none of them makes you a ""data scientist"".In general, I agree with the others here. A certificate from Coursera or Cloudera just signalizes an interest but it does not move the needle.  There is a lot more to consider and you can have a bigger impact by providing a comprehensive repository of your work (github profile for example) and by networking with other data scientists. Anyone hiring for a data science profile will always prefer to see your previous work and coding style/abilities.";;;
358;2;2014-06-13T12:30:45.290;;Sounds interesting. Could the solution be to dump the data out, build a fast custom processing thingie to run it through and then import it back to the database? I've seen some blazing fast Java-based text processing tools for topic modeling that handle millions of lines of text per second.If it's an option then you can build a shell script to first dump the data in as good as format as possible. Then some pre-processing to separate the datasets, then real processing of comparison and changes. Lastly something that writes it back to a good format for input into database.Definately not a one-afternoon project, but you could probably get it to work in a couple of weeks.;;;
359;2;2014-06-13T13:49:35.777;;Becoming a Data Scientist – Curriculum via Metromap is a popular reference for this kind of question.;;;
360;2;2014-06-13T14:46:30.393;;That's because something called bias-variance dilema. The overfitted model means that we will have more complex decision boundary if we give more variance on model. The thing is, not only too simple models but also complex models are likely to have dis-classified result on unseen data. Consequently, over-fitted model is not good as under-fitted model. That's why overfitting is bad and we need to fit the model somewhere in the middle.;;;
361;1;2014-06-13T16:44:29.323;When is a Model Underfitted?;"Logic often states that by underfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?Note: This is a followup to my question, ""Why Is Overfitting Bad?""";[education, open-source];2516;3
362;2;2014-06-13T17:14:26.260;;Can't you create a hash for each classes, and then merge rows by rows, field by field only the classes where the hash changed ? It should be faster if most of the classes don't change..Or a hash of each rows or maybe columns.. depending on how the data normally change.. ;;;
363;2;2014-06-13T17:14:57.517;;Models are but abstractions of what is seen in real life. They are designed in order to abstract-away nitty-gritties of the real system in observation, while keeping sufficient information to support desired analysis.If a model is overfit, it takes into account too many details about what is being observed, and small changes on such object may cause the model to lose precision. On the other hand, if a model is underfit, it evaluates so few attributes that noteworthy changes on the object may be ignored.Note also that underfit may be seen as an overfit, depending on the dataset. If your input can be 99%-correctly-classified with a single attribute, you overfit the model to the data by simplifying the abstraction to a single characteristic. And, in this case, you'd be generalizing too much the 1% of the base into the 99%-class -- or also specifying the model so much that it can only see one class.A reasonable way to say that a model is neither over nor underfit is by performing cross-validations. You split your dataset into k parts, and say, pick one of them to perform your analysis, while using the other k - 1 parts to train your model. Considering that the input itself is not biased, you should be able to have as much variance of data to train and evaluate as you'd have while using the model in real life processing.;;;
364;2;2014-06-13T17:36:21.937;;"The Programmer Competency Matrix is just a set of skills, which are more likely to occur when being a real programmer than other skills, they are not a checklist to being a programmer, or for that matter, required to be a programmer; most common way to know someone is a programmer is that they're paid to be a programmer, which honestly has nothing to do with programming skills.To be a data scientist, do data science.";;;
365;2;2014-06-13T18:59:11.493;;@OP: Choosing answers by votes is the WORST idea.  Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.  To answer your question:Q: how seriously DS certifications are viewed at this point by the community.  A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, they are intentionally reserved for classroom setting.  Nonetheless, Coursera classes are very useful.  I'd say it is equivalent to one year of stat grad class, out of a two year Master program.  I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It's a lot easier to get A's in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variation from person to person.;;;
366;2;2014-06-13T20:13:01.913;;To answer your question it is important to understand the frame of reference you are looking for, if you are looking for what philosophically you are trying to achieve in model fitting, check out Rubens answer he does a good job of explaining that context.However, in practice your question is almost entirely defined by business objectives.  To give a concrete example, lets say that you are a loan officer, you issued loans that are \$3,000 and when people pay you back you make \$50.  Naturally you are trying to build a model that predicts how if a person defaults on their loan.  Lets keep this simple and say that the outcomes are either full payment, or default.From a business perspective you can sum up a models performance with a contingency matrix:When the model predicts someone is going to default, do they?  To determining the downsides of over and under fitting I find it helpful to think of it as an optimization problem, because in each cross section of predicted verses actual model performance there is either a cost or profit to be made:In this example predicting a default that is a default means avoiding any risk, and predicted a non-default which doesn't default will make \$50 per loan issued.  Where things get dicey is when you are wrong, if you default when you predicted non-default you lose the entire loan principal and if you predict default when a customer actually would not have you suffer \$50 of missed opportunity.  The numbers here are not important, just the approach.With this framework we can now begin to understand the difficulties associated with over and under fitting.Over fitting in this case would mean that your model works far better on you development/test data then it does in production.  Or to put it another way, your model in production will far underperform what you saw in development, this false confidence will probably cause you to take on far more risky loans then you otherwise would and leaves you very vulnerable to losing money.On the other hand, under fitting in this context will leave you with a model that just does a poor job of matching reality.  While the results of this can be wildly unpredictable, (the opposite word you want to describe your predictive models), commonly what happens is standards are tightened up to compensate for this, leading to less overall customers leading to lost good customers.  Under fitting suffers a kind of opposite difficulty that over fitting does, which is under fitting gives you lower confidence.  Insidiously, the lack of predictability still leads you to take on unexpected risk, all of which is bad news.In my experience the best way to avoid both of these situations is validating your model on data that is completely outside the scope of your training data, so you can have some confidence that you have a representative sample of what you will see 'in the wild'.  Additionally, it is always a good practice to revalidate your models periodically, to determine how quickly your model is degrading, and if it is still accomplishing your objectives.Just to some things up, your model is under fitted when it does a poor job of predicting both the development and production data.;;;
368;2;2014-06-14T07:34:37.643;;"If you know R and it's ggplot library, you could try ggplot for python:I like it, because I do work in R and python, and both are virtually identical.But if you are not familiar you have to deal with a very ""unpythonic"" syntax. But I think it's an easy library overall.";;;
369;1;2014-06-14T18:53:32.243;Difference between using RMSE and nDCG to evaluate Recommender Systems;What kind of error measures do RMSE and nDCG give while evaluating a recommender system, and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well!;[education, open-source];328;2
370;1;2014-06-14T19:54:53.193;Data Science oriented dataset/research question for Statistics MSc thesis;"I'd like to explore 'data science'. The term seems a little vague to me, but I expect it to require:machine learning (rather than traditional statistics);a large enough dataset that you have to run analyses on clusters.What are some good datasets and problems, accessible to a statistician with some programming background, that I can use to explore the field of data science?To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems.";[education, open-source];356;
371;1;2014-06-14T20:32:06.143;How to process natural language queries?;I'm curious about natural language querying.  Stanford has what looks to be a strong set of software for processing natural language.  I've also seen the Apache OpenNLP library, and the General Architecture for Text Engineering.There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?The first rectangle on my flow chart is a bit of a mystery. For example, I might want to know:How many books were sold last month?And I'd want that translated intoSelect count(*)   from sales   where    item_type='book' and    sales_date >= '5/1/2014' and    sales_date <= '5/31/2014';[education, open-source];89;
372;2;2014-06-14T20:52:00.873;;"Just head to kaggle.com; it'll keep you busy for a long time. For open data there's the UC Irvine Machine Learning Repository. In fact, there's a whole Stackexchange site devoted to this; look there.";;;
373;2;2014-06-14T21:13:20.110;;The Sunlight Foundation is an organization that is focused on opening up and encouraging non-partisan analysis of government data.There is a ton of analysis out there in the wild that can be used for comparison, and a wide variety of topics.They provide tools and apis for accessing data, and have helped push to make data available in places like data.gov.One interesting project is Influence Explorer.  You can get source data here as well as access to real time data.You might also want to take a look at one of our more popular questions:Publicly available datasets.;;;
374;2;2014-06-14T21:39:15.833;;"Natural language querying poses very many intricacies which can be very difficult to generalize.  From a high level, I would start with trying to think of things in terms of nouns and verbs.So for the sentence: How many books were sold last month?You would start by breaking the sentence down with a parser which will return a tree format similar to this:You can see that there is a subject books, a compound verbal phrase indicating the past action of sell, and then a noun phrase where you have the time focus of a month.We can further break down the subject for modifiers: ""how many"" for books, and ""last"" for month.Once you have broken the sentence down you need to map those elements to sql language e.g.: how many => count, books => book, sold => sales, month => sales_date (interval), and so on.  Finally, once you have the elements of the language you just need to come up with a set of rules for how different entities interact with each other, which leaves you with:Select count(*)   from sales   where    item_type='book' and    sales_date >= '5/1/2014' and    sales_date <= '5/31/2014'This is at a high level how I would begin, while almost every step I have mentioned is non-trivial and really the rabbit hole can be endless, this should give you many of the dots to connect.  ";;;
375;1;2014-06-14T23:52:10.490;Amazon S3 vs Google Drive;The majority of people use S3. However, Google Drive seems a promising alternative solution for storing large amounts of data. Are there specific reasons why one is better than the other?;[education, open-source];871;1
376;1;2014-06-15T00:00:51.657;How to compare experiments run over different infrastructures;"I'm developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we're able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.The problem is the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), and depending on what is being evaluated, it may raise false/unfair comparisons. For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.So, how can one honestly compare different experiments to point out efficiency gains? ";[education, open-source];75;1
377;2;2014-06-15T00:11:54.950;;From our perspective on here, the big benefit of S3 is the ease of accessing the data from within EC2.Google Drive is directly accessible from the Google Cloud platform.There are a host of other differences that might matter depending on your usage requirements, but that's the one that would matter most around here.The only other difference I can think of that would matter to the DS community is that when you are sharing something, you have no control of the address of a given file on google drive.;;;
379;2;2014-06-15T01:25:48.563;;"Financial Services is a big user of Big Data, and innovator too.  One example is mortgage bond trading.  To answer your questions for it: What kinda data these companies used. What was the size of the data?Long histories of each mortgage issued for the past many years, and payments by month against them.  (Billions of rows)Long histories of credit histories.  (Billions of rows)Home price indices.  (Not as big) What kinda of tools technologies they used to process the data?It varies.  Some use in-house solutions built on databases like Netezza or Teradata.  Others access the data via systems provided by the data providers.  (Corelogic, Experian, etc)  Some banks use columnal database technologies like KDB, or 1010data. What was the problem they were facing and how the insight they got the  data helped them to resolve the issue.The key issue is determining when mortgage bonds (mortgage backed-securities) will prepay or default.  This is especially important for bonds that lack the government guarantee.  By digging into payment histories, credit files, and understanding the current value of the house, it's possible to predict the likelihood of a default.  Adding an interest rate model and prepayment model also helps predict the likelihood of a prepayment. How they selected the tool\technology to suit their need.If the project is driven by internal IT, usually it's based off of a large database vendor like Oracle, Teradata or Netezza.  If it's driven by the quants, then they are more likely to go straight to the data vendor, or a 3rd party ""All in"" system. What kinda pattern they identified from the data & what kind of  patterns they were looking from the data.Linking the data gives great insights into who is likely to default on their loans, and prepay them.  When you aggregated the loans into bonds, it can be the difference between a bond issued at $100,000,000 being worth that amount, or as little as $20,000,000.";;;
380;2;2014-06-15T01:29:15.240;;I suspect this will get closed since it is very narrow, but my 2 cents...Data Science requires 3 skills:Math/StatsProgrammingDomain KnowledgeIt can be very hard to show all three.  #1 and #2 can be signaled via degrees, but a hiring manager who may not have them doesn't want to trust a liberal arts degree.  If you're looking to get into Data Science, position yourself as a domain expert first.  Publish election predictions.  If you're correct, cite them.  That will get you noticed.If you're Domain knowledge is A+ level, you don't need A+ level programming skills, but learn programming enough so that you don't need someone else to fetch data for you.;;;
381;2;2014-06-15T01:36:51.693;;"CAPM (Capital Asset Pricing Model) in Finance is a classic example of an underfit model.  It was built on the beautiful theory that ""Investors only pay for risk they can't diversify away"" so expected excess returns are equal to correlation to market returns.As a formula [0] Ra = Rf + B (Rm - Rf)where Ra is the expected return of the asset, Rf is the risk free rate, Rm is the market rate of return, and Beta is the correlation to the Equity premium (Rm - Rf)This is beautiful, elegant, and wrong.  Investors seem to require a higher rate of small stocks and value (defined by book to market, or dividend yield) stocks.  Fama and French [1] presented an update to the model, which adds additional Betas for Size and Value.  So how do you know in a general sense?  When the predictions you are making are wrong, and another variable with a logical explanation increases the prediction quality.  It's easy to understand why someone might think small stocks are risky, independent of non-diversifiable risk.  It's a good story, backed by the data.[0]  http://www.investopedia.com/terms/c/capm.asp[1]  http://en.wikipedia.org/wiki/Fama%E2%80%93French_three-factor_model";;;
382;1;2014-06-15T12:26:50.060;How to debug data analysis?;I've came across the following problem, that I recon is rather typical.I have some large data, say, a few million rows. I run some non-trivial analysis on it, e.g. an SQL query consisting of several sub-queries. I get some result, stating, for example, that property X is increasing over time.Now, there are two possible things that could lead to that:X is indeed increasing over timeI have a bug in my analysisHow can I test that the first happened, rather than the second? A step-wise debugger, even if one exists, won't help, since intermediate results can still consist of millions of lines.The only thing I could think of was to somehow generate a small, synthetic data set with the property that I want to test and run the analysis on it as a unit test. Are there tools to do this? Particularly, but not limited to, SQL.;[education, open-source];129;1
384;1;2014-06-15T14:01:38.233;Choose binary classification algorithm;I have a binary classification problem:Approximately 1000 samples in training set10 attributes, including binary, numeric and categoricalWhich algorithm is the best choice for this type of problem?By default I'm going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. ;[education, open-source];1117;0
386;2;2014-06-15T14:23:19.793;;For low parameters, pretty limited sample size,  and a binary classifier logistic regression should be plenty powerful enough.  You can use a more advanced algorithm but it's probably overkill. ;;;
387;1;2014-06-15T15:11:29.970;Filtering spam from retrieved data;I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it'd become a burden to continuously validate the current state of each spammer blocked, checking if the site/domain still disseminate spam data.Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?Edit: if possible, any example of strategy, even if just the intuition behind it, would be very welcome along with the answer.;[education, open-source];129;
388;2;2014-06-15T15:20:31.300;;"Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:On Neural Networks And The Future Of Spam A. C. Cosoi, M. S. Vlad, V. Sgarciu http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8Intelligent Word-Based Spam Filter Detection Using Multi-Neural Networks Ann Nosseir, Khaled Nagati and Islam Taj-Eddinhttp://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdfSpam Detection using Adaptive Neural Networks: Adaptive Resonance Theory David Ndumiyana, Richard Gotora, and Tarisai Mupamombehttp://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdfEDIT:The basic intuition behind using a neural network to help with spam filtering is by providing a weight to terms based on how often they are associated with spam.Neural networks can be trained most quickly in a supervised -- you explicitly provide the classification of the sentence in the training set -- environment.  Without going into the nitty gritty the basic idea can be illustrated with these sentences:Text = ""How is the loss of the Viagra patent going to affect Pfizer"", Spam = falseText = ""Cheap Viagra Buy Now"", Spam = trueText = ""Online pharmacy Viagra Cialis Lipitor"", Spam = trueFor a two stage neural network, the first stage will calculate the likelihood of spam based off of if the word exists in the sentence.  So from our example:viagra => 66%buy => 100%Pfizer => 0%etc..Then for the second stage the results in the first stage are used as variables in the second stage:viagra & buy => 100%Pfizer & viagra=> 0%This basic idea is run for many of the permutations of the all the words in your training data.  The end results once trained is basically just an equation that based of the context of the words in the sentence can assign a probability of being spam.  Set spamminess threshold, and filter out any data higher then said threshold.";;;
389;2;2014-06-15T15:49:12.907;;Here is a suggestion:Code your analysis in such a way that it can be run on sub-samples.Code a complementary routine which can sample, either randomly, or by time, or by region, or ...  This may be domain-specific. This is where your knowledge enters.Combine the two and see if the results are stable across subsamples.;;;
390;2;2014-06-15T16:07:35.543;;When categorical variables are in the mix, I reach for Random Decision Forests, as it handles categorical variables directly without the 1-of-n encoding transformation. This loses less information.;;;
391;2;2014-06-15T16:59:50.190;;This is what I normally do - take up the most important variables (basis your business understanding and hypothesis - you can always revise it later), group by on these attributes to reduce the number of rows, which can then be imported into a Pivot. You should include the sum and count of the relevant metrics on each row.Make sure that you don't put any filters in the previous step. Once you have entire data at a summarized level, you can play around in Pivot tables and see what things are changing / increasing or decreasing.If the data is too big to be summarized even on important parameters, you need to partition it in 3 - 4 subsets and then do this again.Hope it helps.;;;
393;2;2014-06-15T22:33:17.670;;Linear SVM should be a good starting point. Take a look at this guide to choose the right estimator.;;;
394;2;2014-06-16T04:21:36.340;;Personally, we use S3 on top of GCE and really love it. Depending on how much data you're dealing with, Google Drive just doesn't quite match the 5 TB max that S3 gives you. Also, if you're using python, boto does a pretty fantastic job of making most aws services pretty accessible regardless of what stack you're dealing with. Even if you're not using python, they've got a pretty straightforward API that generally is more accessible than Google Drive.Instead of google drive, though google did recently release a cloud storage service, apart from drive, that lets you more closely integrate your storage with any gce instance you've got, https://cloud.google.com/products/cloud-storage/They've got an API which seems to be pretty comparable to S3's, but I can't profess to having really played around with it much. Pricing-wise the two are identical, but I think that the large community and experience with aws in general still puts S3 squarely above both google's cloud storage and google drive.;;;
395;2;2014-06-16T04:37:58.817;;It's hard to say without knowing a little more about your dataset, and how separable your dataset is based on your feature vector, but I would probably suggest using extreme random forest over standard random forests because of your relatively small sample set.Extreme random forests are pretty similar to standard random forests with the one exception that instead of optimizing splits on trees, extreme random forest makes splits at random. Initially this would seem like a negative, but it generally means that you have significantly better generalization and speed, though the AUC on your training set is likely to be a little worse.Logistic regression is also a pretty solid bet for these kinds of tasks, though with your relatively low dimensionality and small sample size I would be worried about overfitting. You might want to check out using K-Nearest Neighbors since it often performs very will with low dimensionalities, but it doesn't usually handle categorical variables very well.If I had to pick one without knowing more about the problem I would certainly place my bets on extreme random forest, as it's very likely to give you good generalization on this kind of dataset, and it also handles a mix of numerical and categorical data better than most other methods.;;;
396;2;2014-06-16T04:51:47.847;;Alex made a number of good points, though I might have to push back a bit on his implication that DBSCAN is the best clustering algorithm to use here. Depending on your implementation, and whether or not you're using accelerated indices (many implementations do not), your time and space complexity will both be O(n2), which is far from ideal.Personally, my go-to clustering algorithms are OpenOrd for winner-takes-all clustering and FLAME for fuzzy clustering. Both methods are indifferent to whether the metrics used are similarity or distance (FLAME in particular is nearly identical in both constructions). The implementation of OpenOrd in Gephi is O(nlogn) and is known to be more scalable than any of the other clustering algorithms present in the Gephi package.FLAME on the other hand is great if you're looking for a fuzzy clustering method. While the complexity of FLAME is a little harder to determine since it's an iterative process, it has been shown to be sub-quadratic, and similar in run-speed to knn.;;;
397;2;2014-06-16T06:34:53.683;;First you need to verify that your implementation of the algorithm is accurate. For that use a small sample of data and check whether the result is correct. At this stage the sample doesn't need to be representative of the population. Once the implementation is verified, you need to verify that there is a significant relationship among the variables that you try to predict. To do that define null hypothesis and try to reject the null hypothesis with a significant confidence level. (hypothesis testing for linear regression) There might be unit test frameworks for your SQL distribution. But using a programming language like R will be more easier to implement.;;;
398;1;2014-06-16T07:32:29.137;What to consider before learning a new language for data analysis;I'm currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.In view of this I face the following dilemma: Should I learn Python to be able to use its extensive scientific libraries (Pandas, Numpy, Scipy, ...), or should I just dive into similar packages of a language I'm already acquainted with (Racket, or to a lesser extent Scala)?(Ideally I would learn Python in parallel with using statistical libraries in Racket, but I'm not sure I'll have time for both)I'm not looking for an answer to this dilemma, but rather for feedback on my different considerations:My current position is as follows:In favour of Python:Extensively used librariesWidely used (may be decisive in case of collaboration with others)A lot of online material to start learning itConferences that are specifically dedicated to Scientific Computing with PythonLearning Python won't be a waste of time anywayIn favour of a language I already know:It's a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)It is feasible. Both Racket and Scala have good mathematics and statistics librariesI can start right away with learning what I need to know rather than first having to learn the basicsTwo concrete questions:What am I forgetting?How big of a nuisance could the Python 2 vs 3 issue be?;[education, open-source];571;
399;2;2014-06-16T09:52:24.913;;According to me, all the factors, you have mentioned are superficial in nature. You have not considered the core of tool selection. In this case, there are 2 aspects, you mentioned:Data analysis - What kind of analysis are you working on? There might be some analysis which are easier in some languages and more difficult in other.Visualization - R provides similar community and learning material (as Python) and has the best visualizations compared to other languages here.At this stage, you can be flexible with what language to learn, since you are starting from scratch.Hope this helps.;;;
400;2;2014-06-16T11:45:05.800;;From my experience, the points to keep in mind when considering a data analysis platform are:Can it handle the size of the data that I need? If your data sets fit in memory, there's usually no big trouble, although AFAIK Python is somewhat more memory-efficient than R. If you need to handle larger-than-memory data sets, the platform need to handle it conveniently. In this case, SQL would cover for basic statistics, Python + Apache Spark is another option.Does the platform covers all of my analysis needs? The greatest annoyance I've encountered in data mining projects is having to juggle between several tools, because tool A handles web connections well, tool B does the statistics and tool C renders nice pictures. You want your weapon-of-choice to cover as many aspects of your projects as possible. When considering this issue, Python is very comprehensive, but R has a lot of build-in statistical tests ready-to-use, if that's what you need. ;;;
401;2;2014-06-16T13:21:58.777;;"I like a multiple step strategy:Write clean easy to understand code, as opposed to short-tricky code. I know statisticians like tricky code, but spotting problems in tricky code is dangerous. ( I am mentioning this because a supervisor of mine was fond of undocumented 500 lines python scrips - have fun debugging that mess and I have seen that pattern a lot, especially from people who are not from an IT background)Break down your code in smaller functions, which can be tested and evaluated in smaller stes.Look for connected elements, e.g. the number of cases with condition X is Y - so this query MUST return Y. Most often this is more complex, but doable.When you are running your script the first time, test it with a small subsample and carefully check if everything is in order. While I like unit tests in IT, bugs in statistics scripts are often so pronounced that they are easily visible doing a carefully check. Or they are methodical errors, which are probably never caught by unit tests.That should suffice to ensure a clean ""one - off "" job. But for a time series as you seem to have, I would add that you should check for values out of range, impossible combinations etc. For me, most scripts that have reached step 4 are probably bug free - and they will stay that way unless something changes. And most often, the data are changing - and that is something which should be checked for every run. Writing code for that can be time consuming and annoying, but it beats subtle errors due to data entry errors.";;;
402;2;2014-06-16T13:25:48.453;;The brat annotation tool might be useful for you as per my comment.  I have tried many of them and this is the best I have found.  It has a nice user interface and can support a number of different types of annotations.  The annotations are stored in a separate .annot file which contain each annotation as well as its location within the original document.  A word of warning though, if you ultimately want to feed the annotations into a classifier like the Stanford NER tool then you will have to do some manipulation to get the data into a format that it will accept.;;;
403;1;2014-06-16T13:30:01.320;Why is there such a mismatch between the Model's predicted probability and theoretical probability in logistic regression?;I am trying to do Logistic Regression using SAS Enterprise Miner. My Independent variables are CPR/Inc (Categorical 1 to 7)OD/Inc (Categorical 1 to 4)Insurance (Binary 0 or 1)Income Loss (Binary 0 or 1)Living Arrangement (Categorical 1 to 7)Employment Status (categorical 1 to 8)My Dependent Variable is Default (Binary 0 or 1)The following is the output from running Regression Model.Analysis of Maximum Likelihood Estimates                                  Standard          WaldParameter       DF    Estimate       Error    Chi-Square    Pr > ChiSq    Exp(Est)Intercept        1     -0.4148      0.0645         41.30        <.0001       0.660CPR___Inc  1     1     -0.8022      0.1051         58.26        <.0001       0.448CPR___Inc  2     1     -0.4380      0.0966         20.57        <.0001       0.645CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795Emp_Status 2     1      0.4061      0.0940         18.66        <.0001       1.501Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796Emp_Status 6     1      0.3761      0.0943         15.91        <.0001       1.457Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716Inc_Loss   0     1     -0.1996      0.0449         19.76        <.0001       0.819Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975Now I used this Model to Score a new set of data. An example row of my new data isCPR - 7OD - 4Living Arrangement - 4Employment Status - 4Insurance - 0Income Loss - 1For this sample row, the model predicted output (Probability of default = 1) as 0.7335 To check this manually, I added the estimatesIntercept + Emp Status 4 + Liv Arran 4 + Insurance 0-0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839Odds ratio = Exponential(-0.0839) = 0.9195Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790I am unable to understand why there is such a mismatch between the Model's predicted probability and theoretical probability. Any help would be much appreciated .Thanks;[education, open-source];140;
404;2;2014-06-16T14:35:20.980;;"Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)It might seem strange, but I would honestly suggest JSON. It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this:{'text': 'I saw the company's manager last day."", {'Person': [{'name': 'John'}, {'indices': [0:1]}, etc...]}The one big advantage you've got over any NLP-specific formats here is that JSON can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.You can also implicitly store tokenization information if you want:{""text"": [""I"", ""saw"", ""the"", ""company's"", ""manager"", ""last"", ""day.""]}EDIT: To clarify the mapping of metadata is pretty open, but here's an example:{'body': '<some_text>', 'metadata':   {'<entity>':    {'<attribute>': '<value>',     'location': [<start_index>, <end_index>]    }  }}Hope that helps, let me know if you've got any more questions.";;;
405;2;2014-06-16T15:00:04.577;;Personally going to make a strong argument in favor of Python here. There are a large number of reasons for this, but I'm going to build on some of the points that other people have mentioned here:Picking a single language: It's definitely possible to mix and match languages, picking d3 for your visualization needs, FORTRAN for your fast matrix multiplies, and python for all of your networking and scripting. You can do this down the line, but keeping your stack as simple as possible is a good move, especially early on.Picking something bigger than you: You never want to be pushing up against the barriers of the language you want to use. This is a huge issue when it comes to languages like Julia and FORTRAN, which simply don't offer the full functionality of languages like python or R.Pick Community: The one most difficult thing to find in any language is community. Python is the clear winner here. If you get stuck, you ask something on SO, and someone will answer in a matter of minutes, which is simply not the case for most other languages. If you're learning something in a vacuum you will simply learn much slower.In terms of the minus points, I might actually push back on them.Deepening your knowledge of one language is a decent idea, but knowing only one language, without having practice generalizing that knowledge to other languages is a good way to shoot yourself in the foot. I have changed my entire favored development stack three time over as many years, moving from MATLAB to Java to haskell to python. Learning to transfer your knowledge to another language is far more valuable than just knowing one.As far as feasibility, this is something you're going to see again and again in any programming career. Turing completeness means you could technically do everything with HTML4 and CSS3, but you want to pick the right tool for the job. If you see the ideal tool and decide to leave it by the roadside you're going to find yourself slowed down wishing you had some of the tools you left behind.A great example of that last point is trying to deploy R code. 'R''s networking capabilities are hugely lacking compared to python, and if you want to deploy a service, or use slightly off-the-beaten path packages, the fact that pip has an order of magnitude more packages than CRAN is a huge help.;;;
406;1;2014-06-16T15:49:55.673;How can I predict traffic based on previous time series data?;If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  I have looked into machine learning algorithms, but I'm not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN (with what I think are sensible parameters and distance function).It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?;[education, open-source];1155;3
407;2;2014-06-16T16:10:34.423;;Other answers recommended a good set of books about the mathematics behind data science. But as you mentioned, its not just mathematics and activities like data collection and inference from data has their own rules and theories, even if not being as rigorous as mathematical backgrounds (yet).For theses parts, I suggest the book Beautiful Data: The Stories Behind Elegant Data Solutions which contains twenty case-study like chapters written by people really engaged with real world data analysis problems. It does not contain any mathematics, but explores areas like collecting data, finding practical ways of using data in analyses, scaling and selecting the best solutions very well.Another really interesting book is Thinking with Data: How to Turn Information into Insights, which is not technical (=programming tutorial) either, but covers important topics on how to really use the data science power in decision making and real world problems.;;;
408;2;2014-06-16T16:34:50.317;;The problem with models like KNN is that they do not take into account seasonality (time-dependent variations in trend).  To take those into account, you should use Time Series analysis.For count data, such as yours, you can use generalized linear auto-regressive moving average models (GLARMA).  Fortunately, there is an R package that implements them (glarma).  The vignette is a good resource for the theory behind the tool.;;;
409;2;2014-06-16T17:26:17.697;;I think Christopher's answers above are entirely sensible.  As an alternate approach (or perhaps just in addition to the advise he's given), I might start by just visualizing the data a bit to try get a rough sense of what's going on.If you haven't already done this, you might try adding a date's month and day of week as features -- if you end up sticking with KNN, this will help the model pick up seasonality. As a different way of taking this on, you might consider starting with a really, really basic model (like OLS).. these often go a long way in generating reasonable predictions.  Finally, the more we know about your data, the easier it will be for us to help generate suggestions -- What time frame are you observing?  What are the features you're currently using?  etc.Hope this helps --;;;
410;1;2014-06-16T18:08:38.623;Choosing a learning rate;I'm currently working on implementing Stochastic Gradient Descent (SGD) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?If so, how do you use this information to inform your decision about a value?If it's not what sort of values should I choose, and how should I choose them?It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?In short: How do I choose the learning rate for SGD?;[education, open-source];1441;5
411;1;2014-06-16T19:14:38.553;Best languages for scientific computing;It seems as though most languages have some number of scientific computing libraries available. Python has ScipyRust has SciRustC++ has several including ViennaCL and ArmadilloJava has Java Numerics and Colt as well as several otherNot to mention languages like R and Julia designed explicitly for scientific computing.With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? Python and R seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. And will anything ever outperform Fortran? Additionally compiled languages tend to have GPU acceleration, while interpreted languages like R and Python don't. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I've missed?;[education, open-source];731;3
412;1;2014-06-16T19:48:31.797;How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?;"MotivationI work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.Anonymizing Confidential DataOne task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as ""Dave"" and in another as ""David,"" commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.What Doesn't WorkFor instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance <= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance <= 1.But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, ""promise you won't flip the paper over?""Another bad solution would be to abbreviate everything. ""Ellen Roberts"" becomes ""ER"" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; ""Benjamin Othello Ames"" and ""Bank of America"" will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:+-----+----+-------------------+-----------+--------+| Row | ID | Name              | WordChars | Origin |+-----+----+-------------------+-----------+--------+| 1   | 17 | ""AMELIA BEDELIA""  | (6, 7)    | Eng    |+-----+----+-------------------+-----------+--------+| 2   | 18 | ""CHRISTOPH BAUER"" | (9, 5)    | Ger    |+-----+----+-------------------+-----------+--------+| 3   | 18 | ""C J BAUER""       | (1, 1, 5) | Ger    |+-----+----+-------------------+-----------+--------+| 4   | 19 | ""FRANZ HELLER""    | (5, 6)    | Ger    |+-----+----+-------------------+-----------+--------+I call this ""inelegant"" because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 & 3, or about the distance between rows 2 & 4 (i.e., how close they are to matching).ConclusionThe goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.I've found a couple papers that might be relevant, but they're a bit over my head:Privacy Preserving String Comparisons Based on Levenshtein DistanceAn Empirical Comparison of Approaches to Approximate String Matching in Private Record Linkage";[education, open-source];420;2
413;2;2014-06-16T19:49:24.377;;Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to PCA, ICA, and Matrix Feature Factorization. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Feature extraction usually involves generating new features which are composites of existing features. Both of these techniques fall into the category of feature engineering. Generally feature engineering is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset, and increasing your signal to noise ratio.;;;
414;2;2014-06-16T19:53:09.957;;Is the learning rate related to the shape of the error gradient, asit dictates the rate of descent?In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.If so, how do you use this information to inform your decision about a value?Adagrad is the most widely known of these and scales a global learning rate η on each dimension based on l2 norm of the history of the error gradient gt on each dimension:Adadelta is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of not having to set a learning rate at all.If it's not what sort of values should I choose, and how should I choose them?Setting learning rates for plain SGD in neural nets is usually aprocess of starting with a sane value such as 0.01 and then doing cross validationto find an optimal value. Typical values range over a few orders ofmagnitude from 0.0001 up to 1.It seems like you would want small values to avoid overshooting, buthow do you choose one such that you don't get stuck in local minimaor take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?Usually the value that's best is near the highest stable learningrate and learning rate decay/annealing (either linear orexponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.;;;
415;2;2014-06-16T20:10:12.167;;This is a pretty massive question, so this is not intended to be a full answer, but hopefully this can help to inform general practice around determining the best tool for the job when it comes to data science. Generally, I have a relatively short list of qualifications I look for when it comes to any tool in this space. In no particular order they are:Performance: Basically boils down to how quickly the language does matrix multiplication, as that is more or less the most important task in data science.Scalability: At least for me personally, this comes down to ease of building a distributed system. This is somewhere where languages like Julia really shine.Community: With any language, you're really looking for an active community that can help you when you get stuck using whichever tool you're using. This is where python pulls very far ahead of most other languages. Flexibility: Nothing is worse than being limited by the language that you use. It doesn't happen very often, but trying to represent graph structures in haskell is a notorious pain, and Julia is filled with a lot of code architectures pains as a result of being such a young language.Ease of Use: If you want to use something in a larger environment, you want to make sure that setup is a straightforward and it can be automated. Nothing is worse than having to set up a finnicky build on half a dozen machines.There are a ton of articles out there about performance and scalability, but in general you're going to be looking at a performance differential of maybe 5-10x between languages, which may or may not matter depending on your specific application. As far as GPU acceleration goes, cudamat is a really seamless way of getting it working with python, and the cuda library in general has made GPU acceleration far more accessible than it used to be.The two primary metrics I use for both community and flexibility are to look at the language's package manager, and the language questions on a site like SO. If there are a large number of high-quality questions and answers, it's a good sign that the community is active. Number of packages and the general activity on those packages can also be a good proxy for this metric.As far as ease of use goes, I am a firm believer that the only way to actually know is to actually set it up yourself. There's a lot of superstition around a lot of Data Science tools, specifically things like databases and distributed computing architecture, but there's no way to really know if something is easy or hard to setup up and deploy without just building it yourself.;;;
416;2;2014-06-16T20:12:39.130;;"If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a salted hash of the string that is used to represent all related records (e.g., David instead of Dave).I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.edit: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that's too revealing, you could bin some of the letters or names; e.g., [A-E] -> 1, [F-J] -> 2, etc. so BOA would become 1OA, or [""Bank"", ""Barry"", ""Bruce"", etc.] -> 1 so Bank of America is again 1OA. For more information see k-anonymity.";;;
417;2;2014-06-16T20:18:50.123;;Blacklists aren't  have value for a number of reasons:They're easy to set up and scale - it's just a key/value store, and you can probably just re-use some of your caching logic for the most basic implementation.Depending on the size and type of the spam attack, there will probably be some very specific terms or URLs being used. It's much faster to throw that term into a blacklist than wait for your model to adapt.You can remove items just as quickly as you added them.Everybody understands how they work and any admin can use them.The key to fighting spam is monitoring. Make sure you have some sort of interface showing which items are on your blacklist, how often they've been hit in the last 10 minutes / hour / day / month, and the ability to easily add and remove items.You'll want to combine a number of different spam detection models and tactics. Neural nets seem to be a good suggestion, and I'd recommend looking at user behavior patterns in addition to just content. Normal humans don't do things like send batches of 1,000 emails every 30 seconds for 12 consecutive hours.;;;
418;1;2014-06-17T00:16:24.287;Best way to classify datasets with mixed types of attributes;I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well.;[education, open-source];143;
419;2;2014-06-17T00:19:09.773;;First you need to decide what you want to do, then look for the right tool for that task.A very general approach is to use R for first versions and to see if your approach is correct. It lacks a little in speed, but has very powerful commands and addon libraries, that you can try almost anything with it:http://www.r-project.org/The second idea is if you want to understand the algorithms behind the libraries, you might wanna take a look at the Numerical Recipies. They are available for different languages and free to use for learning. If you want to use them in commercial products, you need to ourchase a licence:http://en.wikipedia.org/wiki/Numerical_RecipesMost of the time performance will not be the issue but finding the right algorithms and parameters for them, so it is important to have a fast scripting language instead of a monster program that first needs to compile 10 mins before calculating two numbers and putting out the result.And a big plus in using R is that it has built-in functions or libraries for almost any kind of diagram you might wanna need to visualize your data.If you then have a working version, it is almost easy to port it to any other language you think is more performant.;;;
420;2;2014-06-17T00:39:15.990;;It is hard to answer this question without knowing more about the data.  That said, I would offer the following advice:Most machine learning techniques can handle mixed-type data.  Tree based methods (such as AdaBoost and Random Forests) do well with this type of data.  The more important issue is actually the dimensionality, about which you are correct to be concerned.I would suggest that you do something to reduce that dimensionality.  For example, look for the words or phrases that separate the data the best and discard the other words (note: tree based methods do this automatically).;;;
421;1;2014-06-17T04:31:34.067;Online machine learning tutorial;"Does anyone know some good tutorials on online machine learning technics?I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.UPD: Thank you everyone for answers, by ""online"" I mean methods which can be trained in a real-time mode, based on a new inputs one by one.";[education, open-source];470;3
422;1;2014-06-17T05:29:11.830;Publicly available social network datasets/APIs;"As an extension to our great list of publicly available datasets, I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:the name of the social network;what kind of user information it provides (posts, profile, friendship network, ...);whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);whether it simply provides a snapshot of the whole dataset.Any suggestions and further characteristics to be added are very welcome.";[education, open-source];1849;6
423;1;2014-06-17T05:55:04.710;How to measure execution time on distributed system;I'm planning to run experiments with large datasets on distributed system in order to evaluate efficiency gains in comparison with previous proposals.I have limited number of machines nearly ten machines having 200 GB of free space on hard disk on each. On the contrary, I wished to perform experiments on more than available nodes in order to measure scalability, more precisely. Since I don't have any, I thought about using a commodity cluster. However, I'm not sure about the policies of usage, and I need to reliably measure execution times. Are there commodity services which will grant me that only my application would be running at a given time?;[education, open-source];144;
424;1;2014-06-17T06:05:39.653;How to grow a list of related words based on initial keywords?;"I recently saw a cool feature that was once available in Google Sheets: you start by writing a few related keywords in consecutive cells, say: ""blue"", ""green"", ""yellow"", and it automatically generates similar keywords (in this case, other colors). See more examples in this YouTube video.I would like to reproduce this in my own program. I'm thinking of using Freebase, and it would work like this intuitively: Retrieve the list of given words in Freebase;Find their ""common denominator(s)"" and construct a distance metric based on this;Rank other concepts based on their ""distance"" to the original keywords;Display the next closest concepts.As I'm not familiar with this area, my questions are:Is there a better way to do this?What tools are available for each step?";[education, open-source];197;3
425;2;2014-06-17T06:32:22.883;;An example from germany: Xing a site similar to linkedin but limited to german speaking countries.Link to it's developer central: https://dev.xing.com/overviewProvides access to: User profiles, Conversations between users (limited to the user itself), Job advertisings, Contacts and Contacts of Contacts, news from the network and some geolocation api.Yes it has an api, but I did not find information about the rate. But it seems to me, that some information is limited to the consent of the user.;;;
426;2;2014-06-17T06:53:44.007;;There are plenty on youtube and here's a famous one by Andrew Ng from coursea: https://www.coursera.org/course/ml;;;
427;1;2014-06-17T07:46:39.783;does anyone use Julia programming language?;Are anyone using Julia (http://julialang.org/) for professional jobs?Or use it instead of R , Matlab or Mathematica? Is it a good language?If you have to predict next 5-10 years: Do you think it grow up enough to became such a standard in datascience like R or similar?;[education, open-source];357;
428;2;2014-06-17T09:33:37.230;;This critique is no longer justified:While it is true that most of the standard and most respected R libraries were restricted to in-memory computations, there is a growing number of specialized libraries to deal with data that doesn't fit into memory.For instance, for random forests on large datasets, you have the library bigrf. More info here: http://cran.r-project.org/web/packages/bigrf/Another area of growth is R's connectedness to big data environments like hadoop, which opens another world of possibilities.;;;
429;2;2014-06-17T09:56:49.180;;It's not a social network per se, but Stackexchange publish their entire database dump periodically:Stackexchange data dump hosted on the archive.orgPost describing the database dump schemaYou can extract some social information by analyzing which users ask and answer to each other. One nice thing is that since posts are tagged, you can analyze sub-communities easily.;;;
430;1;2014-06-17T10:37:22.987;Looking for example infrastructure stacks/workflows/pipelines;"I'm trying to understand how all the ""big data"" components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can't run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.";[education, open-source];532;10
431;2;2014-06-17T12:37:17.150;;A small collection of such links can be found at here. Many of them are social graphs.;;;
432;2;2014-06-17T12:41:58.900;;There is a very nice library of online machine learning algorithms from a group at NTU, called LIBOL. This would be a very good place to start experimenting with the algorithms.The accompanying user guide, and associated JMLR publication are very nice introductions to the basic algorithms in this field.  Avrim Blum has an older and more technical survey paper on online learning algorithms. ;;;
433;2;2014-06-17T13:31:30.503;;One of the most detailed and clear explanations of setting up a complex analytics pipeline is from the folks over at Twitch.They give detailed motivations of each of the architecture choices for collection, transportation, coordination, processing, storage, and querying their data.Compelling reading! Find it here and here. ;;;
434;2;2014-06-17T13:44:43.807;;There is really no question here as you ask for pure conjectures but consider at least that this week has Julia Con, the first Julia conferenceyou could search GitHub and/or the registered Julia modules;;;
435;1;2014-06-17T13:46:06.367;Relational Data Mining without ILP;I have a huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use Inductive Logic Programming (ILP), but due to special circumstances I can't do that.The other way to tackle this would be just to try to aggregate the values when I have a foreign relation. However, I have thousands of important and distinct rows for some nominal attributes (e.g.: A patient with a relation to several distinct drug prescriptions). So, I just can't do that without creating a new attribute for each distinct row of that nominal attribute, and furthermore most of the new columns would have NULL values if I do that.Is there any non-ILP algorithm that allows me to data mine relational databases without resorting to techniques like pivoting, which would create thousands of new columns?;[education, open-source];144;1
436;2;2014-06-17T14:55:47.103;;I personally have used Julia for a good number of professional projects, and while, as Dirk mentioned, this is purely conjecture, I can give some insights on where Julia really stands out. The question of whether or not these reasons will prove enough to have Julia succeed as a language is anyone's guess.Distributed Systems: Julia is the easiest language I've ever dealt with in terms of building distributed systems. This is becoming more and more relevant in computing, and will potentially become a deciding factor, but the question of whether or not Julia'a relative ease decides this is up for debateJIT Performance: Julia's JIT compiler is extremely fast, and while there is a lot of debate as to how accurate these benchmark numbers are, the Julia Website shows a series of relevant benchmarksCommunity: This is an area where Julia just isn't quite there. The community that is there is generally supportive, but not quite as knowledgable as the R or python communities, which is a definite minus.Extensibility: This is another place where Julia is currently lacking, there is a large disconnect between the implies code patterns that Julia steers you toward and what it can actually support. The type system is currently overly bulky and difficult to use effectively.Again, can't say what this means for the future, but these are just a couple of relevant points when it comes to evaluating Julia in my opinion.;;;
437;1;2014-06-17T18:13:46.230;What are good sources to learn about Bootstrap?;I think that Bootstrap can be useful in my work, where we have a lot a variables that we don't know the distribution of it. So, simulations could help.What are good sources to learn about Bootstrap/other useful simulation methods?;[education, open-source];107;
439;2;2014-06-17T18:42:55.423;;Halfway through reading your question, I realized Levenshtein Distance could be a nice solution to your problem.  Its good to see that you have a link to a paper on the topic, let me see if I can shed some light into what a Levenshtein solution would look like.Levenshtein distance is used across many industries for entity resolution, what makes it useful is that it is a measure of the difference between two sequences.  In the case of string comparison it is just sequences characters.This could help solve your problem by allowing you to provide one number that gives a measure of how similar the text of another field is.  Here is an example of a basic way of using Levenshtein with the data you gave:This provides an ok solution, the distance of 8 provides some indication of a relationship, and it is very PII compliant.  However, it is still not super useful, let see what happens if we do some text magic to take only the first initial of the first name and the full last name dropping anything in the middle:As you can see the Levenshtein distance of 0 is pretty indicative of a relationship.  Commonly data providers will combine a bunch of Levenshtein permutations of the first and last names with 1, 2, or all of the characters just to give some dimensionality as to how entities are related while still maintaining anonymity within the data.;;;
440;2;2014-06-17T18:59:14.947;;"The word2vec algorithm may be a good way to retrieve more elements for a list of similar words. It is an unsupervised ""deep learning"" algorithm that has previously been demonstrated with Wikipedia-based training data (helper scripts are provided on the Google code page).There are currently C and Python implementations. This tutorial by Radim Řehůřek, the author of the Gensim topic modelling library, is an excellent place to start.The ""single topic"" demonstration on the tutorial is a good example of retreiving similar words to a single term (try searching on 'red' or 'yellow'). It should be possible to extend this technique to find the words that have the greatest overall similarity to a set of input words.";;;
441;1;2014-06-17T20:48:35.267;What are the use cases for Apache Spark vs Hadoop;With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I've read through the introduction documentation for Spark, but I'm curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop.;[education, open-source];6635;12
442;2;2014-06-17T21:17:31.210;;Christopher's answers seem very reasonable. In particular tree based methods do well with this sort of data because they branch on discriminative features. It's a little hard to say without knowing your specific application, but in general if you think that some of your features might be significantly more discriminative than others, you could try some dimensionality reduction techniques to clean this up a bit.Also if you use a dimensionality reduction technique you end up getting a slightly more robust format for your feature vector (they generally end up being straight numerical vectors instead of mixed data types), which might let you leverage different methods. You could also look into hand engineering features. With properly hand engineered features Random Forest will get you very close to state of the art on most tasks.;;;
443;2;2014-06-17T22:28:36.070;;"One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you're using) as an additional dataset.E.g.:Generate a set of unique names in the datasetFor each name, calculate edit distance to each other nameGenerate an ID or irreversable hash for each nameReplace names in the original dataset with this IDProvide matrix of edit distances between ID numbers as new datasetThough there's still a lot that could be done to deanonymise the data from these even. E.g. if ""Tim"" is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to ""Tom"" or ""Jim"" (when combined with other info).";;;
444;2;2014-06-17T22:29:36.720;;"A classic book is by B. Efron who created the technique:Bradley Efron; Robert Tibshirani (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC. ISBN 978-0-412-04231-7.";;;
445;2;2014-06-17T23:30:45.897;;"Is your Masters in Computer Science?  Statistics?Is 'data science' going to be at the center of your thesis?  Or a side topic?I'll assume your in Statistics and that you want to focus your thesis on a 'data science' problem.  If so, then I'm going to go against the grain and suggest that you should not start with a data set or an ML method.  Instead, you should seek an interesting research problem that's poorly understood or where ML methods have not yet been proven successful, or where there are many competing ML methods but none seem better than others.Consider this data source: Stanford Large Network Dataset Collection.  While you could pick one of these data sets, make up a problem statement, and then run some list of ML methods, that approach really doesn't tell you very much about what data science is all about, and in my opinion doesn't lead to a very good Masters thesis.Instead, you might do this: look for all the research papers that use ML on some specific category -- e.g. Collaboration networks (a.k.a. co-authorship). As you read each paper, try to find out what they were able to accomplish with each ML method and what they weren't able to address.  Especially look for their suggestions for ""future research"".Maybe they all use the same method, but never tried competing ML methods.  Or maybe they don't adequately validate their results, or maybe there data sets are small, or maybe their research questions and hypothesis were simplistic or limited.Most important: try to find out where this line of research is going.  Why are they even bothering to do this?  What is significant about it?  Where and why are they encountering difficulties?";;;
446;2;2014-06-17T23:38:20.133;;"One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in ""Privacy-preserving record linkage using Bloom filters"" (doi:10.1186/1472-6947-9-41): A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.The article goes into detail about the method, which I will summarize here to the best of my ability.A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9. The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, ""hello"" might yield the following set of 2-grams:[""_h"", ""he"", ""el"", ""ll"", ""lo"", ""o_""]Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters A, B by means of their Dice coefficient: DA, B = 2h / (a + b)Where h is the number of bits that are set to 1 in both filters, a is the number of bits set to 1 in only filter A, and b is the number of bits set to 1 in only filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to 0.Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a probability that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.I found this tutorial to be very helpful for understanding the Bloom filter.There is some flexibility in the implementation of this method; see also this 2010 paper (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters.";;;
447;1;2014-06-18T03:19:07.557;Learning signal encoding;I have a large number of samples which represent Manchester encoded bit streams as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background.I have manually decoded these streams, but I was wondering if I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently.Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? Independent Component Analysis (ICA) seems like could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?;[education, open-source];107;
448;2;2014-06-18T05:35:35.700;;Not sure about the YARN, but I think that Spark makes a real difference compared to Hadoop (advertised as 100 times faster) if data can fit nicely in the memory of the computational nodes. Simply because it avoids hard disk access. If data doesn't fit memory there's still some gain because of buffering.;;;
449;2;2014-06-18T07:27:22.677;;"Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs MapReduce? Because Spark runs on/with Hadoop, which is rather the point.The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be ""100x"" faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it's not in general faster.Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark). Crunch and Cascading already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice.Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It's nice to have it in one package, albeit not yet 'baked'.Why would you not use Spark? paraphrasing myself:Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developersThere is more MapReduce expertise out there now than SparkFor the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalentSpark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don't think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune";;;
450;2;2014-06-18T14:15:37.957;;"I'm not quite sure, but maybe  locality-sensitive hashing is a good solution. It does hashing of input data (in your case - names), so original strings would be preserved. On the other side, the main idea of LSH is to maximize hashes likelihood for similar items. There are a lot of different LSH-implementations. I tried Nilsimsa-hash for comparing tweet texts, and it worked quite well. But I'm not sure, how well it will work in case of short strings (names) - this issue require testing. I tried your examples, and here is the result (name A, name B, ""distance"" - maximum is 120):1. AMELIA BEDELIA  - CHRISTOPH BAUER - 1072. AMELIA BEDELIA  - C J BAUER       - 823. AMELIA BEDELIA  - FRANZ HELLER    - 914. CHRISTOPH BAUER - C J BAUER       - 815. CHRISTOPH BAUER - FRANZ HELLER    - 986. C J BAUER       - FRANZ HELLER    - 83As you see, CHRISTOPH BAUER and C J BAUER turned up to be the closest pair. But difference is not significant.And just for example - hash representation of these names:AMELIA BEDELIA  6b208299602b5000c3005a048122a43a828020889042240005011c1880864502CHRISTOPH BAUER 22226448000ab10102e2860b52062487ff0000928e0822ee106028016cc01237C J BAUER       2282204100961060048050004400240006032400148000802000a80130402002FRANZ HELLER    58002002400880080b49172044020008030002442631e004009195020ad01158";;;
451;1;2014-06-18T14:36:07.980;How to speedup message passing between computing nodes;I'm developing a distributed application, and as it's been designed, there'll be a great load of communication during the processing. Since the communication is already as much spread along the entire process as possible, I'm wondering if there any standard solutions to improve the performance of the message passing layer of my application.What changes/improvements could I apply to my code to reduce the time spent sending messages? For what it's worth, I'm communicating up to 10GB between 9 computing nodes, and the framework I'm using is implemented with OpenMPI.;[education, open-source];280;
452;2;2014-06-18T15:10:22.637;;"As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional ""machine learning approach"", something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic.Your feature matrix would be something like:t1 | t2 | ... | tNt2 | t3 | ... | tN+1t3 | t4 | ... | tN+2...tW | tW+1 | ... |tN+Wwhere tI is the traffic on day I. The feature you'll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day's traffic. Any sort of ML model would work for this. EditIn response to the question, ""can you elaborate on how you use this feature matrix"":The feature matrix has values indicating past traffic over a period of time (for instance, hourly traffic over 1 week), and we use this to predict traffic for some specified time period in the future. We take our historic data and build a feature matrix of historic traffic and label this with the traffic at some period in the future (e.g. 2 days after the window in the feature). Using some sort of regression machine learning model, we can take historic traffic data, and try and build a model that can predict how traffic moved in our historic data set. The presumption is that future traffic will resemble past traffic.";;;
453;1;2014-06-18T15:27:23.313;What is the difference between global and universal compression methods?;"I understand that compression methods may be split into two main sets: global and local. The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing over any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting informations that usually improve the compression rate.While reading about some of these methods, I noticed that the unary method is not universal, which surprised me since I thought ""globality"" and ""universality"" referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?What is the difference between universal and global methods? Aren't these classifications synonyms?";[education, open-source];59;
454;1;2014-06-18T15:48:19.497;What are the implications for training a Tree Ensemble with highly biased datasets?;I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it's difficult to create training datasets that contain enough examples of the positive class.What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?;[education, open-source];234;4
455;1;2014-06-18T16:21:12.203;Suggest text classifier training datasets;Which freely available datasets can I use to train a text classifier?We are trying to enhance our users engagement by recommending the most related content for him, so we thought If we classified our content based on a predefined bag of words we can recommend to him engaging content by getting his feedback on random number of posts already classified before.We can use this info to recommend for him pulses labeled with those classes. But we found If we used a predefined bag of words not related to our content the feature vector will be full of zeros, also categories may be not relevant to our content. so for those reasons we tried another solution that will be clustering our content not classifying it.Thanks :);[education, open-source];243;2
456;2;2014-06-18T17:22:46.207;;"If you expect (or find) that nodes are requesting the same data more than once, perhaps you could benefit from a caching strategy? Especially where some data is used much more often than others, so you can target only the most frequently-used information.If the data is mutable, you also need a way to confirm that it hasn't changed since the last request that's less expensive than repeating the request.This is further complicated if each node has its own separate cache. Depending on the nature of your system and task(s), you could consider adding a node dedicated to serving information between the processing nodes, and building a single cache on that node.For an example of when that might be a good idea, let's suppose I retrieve some data from a remote data store over a low-bandwidth connection, and I have some task(s) requiring that data, which are distributed exclusively among local nodes. I definitely wouldn't want each node requesting information separately over that low-bandwidth connection, which another node might have previously requested. Since my local I/O is much less expensive than my I/O over the low-bandwidth connection, I might add a node between the processing nodes and the remote source that acts as an intermediate server. This node would take requests from the processing nodes, communicate with the remote data store, and cache frequently-requested data to minimize the use of that low-bandwidth connection.The core concepts here that may be applicable to your specific case are:Eliminate or reduce redundant I/O;Take advantage of trade-offs between memory use and computation time;Not all I/O is created equal.";;;
457;2;2014-06-18T18:22:11.680;;Firstly, I would generally agree with everything that AirThomas suggested. Caching things is generally good if you can, but I find it slightly brittle since that's very dependent on exactly what your application is. Data compression is another very solid suggestion, but my impression on both of these is that the speedups you're looking at are going to be relatively marginal. Maybe as high as 2-5x, but I would be very surprised if they were any faster than that.Under the assumption that pure I/O (writing to/reading from memory) is not your limiting factor (if it is, you're probably not going to get a lot faster), I would make a strong plug for zeromq. In the words of the creators: We took a normal TCP socket, injected it with a mix of radioactive  isotopes stolen from a secret Soviet atomic research project,  bombarded it with 1950-era cosmic rays, and put it into the hands of a  drug-addled comic book author with a badly-disguised fetish for  bulging muscles clad in spandex. Yes, ØMQ sockets are the world-saving  superheroes of the networking world.While that may be a little dramatic, zeromq sockets in my opinion are one of the most amazing pieces of software that the world of computer networks has put together in several years. I'm not sure what you're using for your message-passing layer right now, but if you're using something traditional like rabbitmq, you're liable to see speedups of multiple orders of magnitude (personally noticed about 500x, but depends a lot of architecture)Check out some basic benchmarks here.;;;
458;1;2014-06-18T19:48:54.883;K-means vs. online K-means;K-means is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?;[education, open-source];140;1
459;2;2014-06-18T20:07:05.017;;Online k-means (more commonly known as sequential k-means) and traditional k-means are very similar.  The difference is that online k-means allows you to update the model as new data is received.Online k-means should be used when you expect the data to be received one by one (or maybe in chunks).  This allows you to update your model as you get more information about it.  The drawback of this method is that it is dependent on the order in which the data is received (ref).  ;;;
460;2;2014-06-18T21:56:29.147;;A fast, easy an often effective way to approach this imbalance would be to randomly subsample the bigger class (which in your case is the negative class), run the classification N number of times with members from the two classes (one full and the other subsampled) and report the average metric values, the average being computed over N (say 1000) iterations.A more methodical approach would be to execute the Mapping Convergence (MC) algorithm, which involves identifying a subset of strong negative samples with the help of a one-class classifier, such as OSVM or SVDD, and then iteratively execute binary classification on the set of strong negative and positive samples. More details of the MC algorithm can be found in this paper. ;;;
461;1;2014-06-18T22:10:58.497;Preference Matching Algorithm;"There's this side project I'm working on where I need to structure a solution to the following problem.I have two groups of people (clients). Group A intends to buy and group B intends to sell a determined product X. The product has a series of attributes x_i, and my objective is to facilitate the transaction between A and B by matching their preferences. The main idea is to point out to each member of A a corresponding in B whose product better suits his needs, and vice versa. Some complicating aspects of the problem:The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);Any suggestion on how to approach this problem and solve it in an automated way? The idea is to really think out of the box here, so feel free to ""go wild"" on your suggestions.I would also appreciate some references to other similar problems if possible. Great suggestions! Many similarities in to the way I’m thinking of approaching the problem.The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Let’s take an example of a car. The product “car” has lots and lots of attributes that range from its performance, mechanical structure, price etc.Suppose I just want a cheap car, or an electric car. Ok, that’s easy to map because they represent main features of this product. But let’s say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn’t ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them.  Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn’t work. But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I’m thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.Next step would be a “refined search”. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot “guess” what the buyer needs.  But might get me close.  The weighting criteria suggested is great.  It allows me to quantify the level to which the seller matches the buyer’s needs.  The scaling part might be a problem though, because the importance of each attribute varies from client to client. I’m thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute. I’m not a native English speaker so hope I’m being able to express my ideas properly. ";[education, open-source];230;1
462;2;2014-06-18T22:27:06.503;;I would recommend training on more balanced subsets of your data. Training random forest on sets of randomly selected positive example with a similar number of negative samples. In particular if the discriminative features exhibit a lot of variance this will be fairly effective and avoid over-fitting. However in stratification it is important to find balance as over-fitting can become a problem regardless. I would suggest seeing how the model does with the whole data set then progressively increasing the ratio of positive to negative samples approaching an even ratio, and selecting for the one that maximizes your performance metric on some representative hold out data. This paper seems fairly relevant http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf it talks about a weighted Random Forest which more heavily penalizes misclassification of the minority class.  ;;;
463;2;2014-06-18T22:45:25.677;;My first suggestion would be to somehow map the non-quantifiable attributes to quantities with the help of suitable mapping functions. Otherwise, simply leave them out.Secondly, I don't think that you need to assume that the list of attributes is not finite. A standard and intuitive approach is to represent each attribute as an individual dimension in a vector space. Each product is then simply a point in this space. In that case, if you want to dynamically add more attributes you simply have to remap the product vectors into the new feature space (with additional dimensions).    With this representation, a seller is a point in the feature space with product attributes and a buyer is a point in the same feature space with the preference attributes. The task is then to find out the most similar buyer point for a given seller point.If your dataset (i.e. the number of buyers/sellers) is not very large, you can solve this with a nearest neighbour approach implemented with the help of k-d trees.For very large sized data, you can take an IR approach. Index the set of sellers (i.e. the product attributes) by treating each attribute as a separate term with the term-weight being set to the attribute value. A query in this case is a buyer which is also encoded in the term space as a query vector with appropriate term weights. The retrieval step would return you a list of top K most similar matches.;;;
464;2;2014-06-18T22:48:53.350;;Some standard datasets for text classification are the 20-News group, Reuters (with 8 and 52 classes) and WebKb. You can find all of them here. ;;;
465;2;2014-06-18T22:58:35.260;;nDCG is used to evaluate a golden ranked list (typically human judged) against your output ranked list. The more is the correlation between the two ranked lists, i.e. the more similar are the ranks of the relevant items in the two lists, the closer is the value of nDCG to 1.RMSE (Root Mean Squared Error) is typically used to evaluate regression problems where the output (a predicted scalar value) is compared with the true scalar value output for a given data point.So, if you are simply recommending a score (such as recommending a movie rating), then use RMSE. Whereas, if you are recommending a list of items (such as a list of related movies), then use nDCG.  ;;;
466;1;2014-06-18T23:02:54.200;Gas consumption outliers detection - Neural network project. Bad results;"I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I don't find the reason. I am not an expert so I would like to ask you what I can improve and what I'm doing wrong. This is the complete description: https://github.com/denadai2/Gas-consumption-outliers.The neural network is a FeedFoward Network with Back Propagation. As described here I splitted the dataset in a ""small"" dataset of 41'000 rows, 9 features and I tried to add more features. I trained the networks but the results have 14.14 RMSE, so it can't predict so well the gas consumptions, consecutevely I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.What can I improve? What am I doing wrong? Can you have a look of my description?";[education, open-source];175;
468;1;2014-06-19T03:43:23.853;Learning ordinal regression in R?;I'm working on a project and need resources to get me up to speed.The dataset is around 35000 observations on 30 or so variables.  About half the variables are categorical with some having many different possible values, i.e. if you split the categorical variables into dummy variables you would have a lot more than 30 variables.  But still probably on the order of a couple of hundred max.  (n>p).  The response we want to predict is ordinal with 5 levels (1,2,3,4,5).  Predictors are a mix of continuous and categorical, about half of each.  These are my thoughts/plans so far:1.  Treat the response as continuous and run vanilla linear regression.2.  Run nominal and ordinal logistic and probit regression3.  Use MARS and/or another flavor of non-linear regressionI'm familiar with linear regression.  MARS is well enough described by Hastie and Tibshirani.  But I'm at a loss when it comes to ordinal logit/probit, especially with so many variables and a big data set.  The r package glmnetcr seems to be my best bet so far, but the documentation hardly suffices to get me where I need to be.Where can I go to learn more?;[education, open-source];190;2
469;2;2014-06-19T05:52:57.527;;I suggest this tutorial on ordered logit: http://www.ats.ucla.edu/stat/r/dae/ologit.htmIt showcases the use of polr in the MASS package, and also explains the assumptions and how to interpret the results.;;;
470;2;2014-06-19T06:09:43.963;;Just an idea - your data is highly seasonal: daily and weekly cycles are quite perceptible. So first of all, try to decompose your variables (gas and electricity consumption, temperature, and solar radiation). Here is a nice tutorial on time series decomposition for R. After obtaining trend and seasonal components, the most interesting part begins. It's just an assumption, but I think, gas and electricity consumption variables would be quite predictable by means of time series analysis (e.g., ARIMA model). From my point of view, the most exiting part here is to try to predict residuals after decomposition, using available data (temperature anomalies, solar radiation, wind speed). I suppose, these residuals would be outliers, you are looking for. Hope, you will find this useful.;;;
471;2;2014-06-19T07:22:38.987;;One of the most widely used test collection for text categorization research (link below). I've used many times. Enjoy your exploration :)http://www.daviddlewis.com/resources/testcollections/reuters21578/orhttp://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection;;;
473;1;2014-06-19T08:56:46.847;Is logistic regression actually a regression algorithm?;The usual definition of regression (as far as I am aware) is predicting a continuous output variable from a given set of input variables. Logistic regression is a binary classification algorithm, so it produces a categorical output.Is it really a regression algorithm? If so, why?;[education, open-source];526;
474;1;2014-06-19T09:42:28.160;Network structure: k-cliques vs. p-cliques;In network structure, what is the difference between k-cliques and p-cliques, can anyone give a brief explaination with examples? Thanks in advanced!============================EDIT:I found an online ppt while I am googling, please take a look on p.37 and p.39, can you comment on them?;[education, open-source];74;
475;2;2014-06-19T09:50:53.657;;"As you discuss the definition of regression is predicting a continuous variable. Logistic regression is a binary classifier.  Logistic regression is the application of a logit function on the output of a usual regression approach. Logit function turns (-inf,+inf) to [0,1]. I think it is just for historical reasons that keeps that name. Saying something like ""I did some regression to classify images. In particular I used logistic regression."" is wrong.";;;
477;1;2014-06-18T22:15:43.820;Preference Matching Algorithm;"There's this side project I'm working on where I need to structure a solution to the following problem.I have two groups of people (clients). Group ""A"" intends to buy and group ""B"" intends to sell a determined product ""X"". The product has a series of attributes x_i and my objective is to facilitate the transaction between ""A"" e ""B"" by matching their preferences. The main idea is to point out to each member of ""A"" a corresponding in ""B"" who’s product better suits his needs, and vice versa. Some complicating aspects of the problem:The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).Any suggestion on how to approach this problem and solve it in an automated way?The idea is to really think out of the box here so feel free to ""go wild"" on your suggestions. I would also appreciate some references to other similar problems if possible. ";[education, open-source];55;
478;2;2014-06-19T03:20:00.847;;This can be a cross between machine learning and simple matching exercise.I think X_i tend to be rather defined and finite, while A_i can be vague and not finite. From a pure algorithm perspective I would search for instances where X_i = A_i and store the results into a container of sort. The more hits for certain X'es where X_i_n = A_i_k the more points X scores. X'es are then presented to A in the order of points from best match to lowest match.Onto the machine learning mechanism, as the algorithm serves a lot of As (by that mean thousands and thousands, even millions) patterns will start to develop and certain combination of A_i's will be more prevalent, or in other words, worth more to other A_i's for a certain category of A. Using these patterns, the weighting of points will be re-balanced for higher chance of hitting the correct offers.Kind of like how a search engine works.;;;
480;2;2014-06-19T10:35:37.190;;"One fairly powerful R package for regression with an ordinal categorical response is VGAM, on the CRAN. The vignette contains some examples of ordinal regression, but admittedly I have never tried it on such a large dataset, so I cannot estimate how long it may take. You may find some additional material about VGAM on the author's page. Alternatively you could take a look at Laura Thompson's companion to Agresti's book ""Categorical Data Analysis"". Chapter 7 of Thompson's book describes cumulative logit models, which are frequently used with ordinal responses.Hope this helps!";;;
481;2;2014-06-19T12:22:42.017;;In graph theory a clique indicates a fully connected set of nodes: as noted here, a p-clique simply indicates a clique comoprised of p nodes. A k-clique is an undirected graph and a number k, and the output is a clique of size k if one exists.Clique Problem;;;
482;2;2014-06-19T13:23:53.387;;Short AnswerYes, logistic regression is a regression algorithm and it does predict a continuous outcome: the probability of an event.  That we use it as a binary classifier is due to the interpretation of the outcome.DetailLogistic regression is a type of generalize linear regression model.In an ordinary linear regression model, a continuous outcome, y, is modeled as the sum of the product of predictors and their effect:y = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + ewhere e is the error.Generalized linear models do not model y directly.  Instead, they use transformations to expand the domain of y to all real numbers.  This transformation is called the link function.  For logistic regression the link function is the logit function (usually, see note below).The logit function is defined as ln(y/(1 + y))Thus the form of logistic regression is:ln(y/(1 + y)) = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + ewhere y is the probability of an event.The fact that we use it as a binary classifier is due to the interpretation of the outcome.Note: probit is another link function used for logistic regression but logit is the most widely used.;;;
483;2;2014-06-19T14:15:07.433;;You could try Neural Network. You can find 2 great explanations on how to apply NN on time series here and here.Note that it is best practice to :Deseasonalize/detrend the input data (so that the NN will not learn the seasonality).Rescale/Normalize the input data.Because what you are looking for is a regression problem, the activation functions should be linear and not sigmoid or tanh and you aim to minimize the sum-of-squares error (as opposition to the maximization of the negative log-likelihood in a classification problem).;;;
484;2;2014-06-19T14:52:59.877;;"Logistic regression is regression, first and foremost. It becomes a classifier by adding a decision rule. I will give an example that goes backwards. That is, instead of taking data and fitting a model, I'm going to start with the model in order to show how this is truly a regression problem.In logistic regression, we are modeling the log odds, or logit, that an event occurs, which is a continuous quantity. If the probability that event A occurs is P(A), the odds are:P(A) / (1 - P(A)) The log odds, then, are:log { (P(A) / (1 - P(A))) }As in linear regression, we model this with a linear combination of coefficients and predictors:logit = b0 + b1 * x1 + b2 * x2 ...Imagine we are given a model of whether a person has gray hair. Our model uses age as the only predictor. Here, our event A = a person has gray hair:log odds of gray hair = -10 + 0.25 * age...Regression! Here is some Python code and a plot:%matplotlib inlineimport matplotlib.pyplot as pltimport numpy as npimport seaborn as snsx = np.linspace(0, 100, 100)def log_odds(x):    return -10 + .25 * xplt.plot(x, log_odds(x))plt.xlabel(""age"")plt.ylabel(""log odds of gray hair"")Now, let's make it a classifier. First, we need to transform the log odds to get out our probability P(A). We can use the sigmoid function:P(A) = 1 / (1 + exp(-log odds))Here's the code:plt.plot(x, 1 / (1 + np.exp(-log_odds(x))))plt.xlabel(""age"")plt.ylabel(""probability of gray hair"")The last thing we need to make this a classifier is to add a decision rule. One very common rule is to classify a success whenever P(A) > 0.5. We will adopt that rule, which implies that our classifier will predict gray hair whenever a person is older than 40 and will predict non-gray hair whenever a person is under 40.Logistic regression works great as a classifier in more realistic examples too, but before it can be a classifier, it must be a regression technique!";;;
485;2;2014-06-19T16:07:29.907;;"My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science. Data Mining is mostly considered on yielding questions rather than answering them. It is often termed as ""detecting something new"", when compared to Data science, where the data scientist try to solve complex problems to be able to reach their end results. However both terms have many commonalities between them.For example..if u have an agricultural land where u aim to find the affected plants..Here spatial data mining plays a key role in doing this job.There are good chances that you may end  up with not only finding out the affected plants in the land but also the extent to which they are affected.......this is something that is not possible with data science.";;;
487;2;2014-06-19T18:01:36.120;;A good list of publicly available social network datasets can be found on the Stanford Network Analysis Project website: SNAP datasetsThe site contains internet social network data (Facebook, Twitter, Google Plus), Citation networks for academic journals, co-purchasing networks from Amazon and several others kinds of networks. They have directed, undirected, and bipartite graphs and all datasets are snapshots that can be downloaded in compressed form.;;;
488;1;2014-06-19T18:02:24.650;Is GLM a statistical or machine learning model?;I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.;[education, open-source];758;1
489;2;2014-06-19T18:05:51.070;;A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal.;;;
491;2;2014-06-19T18:24:32.023;;"As suggested, going wild. First of all, correct me if I’m wrong:Just a few features exist for each unique product;There is no ultimate features list, and clients are able to add new features to their products.If so, constructing full product-feature table could be computational expensive. And final data table would be extremely sparse.The first step is narrowing customers (products) list for matching. Let’s build a bipartite graph, where sellers would be type-1 nodes, and buyers would be type-2 nodes. Create an edge between any seller and buyer every time they reference a similar product feature, as in the following sketch:Using the above graph, for every unique seller’s product you can select only buyers who are interested in features that match the product (it’s possible to filter at least one common feature, match the full set of features, or set a threshold level). But certainly, that’s not enough. The next step is to compare feature values, as described by the seller and buyer. There are a lot of variants (e.g., k-Nearest-Neighbors). But why not try to solve this question using the existing graph? Let’s add weights to the edges:for continuous features (e.g., price): for binary and non-quantifiable features - just logical biconditional:The main idea here is to scale every feature to the interval [0, 1]. Additionally, we can use feature coefficients to determine most important features. E.g., assuming price is twice as important as availability of some rare function: One of the final steps is simplifying the graph structure and reducing many edges to one edge with weight equal to the sum of the previously calculated weights of each feature. With such a reduced structure every pair of customers/products could have only one edge (no parallel edges). So, to find the best deal for exact seller you just need to select connected buyers with max weighted edges.Future challenge: introduce a cheap method for weighting edges on first step :)";;;
492;1;2014-06-19T19:29:57.797;Word2Vec for Named Entity Recognition;I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?;[education, open-source];1082;
493;2;2014-06-19T19:33:12.067;;Well, firstly, I would not even use things like Machine learning without having in depth knowledge. Simplistic things I would do if I had this time series is:Write sql queries to understand which of the times you have the busiest, average and low foot traffic.Then try to visualize the whole time series, and you could use basic pattern matching algorithms to pick up patterns.This two things will help you understand what your data set is telling you. Then, with that in hand, you will probably be in a better state to use machine learning algorithms.Also, I'm currently working in building something on time series, and using time series analysis will help you much more than machine learning. For example, there are pattern recognition algorithms that you can use that uses every day data to show patterns, and ones which use up to as much as 3 to 6 months of data to catch a pattern.;;;
494;1;2014-06-19T19:33:40.620;Techniques for trend extraction from unbalanced panel data;"My data set is formatted like this:User-id | Threat_scoreaaa       45bbb       32ccc       20The list contains the top 100 users with the highest threat scores. I generate such a list monthly and store each month's list in its own file.There are three things I would like to get from this data:1. Users who are consistently showing up in this list2. Users who are consistently showing up in this list with high threat scores3. Users whose threat scores are increasing very quicklyI am thinking a visual summary would be nice; each month (somehow) decide which users I want to plot on a graph of historic threat scores.Are there any known visualization techniques that deal with similar requirements?How should I be transforming my current data to achieve what I am looking for?";[education, open-source];117;1
495;2;2014-06-19T22:02:44.050;;I would add a third column called month and then concatenate each list. So if you have a top 100 list for 5 months you will create one big table with 500 entries:User-id | Threat_score | monthaaa       45             1bbb       32             1ccc       20             1...       ...            ...bbb       64             2ccc       29             2...       ...            ...Then, to answer your first question, you could simply count the occurrences of each user-id. For example, if user bbb is in your concatenated table five times, then you know that person made your list all five months.To answer you second question, you could do a group by operation to compute some aggregate function of the users. A group by operation with an average function is a little crude and sensitive to outliers, but it would probably get you close to what you are looking for.One possibility for the third question is to compute the difference in threat score between month n-1 and month n. That is, for each month (not including the first month) you subtract the user's previous threat score from the current threat score. You can make this a new column so your table would now look like:User-id | Threat_score | month | differenceaaa       45             1       nullbbb       32             1       nullccc       20             1       null...       ...            ...     ...bbb       64             2       32ccc       29             2       9...       ...            ...With this table, you could again do a group by operation to find people who consistently have a higher threat score than the previous month or you could simply find people with a large difference between the current month and the previous month.As you suggest, visualizing this data is a really good idea. If you care about these threat scores over time (which I think you do), I strongly recommend a simple line chart, with month on the x-axis and threat score on the y-axis. It's not fancy, but it's extremely easy to interpret and should give you useful information about the trends.Most of this stuff (not the visualization) can be done in SQL and all of it can be done in R or Python (and many other languages). Good luck!;;;
496;2;2014-06-20T03:18:07.653;;You have three questions to answer and 100 records per month to analyze.Based on this size, I'd recommend doing analysis in a simple SQL database or a spreadsheet to start off with.  The first two questions are fairly easy to figure out.  The third is a little more difficult.I'd definitely add a column for month and group all of that data together into a spreadsheet or database table given the questions you want to answer.question 1. Users who are consistently showing up in this listIn excel, this answer should help you out:  http://superuser.com/questions/442653/ms-excel-how-count-occurence-of-item-in-a-listFor a SQL database:  http://stackoverflow.com/questions/2516546/select-count-duplicatesquestion 2. Users who are consistently showing up in this list with high risk score This is just adding a little complexity to the above.  For SQL, you would further qualify your query based on a minimum risk score value.In excel, a straight pivot isn't going to work, you'll have to copy the unique values in one column to another, then drag a CountIf function adjacent to each unique value, qualifying the countif function with a minimum risk score.question 3. Users who have/reaching the high risk level very fast.A fast rise in risk level could be defined as the difference between two months being larger than a given value.For each user record you want to know the previous month's threat value, or assume zero as the previous threat value.If that difference is greater than your risk threshold, you want to include it in your report.  If not, they can be filtered from the list.  If I had to do this month after month, I would spend the two hours it might take to automate a report after the first couple of months.  I'd throw all the data in a SQL database and write a quick script in perl or java to iterate through the 100 records, do the calculation, and output the users who crossed the threshold.If I needed it to look pretty, I'd use a reporting tool.  I'm not particularly partial to any of them.If I needed to trend threshold values over time, I'd output the results for all people into a second table add records to that table each month.If I just needed to do it once or twice, figuring out how to do it in excel by adding a new column using VLookUp and some basic math and a filter would probably be the fastest and easiest way to get it done.  I tend to avoid using excel for things I'll need to use with consistency because there are limits that you run into when your data gets sizeable.;;;
497;1;2014-06-20T03:18:59.477;What statistical model should I use to analyze the likelihood that a single event influenced longitudinal data;I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.Here is an example scenario:Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business's daily walk in client's following a specific event?I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:I am attempting to determine the impact that a particular marketing agency has on their client's website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!;[education, open-source];250;4
498;2;2014-06-20T04:09:04.230;;"Have you considered a frequency-based approach exploiting simple word co-occurence in corpora? At least, that's what I've seen most folks use for this. I think it might be covered briefly in Manning and Schütze's book, and I seem to remember something like this as a homework assignment back in grad school...More background here: http://nlp.stanford.edu/IR-book/html/htmledition/automatic-thesaurus-generation-1.htmlFor this step: Rank other concepts based on their ""distance"" to the original keywords;There are several semantic similarity metrics you could look into. Here's a link to some slides I put together for a class project using a few of these similarity metrics in WordNet: http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/greenbacker-WordNet-Similarity.pdf";;;
499;2;2014-06-20T04:27:02.467;;Anecdotally, I've never been impressed with the output from hierarchical LDA. It just doesn't seem to find an optimal level of granularity for choosing the number of topics. I've gotten much better results by running a few iterations of regular LDA, manually inspecting the topics it produced, deciding whether to increase or decrease the number of topics, and continue iterating until I get the granularity I'm looking for.Remember: hierarchical LDA can't read your mind... it doesn't know what you actually intend to use the topic modeling for. Just like with k-means clustering, you should choose the k that makes the most sense for your use case.;;;
500;2;2014-06-20T06:49:25.237;;In addition to Ben's answer, the subtle distinction between statistical models and machine learning models is that, in statistical models, you explicitly decide the output equation structure prior to building the model. The model is built to compute the parameters/coefficients.Take linear model or GLM for example,y = a1x1 + a2x2 + a3x3Your independent variables are x1, x2, x3 and the coefficients to be determined are a1,a2,a3. You define your equation structure this way prior to building the model and compute a1,a2,a3. If you believe that y is somehow correlated to x2 in a non-linear way, you could try something like this.y = a1x1 + a2(x2)^2 + a3x3.Thus, you put a restriction in terms of the output structure. Inherently statistical models are linear models unless you explicitly apply transformations like sigmoid or kernel to make them nonlinear (GLM and SVM).In case of machine learning models, you rarely specify output structure and algorithms like decision trees are inherently non-linear and work efficiently.Contrary to what Ben pointed out, machine learning models aren't just about prediction, they do classification, regression etc which can be used to make predictions which are also done by various statistical models.;;;
501;2;2014-06-20T07:11:40.053;;The best language depends on what you want to do. First remark: don't limit yourself to one language. Learning a new language is always a good thing, but at some point you will need to choose. Facilities offered by the language itself are an obvious thing to keep into account but in my opinion the following are more important:available libraries: do you have to implement everything from scratch or can you reuse existing stuff? Note that this these libraries need not be in whatever language you are considering, as long as you can interface easily. Working in a language without library access won't help you get things done.number of experts: if you want external developers or start working in a team, you have to consider how many people actually know the language. As an extreme example: if you decide to work in Brainfuck because you happen to like it, know that you will likely work alone. Many surveys exists that can help assess the popularity of languages, including the number of questions per language on SO.toolchain: do you have access to good debuggers, profilers, documentation tools and (if you're into that) IDEs?I am aware that most of my points favor established languages. This is from a 'get-things-done' perspective.That said, I personally believe it is far better to become proficient in a low level language and a high level language:low level: C++, C, Fortran, ... using which you can implement certain profiling hot spots only if you need to because developing in these languages is typically slower (though this is subject to debate). These languages remain king of the hill in terms of critical performance and are likely to stay on top for a long time.high level: Python, R, Clojure, ... to 'glue' stuff together and do non-performance critical stuff (preprocessing, data handling, ...). I find this to be important simply because it is much easier to do rapid development and prototyping in these languages.;;;
502;2;2014-06-20T10:01:03.363;;Regarding prediction, statistics and machine learning sciences started to solve mostly the same problem from different perspectives. Basically statistics assumes that the data were produced by a given stochastic model. So, from a statistical perspective, a model is assumed and given various assumptions the errors are treated and the model parameters and other questions are inferred. Machine learning comes from a computer science perspective. The models are algorithmic and usually very few assumptions are required regarding the data. We work with hypothesis space and learning bias. The best exposition of machine learning I found is contained in Tom Mitchell's book called Machine Learning.For a more exhaustive and complete idea regarding the two cultures you can read the Leo Broiman paper called Statistical Modeling: The Two CulturesHowever what must be added is that even if the two sciences started with different perspectives, both of them now now share a fair amount of common knowledge and techniques. Why, because the problems were the same, but the tools were different. So now machine learning is mostly treated from a statistical perspective (check the Hastie,Tibshirani, Friedman book The Elements of Statistical Learning from a machine learning point of view with a statistical treatement, and perhaps Kevin P. Murphy 's book Machine Learning: A probabilistic perspective, to name just a few of the best books available today). Even the history of the development of this field show the benefits of this merge of perspectives. I will describe two events. The first is the creation of CART trees, which was created by Breiman with a solid statistical background. At approximately the same time, Quinlan developed ID3,C45,See5, and so on, decision tree suite with a more computer science background. Now both this families of trees and the ensemble methods like bagging and forests become quite similar. The second story is about boosting. Initially they were developed by Freund and Shapire when they discovered AdaBoost. The choices for designing AdaBoost were done mostly from a computational perspective. Even the authors did not understood well why it works. Only 5 years later Breiman (again!) described the adaboost model from a statistical perspective and gave an explanation for why that works. Since then, various eminent scientists, with both type of backgrounds, developed further those ideas leading to a Pleiads of boosting algorithms, like logistic boosting, gradient boosting, gentle boosting ans so on. It is hard now to think at boosting without a solid statistical background.GLM is a statistical development. However new Bayesian treatments puts this algorithm also in machine learning playground. So I believe both claims could be right, since the interpretation and treatment of how it works could be different. ;;;
503;2;2014-06-20T14:17:27.970;;Edit: Warning, i leave my message but my answer seems wrong, please check out the comment below!I'm not an expert but I guess the main problem is to answer this question:Has an/any event affected the number of hits on a certain day?But I don't know how to treat multiple events, so I would try to answer this question:Does event X affected the number of hits on a certain day?Which can be answered using hypothesis testing with p-values (what scientist do to evaluate for instance if a medicine affects a disease or not).By using p-values, you could determinate if the number of hits in a certain day were mere random and acceptable under normal circumstances or that they must correspond to a change in your model.You can read more about p-values in Open Intro to Statistics Book, I've actually learn about them from there.Then, the other parts of the problem are how to identify your events and calculate the necessary parameters to answer your question (average/median, variance, etc.) and also how to keep that up-to-date and working.;;;
504;1;2014-06-20T14:58:09.320;Dealing with diverse text data;"I'm currently working with a dataset with a wide range of document lengths -- anywhere from a single word to a full page of text.  In addition, the grammatical structure and use of punctuation varies wildly from document to document.  The goal is to classify those documents into one of about 10-15 categories.  I'm currently using ridge regression and logistic regression for the task, and CV for the alpha values of ridge.  The feature vectors are tf-idf ngrams.Recently I've noticed that longer documents are much less likely to be categorized. Why might this be the case, and how can one ""normalize"" for this kind of variation?  As a more general question, how does one typically deal with diverse data sets?  Should documents be grouped based off of metrics like document length, use of punctuation, grammatical rigor, etc. and then fed through different classifiers?  ";[education, open-source];80;
506;2;2014-06-20T16:09:41.637;;"For the record, I think this is the type of question that's perfect for the data science Stack Exchange. I hope we get a bunch of real world examples of data problems and several perspectives on how best to solve them.I would encourage you not to use p-values as they can be pretty misleading (1, 2). My approach hinges on you being able to summarize traffic on a given page before and after some intervention. What you care about is the difference in the rate before and after the intervention. That is, how does the number of hits per day change? Below, I explain a first stab approach with some simulated example data. I will then explain one potential pitfall (and what I would do about it).First, let's think about one page before and after an intervention. Pretend the intervention increases hits per day by roughly 15%:import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsdef simulate_data(true_diff=0):    #First choose a number of days between [1, 1000] before the intervention    num_before = np.random.randint(1, 1001)    #Next choose a number of days between [1, 1000] after the intervention    num_after = np.random.randint(1, 1001)    #Next choose a rate for before the intervention. How many views per day on average?    rate_before = np.random.randint(50, 151)    #The intervention causes a `true_diff` increase on average (but is also random)    rate_after = np.random.normal(1 + true_diff, .1) * rate_before    #Simulate viewers per day:    vpd_before = np.random.poisson(rate_before, size=num_before)    vpd_after = np.random.poisson(rate_after, size=num_after)    return vpd_before, vpd_aftervpd_before, vpd_after = simulate_data(.15)plt.hist(vpd_before, histtype=""step"", bins=20, normed=True, lw=2)plt.hist(vpd_after, histtype=""step"", bins=20, normed=True, lw=2)plt.legend((""before"", ""after""))plt.title(""Views per day before and after intervention"")plt.xlabel(""Views per day"")plt.ylabel(""Frequency"")plt.show()We can clearly see that the intervention increased the number of hits per day, on average. But in order to quantify the difference in rates, we should use one company's intervention for multiple pages. Since the underlying rate will be different for each page, we should compute the percent change in rate (again, the rate here is hits per day).Now, let's pretend we have data for n = 100 pages, each of which received an intervention from the same company. To get the percent difference we take (mean(hits per day before) - mean(hits per day after)) / mean(hits per day before):n = 100pct_diff = np.zeros(n)for i in xrange(n):    vpd_before, vpd_after = simulate_data(.15)    # % difference. Note: this is the thing we want to infer    pct_diff[i] = (vpd_after.mean() - vpd_before.mean()) / vpd_before.mean()plt.hist(pct_diff)plt.title(""Distribution of percent change"")plt.xlabel(""Percent change"")plt.ylabel(""Frequency"")plt.show()Now we have the distribution of our parameter of interest! We can query this result in different ways. For example, we might want to know the mode, or (approximation of) the most likely value for this percent change:def mode_continuous(x, num_bins=None):    if num_bins is None:        counts, bins = np.histogram(x)    else:        counts, bins = np.histogram(x, bins=num_bins)    ndx = np.argmax(counts)    return bins[ndx:(ndx+1)].mean()mode_continuous(pct_diff, 20)When I ran this I got 0.126, which is not bad, considering our true percent change is 15. We can also see the number of positive changes, which approximates the probability that a given company's intervention improves hits per day:(pct_diff > 0).mean()Here, my result is 0.93, so we could say there's a pretty good chance that this company is effective.Finally, a potential pitfall: Each page probably has some underlying trend that you should probably account for. That is, even without the intervention, hits per day may increase. To account for this, I would estimate a simple linear regression where the outcome variable is hits per day and the independent variable is day (start at day=0 and simply increment for all the days in your sample). Then subtract the estimate, y_hat, from each number of hits per day to de-trend your data. Then you can do the above procedure and be confident that a positive percent difference is not due to the underlying trend. Of course, the trend may not be linear, so use discretion! Good luck!";;;
507;2;2014-06-20T16:29:27.673;;"Back in my data analyst days this type of problem was pretty typical.  Basically, everyone in marketing would come up with a crazy idea that the sold to higher ups as the single event that would boost KPI's by 2000%.  The higher ups would approve them and then they would begin their ""test"".  Results would come back, and management would dump it on the data analysts to determine what worked and who did it.The short answer is you cant really know if it wasn't run as a random A/B style test on like time periods.  But I am very aware of how deficient that answer is, especially if the fact that a pure answer doesn't exist is irrelevant to the urgency of future business decisions.  Here are some of the techniques I would use to salvage the analysis in this situation, bear in mind this is more of an art then a science.HandlesA handle is something that exists in the data that you can hold onto.  From what you are telling me in your situation you have a lot of info on who the marketing agency is, when they tried a tactic, and to which site they applied it to.  These are your starting point and information like this going to be the corner stone of your analysis.MethodologyThe methodology is going to probably hold the strongest impact on which agencies are given credit for any and all gains so you are going to need to make sure that it is clearly outlines and all stake holders agree that it makes sense.  If you cant do that it is going to be difficult for people to trust your analysis.  An example of this are conversions. Say the marketing department purchases some leads and they arrive at our landing page, we would track them for 3 days, if they made a purchase within that time we would count them as having been converted.  Why 3 days, why not 5 or 1?  Thats not important as long as everyone agrees, you now have a definition you can build off of.  ComparisonsIn an ideal would you would have a nice A/B test to prove a definitive relationship, I am going to assume that you are running short on those, still, you can learn something from a simple comparison of like data.  When companies are trying to determine the efficacy of radio advertising they will often run ads on offset months in the same market, or for several months in one market and compare that with the results in a separate but similar market.  Its doesn't pass for science, but even with all that noise a strong results will almost always be noticeable.I would combine these in your case to determine how long an event is given to register an effect.  Once you have the data from that time period run it against your modeled out traffic prediction, week over week growth, month over month etc. Which, can then allow a meaningful comparison between agencies, and across time periods.PragmatismThe aspiration is to be able to provide a deep understanding of cause and effect, but it is probably not realistic.  Because of how messy outside factors make your analysis, you are constantly going to run up against the question over and over again: Did this event raise volume/sales/click throughs, or would doing anything at all have had the same effect?  The best advise I can give for this is set very realistic goals for what you are looking to measure.  A good starting point is, within the methodology you have, which event had the largest impact.  Once you have those open your aperture from there.SummaryOnce you have reasoned out all of these aspects you can go about building a general solution which can then be automated.  The advantage to designing your solution in this manner is that the business logic is already built in.  This will make your results much more approachable and intuitive to non-technical business leaders.";;;
508;2;2014-06-20T16:56:41.160;;I'm not sure how you are applying a regression framework for document classification. The way I'd approach the problem is to apply a standard discriminative classification approach such as SVM.In a discriminative classification approach the notion of similarity or inverse distance between data points (documents in this case) is pivotal. Fortunately for documents, there is a standard way of defining pairwise similarity. This is the standard cosine similarity measure which makes use of document length normalization to take different document lengths into account.Thus, practically speaking, in cosine similarity you would work with relative term weights normalized by document lengths and hence document length diversity should not be a major issue in the similarity computation.One also has to be careful when applying idf in term weights. If the number of documents is not significantly large the idf measure may be statistically imprecise thus adding noise to the term weights. It's also a standard practice to ignore stopwords and punctuations.;;;
509;2;2014-06-20T17:30:39.097;;For example...if u have an agricultural land then selecting one particular area of that land would be feature selection.If u aim to find the affected plants in that area den u need to observe each plant based on a particular feature that is common in each plant so as to find the abnormalities...for this u would be considering feature extraction.In this example the original agricultural land corresponds to Dimensionality reduction.;;;
510;2;2014-06-20T17:34:51.633;;"A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks’ APIs for researchers. Unfortunately, it is in Russian. Here is a summary:Twitter (https://dev.twitter.com/docs/api/1.1)almost all data about tweets/texts and users is available;lack of sociodemographic data;great streaming API: useful for real time text processing;a lot of wrappers for programing languages;getting network structure (connections) is possible, but time-expensive (1 request per 1 minute).Facebook (https://developers.facebook.com/docs/reference/api/)rate limits: about 1 request per second;well documented, sandbox present;FQL (SQL-like) and «regular Rest» Graph API;friendship data and sociodemographic features present;a lot of data is beyond event horizon: only friends' and friends' of friends data is more or less complete, almost nothing could be investigated about random user;some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym).Instagram (http://instagram.com/developer/)rate limits: 5000 requests per hour;real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used;lack of sociodemographic data;photos, filters data available;unexpected imperfections (e.g., it’s possible to collect only 150 comments to post/photo).Foursquare (https://developer.foursquare.com/overview/)rate limits: 5000 requests per hour;kingdom of geosocial data :)quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once;again: lack of sociodemographic data.Google+ (https://developers.google.com/+/api/latest/)about 5 requests per second (try to verify);main methods: activities and people;like on Facebook, a lot of personal data for random user is hidden;lack of user connections data.And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is vk.com. It’s translated to many languages, but popular only in Russia and other CIS countries. API docs link: http://vk.com/dev/. And from my point of view, it’s the best choice for homebrew social media research. At least, in Russia. That’s why:rate limits: 3 requests per second;public text and media data available;sociodemographic data available: for random user availability level is about 60-70%;connections between users are also available: almost all friendships data for random user is available;some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.";;;
511;1;2014-06-20T17:57:46.363;Cross-validation: K-fold vs Repeated random sub-sampling;I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.Classification algorithms used: random forest & logistic regression.;[education, open-source];784;
512;2;2014-06-20T18:56:52.110;;Here's an approach I didn't see mentioned: separate the process into two steps: the first step focused on encoding names so that alternative versions of the same name are encoded the same (or nearly the same), and the second step focused on making them anonymous.For the first step, you could use one of the Phonetic Algorithms (Soundex and variants), applied to first name, last name, and initials in various orders.  (See this article, also).  It's in this step where you resolve similarities vs. differences in names to balance false positives from false negatives.For the second step, you can pick any hashing or cryptographic method you like, without concern for how that method affects name matching.  This gives you freedom to use a method that has the best characteristics for both performance, robustness, and anonymity.;;;
513;2;2014-06-20T19:15:52.093;;If you have an adequate number of samples and want to use all the data, then k-fold cross-validation is the way to go. Having ~1,500 seems like a lot but whether it is adequate for k-fold cross-validation also depends on the dimensionality of the data (number of attributes and number of attribute values). For example, if each observation has 100 attributes, then 1,500 observations is low.Another potential downside to k-fold cross-validation is the possibility of a single, extreme outlier skewing the results. For example, if you have one extreme outlier that can heavily bias your classifier, then in a 10-fold cross-validation, 9 of the 10 partitions will be affected (though for random forests, I don't think you would have that problem).Random subsampling (e.g., bootstrap sampling) is preferable when you are either undersampled or when you have the situation above, where you don't want each observation to appear in k-1 folds.;;;
514;2;2014-06-20T20:03:51.247;;I like Amir Ali Akbari's suggestions, and I'll add a few of my own, focusing on topics and skills that are not adequately covered in most machine learning and data analysis books that focus on math and/or programming.Data Cleaning:Osborne 2012, Best Practices in Data CleaningMcCallom 2012, Bad Data Handbook: Cleaning Up The Data So You Can Get Back To WorkBayesian Data Analysis (alternative to Fisher-style Null Hypothesis Significance Testing):Kruschke 2011, Doing Bayesian Data AnalysisInference in the face of uncertainty, incompleteness, contradictions, ambiguity, imprecision, ignorance, etc.:Schum & Starace, 2001, The Evidential Foundations of Probabilistic ReasoningBammer & Smithson 2008, Uncertainty and Risk: Multidisciplinary PerspectivesSmithson 1989, Ignorance and UncertaintyCIA 2008, A Tradecraft Primer: Structured Analytic Techniques for Improving Intelligence Analysis (FREE! as PDF)Morgan & Winship 2007, Counterfactuals and Causal Inference: Methods and Principles for Social ResearchExperiments:Glennerster & Takavarasha 2013, Running Randomized Evaluations: A Practical GuideDunning 2012, Natural Experiments in the Social SciencesSimulation:Epstein 2006, Generative Social Science: Studies in Agent-Based Computational ModelingNelson 2010, Stochastic Modeling: Analysis and SimulationExpert elicitation, probabilistic estimation:Hubbard 2014, How to Measure Anything: Finding the Value of Intangibles in Business;;;
515;2;2014-06-20T20:44:39.280;;I can't add a comment yet, but just FYI ESL is available for free online as a pdf;;;
516;1;2014-06-21T04:59:06.620;Efficient solution of fmincg without providing gradient?;I'm working on multiclass logistic regression model with a large number of features (numFeatures > 100). Using a Maximum Likelihood Estimation based on the cost function and gradient, the fmincg algorithm solves the problem quickly. However, I'm also experimenting with a different cost function and do not have a gradient.Is there a good way to speed up the calculation process? E.g., is there a different algorithm or fmincg setting that I can use?;[education, open-source];83;
517;2;2014-06-21T07:02:10.687;;If you do not have a gradient available, but the problem is convex, you can use the Nelder-Mead simplex method. It is available in most optimization packages, for example in scipy.optimize.;;;
518;1;2014-06-21T10:55:41.700;Online k-means explanation;Please, could someone recommend a paper or blog post that describes the online k-means algorithm.Thanks :);[education, open-source];80;0
519;2;2014-06-21T12:59:15.373;;The main problem here is that even before attempting to apply anomaly detection algorithms, you are not getting good enough predictions of gas consumption using neural networks. If the main goal here is to reach the stage when anomaly detection algorithms could be used and you state that you have access to examples of successful application of linear regression for this problem, this approach could be more productive. One of the principles of successful machine learning application is that several different algorithms can be tried out before final selection based on results.It you choose to tune your neural network performance, learning curve plotting the effect of change in different hyperparameters on the error rate can be used. Hyperparameters that can be modified are: number of featuresorder of the polynomialregularization parameter number of layers in the networkBest settings can be selected by the performance on cross validation set.;;;
520;2;2014-06-21T19:13:47.757;;In your training notebook you present results for training with 20 epochs. Have you tried varying that parameter, to see if it affects your performance? This is an important parameter for back-propagation. For estimating your model parameters, as user tomaskazemekas pointed out, plotting Learning Curves is a very good approach. In addition to that, you could also create a plot using a model parameter (e.g. training epochs or hidden layer size) vs. Training and Validation error. This will allow you to understand the bias/variance tradeoff, and help you pick a good value for your parameters. Some info can be found here. Naturally, it is a good idea to keep a small percentage of your data for a (third) Test set.As a side note, it seems that increasing the number of neurons in your model show no significant improvement for your RMSE. This suggests that you could also try with a simpler model, i.e. with less neurons and see how your model behaves.In fact, I would suggest (if you haven't done so already) trying a simple model with few or no parameters first e.g. Linear Regression, and compare your results with the literature, just as a sanity check. ;;;
521;2;2014-06-22T12:02:52.773;;Conjugate Gradient -- the cg in Function MINimization (nonlinear) Conjugate Gradiant -- requires you to have a gradient function (or approximation) since that is a critical part of the algorithm itself: it needs to find the steepest descent direction quickly.fminsearch implements Nelder-Mead, a nonlinear gradient-free method. Its convergence properties are not anywhere near as good.What is your cost function? Are there approximations that are differentiable (pref. twice so you can use the very powerful quasi-Newton methods)?;;;
522;2;2014-06-22T13:18:38.760;;I have been able to optimize very strange functions with simulated annealing, and it does not require a gradient. Instead, it uses random numbers in a way very similar to Markov Chain Monte Carlo, which helps it avoid getting stuck in local optima. A decent explanation that gives the intuition behind it can be found in this lecture: Simulated Annealing. scipy 0.14 includes this algorithm in its optimization module: scipy.optimize.anneal.;;;
523;2;2014-06-22T16:03:39.047;;In your notebooks, I did not see your neural network model, can you point which library is using, how many layers you have and what type of neural network are you using?In your notebooks, it seems you are using the noisy and outlier dataset to train the neural network, I think you should train the neural network on the dataset that you do not have any outliers so that you could see the observation distance from the prediction of the neural network to label the observation either outlier or not. I wrote couple of things on outlier detection in time-series signals, your data is highly seasonal as sobach mentioned and you could use FFT(first link above) to get the overall trend in the signal. After you get the frequency component in the gas consumption, you could look at the high frequency components to get the outliers. Also if you want to insist on using neural network for seasonal data, you may want to check recurrent neural networks out as they could incorporate the past observations better than a vanilla neural network, and supposedly may provide a better result for the data that you have. ;;;
525;2;2014-06-22T18:35:22.207;;In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.People use a wide range of solutions for model training and deployment:Build a model, then export and deploy it with PMML  AirBnB describes their model training in R/Python and deployment of PMML models via OpenScoring. Pattern is project related to Cascading that can consume PMML and deploy predictive models. Build a model in MapReduce and access values in a custom system  Conjecture is an open source project from Etsy that allows for model training with Scalding, an easier to use scala wrapper around MapReduce, and  deployment via Php. Kiji is an open source project from WibiData that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via Scalding. Use an online system that allows for continuously updating model parameters.  Google released a great paper about an online collaborative filtering they implemented to deal with recommendations in Google News. ;;;
526;2;2014-06-22T18:50:07.193;;Chapter 1 of Practical Data Science with R (http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.  ;;;
527;1;2014-06-22T21:45:35.800;Score matrix string similarity;"I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the Smith-Waterman Algorithm to compare the similarity.So I've drawn a picture of how I'm thinking about representing the data - The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).Image that this matrix represents a key type of ""things"" I then need to add the ""things"" similarity score into a vector of 0 or 1. Thats ok.What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).";[education, open-source];268;
529;2;2014-06-23T03:44:57.643;;Airbnb and Etsy both recently posted detailed information about their workflows.;;;
530;1;2014-06-23T04:39:26.623;How to select algorithms for ensemble methods?;There is a general recommendation that algorithms in ensemble learning combinations should be different in nature. Is there a classification table, a scale or some rules that allow to evaluate how far away are the algorithms from each other? What are the best combinations? ;[education, open-source];220;
531;1;2014-06-23T07:03:15.643;Binary classification model for sparse / biased data;I have a dataset with following specifications:Training dataset with 193176 samples with 2821 positivesTest Dataset with 82887 samples with 673 positivesThere are 10 features.I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:mean square error : 0.00804710026904Confusion matrix : [[82214   667]                   [    0     6]]i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:Different algorithms like RandomForest, DecisionTree, SVMChanging parameters value to call the functionSome intuition based feature engineering to include compounded featuresNow, my questions are:What can I do to improve the number of positive hits ? How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?I am using the scikit-learn library with Python and all implementations are library functions.edit:Here are the results with a few other algorithms:Random Forest Classifier(n_estimators=100)[[82211   667][    3     6]]Decision Trees:[[78611   635][ 3603    38]];[education, open-source];214;1
532;2;2014-06-23T08:03:50.327;;"In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifiers to be different from each other, that is you don't want to ask the same ""expert"" twice for the same thing.In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of features (or both). If you use different training sets you end up with different models and different ""independent"" classifiers. There is no golden rule on what works best in general. You have to try to see if there is an improvement for your specific problem. ";;;
533;2;2014-06-23T08:13:22.837;;Since you are doing binary classification, have you tried adjusting the classification threshold? Since your algorithm seems rather insensitive, I would try lowering it and check if there is an improvement.You can always use Learning Curves, or a plot of one model parameter vs. Training and Validation error to determine whether your model is overfitting. It seems it is under fitting in your case, but that's just intuition.Well, ultimately it depends on your dataset, and the different models you have tried. At this point, and without further testing, there can not be a definite answer.Without claiming to be an expert on the topic, there are a number of different techniques you may follow (hint: first link on google), but in my opinion you should first make sure you choose your cost function carefully, so that it represents what you are actually looking for. Not sure what you mean by pattern intuition, can you elaborate?By the way, what were your results with the different algorithms you tried? Were they any different?;;;
534;2;2014-06-23T09:23:27.617;;If your goal is to transform your matrix into a number (your similarity measure), you may want to use a matrix norm.For instance, using the Frobenius norm on your example would return 1.488086.;;;
535;2;2014-06-23T09:42:46.450;;As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:Sum of max. vals is equal to 0.Select maximum value from doc-doc matrix and add it to Sum of max. vals.Remove row and column with maximum value from the matrix.Repeat steps 2-3 until rows or columns are ended.Denominate Sum of max. vals by average number of key words in two texts.Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.You haven't mentioned software, you are using, but here is R example of function, computing such similarity (it takes object of class matrix as input):eval.sim <- function(sim.matrix){  similarity <- 0  denominator <- sum(dim(sim.matrix)) / 2  for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){    extract <- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]    similarity <- similarity + sim.matrix[extract[1], extract[2]]    sim.matrix <- sim.matrix[-extract[1], -extract[2]]  }  similarity <- similarity + max(sm.copy)  similarity <- similarity / denominator}In python - import numpy as npdef score_matrix(sim_matrix):    similarity = 0    denominator = sum(sim_matrix.shape) / 2    for i in range(min(sim_matrix.shape)):        x, y = np.where(sim_matrix == np.max(sim_matrix))[0][0], np.where(sim_matrix == np.max(sim_matrix))[1][0]        similarity += sim_matrix[x, y]        sim_matrix = np.delete(sim_matrix,(x),axis=0)        sim_matrix = np.delete(sim_matrix,(y),axis=1)    return similarity / denominator;;;
536;1;2014-06-23T11:03:09.767;Good practices for manual modifications of data;"More often than not, data I am working with is not 100% clean. Even if it is reasonably clean, still there are portions that need to be fixed. When a fraction of data needs it, I write a script and incorporate it in data processing.But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on ""small data"",  such as that in CSV files or a relational database.The practical problems I encountered:Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).Copying and modifying data may make a mess, as:Generating it again will destroy all fixes.When there are more errors of different kinds, too many copies of the same file result, and it is hard to keep track of them all.Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, seems to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.What are the best practices in such a case as this?EDIT: The question is on the workflow, not whether to use it or not.(In my particular case I don't want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)";[education, open-source];182;1
538;1;2014-06-23T13:31:41.443;What are your favorite sources for news about Machine Learning and Data Science?;Data Science and Machine Learning include a lot of different topics and it´s hard to stay up-to-date about all the news about papers, researches or new tutorials and tools.What sources do you use to get all the information?I use mostly Reddit as my first source and the subreddits Machine Learning and Rhttp://www.reddit.com/r/MachineLearninghttp://www.reddit.com/r/rstatsBut also datatau.com and of course the great KDNuggets page ;[education, open-source];201;
539;2;2014-06-23T13:36:18.273;;I like following certain tags on stats.stackexchange.com (a.k.a. CrossValidated), as well as the r tag on StackOverflow.Arxiv.org is also pretty cool, it has several subsections devoted to stats and ML. ;;;
540;1;2014-06-23T15:38:10.920;First steps on a new cleaned dataset;What is the very first thing you do when you get your hands on a new data set (assuming it is cleaned and well structured)? Please share sample code snippets as I am sure this would be extremely helpful for both beginners and experienced. ;[education, open-source];144;1
541;2;2014-06-23T15:48:10.033;;"The #1 most important thing is to explicitly document your process.Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review.You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it's not the only tool at your disposal. Sometimes you need to use your judgment as the scientist.But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you're going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify not correcting the other 99,992 observations.You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.On the other hand, why do all this extra work before you know it's necessary? Plenty of ""dirty"" data sets will still produce useful results. Perhaps 0.5% of your data is ""dirty"" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise.For example...Say you have a set of test results from a survey of wells, showing the concentrations of certain dissolved substances, hourly over the course of a year. And you observe in this set certain spikes, of short duration, and orders of magnitude higher than the surrounding data. If they are few, and obvious, and you know that the sensors used to produce the data set occasionally malfunction, then there's no reason to apply an algorithmic solution to the entire data set. You have a choice between excluding some data or modifying it. I recommend the exclusion route whenever possible, since you are making fewer assumptions when you don't have to additionally choose a ""correct"" value. But if your analysis will absolutely fail with a discontinuity, there are many options. You could choose to replace ""bad"" values via linear interpolation. You could hold constant the previous value. If there is some great previous literature to apply, and you really have a strong case that the ""bad"" data is purely due to equipment malfunction, perhaps there's an established model that you can apply to fill in those regions.There are a great many approaches, which is why the most important thing is to document your process, explicitly and exhaustively. Arguing your case is important too, but not strictly necessary; the community is just more likely to ignore you if you don't make a full effort to engage the review process.";;;
543;2;2014-06-23T17:50:51.320;;Well, you mention in your question of the data being 'clean and well structured'. In practice, close to 70% of the time is spent in doing these two steps. Of course the first thing you do is to separate the training and test data. Considering the plethora of libraries and tools available irrespective of which technology/language you prefer to use, the next step would be to understand the data via graph plots and drawing useful intuitions specific to your target goal. This would then be followed by various other problem specific methods. As pointed out, the question is very broad and citing code snippets is simply not feasible.;;;
544;2;2014-06-23T17:56:32.917;;You could also try:Quora Machine Learning Daily;;;
545;2;2014-06-23T18:38:34.260;;"I think this is a reasonable question. Here is what I do:Peak at the first few rowsVisualize the distribution of the features I care about (histograms)Visualize the relationship between pairs of features (scatterplots)I downloaded the abalone dataset from the UCI Machine Learning repository here. Let's say I care about how height and diameter can be used to predict whole weight. For completeness, I've included the step of reading the data from file.import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsdata = pd.read_csv(""abalone.data"", header=False)data.columns = [""sex"", ""length"", ""diameter"", ""height"",                 ""whole_weight"", ""shucked_weight"",                ""viscera_weight"", ""shell_weight"", ""rings""]Now we can take a peak at the first few rows:data.head()Now, I know that the variables I care about are floating point values and they can be treated as continuous. I want to take a look to see how these three variables are distributed:fig = plt.figure(figsize=(20,5))plt.subplot(1, 3, 1)plt.hist(data['diameter'], normed=True)plt.title(""Diameter"")plt.subplot(1, 3, 2)plt.hist(data['height'], normed=True)plt.title(""Height"")plt.subplot(1, 3, 3)plt.hist(data['whole_weight'], normed=True)plt.title(""Whole Weight"")plt.show()Great! Now, I know that diameter and whole weight are skewed left and right (respectively). I also know that there are some outliers in terms of height (which is why matplotlib gives me extra room to the right of the distribution). Finally, I'd like to see if I can find any visual patterns between my predictors and outcome variable. I use a scatter plot for this:plt.figure(figsize=(15,5))plt.subplot(1, 2, 1)plt.plot(data['diameter'], data['whole_weight'], 'o')plt.title(""Diameter vs. Whole Weight"")plt.ylabel(""Whole Weight"")plt.xlabel(""Diameter"")plt.subplot(1, 2, 2)plt.plot(data['height'], data['whole_weight'], 'o')plt.title(""Height vs. Whole Weight"")plt.ylabel(""Whole Weight"")plt.xlabel(""Height"")plt.show()Here, I see there is a non-linear relationship between diameter and whole weight and I'm going to have to deal with my height outliers. Now, I'm ready to do some analysis!";;;
546;2;2014-06-23T18:56:11.577;;"Even if you are effectively modifying certain records by hand, as in the city name example you give, I would recommend doing it in code. The reason to strongly prefer code over hand-tweaking records is that the code makes a result reproducible. You want to make sure that you can always go from raw data to final result without any human intervention.Here's a quick example. Let's say I have a list of city names in a pandas dataframe and I am certain they should all be ""omaha"" (you need to be absolutely certain, because changing values by hand is fraught with danger). But instead I have the following strings:pd.unique(data.city)array(['omaha', 'omahd', 'imaha', 'omaka'], dtype=object)You could make the change like this:data.city.values[data.city.values == 'omahd'] = 'omaha'data.city.values[data.city.values == 'imaha'] = 'omaha'data.city.values[data.city.values == 'omaka'] = 'omaha'That code is ugly, but if you run it on the same raw dataset, you will always get the same result.";;;
548;1;2014-06-23T23:20:36.180;Feature Extraction Technique - Summarizing a Sequence of Data;"I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements). I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? ADD:As mentioned in a response below, I also considered (but forgot to add here) using Dynamic Time Warping (DTW) and then hierarchical clustering on the resulting distance matrix - creating some number of clusters and then using the cluster membership as a feature. Scoring test data would likely have to follow a process where the DTW was done on new cases and the cluster centroids - matching the new data series to their closest centroids... ";[education, open-source];547;2
549;2;2014-06-24T00:54:13.110;;What you're trying to do here is reduce the dimensionality of your features. You can search for dimensionality reduction to get several options, but one very popular technique is principal components analysis (PCA). Principal components are not interpretable like the options you've mentioned, but they do a good job of summarizing all of the information.;;;
550;2;2014-06-24T03:50:41.410;;Feature extraction is always a challenge and the less addressed topic in literature, since it's widely application dependant.Some ideas you can try:Raw data, measured day-by-day. That's kind of obvious with some implications and extra preprocessing (normalisation) in order to make timelines of different length comparable.Higher moments: skewness, kurtosis, etcDerivative(s): speed of evolutionTime span is not that large but maybe it is worth trying some time series analysis features like for example autocorrelation. Some customised features like breaking timeline in weeks and measure the quantities you already measure in each week separately. Then a non-linear classifier would be able to combine e.g first-week features with last-week features in order to get insight of evolution in time.;;;
552;2;2014-06-24T07:03:35.567;;At first glance, you need to extract features from your time series (x - 12) - x. One possible approach is to compute summary metrics: average, dispersion, etc. But doing so, you will loose all time-series related information. But data, extracted from curve shape may be quite useful. I recommend you to look through this article, where authors propose algorithm for time series clustering. Hope, it will be useful. Additionally to such clustering you can add summary statistics to your feature list. ;;;
554;1;2014-06-24T07:29:57.787;Datasets understanding best practices;I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.My questions:What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?Practices to make sure the dataset is clean?Practices to make sure the dataset doesn't have wrong values or so?;[education, open-source];251;3
555;2;2014-06-24T07:35:16.930;;"1.  Do not modify the original dataHaving the original data source intact is important.  You may find that updates you make to the data are not valid.  You may also find a more efficient way to make updates and you will want to regression test those updates.Always work with a copy of the data, and add columns/properties/metadata that includes any processed corrections.Example, if your data is a .csv file that includes a city name that contains several misspellings:1. Copy the .csv to a new file2. Add a column for containing ""adjusted_city_name""3. Copy data from the city_name column to the    adjusted_city_name column and make corrections in that column.2. Document proposed changesAny changes you want to make to data should be documented so that they can be replicated moving forward.Version control and timestamp the document every time you change it.  That will help in troubleshooting at a later date.Be explicit.  Do not simply state ""correct capitalization problems"", state ""ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case.""Update the document with references to any automation routines that have been built to manage data cleansing.3. Decide on a standard data cleansing technologyWhether you use perl, python, java, a particular utility, a manual process or something else is not the issue.  The issue is that in the future you want to hand the data cleansing process to somebody else.  If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult.4. Standardize the workflowThere should be a standard way to handle new data.  Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step.5. Make as few changes as is absolutely necessaryIt's always better to have a fault tolerant analysis than one that makes assumptions about the data.  6. Avoid manual updatesIt's always tempting, but people are error-prone and again it makes delegation difficult.Notes on manual processingTo more completely address the original question as to whether there's a ""good"" way to do manual processing, I would say no, there is not.  My answer is based on experience and is not one that I make arbitrarily.I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally.  You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale.It's much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data.  (And then automating that rule set)If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital.In the end, routines to do something like correct city names can be applied to other data sets.";;;
556;2;2014-06-24T08:09:49.300;;"There are basic things you can do with any set of data:Validate values (String length tolerance, data type, formatting masks, required field presence, etc.)Range correctness (Does this seemingly correct data fall within expected ranges of values)Preliminary processing (If I attempt to analyze this data, can I perform the basics without running into errors)Preliminary reporting (run a report against a data set and ensure that it passes a sanity test)Defining null vs. empty vs. zero vs. False for any given column of dataIdentifying data that is out of place (numeric values dramatically different than other values in a data set, string values that look like they might be misspelled, etc.)Eliminating or correcting obviously errant dataUnderstanding data to identify errors is a whole different ball game, and it is very important.For instance, you can have a rule that says a serial number must be present in a given data set and that serial number must be alphanumeric with a maximum string length of 255 and a minimum string length of 5.Looking at the data, you may find one particular serial number value reads ""PLEASE ENTER SERIAL""  It's perfectly valid, but wrong.That's kind of an obvious one, but say you're processing stock data and you had a price range for 1000 stocks that was under a dollar.  A lot of people would not know that a stock price so low is invalid on certain exchanges and perfectly valid on others.  You need knowledge about your data to understand if what you are seeing is problematic or not.  In the real world, you don't always have the luxury of understanding your data intimately.The way I avoid problems is by leveraging the people around me.  For small data sets, I can ask someone to review the data in it's entirety.  For large ones, pulling a set of random samples and asking someone to do a sanity check on the data is more appropriate.Further, questioning the source of the data and how well that data source can be trusted is imperative.  I often have multiple conflicting sources of data and we create rules to determine the ""source of truth"".  Sometimes one data set has great data in a given aspect, but other data sets are stronger in other areas.Manually entered data is usually what I'm most skeptical about, but in some cases it is stronger than anything that can be acquired through automation.";;;
559;1;2014-06-24T12:28:10.990;Detecting cats visually by means of anomaly detection;"I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.The task is the following:Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there's a cat present so that I may go chase it off using my super soaker. For simplicity's sake, say that I only care about a cat with black and white coloring.I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden. Sample image:My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day. Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.";[education, open-source];1672;3
560;2;2014-06-24T12:55:26.457;;"This is an interesting and also quite ambitious project :)I am not sure anomaly detection (at least in the sense described in the course you followed) would be a very fitting algorithm in this case. I would consider a more viable approach to be what has been discussed at the end of that course where a Photo OCR workflow was demonstrated.The approach would consist of segmenting your image in smaller ""blocks"", and going through them one-by-one using a supervised learning algorithm and try to classify each block according to whether it contains a cat or not. If one block contains a cat, the alarm goes off. As a bonus, you get the position of the cat as well, so that you may think of incorporating some ""automatic"" response as a future step to your project. The benefit here is that you will not have to train your algorithm using a dataset specific to your garden (which, as you mention is difficult to create), but you can use images of cats taken off the net (e.g. perhaps you can search for ""cat on grass"" or something), and perhaps patches of photos from your (or other) gardens. Therefore you don;t have to spend your time collecting photos from your camera, and you avoid the risk of having a very small (comparable) sample of positives (i.e. cats).Now, of course how easy it is to build an accurate cat detector is another topic..";;;
561;2;2014-06-24T13:34:12.107;;You could simplify your problem significantly by using a motion/change detection approach. For example, you could compare each image/frame with one from an early time (e.g., a minute earlier), then only consider pixels that have changed since the earlier time. You could then extract the rectangular region of change and use that as the basis for your classification or anomaly detection.Taking this type of approach can significantly simplify your classifier and reduce your false target rate because you can ignore anything that is not roughly the size of a cat (e.g., a person or bird). You would then use the extracted change regions that were not filtered out to form the training set for your classifier (or anomaly detector).Just be sure to get your false target rate sufficiently low before mounting a laser turret to your feline intrusion detection system.;;;
562;2;2014-06-24T15:44:52.540;;As a rule of thumb I always propose three different options:Use a bagging learning technique, similar to that one followed by Random Forest. This technique allows the training of 'small' classifiers which see a small portion of the whole data. Afterwards, a simple voting scheme (as in Random Forest) will lead you to a very interesting and robust classification.Use any technique related to fusioning information or probabilistic fusion. This is a very suitable solution in order to combine different likelihoods from different classifiers.My last suggestion is the use of fuzzy logic, a very adequate tool in order to combine information properly from a probabilistic (belonging) perspective.The selection of specific methods or strategies will depend enormously on the data.;;;
563;2;2014-06-24T15:55:37.110;;The strategy of motion/change detection is certainly adequate, but I would add an extra operation. I would detect those regions that are more likely to be changed, for instance, the ladder seems a place where humans can be (also cats) and grass where dogs, cats or humans can be.I would capture a map with size of the object and trajectory and with this I would create a cluster with the aim of detecting an object (with specific size within the image in terms of pixels) that moves with a certain speed and trajectory.You can achieve this by using R or I would suggest OpenCV in order to detect movement and follow different objects.;;;
564;1;2014-06-24T18:59:30.560;What visualization technique to best describe a recommendation dataset?;I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)a,b,.2a,c,.3a,d,.4a,e,.1e,b,.3e,c,.5e,d,.8I've looked at some graphs, but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business that for these users the recommendation system is useful.@Steve Kallestad do you mean something like this : ;[education, open-source];99;
565;1;2014-06-24T19:43:24.643;Why does Gradient Boosting regression predict negative values when there are no negative y-values in my training set?;"As I increase the number of trees in scikit learn's GradientBoostingRegressor, I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features, most of which are binary.Some of the parameters that I was tuning were:the number of trees/iterations;learning depth;and learning rate.The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.My code is something like:from sklearn.ensemble import GradientBoostingRegressorX_train, X_test, y_train, y_test = train_test_split(X, y)reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = 'ls', learning_rate = .01)reg.fit(X_train, y_train)ypred = reg.predict(X_test)";[education, open-source];442;
566;1;2014-06-24T17:06:10.310;Named Entity Recognition: NLTK using Regular Expression;"Many times Named Entity Recognition (NER) doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.For example, consider the following input: ""Barack Obama is a great person.""And the output:Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]),    ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])where as for the input:  'Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he ""was honored"" to be compared to Darth Vader while in office.'the output is:Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'),    Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'),    ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]),    ('that', 'IN'), ('he', 'PRP'), ('``', '``'), ('was', 'VBD'), ('honored', 'VBN'),    (""''"", ""''""), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'),    Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'),    ('office', 'NN'), ('.', '.')])Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) is correctly extracted. So, I think if nltk.ne_chunk is used first, and then if two consecutive trees are NNP, there are higher chances that both refer to one entity. I have been playing with NLTK toolkit, and I came across this problem a lot, but couldn't find a satisfying answer. Any suggestion will be really appreciated. I'm looking for flaws in my approach.";[education, open-source];280;1
567;2;2014-06-24T20:28:33.693;;I think you're looking for a similarity matrix (see bottom of the page). If you don't have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what's going on. I'd say 25 rows / columns maximum.In a similarity matrix, x, and y coordinates correspond to the two things you're comparing, while a colormap magnitude represents similarity EDIT:One thing you could do to replace the colormap is the insert, say, circles of different sizes according to the similarity metric. Or you could insert the numbers themselves, again, varying the size of the number as the magnitude of that number varies. Size usually works best is business visualizations.;;;
568;2;2014-06-24T20:34:18.473;;I suggest the use of Hidden Markov Models, with two possible states: (1) high levels and (0) low levels. This technique might be helpful to decode your signal. Probably you would need a specific HMM for each codification.If noise is an issue an FIR filter with a Blackman-Harris window function would allow you to isolate the frequency you're concerned with. ;;;
569;2;2014-06-24T20:35:44.713;;Is it a bird? Is it a cat? We have black-and-white cat-sized! magpies here. so that would fail.First thing would be to exclude all areas that are green, cats are seldom green.Then compare the rest to a reference image to remove static things like stones and stairs.Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone.With two cameras you could do a 3d mapping of the objects and eliminate flying objects.;;;
570;2;2014-06-25T00:13:50.803;;Personally, I think Netflix got it right.  Break it down into a confidence rating from 1-5 and show your recommendations based on the number of yellow stars.  It doesn't have to be stars, but those icon based graphs are very easy to interpret and get the point across clearly.;;;
571;1;2014-06-25T02:33:22.673;Standardize numbers for ranking ratios;I'm trying to rank some percentages. I have numerators and denominators for each ratio. To give a concrete example, consider ratio as total graduates / total students in a school.But the issue is that total students vary over a long range (1000-20000). Smaller schools seem to have higher percentage of students graduating, but I want to standardize it, and not let the size of the school affect the ranking. Is there a way to do it?;[education, open-source];82;1
572;2;2014-06-25T03:11:02.457;;"This is relatively simple to do mathematically.  First, fit a regression line to the scatter plot of ""total graduates"" (y) vs. ""total students"" (x).  You will probably see a downward sloping line if your assertion is correct (smaller schools graduate a higher %).You can identify the slope and y-intercept for this line to convert it into an equation y = mx + b, and then do a little algebra to convert the equation  into normalized form: ""y / x = m + b / x""Then, with all the ratios in your data , you should subtract this RHS:  normalized ratio = (total grads / total students) - (m + b / total students)If the result is postive, then the ratio is above normal for that size (i.e. above the regression line) and if it is negative it is below the regression line.  If you want all positive numbers, you can add a positive constant to move all results above zero.This is how to do it mathematically, but I suggest that you consider whether it is wise, from a data analysis point of view, to normalize by school size.  This depends on the purpose of your analysis and specifically how this ratio is being analyzed in relation to other data.";;;
573;2;2014-06-25T03:30:19.787;;OpenCV's background subtraction will find objects moving about your harden. After that you could use a classifier or shape analysis to differentiate between cats, people, trees and etc.;;;
574;2;2014-06-25T07:22:33.170;;This is a very interesting problem.I faced a similar one by analyzing the pictures users upload to the social network. I did the following approach:Rather than associating data to ages (15 y.o., 27 y.o., ...) what I did is to establish different groups of ages: Less than 18, from 18 to 30 and greater than 30 (this is due to the specific problem we were facing, but you can choose whatever intervals you want). This division helps a lot to solve the problem.Afterwards, I created a hierarchical clustering (divisive or aggregative). Then I choose those branches where I had users with known ages (or group ages) and then for that branch I extended the same age to that group.This approach is semi-supervised learning and I recommended it in case you only have some data labeled.Please, notice that on a social network, people usually lie about the age (just for fun, or sometimes because they want to camuflate themselves on the social net).;;;
575;2;2014-06-25T09:19:25.560;;This is a very good question and a common situation.In my opinion there are three different factors that must be controlled:Data: There exist already different benchmarks in order to evaluate algorithms and architectures. These data must be publicly available so that everybody can contrast their approaches.Architecture: My suggestion is to test everything on the cloud, so that everyone can contrast their results and also there is no doubt the same machines and software is used.Algorithms: If you have developed a distributed algorithm, it would be great to compare your algorithm on a specific data. In this case, algorithms must not be public.So, answering your question, if you want to compare different experiments and state to what extent your distributed algorithm outperforms others, you should try to replicate as accurate as possible the same environment (data and architecture) where the experiments were carried out.If this is not possible, my suggestion is that you test your algorithm with public data and cloud architecture so that you become a referent as you are facilitating the comparison of future algorithms.;;;
576;2;2014-06-25T10:15:31.477;;You should try arules package in R. It allows you to create not only the association rules but also to specify the length of each rule, the importance of each rule and also you can filter them, which is what you are looking for (try the rhs() command of this package).;;;
577;2;2014-06-25T10:59:56.737;;Though it's easy to say, it's better to treat the environment that changes as variables, describe/estimate your algorithm's performance base on these variables. And hopefully others will do the same. Of interest, Experiments as Research Validation -- Have We Gone too Far?.;;;
580;2;2014-06-25T14:30:10.597;;I have read some about it and I had the following blog in mind:http://fellgernon.tumblr.com/post/46117939292/predicting-who-will-win-a-nfl-match-at-half-time#.UtehM7TWtQgThis blog deals with the prediction of a NFL match after the half time is already over. The prediction is 80% accurate with simple GLM model.I do not know if that is suitable for soccer.;;;
581;2;2014-06-25T14:36:29.163;;In my opinion, there are solutions to deal with categorical data in clustering. R comes with a specific distance for categorical data. This distance is called Gower (http://www.inside-r.org/packages/cran/StatMatch/docs/gower.dist) and it works pretty well.;;;
582;2;2014-06-25T14:43:53.680;;Some research from D. Nguyen et al. try to predict twitter user's age based on their tweets. Maybe you find them useful. They use logistic and linear regression.;;;
583;2;2014-06-25T15:42:19.287;;"The original MacQueen k-means publication (the first to use the name ""kmeans"") is an online algorithm. MacQueen, J. B. (1967). ""Some Methods for classification and Analysis of Multivariate Observations"". Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281–297After assigning each point, the mean is incrementally updated.As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled. On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer.When you implement a parallel version of k-means, make sure to study the update formulas in MacQueens publication. They're useful.";;;
584;2;2014-06-25T15:46:21.267;;"The original MacQueen k-means publication (the first to use the name ""kmeans"") is an online algorithm. MacQueen, J. B. (1967). ""Some Methods for classification and Analysis of Multivariate Observations"". Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281–297After assigning each point, the mean is incrementally updated using a simple weighted-average formula (old mean is weighted with n, the new observation is weighted with 1, if the mean had n observations before).As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled (because it updates the mean faster!). On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer (additional math operations, obviously).";;;
585;2;2014-06-25T15:53:30.723;;"DBSCAN (see also: Generalized DBSCAN) does not require a distance.All it needs is a binary decision. Commonly, one would use ""distance < epsilon"" but nothing says you cannot use ""similarity > epsilon"" instead. Triangle inequality etc. are not required.Affinity propagation, as the name says, uses similarities.Hierarchical clustering, except for maybe Ward linkage, does not make any assumption. In many implementations you can just use negative distances when you have similarities, and it will work just fine. Because all that is needed is min, max, and <.Kernel k-means could work IF your similarity is a good kernel function. Think of it as computing k-means in a different vector space, where Euclidean distance corresponds to your similarity function. But then you need to know k.PAM (K-medoids) should work. Assign each object to the most similary medoid, then choose the object with the highest average similarity as new medoid... no triangle inequality needed.... and probably many many more. There are literally hundreds of clustering algorithms. Most should work IMHO. Very few seem to actually require metric properties. K-means has probably the strongest requirements: it minimizes variance (not distance, or similarity), and you must be able to compute means.";;;
586;1;2014-06-25T17:37:23.380;What are the main types of NLP annotators?;I'm new to the world of text mining and have been reading up on annotators at places like the UIMA website. I'm encountering many new terms like named entity recognition, tokenizer, lemmatizer, gazetteer, etc. Coming from a layman background, this is all very confusing so can anyone tell me or link to resources that can explain what the main categories of annotators are and what they do?;[education, open-source];141;
587;1;2014-06-25T18:06:14.293;Where can I download historical market capitalization and daily turnover data for stocks?;There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they're restricted to US stocks. Yahoo Finance provides this data online but there's no option to download it ( or none I am aware of ). Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?Is there some way to download it via Yahoo Finance or Google Finance ?I need data for the last decade or so and hence need some script or API which would do this.;[education, open-source];1889;2
588;2;2014-06-25T18:12:04.010;;Quant SE is better place for questions related to getting financial data:What data sources are available onlinehttp://quant.stackexchange.com/search?q=market+capitalization+data;;;
591;2;2014-06-26T06:51:12.243;;"If you are totally unfamiliar with ordinal regression, I would try to read the Tabachnick / Fidell (http://www.pearsonhighered.com/educator/product/Using-Multivariate-Statistics-6E/0205849571.page)  chapter on the topic first - while not written for R, the book is very good at conveying the general logic and the ""do's"" and ""do nots"". As a question: What are your response catgeories exactly? If they are some sort of scale, like ""good - bad"" it would be ok to use a linear regression (market research does it all the time...), but if the items are more disjunct, an ordinal regression might be better.I dimly remember that some books about structural equatiotion modelling mentioned that linear regression was superior for good scales than probit - bit I cannot recall the book at the moment, sorry!The most serious problem might be the number of dummy variables - a couple of hundred dummy variables will make the analysis slow, hard to interpret and probably unstable - are there enough cases for each dummy / dummy-combination?";;;
592;2;2014-06-26T07:41:52.403;;I usually take a two-step approach compute univariate (variable by variable) summary statistics such as mean, range, variance, number of missing, cardinality, etc. for each variable and look for oddities (e.g. range not plausible given the meaning of the variable). Plot histograms for those odd variables.split the data into manageable subsets (choose a meaningful variable and split the data according to it e.g. all positive examples, and all negative) and explore them visually (e.g. with ggobi ). Especially use tools like brushing and scatter plots to understand how variables are linked together.And when you start building models, make sure to plot the residuals, looking for extreme errors that might be due to an outlier, or look at the confusion matrix and make sure it is balanced. Use k-fold cross validation to optimize your models and look at the variance of the training error for each fold, if one fold performs much worse than the others, it may contain outliers.;;;
593;2;2014-06-26T08:01:40.763;;As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though.Best practice is often tokeep an untouched version of the databuild a working copy of the data with errors fixedhave a way to re-create a working copy from the original dataThe last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly.If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy.;;;
594;1;2014-06-26T12:11:51.253;How does Google categorize results from its image search?;"While doing a Google image search, the page displays some figured out categories for the images of the topic being searched for. I'm interested in learning how this works, and how it chooses and creates categories.Unfortunately, I couldn't find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this, and what basis these categories are created from?For example, if I search for ""animals"" I get the categories: ""cute"", ""baby"", ""wild"", ""farm"", ""zoo"", ""clipart"".If I go into ""wild"", I then have subcategories: ""forest"", ""baby"", ""africa"", ""clipart"", ""rainforest"", ""domestic"".";[education, open-source];120;
595;1;2014-06-26T12:25:55.663;How to use neural networks with large and variable number of inputs?;I'm new to machine learning, but I have an interesting problem. I have a large sample of people and visited sites. Some people have indicated gender, age, and other parameters. Now I want to restore these parameters to each user.Which way do I look for? Which algorithm is suitable to solve this problem? I'm familiar with Neural Networks (supervised learning), but it seems they don't fit.;[education, open-source];113;
596;1;2014-06-26T13:32:18.050;Network analysis classic datasets;"There are several classic datasets for machine learning classification/regression tasks. The most popular are:Iris Flower Data Set;Titanic Data Set;Motor Trend Cars;etc.But does anyone know similar datasets for networks analysis / graph theory? More concrete - I'm looking for Gold standard datasets for comparing/evaluating/learning:centrality measures;network clustering algorithms.I don't need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.EDIT:It's quite difficult to provide exact features for ""gold standard dataset"", but here are some thoughts. I think, real classic dataset should satisfy these criteria:Multiple references in articles and textbooks;Inclusion in well-known network analysis software packages;Sufficient time of existence;Usage in a number of courses on graph analysis.Concerning my field of interest, I also need labeled classes for vertices and/or precomputed (or predefined) ""authority scores"" (i.e. centrality estimates). After asking this question I continued searching, and here are some suitable examples:Zachary's Karate Club: introduced in 1977, cited more than 1.5k times (according to Google Scholar), vertexes have attribute Faction (which can be used for clustering).Erdos Collaboration Network: unfortunately, I haven't find this network in form of data-file, but it's rather famous, and if someone will enrich network with  mathematicians' specialisations data, it also could be used for testing clustering algorithms.";[education, open-source];130;2
597;2;2014-06-26T13:45:16.573;;"I am not working at Google, but I think it is some sort of recommendation system based on the words which millions of users searched before. So those people who search for ""animals"" often search for ""wild animals"" for example. Like in many online stores they recommend you to buy something in addition to the product you are looking for based on the previous purchases of other users.There are many approaches how to build such recommendation system using machine learning, no one knows for sure what google uses.";;;
598;2;2014-06-26T14:17:44.077;;There exist many possibilities for populating empty gaps on data.Most repeated value: Fill the gaps with the most common value.Create a distribution: Make the histogram and drop values according to that distribution.Create a new label: Since you do not have information, do not assume any value and create another label/category to indicate that value is empty.Create a classifier: Make a relation among the variable with empty gaps and the rest of the data and create a simple classifier. With this, populate the rest of the data.There exist many others, but these are the most common strategies. My suggestion is not to populate and to keep unknown what is unknown.;;;
599;2;2014-06-26T14:21:10.513;;The only thing I know about is benchmark data for Graph Databases, such as Neo4j.You may find links similar to this one:http://istc-bigdata.org/index.php/benchmarking-graph-databases/where you can find data to test network analysis and graph theory.Furthermore, you could play with the API of Twitter/Facebook to collect your own data. This is also a suggestion in case you do not find the data you are looking for.;;;
600;1;2014-06-26T14:39:06.410;How do you create an optimized walk list given longitude and latitude coordinates?;I am working on a political campaign where dozens of volunteers will be conducting door-knocking promotions over the next few weeks. Given a list with names, addresses and long/lat coordinates, what algorithms can be used to create an optimized walk list.;[education, open-source];188;
602;2;2014-06-26T16:19:46.480;;"I thought of expanding a bit on the answer by Stanpol. While recommendation system is one approach of suggesting related queries, one more standard information retrieval based approach is the query expansion technique.Generally speaking, query expansion involves selecting additional terms from the top ranked documents retrieved in response to an initial query. Terms are typically selected by a combination of a term scoring function such as tf-idf and a co-occurrence based measure.For example, in response to a query term ""animal"", a term selection function may choose the term ""zoo"", because""zoo"" may be a dominating term (high tf-idf) in the top (say 10) documents retrieved in response to the query ""animal""""zoo"" may co-occur frequently (in close proximity) with the original query term ""animal"" in these documents";;;
603;2;2014-06-26T18:01:06.443;;"People see something closely related to the Travelling Salesman Problem and think that it can't be solved. A good deal of work has been done on this topic and not all of it indicates that a solution is not available.  Depending on the parameters and the desired solution, you may be able to find something that will work.You may want to give a look at the OpenOpt python library. Another resource to look at would  be the TSP Solver and Generator.If you are using R, there is a TSP package available.Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point.  Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available.  You have a small geographic region and a small set of ""salespeople"", so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop.  In practical terms, you don't need to find the absolutely most optimal strategy.  You just need a very good one.  Pick a TSP package that looks the least overwhelming and give it a go.";;;
604;2;2014-06-26T19:16:31.470;;Google isn't going to give away their proprietary work, but we can speculate.Here's what I can gather from my limited usage:The recommendations do not seem to be user, geography, or history specific.There is never an empty recommendation (one that returns no results)There is not always a recommendation (some searches just return images)The recommendations are not always the same (consecutive searches sometimes return different recommendations)Result ordering shifts regularly (search for a specific image and it won't always be in the same place)Very popular searches seem to be pre-calculated and more static than unpopular searches.Recommendations are not always one additional word, and recommendations do not always include the base query.It seems to me that they do this based on the history of the general end user population, they rotate recommendations when there are many popular ones, and they do some additional processing to determine that the result set is of a reasonable size.I would postulate that it works as follows:Use consecutive search strings from users (short-to-long-tail searches) as training data for a machine-learning algorithm.Run searches that occur > N amount of times a week against that recommendation algorithm.Validate and clean results.Push them out to the general population in rotation/A-B testing.Track click-throughs.Refine results over time.;;;
605;1;2014-06-26T19:23:46.043;How to normalize results of Singular Value Decomposition (SVD) between 0 and 1?;I'm building a recommender system and using SVD as one of the preprocessing techniques.However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.After I take the SVD (A = USV^T), is there a standard way to normalize the matrix 'A' between 0 and 1? Thanks!Edit: I want all of my similarity measurements to give results between 0 and 1 and my normalized euclidean distance in particular fails if the input matrix does not have values between 0 and 1.;[education, open-source];173;
606;2;2014-06-26T20:49:38.470;;"Folks here stated great steps, but I think there are great information at the following link what I do when I get a new data set as told through tweets,It sums up the steps the folks tweeted answering the great @hmason question ""Data people: What is the very first thing you do when you get your hands on a new data set?""Hope it will be useful. ";;;
607;1;2014-06-26T22:17:52.570;Do you need a virtual machine as an instrument for your data science practice?;"I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:datasciencetoolKIT.org -> vm is on vagrant cloud (4 GB) and seems to be more ""hip"" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.datasciencetoolBOX.org -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.";[education, open-source];204;
608;1;2014-06-26T22:58:32.380;Any differences in regularisation in MLP between batch and individual updates?;I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and Multilayer perceptron (MLP) that I put together.Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of lambda * current_weight is added as well before the combined delta is subtracted at the end of the batch, where lambda is the regularisation parameter.My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. Does a smaller regularisation term per item work just as well? For instance lambda * current_weight / N where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.;[education, open-source];62;1
609;2;2014-06-26T23:46:22.487;;"Below is a very good note (page 12) on learning rate in Neural Nets (Back Propagation) by Andrew Ng. You will find details relating to learning rate. http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdfFor your 4th point, you're right that normally one has to choose a ""balanced"" learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get a ""good enough"" learning rate.";;;
611;2;2014-06-27T07:06:14.297;;I had almost the same problem: 'restoring' age, gender, location for social network users. But I used users' ego-networks, not visited sites statistics. And I faced with two almost independent tasks:'Restoring' or 'predicting' data. You can use a bunch of different technics to complete this task, but my vote is for simplest ones (KISS, yes). E.g., in my case, for age prediction, mean of ego-network users' ages gave satisfactory results (for about 70% of users error was less than +/-3 years, in my case it was enough). It's just an idea, but you can try to use for age prediction weighted average, defining weight as similarity measure between visited sites sets of current user and others.Evaluating prediction quality. Algorithm from task-1 will produce prediction almost in all cases. And second task is to determine, if prediction is reliable. E.g., in case of ego network and age prediction: can we trust in prediction, if a user has only one 'friend' in his ego network? This task is more about machine-learning: it's a binary classification problem. You need to compose features set, form training and test samples from your data with both right and wrong predictions. Creating appropriate classifier will help you to filter out unpredictable users. But you need to determine, what are your features set. I used a number of network metrics, and summary statistics on feature of interest distribution among ego-network.This approach wouldn't populate all the gaps, but only predictable ones.;;;
612;2;2014-06-27T07:19:11.420;;Regularization is relevant in per-item learning as well. I would suggest to start with a basic validation approach for finding out lambda, whether you are doing batch or per-item learning. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method. As a side note, I believe the value of lambda should be considered in relation to the updates of the parameter vector, rather than the training set size. For batch training you have one parameter update per dataset pass, while for online one update per sample (regardless of the training set size).I recently stumbled upon this Crossvalidated Question, which seems quite similar to yours. There is a link to a paper about a new SGD algorithm, with some relevant content. It might be useful to take a look (especially pages 1742-1743).;;;
613;1;2014-06-27T12:03:53.357;Difference Between Hadoop Mapreduce(Java) and RHadoop mapreduce;I understand Hadoop MapReduce and its features but I am confused about R MapReduce.One difference I have read is that R utilizes maximum RAM. So do perform parallel processing integrated R with Hadoop.My doubt is:R can do all stats, math and data science related stuff, but why R MapReduce?Is there any new task I can achieve by using R MapReduce instead of Hadoop MapReduce? If yes, please specify.We can achieve the task by using R with Hadoop (directly) but what is the importance of MapReduce in R and how it is different from normal MapReduce?;[education, open-source];402;0
614;2;2014-06-27T12:28:25.467;;To complement what insys said :Regularization is used when computing the backpropagation for all the weights in your MLP.Therefore, instead of computing the gradient in regard to all the input of the training set (batch) you only use some/one item(s) (stochastic or semi-stochastic).You will end up limiting a result of the update in regard to one item instead of all which is also correct.Also, if i remember correctly, Andrew NG used L2-regularization.The /N in lambda * current_weight / N is not mandatory, it just helps rescaling the input. However if you choose not to use it, you will have (in most of the case) to select another value for lambda.You can also use the Grid-search algorithm to choose the best value for lambda (the hyperparameter => the one you have to choose).;;;
615;2;2014-06-27T13:22:22.577;;"One standard reference written from social science perspective is J Scott Long's Limited Dependent Variables book. It goes much deeper than say Tabachnik suggested in another answer: Tabachnik is a cookbook at best, with little to no explanations of the ""why"", and it seems like you would benefit from figuring this out in more detail that can be found in Long's book. Ordinal regression should be covered in most introductory econometrics courses (Wooldridge's Cross-Section and Panel Data is a great graduate-level book), as well as quantitative social science courses (sociology, psychology), although I would imagine that the latter will loop back to Long's book.Given that your number of variables is wa-a-ay lower than the sample size, the R package you should be looking is probably ordinal rather than glmnetcr. Another answer mentioned that you can find this functionality in a more mainstream MASS package.";;;
616;1;2014-06-27T15:58:19.340;Identifying “clusters” or “groups” in a matrix;I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:[A B B C A]  [A A B A A]  [A B B C C]  [A A A A A]  There would be two separate clusters for A, two separate clusters for C, and one cluster for B.The output I'm looking for would ideally assign a unique ID to each cluster, something like this:[1 2 2 3 4]  [1 1 2 4 4]  [1 2 2 5 5]  [1 1 1 1 1]Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I'm convinced it must be out there.;[education, open-source];140;
617;2;2014-06-27T16:40:17.393;;What do you think is distance measure in your case?I assume there are three dimensions here:RowN (row number)ColN (column number)Value (value: A, B or C)That means data you get from 4x5 matrix looks like:Sample1 -> (1, 1, A)Sample2 -> (1, 2, B)...Sample5 -> (1, 5, A)Sample6 -> (2, 1, A)...Sample15 -> (3, 5, C)...Sample20 -> (4, 5, A)Is value scaled? In other words, is A < B < C? If yes, thenyou can replace {A, B, C} with {0, 1, 2} (or may be {10, 11, 12}, if you want this difference be less important than RowN and ColN attributes)normalize your datause, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from stats R packageIn that case the distance between two will be:Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )If value is not scaled (regular categorical variable), use some modifications of K-Means that work with categorical data.So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size.;;;
618;2;2014-06-27T19:18:11.433;;"Although adesantos has already given a good answer, I would like to add a little background information.The name for the problem you are looking at is ""imputation"". As adesantos already said, one of the possibilities is to fit a distribution. For example, you could fit a multivariate Gaussian to the data. You will get the mean only from the samples you know and you calculate the covariances only from the samples you know. You can then use standard MVG results to impute the missing data linearly.This is probably the simplest probabilistic method of imputation and it is already quite involved. If you are a neural networks, a recently proposed method that can do so are deep latent gaussian models by Rezende et al. However, understand the method will require a lot of neural net knowledge, quite some variational Bayes knowledge about Markov chains.Another method, which I have hear to work well is to train a generative stochastic network (Bengio et al). This is done by training a denoising auto encoder on the data you have (neglecting missing values in the reconstruction loss). Say you have a reconstruction function f and a input x. Then you will reconstruct it via x' = f(x). You then reset the values of x' with those you know from x. (I.e. you only keep the values that were missing before reconstruction.) If you do so many times, you are guaranteed to sample from the distribution given the values you know.But in either case, these methods require quite some knowledge about statistics and neural nets.";;;
619;2;2014-06-27T19:37:44.847;;rhadoop (the part you are interested in is now called rmr2) is simply a client API for MapReduce written in R. You invoke MapReduce using R package APIs, and send an R function to the workers, where it is executed by an R interpreter locally. But it is otherwise exactly the same MapReduce.You can call anything you like in R this way, but no R functions are themselves parallelized to use MapReduce in this way. The point is simply that you can invoke M/R from R. I don't think it somehow lets you do anything more magical than that.;;;
620;2;2014-06-27T21:45:23.237;;Yes, it's problematic.  If you oversample the minority, you risk overfitting.  If you undersample the majority, you risk missing aspects of the majority class.  Stratified sampling, btw, is the equivalent to assigning non-uniform misclassification costs.  Alternatives:(1) Independently sampling several subsets from the majority class and making multiple classifiers by combining each subset with all the minority class data, as suggested in the answer from @Debasis and described in this EasyEnsemble paper, (2) SMOTE (Synthetic Minority Oversampling Technique) or SMOTEBoost, (combining SMOTE with boosting) to create synthetic instances of the minority class by making nearest neighbors in the feature space.  SMOTE is implemented in R in the DMwR package.;;;
621;2;2014-06-28T01:07:27.263;;There is a bunch of datasets made free by UC Irvine to play with here. Among those datasets, there are a few dozen textual datasets that might help you guys with your task.Those are kind of generic datasets, so depending on your purpose they should not be used as the only data to train your models, or else your model -- while it might work -- will not produce quality results.;;;
622;2;2014-06-28T14:34:17.487;;"I'm not sure if your question classifies as a clustering problem. In clustering you are trying to discover clusters of similar examples using unlabelled data. Here, it seems you wish to enumerate existing ""clusters"" of nearby nodes. To be honest, I have no idea of such a function in R. But, as far as the algorithm is concerned, I believe what you are looking for is Connected-Component Labeling. Kind of a bucket fill, for matrices.The wikipedia article is linked above. One of the algorithms presented there, termed as single-pass algorithm, is as follows:One-Pass(Image)        [M, N]=size(Image);        Connected = zeros(M,N);        Mark = Value;        Difference = Increment;        Offsets = [-1; M; 1; -M];        Index = [];        No_of_Objects = 0;    for i: 1:M :       for j: 1:N:            if(Image(i,j)==1)                             No_of_Objects = No_of_Objects +1;                             Index = [((j-1)*M + i)];                            Connected(Index)=Mark;                             while ~isempty(Index)                                      Image(Index)=0;                                      Neighbors = bsxfun(@plus, Index, Offsets');                      Neighbors = unique(Neighbors(:));                                      Index = Neighbors(find(Image(Neighbors)));                                                      Connected(Index)=Mark;                 end                             Mark = Mark + Difference;            end      end  endI guess it'd be easy to roll your own using the above.";;;
623;1;2014-06-28T14:57:18.177;Measuring performance of different classifiers with different sample sizes;"I'm currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.I'm wondering if there's a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that's being classified?Currently, I'm using precision/recall as a measure of performance, so might have something like:                    Precision RecallPerson classifier   65%       40%Company classifier  98%       90%Cheese classifier   10%       50%Egg classifier      100%      100%However, the dataset I'm running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?Let's say we had hundreds of such classifiers, I guess I'm looking for a good way to answer questions like ""Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they're underperforming?"". ";[education, open-source];133;
624;2;2014-06-29T07:06:00.430;;In most cases a practicing data scientist creates his own working environment on personal computed installing preferred software packages. Normally it is sufficient and efficient use of computing resources, because to run a virtual machine (VM) on your main machine you have to allocate a significant portion of RAM for it. The software will run noticeably slower on both the main and the virtual machine unless a lot of RAM. Due to this impact on speed it is not common to use VMs as main working environment but they are a good solution in several cases when there is a need of additional working environment. The VMs be considered when:There is a need to easily replicate a number of identical computingenvironments when teaching a course or doing a presentation on aconference.There is a need to save and recreate an exact environment for an experiment or a calculation.There is a need to run a different OS or to test a solution on a tool that runs on a different OS. One wants to try out a bundle of software tools before installingthem on the main machine. E.g. there is an opportunity to instal an instance of Hadoop (CDH) on a VM during an Intro to Hadoop course on Udacity. VMs are sometimes used for fast deployment in the cloud like AWS EC, Rackspace etc.The VMs mentioned in the original question are made as easily installable data science software bundles. There are more than these two. This blog post by Jeroen Janssens gives a comparison of at least four:Data Science Toolbox Mining the Social Web Data Science Toolkit Data Science Box  ;;;
625;2;2014-06-29T12:48:08.840;;Maybe you can check here - http://snap.stanford.edu/data/For each data set you will also see references of the works where they have been used;;;
626;2;2014-06-29T21:11:47.727;;In my opinion, it is difficult to compare the performance when there is such a big difference of size. On this link, (please check it out here in Wikipedia http://en.wikipedia.org/wiki/Effect_size), you may see different strategies.The one I suggest is one related to the variance. For instance, consider the performance of the classifier (100%) and the person classifier (65%). The minimum error you commit with the former classifier is 100%. However, the minimum error you can commit with the latter classifier is 10e-5.So one way to compare classifier is to have on mind this Rule of Three (http://en.wikipedia.org/wiki/Rule_of_three_(statistics) where you can compare the performance and its variability.Other possibility is F-measure which is a combination of Precision and Recall and it is somehow independent to the effect size.;;;
627;2;2014-06-29T22:44:02.560;;"Roughly speaking, over-fitting typically occurs when the ratiois too high.Think of over-fitting as a situation where your model learn the training data by heart instead of learning the big pictures which prevent it from being able to generalized to the test data: this happens when the model is too complex with respect to the size of the training data, that is to say when the size of the training data is to small in comparison with the model complexity.Examples: if your data is in two dimensions, you have 10000 points in the training set and the model is a line, you are likely to under-fit.if your data is in two dimensions, you have 10 points in the training set and the model is 100-degree polynomial, you are likely to over-fit.From a theoretical standpoint, the amount of data you need to properly train your model is a crucial yet far-to-be-answered question in machine learning. One such approach to answer this question is the VC dimension. Another is the bias-variance tradeoff.From an empirical standpoint, people typically plot the training error and the test error on the same plot and make sure that they don't reduce the training error at the expense of the test error:I would advise to watch Coursera' Machine Learning course, section ""10: Advice for applying Machine Learning"".(PS: please go here to ask for TeX support on this SE.) ";;;
628;2;2014-06-29T23:14:39.513;;"A model underfits when it is too simple with regards to the data it is trying to model.One way to detect such situation is to use the bias–variance approach, which can represented like this:Your model is underfitted when you have a high bias.To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.If an algorithm is suffering from high variance:more data will probably helpotherwise reduce the model complexityIf an algorithm is suffering from high bias:increase the model complexityI would advise to watch Coursera' Machine Learning course, section ""10: Advice for applying Machine Learning"", from which I took the above graphs.";;;
629;2;2014-06-30T01:51:11.027;;"Do you need a VM?You need to keep in mind that a virtual machine is a software emulation of your own or another machine hardware configuration that can run an operating systems. In most basic terms, it acts as a layer interfacing between the virtual OS, and your own OS which then communicates with the lower level hardware to provide support to the virtual OS. What this means for you is:ConsHardware SupportA drawback of virtual machine technology is that it supports only the hardware that both the virtual machine hypervisor and the guest operating system support. Even if the guest operating system supports the physical hardware, it sees only the virtual hardware presented by the virtual machine. The second aspect of virtual machine hardware support is the hardware presented to the guest operating system. No matter the hardware in the host, the hardware presented to the guest environment is usually the same (with the exception of the CPU, which shows through). For example, VMware GSX Server presents an AMD PCnet32 Fast Ethernet card or an optimized VMware-proprietary network card, depending on which you choose. The network card in the host machine does not matter. VMware GSX Server performs the translation between the guest environment's network card and the host environment's network card. This is great for standardization, but it also means that host hardware that VMware does not understand will not be present in the guest environment.Performance PenaltyVirtual machine technology imposes a performance penalty from running an additional layer above the physical hardware but beneath the guest operating system. The performance penalty varies based on the virtualization software used and the guest software being run. This is significant.ProsIsolation One of the key reasons to employ virtualization is to isolate applications from each other. Running everything on one machine would be great if it all worked, but many times it results in undesirable interactions or even outright conflicts. The cause often is software problems or business requirements, such as the need for isolated security. Virtual machines allow you to isolate each application (or group of applications) in its own sandbox environment. The virtual machines can run on the same physical machine (simplifying IT hardware management), yet appear as independent machines to the software you are running. For all intents and purposes—except performance, the virtual machines are independent machines. If one virtual machine goes down due to application or operating system error, the others continue running, providing services your business needs to function smoothly.Standardization Another key benefit virtual machines provide is standardization. The hardware that is presented to the guest operating system is uniform for the most part, usually with the CPU being the only component that is ""pass-through"" in the sense that the guest sees what is on the host. A standardized hardware platform reduces support costs and increases the share of IT resources that you can devote to accomplishing goals that give your business a competitive advantage. The host machines can be different (as indeed they often are when hardware is acquired at different times), but the virtual machines will appear to be the same across all of them.Ease of Testing Virtual machines let you test scenarios easily. Most virtual machine software today provides snapshot and rollback capabilities. This means you can stop a virtual machine, create a snapshot, perform more operations in the virtual machine, and then roll back again and again until you have finished your testing. This is very handy for software development, but it is also useful for system administration. Admins can snapshot a system and install some software or make some configuration changes that they suspect may destabilize the system. If the software installs or changes work, then the admin can commit the updates. If the updates damage or destroy the system, the admin can roll them back.  Virtual machines also facilitate scenario testing by enabling virtual networks. In VMware Workstation, for example, you can set up multiple virtual machines on a virtual network with configurable parameters, such as packet loss from congestion and latency. You can thus test timing-sensitive or load-sensitive applications to see how they perform under the stress of a simulated heavy workload.Mobility Virtual machines are easy to move between physical machines. Most of the virtual machine software on the market today stores a whole disk in the guest environment as a single file in the host environment. Snapshot and rollback capabilities are implemented by storing the change in state in a separate file in the host information. Having a single file represent an entire guest environment disk promotes the mobility of virtual machines. Transferring the virtual machine to another physical machine is as easy as moving the virtual disk file and some configuration files to the other physical machine. Deploying another copy of a virtual machine is the same as transferring a virtual machine, except that instead of moving the files, you copy them.Which VM should I use if I am starting out?The Data Science Box or the Data Science Toolbox are your best bets if you just getting into data science. They have the basic software that you will need, with the primary difference being the virtual environment in which each of these can run. The DSB can run on AWS while the DST can run on Virtual Box (which is the most common tool used for VMs).Sourceshttp://www.devx.com/vmspecialreport/Article/30383http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html";;;
630;2;2014-06-30T04:00:35.077;;Here's a crazy idea: talk to the volunteers who know the neighborhoods and who have done door-to-door work before.  Get their advice and ideas.  They will probably have insights that no algorithm will produce, and those modifications will be valuable to any computer-generated route list.  One example: Avoiding crossing heavily traveled streets with slow lights or no lights.  Another example: pairs of volunteers working on opposite sides of the same street will feel safer than a volunteer working that street alone.;;;
634;1;2014-06-30T09:43:01.940;Handling a regularly increasing feature set;I'm working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis. I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.I'm thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How can I choose an algorithm for the overall classifier?;[education, open-source];80;
635;2;2014-06-30T10:47:06.453;;In an ideal world, you retain all of your historical data, and do indeed run a new model with the new feature extracted retroactively from historical data. I'd argue that the computing resource spent on this is quite useful actually. Is it really a problem?Yes, it's a widely accepted technique to build an ensemble of classifiers and combine their results. You can build a new model in parallel just on new features and average in its prediction. This should add value, but, you will never capture interaction between the new and old features this way, since they will never appear together in a classifier. ;;;
636;1;2014-06-30T12:05:38.597;Data preparation and machine learning algorithm for click prediction;I am new to machine learning. I have a task at hand of predicting click probability given user information like city, state, OS version, OS family, device, browser family, browser version, etc. I have been advised to try logit since logit seems to be what MS and Google are using. I have some questions regarding logistic regression:Click and non click is a very very unbalanced class and the simple GLM predictions do not look good. How can I make the data work better with the GLM?All the variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. How can I deal with this distribution of categorical variables?One of the variables that we get is device ID. This is a very unique feature that can be translated to a user's identity. How can I make use of it in logit, or should it be used in a completely different model based on user identity?;[education, open-source];310;
637;2;2014-06-30T13:56:10.753;;Here's an idea that just popped out of the blue – what if you make use of Random Subspace Sampling (as in fact Sean Owen already suggested) to train a bunch of new classifiers every time a new feature appears (using a random feature subset, including the new set of features). You could train those models on a subset of samples as well to save some training time.This way you can have new classifiers possibly taking on both new and old features, and at the same time keeping your old classifiers. You might even, perhaps using a cross validation technique to measure each classifier's performance, be able to kill-off the worst performing ones after a while, to avoid a bloated model.;;;
640;1;2014-06-30T20:51:58.640;Large Scale Personalization - Per User vs Global Models;I'm currently working on a project that would benefit from personalized predictions.  Given an input document, a set of output documents, and a history of user behavior, I'd like to predict which of the output documents are clicked.  In short, I'm wondering what the typical approach to this kind of personalization problem is.  Are models trained per user, or does a single global model take in summary statistics of past user behavior to help inform that decision?  Per user models won't be accurate until the user has been active for a while, while most global models have to take in a fixed length feature vector (meaning we more or less have to compress a stream of past events into a smaller number of summary statistics).  ;[education, open-source];55;
641;1;2014-06-30T21:02:05.053;Dataset for Named Entity Recognition on Informal Text;"I'm currently searching for labeled datasets to use to train a model to extract named entities from informal text (think something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I'm looking for out of domain data that's a bit more ""informal"" than the news articles and journal entries that many of today's state of the art named entity recognition systems are trained on.  Any recommendations?  So far I've only been able to locate 50k tokens from twitter published here: https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt";[education, open-source];579;
642;2;2014-06-30T23:10:53.397;;"The answer to this question is going to vary pretty wildly depending on the size and nature of your data. At a high level, you could think of it as a special case of multilevel models; you have the option of estimating a model with complete pooling (i.e., a universal model that doesn't distinguish between users), models with no pooling (a separate model for each user), and partially pooled models (a mixture of the two). You should really read Andrew Gelman on this topic if you're interested.You can also think of this as a learning-to-rank problem that either tries to produce point-wise estimates using a single function or instead tries to optimize on some list-wise loss function (e.g., NDCG). As with most machine learning problems, it all depends on what kind of data you have, the quality of it, the sparseness of it, and what kinds of features you are able to extract from it. If you have reason to believe that each and every user is going to be pretty unique in their behavior, you might want to build a per-user model, but that's going to be unwieldy fast -- and what do you do when you are faced with a new user?";;;
643;2;2014-07-01T08:04:30.127;;I guess you say that you want to use 3-fold cross-validation because you know something about your data (that using k=10 would cause overfitting? I'm curious to your reasoning). I am not sure that you know this, if not then you can simply use a larger k.If you still think that you cannot use standard k-fold cross-validation, then you could modify the algorithm a bit: say that you split the data into 30 folds and each time use 20 for training and 10 for evaluation (and then shift up one fold and use the first and the last 9 as evaluation and the rest as training). This means that you're able to use all your data.When I use k-fold cross-validation I usually run the process multiple times with a different randomisation to make sure that I have sufficient data, if you don't you will see different performances depending on the randomisation. In such cases I would suggest sampling. The trick then is to do it often enough.;;;
644;1;2014-07-01T13:44:23.290;"Create most ""average"" cosine similarity observation";"For a recommendation system I'm using cosine similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general ""average"" category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.So for example if these were my observations (rows are observations, cols are features):[[0, 0, 0, 1, 1, 1, 0, 1, 0], [1, 0, 1, 0, 0, 0, 1, 0, 0], [1, 1, 1, 1, 0, 1, 0, 1, 1], [0, 0, 1, 0, 0, 1, 0, 1, 0]]A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.[ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosine similarity. Though, at the moment, I'm having trouble reasoning my way through angles between lines in high dimensional space.Any ideas?";[education, open-source];197;
645;2;2014-07-01T17:01:12.263;;You need to look at the confidence interval of the statistic.  This helps measure how much uncertainty in the statistic, which is largely a function of sample size.;;;
646;1;2014-07-01T17:03:31.907;Spatial index for variable kernel nonparametric density;"I'm trying to build a nonparametric density function for a fairly large dataset that can be evaluated efficently, and can be updated efficiently when new points are added. There will only ever be a maximum of 4 independent variables, but we can start off with 2. Lets use a gaussian kernel. Let the result be a probability density function, i.e. its volume will be 1.In each evaluation, we can omit all points for which the evaluation point is outside a certain ellipsoid corresponding to the minimum gaussian value we care about. We can change this threshold for accuracy or performance, and the maximum number of points inside the threshold will depend on the chosen covariance matrix of the kernel. Then, we can evaluate the distribution approximately using the subset of points.If we use a fixed kernel, then we can use the eigenvalues and eigenvectors we get from the covariance matrix to transform each point so that the threshold ellipsoid is a fixed circle. We can then shove all the transformed points into a spatial index, and efficiently find all points within the required radius of the evaluation point.However, we would the kernel to be variable for two reasons. (1) to fit the data better, and (2) because adding or modifying points would require the fixed kernel to be updated, which would mean that the entire data set would need to be reindexed. With a variable kernel, we could make new/updated points only affect the closest points.Specifically, is there a spatial index that can efficiently find ellipses surrounding a given point from a set of around 10 million ellipses of different shapes and sizes?More generally though, does my approach look sound? I am open to answers like ""give up and precalculate a grid of results"". Answers much appreciated!";[education, open-source];19;
648;2;2014-07-01T18:48:13.757;;Singular Value Decomposition is a linear algebraic technique as a result of which the notion of normalization is hard to define. In principle, you can do this normalization by dividing each element A(i,j) of the matrix by the sum (or max) of the elements in that particular (ith) row, i.e. A(i,j) = A(i, j) / \sum_{k=1}^{n} A(i,k)However, a more elegant way to achieve this would be to apply a probabilistic dimensionality reduction technique such as PLSA or LDA. These dimensionality reduction techniques ensure that you always end up with probability values strictly between 0 and 1.  ;;;
649;2;2014-07-01T23:11:35.627;;You are doing the correct thing. Technically, this averaging leads to computing the centroid in the Euclidean space of a set of N points. The centroid works pretty well with cosine similarities (cosine of the angles between normalized vectors), e.g. the Rocchio algorithm.;;;
650;2;2014-07-02T08:40:09.773;;As @Debasis writes, what you are doing is correct. Do not worry about the relative values as the cosine similarity focusses on the angle, not the length of the vector. Note that the averages that you compute actually are the proportion of observations that have that feature in your subset. If the subset is sufficiently large, you can interpret them as probabilities.;;;
651;2;2014-07-02T08:50:55.980;;The number of data in the class is sometimes referred to as the support of the classifier. It tells how much you can trust your result, like a p-value would allow you to trust or distrust some test.One approach you can use is to compute several classifier performance measures, not only precision and recall, but also true positive rate, false positive rate, specificity, sensitivity, positive likelihood, negative likelihood, etc. and see whether they are consistent with one another. If one of the measure maxes out (100%) and the other do not, it is often, in my experience, indicative of something went wrong (e.g. poor support, trivial classifier, biased classifier, etc.). See this for a list of classifier performance measures.;;;
652;2;2014-07-02T08:58:25.413;;Simply, one common approach is to increase the complexity of the model, making it simple, and most probably underfitting at first, and increasing the complexity of the model  until early signs of overfitting are witnessed using a resampling technique such as cross validation, bootstrap, etc.You increase the complexity either by adding parameters (number of hidden neurons for artificial neural networks, number of trees in a random forest) or by relaxing the regularization (often named lambda, or C for support vector machines) term in your model.;;;
653;1;2014-07-02T13:40:27.000;Which non-training classification methods are available?;I am trying to find which classification methods, that do not use a training phase, are available. The scenario is gene expression based classification, in which you have a matrix of gene expression of m genes (features) and n samples (observations).A signature for each class is also provided (that is a list of the features to consider to define to which class belongs a sample).An application (non-training) is the Nearest Template Prediction method. In this case it is computed the cosine distance between each sample and each signature (on the common set of features). Then each sample is assigned to the nearest class (the sample-class comparison resulting in a smaller distance). No already classified samples are needed in this case.A different application (training) is the kNN method, in which we have a set of already labeled samples. Then, each new sample is labeled depending on how are labeled the k nearest samples.Are there any other non-training methods?Thanks;[education, open-source];116;
654;2;2014-07-02T14:14:22.593;;"What you are asking about is Instance-Based Learning. k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is Analogical Modeling, which uses instances as exemplars for comparison with new data.You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are ""training examples"" (labeled instances) but the classifier doesn't learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a ""lazy"" learner.Note that the Nearest Template Prediction method you mention effectively is a form of kNN with k=1 and cosine distance as the distance measure.";;;
655;1;2014-07-02T14:40:55.413;Foreign exchange market forecasting with neural networks;I would like to use ANN to automate trading currencies, preferably USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some papers and done some experiments but without much luck. I would like to get advice from EXPERTS to make this work.Here is what I did so far:I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.Extracted all ticks for the time frame 12PM to 14PM for all days.From this data, created a data set where each entry consists of n bid values in sequence. Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 output neuron. Input layer had linear TF, hidden had log TF and output had linear TF.Trained the network with back propagation with n-125 first and then 10.For both n, the MSE did not drop below 0.5 and stayed at that value during full training. Assuming that this could be due to the time series being totally random, I used the R package to find partial autocorrelation on the data set (pacf). This gave non zero values for 2 and 3 lags only. Question 1: What does this mean exactly? Then I used hurst exponent to evaluate the randomness. In R, hurst(values) showed values above 0.9.Question 2: It is supposed to be nearly random. Should it have a value closer to 0.5?I repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the (n-1)th bid value. It looks like ANN just takes the last bid as the next bid! I tried different network structures (all multilayer perceptions), different training parameters, etc, but results are same.Question 3: How can I improve the accuracy? Are there any other training methods than backpropagation?;[education, open-source];203;
656;2;2014-07-02T15:52:44.470;;Some suggestionsRemove items appearing too infrequently in the data. That will reduce the dimensionality by several orders of magnitude. If a feature occurs less than say 10 times, it's likely that it's not adding any predictive value, and it may lead to overfitting due to low frequencyTry a Linear SVM instead. They handle large dimensional data very well in terms of not overfitting. They also often have the option to assign relative weights to different classes, which may help address your unbalanced problem above. The sklearn svm (which simply wraps some other packages such as libsvm) has this option.Don't use the ID column. Producing a model per user will most probably lead to overfitting. Instead, feed in attributes that describe the user that allows the model to generalize over similar users. You could try fitting a separate model per user, but you need a lot of data per user to do this well.It sounds like you really need to try some feature selection here, to reduce the dimensionality of the problem. But try 1 and 2 first, as they may give you good results sooner (although the end solution may still work better with some good feature selection). Sklearn again has a number of options for feature selection.;;;
657;1;2014-07-02T16:41:47.467;How are selected the features for a decision tree in CART?;Suppose I want to use CART as classification tree (I want a categorical response). I have the training set, and I split it using observation labels.Now, to build the decision tree (classification tree) how are selected the features to decide which label apply to testing observations?Supposing we are working on gene expression matrix, in which each element is a real number, is that done using features that are more distant between classes?;[education, open-source];45;
658;1;2014-07-02T20:54:53.767;Prediction with non-atomic features;"I would like to use non-atomic data, as a feature for a prediction. Suppose I have a Table with these features:- Column 1: Categorical - House- Column 2: Numerical - 23.22- Column 3: A Vector - [ 12, 22, 32 ]- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]- Column 5: A List [ 122, Boolean ]I would like to predict/classify, for instance, Column 2.I am making something to automatically respond to questions, any type of question, like ""Where was Foo Born?"" ...I first make a query to a search engine, then I get some text data as a result, then I do all the parsing stuff (tagging, stemming, parsing, splitting ... )My first approach was to make a table, each row with a line of text and a lot of features, like ""First Word"", ""Tag of First Word"", ""Chunks"", etc...But with this approach I am missing the relationships between the sentences. I would like to know if there is an algorithm that looks inside the tree structures (or vectors) and makes the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement.";[education, open-source];96;
659;2;2014-07-03T08:00:10.823;;As Steve Kallestad has said, this is a TSP problem, and there are wonderful free solvers to find approximate solutions.It may be too much work for what you are looking for, but you may try use one of those solvers in combination with the Google Maps API, to find real walking distances beetwen your coordinates:https://developers.google.com/maps/documentation/directions/#DirectionsRequests(I have never used this API, so I don't know how easy or effective it would be);;;
660;1;2014-07-03T10:49:50.993;Linear Regression in R Mapreduce(RHadoop);"I m new to RHadoop and also to RMR...I had an requirement to write a Mapreduce Job in R Mapreduce. I have Tried writing but While executing this it gives an Error.Tring to read the file from hdfsError:Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  :    hadoop streaming failed with error code 1Code :Sys.setenv(HADOOP_HOME=""/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop"")Sys.setenv(HADOOP_CMD=""/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/bin/hadoop"")Sys.setenv(HADOOP_STREAMING=""/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar"")library(rmr2)library(rhdfs)hdfs.init()day_file = hdfs.file(""/hdfs/bikes_LR/day.csv"",""r"")day_read = hdfs.read(day_file)c = rawToChar(day_read)XtX =  values(from.dfs(    mapreduce(      input = ""/hdfs/bikes_LR/day.csv"",      map=        function(.,Xi){         yi =c[Xi[,1],]         Xi = Xi[,-1]         keyval(1,list(t(Xi)%*%Xi))       },  reduce = function(k,v )  {    vals =as.numeric(v)    keyval(k,sum(vals))  } ,  combine = TRUE)))[[1]]XtY = values(from.dfs(    mapreduce(     input = ""/hdfs/bikes_LR/day.csv"",     map=       function(.,Xi){         yi =c[Xi[,1],]         Xi = Xi[,-1]        keyval(1,list(t(Xi)%*%yi))       },     reduce = TRUE ,     combine = TRUE)))[[1]]solve(XtX,XtY)Input:------------instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,9852,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,8013,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,13494,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,15625,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,16006,2011-01-06,1,0,1,0,4,1,1,0.204348,0.233209,0.518261,0.0895652,88,1518,16067,2011-01-07,1,0,1,0,5,1,2,0.196522,0.208839,0.498696,0.168726,148,1362,15108,2011-01-08,1,0,1,0,6,0,2,0.165,0.162254,0.535833,0.266804,68,891,9599,2011-01-09,1,0,1,0,0,0,1,0.138333,0.116175,0.434167,0.36195,54,768,82210,2011-01-10,1,0,1,0,1,1,1,0.150833,0.150888,0.482917,0.223267,41,1280,1321 Please Suggest me any mistakes.";[education, open-source];615;2
661;1;2014-07-03T14:18:14.387;Cannot make user directory on a new CDH5 installation (Hadoop);I download and installed CDH 5 package succesfully on a single linux node in pseudo-distributed Mode on my CentOS 6.5Starting Hadoop and verifying it is Working Properly as in this linkI succesfully finished the following stepsStep 1: Format the NameNode.Step 2: Start HDFSStep 3: Create the /tmp DirectoryStep 4: Create the MapReduce system directories:Step 5: Verify the HDFS File StructureStep 6: Start MapReducewhile following command in step 7 I get the following error.Step 7: Create User Directories$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopusermkdir: '/user/hadoopuser': No such file or directory(where hadoopuser is my linux login username)If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.How to success the step 7:?Please provide the sloution to procced the remaining installation.;[education, open-source];337;1
662;1;2014-07-03T16:11:22.637;What algorithms should I use to perform job classification based on resume data?;Note that I am doing everything in R. The problem goes as follow: Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. My original idea:  make this a supervised learning problem. Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those unlabelled data, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. UpdateQuestion 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.Is this approach wrong ? Please correct me if you think my approach is wrong. Question 3: The tricky part is: how to identify and extract the keywords ? Using the tm package in R ? what algorithm is the tm   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. Any ideas would be great.;[education, open-source];1468;7
663;2;2014-07-03T17:06:13.143;;"Not enough rep for a comment, but check out: http://www.rdatamining.com/examples/text-miningHere, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, 'programed' and 'programming' could be stemmed to 'program'.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.Example:1) Load libraries and build the example datalibrary(tm)library(SnowballC)doc1 = ""I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C.""job1 = ""Software Engineer""doc2 = ""Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting.""job2 = ""Quality Assurance""doc3 = ""Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases.""job3 = ""Software Engineer""jobInfo = data.frame(""text"" = c(doc1,doc2,doc3),                     ""job"" = c(job1,job2,job3))2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.# Convert to lowercasejobInfo$text = sapply(jobInfo$text,tolower)# Remove PunctuationjobInfo$text = sapply(jobInfo$text,function(x) gsub(""[[:punct:]]"","" "",x))# Remove extra white spacejobInfo$text = sapply(jobInfo$text,function(x) gsub(""[ ]+"","" "",x))# Remove stop wordsjobInfo$text = sapply(jobInfo$text, function(x){  paste(setdiff(strsplit(x,"" "")[[1]],stopwords()),collapse="" "")})# Stem words (Also try without stemming?)jobInfo$text = sapply(jobInfo$text, function(x)  {  paste(setdiff(wordStem(strsplit(x,"" "")[[1]]),""""),collapse="" "")})3) Make a corpus source and document term matrix.# Create Corpus SourcejobCorpus = Corpus(VectorSource(jobInfo$text))# Create Document Term MatrixjobDTM = DocumentTermMatrix(jobCorpus)# Create Term Frequency MatrixjobFreq = as.matrix(jobDTM)Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.Where you go from here is up to you.  You can keep only specific (more common) words and use them as features in your model.  Another way is to keep it simple and have a percentage of words used in each job description, say ""java"" would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'.Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's.";;;
664;1;2014-07-03T17:11:01.770;Fisher Scoring v/s Coordinate Descent for MLE in R;R base function glm() uses Fishers Scoring for MLE, while the glmnet uses the coordinate descent method to solve the same equation ? Coordinate descent is more time efficient than Fisher Scoring as fisher scoring calculates the second order derivative matrix and some other matrix operation which makes it space and time expensive, while coordinate descent can do the same task in O(np) time.Why R base function uses Fisher Scoring or this method has advantage over other optimization methods? What will be comparison between coordinate descent and Fisher Scoring ? I am relatively new to do this field so any help or resource will be helpfu;[education, open-source];36;
665;2;2014-07-03T17:52:00.963;;"At each split point, CART will choose the feature which ""best"" splits the observations. What qualifies as best varies, but generally the split is done so that the subsequent nodes are more homogenous/pure with respect to the target. There are different ways of measuring homogeneity, for example Gini, Entropy, Chi-square. If you are using software, it may allow you to choose the measure of homogenity that the tree algorithm will use.Distance is not a factor with trees - what matters is whether the value is greater than or less than the split point, not the distance from the split point.";;;
666;2;2014-07-03T18:36:36.050;;As far as gathering data goes, you can check out Quandl (there's a tutorial on using it with R on DataCamp if you're interested).In addition, Aswath Damodaran's site contains a lot of helpful datasets. Though they aren't updated that frequently, they may still be useful, especially as a benchmark for comparing your own output (from the scripts you will inevitably need to write to calculate the necessary metrics).  And, again, Quant SE is probably a better place to be looking...;;;
667;2;2014-07-03T20:47:20.147;;"This is a tricky problem. There are many ways to handle it. I guess, resumes can be treated as semi-structured documents. Sometimes, it's beneficial to have some minimal structure in the documents. I believe, in resumes you would see some tabular data. You might want to treat these as attribute value pairs. For example, you would get a list of terms for the attribute ""Skill set"".The key idea is to manually configure a list of key phrases such as ""skill"", ""education"", ""publication"" etc. The next step is to extract terms which pertain to these key phrases either by exploiting the structure in some way (such as tables) or by utilizing the proximity of terms around these key phrases, e.g. the fact that the word ""Java"" is in close proximity to the term ""skill"" might indicate that the person is skilled in Java.After you extract these information, the next step could be to build up a feature vector for each of these key phrases. You can then represent a document as a vector with different fields (one each for a key phrase). For example, consider the following two resumes represented with two fields, namely project and education.Doc1: {project: (java, 3) (c, 4)}, {education: (computer, 2), (physics, 1)}Doc2: {project: (java, 3) (python, 2)}, {education: (maths, 3), (computer, 2)}In the above example, I show a term with the frequency. Of course, while extracting the terms you need to stem and remove stop-words. It is clear from the examples that the person whose resume is Doc1 is more skilled in C than that of D2. Implementation wise, it's very easy to represent documents as field vectors in Lucene.Now, the next step is to retrieve a ranked list of resumes given a job specification. In fact, that's fairly straight forward if you represent queries (job specs) as field vectors as well. You just need to retrieve a ranked list of candidates (resumes) using Lucene from a collection of indexed resumes.";;;
668;2;2014-07-03T20:51:59.510;;I've done some research in this area. I've found first order Markov chains work well for predicting within game scoring dynamics across a variety of sports.You can read in more detail here:http://www.epjdatascience.com/content/3/1/4;;;
669;2;2014-07-03T23:00:42.350;;The results you're seeing aren't a byproduct of your training product, but rather that neural nets are not a great choice for this task. Neural nets are effectively a means to create a high order non-linear function by composing a number of simpler functions. This is often a really good thing, because it allows neural nets to fit very complex patterns.However, in a stock exchange any complex pattern, when traded upon will quickly decay. Detecting a complicated pattern will generally not generate useful results, because it is typically complex patterns in the short term. Additionally, depending on the metric you choose, there are a number of ways of performing well that actually won't pay off in investing (such as just predicting the last value in your example).In addition the stock market is startlingly chaotic which can result in a neural net overfitting. This means that the patterns it learns will generalize poorly. Something along the lines of just seeing a stock decrease over a day and uniformly deciding that the stock will always decrease just because it was seen on a relatively short term. Instead techniques like ridge and robust regression, which will identify more general, less complex patterns, do better.The winner of a similar Kaggle competition used robust regression for this very reason. You are likely to see better results if you switch to a shallow learning model that will find functions of a lower polynomial order, over the deep complex functions of a neural net.;;;
670;1;2014-07-04T02:31:42.080;Dealing with events that have not yet happened when building a model;I was building a model that predicts user churn for a website, where I have data on all users, both past and present.I can build a model that only uses those users that have left, but then I'm leaving 2/3 of the total user population unused.Is there a good way to incorporate data from these users into a model from a conceptual standpoint?;[education, open-source];53;
671;1;2014-07-04T05:12:44.707;Linearly increasing data with manual reset;"I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a Simple Linear Regression algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120.All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case.I'm new to data science, would appreciate any pointers to move further.Edit: nfmcclure's suggestions appliedBefore applying the suggestionsBelow is the snapshot of what I've got after splitting the dataset where the reset occurs, and the slope of two set.finding the mean of the two slopes and drawing the line from the mean.Is this OK?";[education, open-source];110;
672;2;2014-07-04T06:46:19.180;;This setting is common in reliability, health care, and mortality. The statistical analysis method is called Survival Analysis. All users are coded according to their start date (or week or month).  You use the empirical data to estimate the survival function, which is the probability that the time of defection is later than some specified time t.Your baseline model will estimate survival function for all users.  Then you can do more sophisticated modeling to estimate what factors or behaviors might predict defection (churn), given your baseline survival function.  Basically, any model that is predictive will yield a survival probability that is significantly lower than the baseline.There's another approach which involves attempting to identify precursor events patterns or user behavior pattern that foreshadow defection. Any given event/behavior pattern might occur for users that defect, or for users that stay. For this analysis, you may need to censor your data to only include users that have been members for some minimum period of time. The minimum time period can be estimated using your estimate of survival function, or even simple histogram analysis of the distribution of membership period for users who have defected. ;;;
675;2;2014-07-04T22:46:32.130;;"Just extract keywords and train a classifier on them. That's all, really. Most of the text in CVs is not actually related to skills. E.g. consider sentence ""I'm experienced and highly efficient in Java"". Here only 1 out of 7 words is a skill name, the rest is just a noise that's going to put your classification accuracy down. Most of CVs are not really structured. Or structured too freely. Or use unusual names for sections. Or file formats that don't preserve structure when translated to text. I have experience extracting dates, times, names, addresses and even people intents from unstructured text, but not a skill (or university or anything) list, not even closely. So just tokenize (and possibly stem) your CVs, select only words from predefined list (you can use LinkedIn or something similar to grab this list), create a feature vector and try out a couple of classifiers (say, SVM and Naive Bayes). (Note: I used a similar approach to classify LinkedIn profiles into more than 50 classes with accuracy > 90%, so I'm pretty sure even naive implementation will work well.)";;;
676;2;2014-07-05T14:48:10.570;;"In general regression models (any) can behave in an arbitrary way beyond the domain spanned by training samples. In particular, they are free to assume linearity of the modeled function, so if you for instance train a regression model with points:X     Y10    020    130    2it is reasonable to build a model f(x) = x/10-1, which for x<10 returns negative values.The same applies ""in between"" your data points, it is always possible that due to the assumed famility of functions (which can be modeled by particular method) you will get values ""out of your training samples"". You can think about this in another way - ""what is so special about negative values?"", why do you find existance of negative values weird (if not provided in training set) while you don't get alarmed by existance of lets say... value 2131.23? Unless developed in such a way, no model will treat negative values ""different"" than positive ones. This is just a natural element of the real values which can be attained as any other value.";;;
677;1;2014-07-05T15:01:43.940;Struggling to integrate sklearn and pandas in simple Kaggle task;"I'm trying to use the sklearn_pandas module to extend the work I do in pandas and dip a toe into machine learning but I'm struggling with an error I don't really understand how to fix.I was working through the following dataset on Kaggle.It's essentially an unheadered table (1000 rows, 40 features) with floating point values.import pandas as pdfrom sklearn import neighborsfrom sklearn_pandas import DataFrameMapper, cross_val_scorepath_train =""../kaggle/scikitlearn/train.csv""path_labels =""../kaggle/scikitlearn/trainLabels.csv""path_test = ""../kaggle/scikitlearn/test.csv""train = pd.read_csv(path_train, header=None)labels = pd.read_csv(path_labels, header=None)test = pd.read_csv(path_test, header=None)mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])mapper_trainOutput:DataFrameMapper(features=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',       n_neighbors=3, p=2, weights='uniform'))])So far so good. But then I try the fitmapper_train.fit_transform(train, labels)Output:---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)<ipython-input-6-e3897d6db1b5> in <module>()----> 1 mapper_train.fit_transform(train, labels)//anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)    409         else:    410             # fit method of arity 2 (supervised transformation)--> 411             return self.fit(X, y, **fit_params).transform(X)    412     413 //anaconda/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)    116         for columns, transformer in self.features:    117             if transformer is not None:--> 118                 transformer.fit(self._get_col_subset(X, columns))    119         return self    120 TypeError: fit() takes exactly 3 arguments (2 given)`What am I doing wrong? While the data in this case is all the same, I'm planning to work up a workflow for mixtures categorical, nominal and floating point features and sklearn_pandas seemed to be a logical fit.";[education, open-source];588;
678;1;2014-07-05T16:10:21.580;What are some standard ways of computing the distance between documents?;"When I say ""document"", I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter.";[education, open-source];1932;7
679;1;2014-07-05T16:20:17.963;What are some standard ways of computing the distance between individual search queries?;"I made a similar question asking about distance between ""documents"" (Wikipedia articles, news stories, etc.).  I made this a separate question because search queries are considerably smaller than documents and are considerably noisier.  I hence don't know (and doubt) if the same distance metrics would be used here.  Either vanilla lexical distance metrics or state-of-the-art semantic distance metrics are preferred, with stronger preference for the latter.";[education, open-source];96;1
680;2;2014-07-06T00:01:41.327;;"There's a number of semantic distance measures, each with its pros and cons. Here are just a few of them: cosine distance, inner product between document feature vectors;LSA, another vector-based model, but utilizing SVD for de-noising original term-document matrix;WordNet-based, human verified, though hardly extensible.Start with a simplest approach and then move further based on issues for your specific case. ";;;
682;2;2014-07-06T14:01:57.467;;"Michael Maouboussin, in his book, ""The Success Equation,"" looks at differentiating luck from skill in various endeavors, including sports.  He actually ranks sports by the amount of luck that contributes to performance in the different sports (p. 23) and about 2/3 of performance in football is attributable to skill.  By contrast, I used MM's technique to analyze performance in Formula 1 racing, and found that 60% is attributable to skill (less than I was expecting.)That said, it seems this kind of analysis would imply that a sufficiently detailed and crafted feature set would allow ML algorithms to predict performance of NFL teams, perhaps even to the play level, with the caveat that significant variance will still exist because of the influence of luck in the game.";;;
683;1;2014-07-07T09:34:36.950;Build a binary classifier with only positive and unlabeled data;I have 2 datasets, one with positive instances of what I would like to detect, and one with unlabeled instances. What methods can I use ?As an example, suppose we want to understand detect spam email based on a few structured email characteristics. We have one dataset of 10000 spam emails, and one dataset of 100000 emails for which we don't know whether they are spam or not.How can we tackle this problem (without labeling manually any of the unlabeled data) ?What can we do if we have additional information about the proportion of spam in the unlabeled data (i.e. what if we estimate that between 20-40% of the 100000 unlabeled emails are spam) ?;[education, open-source];85;2
684;2;2014-07-07T10:25:55.323;;"I have never used sklearn_pandas, but from reading their source code, it looks like this is a bug on their side. If you look for the function that is throwing the exception, you can notice that they are discarding the y argument (it does not even survive until the docstring), and the inner fit function expects one argument more, which is probably y:def fit(self, X, y=None):    '''    Fit a transformation from the pipeline    X       the data to fit    '''    for columns, transformer in self.features:        if transformer is not None:            transformer.fit(self._get_col_subset(X, columns))    return selfI would recommend that you open an issue in their bug tracker.UPDATE:You can test this if you run your code from IPython. To summarize, if you use the %pdb on magic right before you run the problematic call, the exception is captured by the Python debugger, so you can play around a bit and see that calling the fit function with the label values y[0] does work  -- see the last line with the pdb> prompt. (The CSV files are downloaded from Kaggle, except for the largest one which is just a part of the real file).In [1]: import pandas as pdIn [2]: from sklearn import neighborsIn [3]: from sklearn_pandas import DataFrameMapper, cross_val_scoreIn [4]: path_train =""train.csv""In [5]: path_labels =""trainLabels.csv""In [6]: path_test = ""test.csv""In [7]: train = pd.read_csv(path_train, header=None)In [8]: labels = pd.read_csv(path_labels, header=None)In [9]: test = pd.read_csv(path_test, header=None)In [10]: mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])In [13]: %pdb onIn [14]: mapper_train.fit_transform(train, labels)---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)<ipython-input-14-e3897d6db1b5> in <module>()----> 1 mapper_train.fit_transform(train, labels)/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)    409         else:    410             # fit method of arity 2 (supervised transformation)--> 411             return self.fit(X, y, **fit_params).transform(X)    412    413/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)    116         for columns, transformer in self.features:    117             if transformer is not None:--> 118                 transformer.fit(self._get_col_subset(X, columns))    119         return self    120TypeError: fit() takes exactly 3 arguments (2 given)> /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.py(118)fit()    117             if transformer is not None:--> 118                 transformer.fit(self._get_col_subset(X, columns))    119         return selfipdb> l    113    114         X       the data to fit    115         '''    116         for columns, transformer in self.features:    117             if transformer is not None:--> 118                 transformer.fit(self._get_col_subset(X, columns))    119         return self    120    121    122     def transform(self, X):    123         '''ipdb> transformer.fit(self._get_col_subset(X, columns), y[0])KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',           n_neighbors=3, p=2, weights='uniform')";;;
685;1;2014-07-07T11:43:48.430;Stochastic gradient descent in logistic regression;"I am very new to machine learning and in my first project have stumbled across a lot of issues which I really want to get through.I'm using logistic regression with R's glmnet package and alpha = 0 for ridge regression.I'm using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn't much of a difference (0.61).My dependent variable/output is probability of click, based on if there is a click or not in historical data.The independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version and OS family.Of these, for prediction I'm using state, device, user age, user gender, IP carrier, browser version, browser family, OS version and OS family; I am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. I am not using city because they are too many or mobile manufacturer because they are too few.Is that okay or should I be using the rejected variables?To start, I create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.After training the model, I save the coefficients and intercept. These are used for new incoming requests using the formula for logistic regression: Where a is intercept, k is the ith coefficient and x is the ith variable value.Is my approach correct so far?Simple GLM in R (that is where there is no regularized regression, right?) gave me 0.56 AUC. With regularization I get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn't happen is almost always greater than the max probability that a click happened.So basically what should I do?I have read how stochastic gradient descent is an effective technique in logit so how do I implement stochastic gradient descent in R? If it's not straightforward, is there a way to implement this system in Python? Is SGD implemented after generating a regularized logistic regression model or is it a different process altogether?Also there is an algorithm called follow the regularized leader (FTRL) that is used in click-through rate prediction. Is there a sample code and use of FTRL that I could go through?";[education, open-source];376;
686;2;2014-07-07T13:23:28.220;;"State of the art appears to be ""paragraph vectors"" introduced in a recent paper: http://cs.stanford.edu/~quocle/paragraph_vector.pdf. Cosine/Euclidean distance between paragraph vectors would likely work better than any other approach. This probably isn't feasible yet due to lack of open source implementations.Next best thing is cosine distance between LSA vectors or cosine distance between raw BOW vectors. Sometimes it works better to choose different weighting schemes, like TF-IDF. ";;;
687;2;2014-07-07T14:15:51.117;;My suggestion would be to attempt to build some kind of clustering on your unlabeled data that somewhat approximates a labelled dataset. The rationale is more or less as follows:You have some feature vector for representing your documentsBased on that feature vector, you can come up with a number of different clusterings, with either fuzzy, rough, or class-based clustering methodsKnowing what a positive example looks like, you can quickly evaluate the overall similarity of a cluster to your positive clusterKnowing that there should really only be two clusters, you can adjust the hyperparameters on your clustering method so that the above two metrics are closer and closer to satisfactionWith the two clusters, you have what is likely a close approximation of a labelled dataset, which you can then use as a silver-standard corpus of sorts to actually train your modelHope that makes sense, if you're specifically looking for clustering algorithms, a few that I personally enjoy that might be good in this scenario are FLAME and tsne. Alternately, looking at the spectacular gensim library in python will get you a long way toward the clustering you're looking for.Hope that helps and makes sense, leave a comment if you've got any questions.;;;
688;2;2014-07-07T14:41:59.523;;"As @SpacedMan has noted in a comment, the street layout will have a massive influence on the optimization of the walk list.  You have included only ""latitude and longitude"" in your question's title; but solving that problem does not lead to a ""walk list"", but to a ""as-the-crow-flies list"".Looking at your street layout as a graph, with edge weights describing distances, and trying to find the shortest traversal between all required addresses, will lead you to think of your problem as a ""Shortest path problem"". Dijkstra's algorithm is the best known solution (there are others); in its naive implementation it converges in O(n2), which may be acceptable if your lists of addresses are moderate in size.  Otherwise, look for optimized versions in the above links.As for libraries and resources to start tackling the problem, since you do not specify languages or platforms, let me point to the compilation of routing solvers in the Open Street Maps wiki and in general their frameworks and libraries page.";;;
689;2;2014-07-07T15:36:40.960;;There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.Cosine Distance - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.Levenshtein Distance - Also known as edit distance, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thingLSA - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called topic modeling. LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementationsLDA - Is also a technique used for topic modeling, but it's different from LSA in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from LDA are better for modeling document similarity than LSA, but not quite as good for learning how to discriminate strongly between topics.Pachinko Allocation - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of LDA, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come byword2vec - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as Count Vectorizers and TF-IDF. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.doc2vec - Also known as paragraph vectors, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The gensim library in python has an implementation of word2vec that is straightforward enough that it can pretty reasonably be leveraged to build doc2vec, but make sure to keep the license in mind if you want to go down this routeHope that helps, let me know if you've got any questions.;;;
690;2;2014-07-07T18:06:24.583;;Train 2 generative models, one for each dataset (spam only, spam plus ham), that will give you the probability that a datapoint is drawn from the same probability distribution of the training data. Assign emails as spam or ham based on which model gives you the highest probability of the document arising from the training data used to train it. Example generative models are RBM's, autoencoders (in that case, which model has the lowest reconstruction error). There are likely some bayesian generative models also that will assign a probability to a data point based on some training data.The best option though would be to take time to curate a second dataset containing only ham. That will give you higher classification accuracy. Assuming a lower proportion of spam to ham emails, that should not be too hard. You can even use Mechanical Turk if you lack the time or resources (or interns \ graduates students or other cheap labor).;;;
691;2;2014-07-07T18:20:36.090;;Empirically I have found LSA vastly superior to LDA every time and on every dataset I have tried it on. I have talked to other people who have said the same thing. It's also been used to win a number of the SemEval competitions for measuring semantic similarity between documents, often in combinations with a wordnet based measure, so I wouldn't say it's going out of fashion, or is definitely inferior to LDA, which is better for topic modelling and not semantic similarity in my experience, contrary to what some responders have stated. If you use gensim (a python library), it has LSA, LDA and word2vec, so you can easily compare the 3. doc2vec is a cool idea, but does not scale very well and you will likely have to implement it yourself as I am unaware of any open source implementations. It does not scale well as for each document, a new and separate model has to be built using SGD, a slow machine learning algorithm. But it will probably give you the most accurate results. LSA and LDA also don't scale well (word2vec does however), LDA scales worse in general. Gensim's implementations are very fast however, as it uses iterative SVD.One other note, if you use word2vec, you will still have to determine a way to compose vectors from documents, as it gives you a different vector per word. The simplest way to do this is to normalize each vector and take the mean over all word vectors in the document, or take a weighted mean by idf weighting of each word. So it's not as simple as 'use word2vec', you will need to do something further to compute document similarity. I would personally go with LSA, as I have seen it work well empirically, and gensim's library scales very well. However, there's no free lunch, so preferably try each method and see which works better for your data.;;;
692;2;2014-07-07T18:28:50.420;;The cosine similarity metric does a good (if not perfect) job of controlling for the document length, so comparing the similarity of 2 documents or 2 queries using the cosine metric and tf idf weights for the words should work well in either case. I would also recommend doing LSA first on tf idf weights, and then computing the cosine distance\similarities.If you are trying to build a search engine, I would recommend using a free open source search engine like solr or elastic search, or just the raw lucene libraries, as they do most of the work for you, and have good built in methods for handling the query to document similarity problem.;;;
693;2;2014-07-07T18:36:23.430;;I work for an online jobs site and we build solutions to recommend jobs based on resumes. Our approach take's a person's job title (or desired job title if a student and known), along with skills we extract from their resume, and their location (which is very important to most people) and find matches with jobs based on that. in terms of document classification, I would take a similar approach. I would recommend computing a tf idf matrix for each resume as a standard bag of words model, extracting just the person's job title and skills (for which you will need to define a list of skills to look for), and feed that into a ML algorithm. I would recommend trying knn, and an SVM, the latter works very well with high dimensional text data. Linear SVM's tend to do better than non-linear (e.g. using RBf kernels). If you have that outputting reasonable results, I would then play with extracting features using a natural language parser \ chunker, and also some custom built phrases matched by regex's.;;;
694;1;2014-07-07T19:17:04.973;Best python library for neural networks;I'm using Neural Networks to solve different Machine learning problems. I'm using Python and pybrain but this library is almost discontinued. Are there other good alternatives in Python?Thanks;[education, open-source];10064;19
695;2;2014-07-07T19:55:51.057;;Pylearn2 is generally considered the library of choice for neural networks and deep learning in python.  Its designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders are provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for Pylearn2 is available on github.Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes.UPDATE: the landscape has changed quite a bit since I answered this question in July '14, and some new players have entered the space.  In particular I would recommend checking out:Lasagne: https://github.com/Lasagne/LasagneKeras: https://github.com/fchollet/kerasDeepy: https://github.com/uaca/deepyNolearn: https://github.com/dnouri/nolearnBlocks: https://github.com/mila-udem/blocksThey each have their strengths and weaknesses, so give them all a go and see which best suits your use case.;;;
697;1;2014-07-07T21:26:36.830;Running huge datasets with R;I'm trying to run some analysis with some big datasets (eg 400k rows vs. 400 columns) with R (e.g. using neural networks and recommendation systems).But, it's taking too long to process the data (with huge matrices, e.g. 400k rows vs. 400k columns).What are some free/cheap ways to improve R performance?I'm accepting packages or web services suggestions (other options are welcome).;[education, open-source];148;2
698;2;2014-07-08T02:08:04.560;;They can't predict, but they can tell you the most likely result. There's an study about this kind of approach from Etienne - Predicting Who Will Win the World Cup with Wolfram Language. This is a very detailed study, so you can check all the methodology used to get the predictions. Interesting enough, 11 from 15 matches were correct! As one might expect, Brazil is the favorite, with a probability to win of 42.5%. This striking result is due to the fact that Brazil has both the highest Elo ranking and plays at home.(Let's go Brazil!);;;
699;2;2014-07-08T06:40:36.923;;"From my experience only some classes of queries can be classified on lexical features (due to ambiguity of natural language). Instead you can try to use boolean search results (sites or segments of sites, not documents, without ranking) as features for classification (instead on words). This approach works well in classes where there is a big lexical ambiguity in a query but exists a lot of good sites relevant to the query (e.g. movies, music, commercial queries and so on).Also, for offline classification you can do LSI on query-site matrix. See ""Introduction to Information Retrieval"" book for details.";;;
700;1;2014-07-08T07:29:34.167;Efficient dynamic clustering;I have a set of datapoints from the unit interval (i.e. 1-dimensional dataset with numerical values). I receive some additional datapoints online, and moreover the value of some datapoints might change dynamically. I'm looking for an ideal clustering algorithm which can handle these issues efficiently.I know sequential k-means clustering copes with the addition of new instances, and I suppose with minor modification it can work with dynamic instance values (i.e. first taking the modified instance from the respective cluster, then updating the mean of the cluster and finally giving the modified instance as an input to the algorithm just as the addition of an unseen instance).My concern with using the k-means algorithm is the requirement of supplying the number of clusters as an input. I know that they beat other clustering algorithms (GAs, MSTs, Hierarchical Methods etc.) in time&space complexity. Honestly I'm not sure, but maybe I can get away with using one of the aforementioned algorithms. Even that my datasets are relatively large, the existence of a single dimension makes me wonder.More specifically a typical test case of mine would contain about 10K-200K 1-dimensional datapoints. I would like to complete the clustering preferably under a second. The dynamic changes in the value points are assumed to be smooth, i.e. relatively small. Thus being able to use existing solutions (i.e. being able to continue clustering on the existing one when a value is changed or new one is added) is highly preferred. So all in all: Can you think of an algorithm which will provide a sweet spot between computational efficiency and the accuracy of clusters wrt. the problem defined above? Are there some nice heuristics for the k-means algorithm to automatically compute the value of K beforehand?;[education, open-source];100;2
701;1;2014-07-08T07:37:57.123;Clustering pair-wise distance dataset;I have generated a dataset of pairwise distances as follows:id_1 id_2 dist_12id_2 id_3 dist_23I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven't been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?;[education, open-source];119;
702;2;2014-07-08T09:18:17.990;;"In the scikit-learn implementation of Spectral clustering and DBSCAN you do not need to precompute the distances, you should input the sample coordinates for all id_1 ... id_n.  Here is a simplification of the documented example comparison of clustering algorithms:import numpy as npfrom sklearn import clusterfrom sklearn.preprocessing import StandardScaler## Prepare the dataX = np.random.rand(1500, 2)# When reading from a file of the form: `id_n coord_x coord_y`# you will need this call instead:# X = np.loadtxt('coords.csv', usecols=(1, 2))X = StandardScaler().fit_transform(X)## Instantiate the algorithmsspectral = cluster.SpectralClustering(n_clusters=2,                                      eigen_solver='arpack',                                      affinity=""nearest_neighbors"")dbscan = cluster.DBSCAN(eps=.2)## Use the algorithmsspectral_labels = spectral.fit_predict(X)dbscan_labels = dbscan.fit_predict(X)";;;
703;2;2014-07-08T10:36:44.220;;Pylearn is relies on Theano and as mentioned in other answer to use the library is really complicated, until you get the hold of it.In the meantime I would suggest using Theanets. It aslo built on top of Theano, but is much more easier to work with. It might be true, that it doesn't have all the features of Pylearn, but for the basic work it's sufficient.Also it's open source, so you can add custom networks on the fly, if you dare. :);;;
704;1;2014-07-08T13:45:07.583;Working with HPC clusters;In my university, we have an HPC computing cluster. I use the cluster to train classifiers and so on. So, usually, to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash script that contains (among others) a command like qsub script.py.However, I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository, so I get the same python script there. Then I write that Bash script or edit it, so I can run the bash script.As you see this is really frustrating since, for every little update for the python script, I need to do many steps to have it executed at the computing cluster. Of course the task gets even more complicated when I have to put the data on the server and use the datasets' path on the server.I'm sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?;[education, open-source];133;
705;2;2014-07-08T14:32:55.827;;"Stochastic gradient descent is a method of setting the parameters of the regressor; since the objective for logistic regression is convex (has only one maximum), this won't be an issue and SGD is generally only needed to improve convergence speed with masses of training data.What your numbers suggest to me is that your features are not adequate to separate the classes. Consider adding extra features if you can think any any that are useful. You might also consider interactions and quadratic features in your original feature space.";;;
707;2;2014-07-09T00:37:51.167;;Try a recurrent neural network, a model well suited for time series data. They're notoriously difficult to train, but seem to perform well when trained properly: http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf;;;
708;2;2014-07-09T00:44:28.207;;There are many solutions to ease the burden of copying the file from a local machine to the computing nodes in the clusters. A simple approach is to use an interface that allows multi-access to the machines in the cluster, like clusterssh (cssh). It allows you to type commands to multiple machines at once via a set of terminal screens (each one a ssh connection to a different machine in the cluster).Since your cluster seem to have qsub set up, your problem may be rather related to replicating the data along the machines (other than simply running a command in each node). So, to address this point, you may either write an scp script, to copy things to and from each node in the cluster (which is surely better addressed with SVN), or you may set up a NFS. This would allow for a simple and transparent access to the data, and also reduce the need for replicating unnecessary data.For example, you could access a node, copy the data to such place, and simply use the data remotely, via network communication. I'm not acquainted with how to set up a NFS, but you already have access to it (in case your home folder is the same across the machines you access). Then, the scripts and data could be sent to a single place, and later accessed from others. This is akin to the SVN approach, except it's more transparent/straightforward.;;;
709;2;2014-07-09T07:08:34.740;;Since you mention you are building a recommendation system, I believe you have a sparse matrix which you are working on. Check sparseMatrix from Matrix package. This should be able to help you with storing your large size matrix in memory and train your model. ;;;
710;1;2014-07-09T11:05:40.813;Commercial Text Summarization Tools;I'm looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:Extractive Multi-Document Summarization (Generic or query-based)Extractive Single-Document Summarization (Generic or query-based)Generative Single-Document Summarization (Generic or query-based)Generative Multi-Document Summarization (Generic or query-based);[education, open-source];154;2
711;1;2014-07-09T12:22:22.400;"Are Support Vector Machines still considered ""state of the art"" in their niche?";"This question is in response to a comment I saw on another question.The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of ""SVMs are not used so much nowadays"".I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a ""niche"" covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's ""sweet spot"" just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?I tried a search for e.g. ""are support vector machines out of fashion"" and found nothing to imply they were being dropped in favour of anything else.And Wikipedia has this: http://en.wikipedia.org/wiki/Support_vector_machine#Issues . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).";[education, open-source];1388;6
712;2;2014-07-09T13:07:13.303;;"SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.There are some significant disadvantages as well.Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.SVMs generally belong to the class of ""Sparse Kernel Machines"". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the Relevance Vector Machine (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.   A very effective classifier, which is very popular nowadays, is the Random Forest. The main advantages are:Only one parameter to tune (i.e. the number of trees in the forest)Not utterly parameter sensitiveCan easily be extended to multiple classesIs based on probabilistic principles (maximizing mutual information gain with the help of decision trees)";;;
713;1;2014-07-09T17:51:40.583;Cloudera QuickStart VM Error;I have installed cloudera CDH5 Quick start VM on VM player. When I login through HUE in the first page I am the following error“Potential misconfiguration detected. Fix and restart Hue.”How to solve this issue.Thanks,Green;[education, open-source];468;
714;2;2014-07-10T08:38:43.353;;Although your question is not very specific so I'll try to give you some generic solutions. There are couple of things you can do here:Check sparseMatrix from Matrix package as mentioned by @SidhhaTry running your model in parallel using packages like snowfall, Parallel. Check this list of packages on Cran which can help you runnning your model in multicore parallel mode.You can also try data.table package. It is quite phenomenal in speed.Good reads:11 Tips on How to Handle Big Data in R (and 1 Bad Pun)Why R is slow & how to improve its Performance?;;;
715;2;2014-07-10T09:16:39.937;;Go into the other link from home - to the cloudera manager.From there, you'll see Hue can be restarted, but there is probably an alert that needs to be resolved in there first.If I remember right there's some initial configuration that needs to be done on the quickstart VM that's spelled out as soon as you log into the manager application.;;;
716;1;2014-07-10T10:07:13.523;How to choose the features for a neural network?;"I know that there is no a clear answer for this question, but let's suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The ""best"" way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like correlation measures (http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf) etc?";[education, open-source];260;
717;1;2014-07-10T11:55:49.637;How to define a custom resampling methodology;I'm using an experimental design to test the robustness of different classification methods, and now I'm searching for the correct definition of such design.I'm creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. Then, I run each classification method on every subset. Finally, I estimate the accuracy of each method as how many classifications on subsets are in agreement with the classification on the full dataset. For example:Classification-full     1    2    3    2    1    1    2Classification-subset1  1    2         2    3    1   Classification-subset2       2    3         1    1    2...Accuracy                1    1    1    1  0.5    1    1Is there a correct name to this methodology? I thought it can fall under bootstrapping but I'm not sure about this.;[education, open-source];45;1
718;2;2014-07-10T15:43:53.177;;A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.A strong linear correlation between the new feature and the predicted variable is an good sign that a new feature will be valuable, but the absence of a high correlation is not necessary a sign of a poor feature, because neural networks are not restricted to linear combinations of variables.   If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them.;;;
719;2;2014-07-10T22:42:13.720;;nsl-I'm a beginner at machine learning, so forgive the lay-like description here, but it sounds like you might be able to use topic modelling, like latent dirichlet analysis (LDA). It's an algorithm widely used to classify documents, according to what topics they are about, based on the words found and the relative frequencies of those words in the overall corpus.  I bring it up mainly because, in LDA it's not necessary to define the topics in advance.Since the help pages on LDA are mostly written for text analysis, the analogy I would use, in order to apply it to your question, is:- Treat each gene expression, or feature, as a 'word' (sometimes called a token in typical LDA text-classification applications)- Treat each sample as a document (ie it contains an assortment of words, or gene expressions)- Treat the signatures as pre-existing topicsIf I'm not mistaken, LDA should give weighted probabilities for each topic, as to how strongly it is present in each document.;;;
720;2;2014-07-10T23:38:58.153;;There are a couple of open source options I know of - LibOTS - http://libots.sourceforge.net/DocSum - http://docsum.sourceforge.net/docsum/web/about.phpA couple of commercial solutions - Intellix Summarizer Pro - http://summarizer.intellexer.com/order_summarizer_pro.phpCopernic Summarizer - http://www.copernic.com/en/products/summarizer/And this one is a web service - TextTeaser - http://www.textteaser.com/I'm sure there are plenty of others out there.  I have used Copernic a good deal and it's pretty good, but I was hoping it could be automated easily, which it can't - at least it couldn't when I used it.  ;;;
721;2;2014-07-11T01:28:24.957;;When it comes to dealing with many disparate kinds of data, especially when the relationships between them are unclear, I would strongly recommend a technique based on decision trees, the most popular ones today to the best of my knowledge are random forest, and extremely randomized trees.Both have implementations in sklearn, and they are pretty straightforward to use. At a very high level, the reason that a decision tree-based approach is advantageous for multiple disparate kinds of data is because decision trees are largely independent from the specific data they are dealing with, just so long as they are capable of understanding your representation.You'll still have to fit your data into a feature vector of some kind, but based on your example that seems like a pretty straightforward task, and if you're willing to go a little deeper on your implementation you could certainly come up with a custom tree-splitting rule without actually having to change anything in the underlying algorithm. The original paper is a pretty decent place to start if you want to give that a shot.If you want pseudo-structural data from your text data though, I might suggest looking into doc2vec, recently developed by Google. I don't think there are any good open-source implementations now, but it's a pretty straightforward improvement on the word2vec algorithm, which has implementations in at least C and python.Hope that helps! Let me know if you've got any more questions.;;;
722;2;2014-07-11T09:49:32.933;;R contains some standard functions for data manipulation, which can be used for data cleaning, in its base package (gsub, transform, etc.), as well as in various third-party packages, such as stringr, reshape/reshape2, and plyr/dplyr. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.Additionally, R offers some packages specifically focused on data cleaning and transformation:editrules (http://cran.r-project.org/web/packages/editrules/index.html)deducorrect (http://cran.r-project.org/web/packages/deducorrect/index.html)StatMatch (http://cran.r-project.org/web/packages/StatMatch/index.html)MatchIt (http://cran.r-project.org/web/packages/MatchIt/index.html)DataCombine (http://cran.r-project.org/web/packages/DataCombine)data.table (http://cran.r-project.org/web/packages/data.table)A comprehensive and coherent approach to data cleaning in R, including examples and use of editrules and deducorrect packages, as well as a description of workflow (framework) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.;;;
723;2;2014-07-11T11:32:13.550;;Random subsampling seems appropriate, bootstrapping is a bit more generic, but also correct.Here are some references and synonyms: http://www.frank-dieterle.com/phd/2_4_3.html;;;
724;2;2014-07-11T14:13:30.403;;Your approach of using a source version repository is a good one and it actually allows you also working on the cluster and then copying everything back.If you find yourself making minor edits to your Python script on your laptop, then updating your SVN directory on the cluster, why not work directly on the cluster frontend, make all needed minor edits, and then, at the end of the day, commit everything there and update on your laptop? All you need is to get familiar with the environment there (OS, editor, etc.) or install your own environment (I usually install in my home directory the latest version of Vim, Tmux, etc. with the proper dotfiles so I feel at home there.)Also, you can version your data, and even your intermediate results if size permits. My repositories often comprise code, data (original and cleaned versions), documentation, and paper sources for publishing (latex)Finally, you can script your job submission to avoid modifying scripts manually. qsub accepts a script from stdin and also accepts all #$ comments as command-line arguments. ;;;
725;2;2014-07-11T14:27:01.603;;What you describe falls in the category of concept drift in machine learning.You might find interesting and actionable ideas in this summary paper and you'll find a taxonomy of the possible approaches in these slides.;;;
726;1;2014-07-11T21:09:58.873;Trying to understand free-energy equations in a Karl Friston neuroscience article;"I am trying to understand a neuroscience article:Friston, Karl J., et al. ""Action and behavior: a free-energy formulation."" Biological cybernetics 102.3 (2010): 227-260. (DOI 10.1007/s00422-010-0364-z)In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5): The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements): • Energy minus entropy • Divergence plus surprise • Complexity minus accuracy Mathematically, these correspond to: The things I am struggling with at this point are:the meaning of the || in the 2nd and 3rd versions of the equations;and the negative logs.Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?";[education, open-source];90;
727;2;2014-07-11T21:35:22.677;;Your problem is that the resets aren't part of your linear model. You either have to cut your data into different fragments at the resets, so that no reset occurs within each fragment, and you can fit a linear model to each fragment. Or you can build a more complicated model that allows for resets. In this case, either the time of occurrence of the resets has to be put into the model manually, or the time of resets has to be a free parameter in the model that is determined by fitting the model to the data.;;;
728;2;2014-07-11T22:03:31.950;;In addition to excellent previous answers, I'd like to recommend two papers on data cleaning. They are not specific to manual data cleaning, but, considering the benefits and advice (which I completely agree with) of expressing even manual data transformations in code, these resources can be as valuable. Also, despite the fact that following papers are somewhat R-focused, I believe that general ideas and workflows for data cleaning can be easily extracted and are equally applicable to non-R environments, as well.The first paper presents the concept of tidy data, as well as examples and best practices of use of standard and specific R packages in data cleaning: http://vita.had.co.nz/papers/tidy-data.pdf.A comprehensive and coherent approach to data cleaning in R, including examples, as well as a description of workflow (framework) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.;;;
729;2;2014-07-11T23:08:16.267;;I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I'm going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural 'reset' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd's, but this can be changed)Here is a plot of the data, and the code to generating it is at the bottom.For starters, we find the breaks and fit each set of y-values and record the slopes.# Find the differences between adjacent pointsdiffs = y_data[-1] - y_data[-length(y_data)]# Find the break points (here I use 4 s.d.'s)break_points = c(0,which(diffs < (mean(diffs) - 4*sd(diffs))),length(y_data))# Create the lists of y-valuesy_lists = sapply(1:(length(break_points)-1),function(x){  y_data[(break_points[x]+1):(break_points[x+1])]})# Create the lists of x-valuesx_lists = lapply(y_lists,function(x) 1:length(x))#Find all the slopes for the lists of pointsslopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))Here are the slopes:(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)And we can just take the mean to find the expected slope (3.920168).Edit: Predicting when series reaches 120I realized I didn't finish predicted when series reaches 120.  If we estimate the slope to be m and we see a reset at time t to a value x (x<120), we can predict how much longer it would take to reach 120 by some simple algebra.Here, t is the time it would take to reach 120 after a reset, x is what it resets to, and m is the estimated slope.  I'm not going to even touch the subject of units here, but it's good practice to work them out and make sure everything makes sense.Edit: Creating The Sample DataThe sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.# Create Sample Dataset.seed(1001)x_data = 1:100 # x-datay_data = rep(0,length(x_data)) # Initialize y-datay_data[1] = 50 reset_level = sample(115:120,1) # Select initial cutofffor (i in x_data[-1]){ # Loop through rest of x-data  if(y_data[i-1]>reset_level){ # check if y-value is above cutoff    y_data[i] = 50             # Reset if it is and    reset_level = sample(115:120,1) # rechoose cutoff  }else {    y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise  }}plot(x_data,y_data) # Plot data;;;
730;1;2014-07-12T17:25:52.907;"Is FPGrowth still considered ""state of the art"" in frequent pattern mining?";"As far as I know the development of algorithms to solve the Frequent Pattern Mining (FPM) problem, the road of improvements have some main checkpoints. Firstly, the Apriori algorithm was proposed in 1993, by Agrawal et al., along with the formalization of the problem. The algorithm was able to strip-off some sets from the 2^n - 1 sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.Later, on year 1997, Zaki et al. proposed the algorithm Eclat, which inserted the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.In 2000, Han et al. proposed an algorithm named FPGrowth, along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to strip-off non-frequent itemsets.Edit:As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered ""state-of-the-art""? What is the intuition/main-contribution of such algorithms?Is the FPGrowth algorithm still considered ""state of the art"" in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?";[education, open-source];173;1
731;1;2014-07-13T09:04:39.703;How to fight underfitting in a deep neural net;When I started with artificial neural networks (NN) I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even beat my score on random forest!I'm seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.For implementing NN I use Theano Stacked Auto Encoder with the code from tutorial that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at tutorial, chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100->100->100, 60->60->60, 60->30->15, etc.), different learning and pre-train rates, etc.And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.How can it be that my deep NN with pre-training behaves so badly? What should I try? ;[education, open-source];603;3
732;2;2014-07-13T09:58:16.387;;The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this gives a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate). Also I found following papers very useful: Visually Debugging Restricted Boltzmann Machine Trainingwith a 3D ExampleA Practical Guide to Training Restricted BoltzmannMachinesThey both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn't provide good representation of features, further layers have almost no chance to fix it. ;;;
733;2;2014-07-13T12:24:36.430;;"Ask your grid administrator to add your local machine as a ""submit host"", and install SGE (which we assume you are using, you don't actually say) so then you can qsub from your machine.OR....Use emacs, then you can edit on your HPC via emacs's ""tramp"" ssh-connection facilities, and keep a shell open in another emacs window. You don't say what editor/operating system you like to use. You can even configure emacs to save a file in two places, so you could save to your local machine for running tests and to the HPC file system simultaneously for big jobs.";;;
734;2;2014-07-14T12:08:13.373;;"I'm not qualified to understand almost all of that paper, but, I might be able to give some intuitions from information theory that help you parse the paper.|| denotes the Kullback-Leibler divergence. It measures an information gain between two distributions. I suppose you could say it indicates the information in the real distribution of data that a model fails to capture.When you see ""negative log"" think ""entropy"".In the first equation, think of it as ""-ln(...) - -ln(...)"". This may help think of it as the difference of entropies. Likewise in the second, read it as ""D(...) + -ln(...)"". This may help think of it as ""plus entropy"".If you look at the divergence definition, you'll see it is defined as the log of the ratio of the PDFs. This may help connect it to logs and negative logs. Look at the definition that writes it as cross-entropy minus entropy. Then this is all a question of differences of entropies of things which may be clearer.";;;
735;2;2014-07-14T13:06:05.523;;"From your question's wording I assume that you have a local machine and a remote machine where you update two files — a Python script and a Bash script.  Both files are under SVN control, and both machines have access to the same SVN server.I am sorry I do not have any advice specific to your grid system, but let me list some general points I have found important for any deployment.Keep production changes limited to configuration changes. You write that you have to ""use the datasets' path on the server""; this sounds to me like you have the paths hardcoded into your Python script.  This is not a good idea, precisely because you will need to change those paths in every other machine where you move the script to.  If you commit those changes back to SVN, then on your local machine you will have the remote paths, and on and on ...  (What if there are not only paths, but also passwords?  You should not have production passwords in an SVN server.)So, keep paths and other setup informations in a .ini file and use ConfigParser to read it, or use a .json file and use the json module. Keep one copy of the file locally and one remotely, both under the same path, both without SVN control, and just keep the path to that configuration file in the Python script (or get it from the command line if you can't keep both configurations under the same path).Keep configuration as small as possible. Any configuration is a ""moving part"" of your application, and any system is more robust the less it has moving parts.  A good indicator of something that belongs into configuration is exactly that you have to edit it every time you move the code; things that have not needed editing can remain as constants in the code.Automate your deployment.  You can do it via a Bash script on your local machine; note that you can run any command on a remote machine through ssh.  For instance:svn export yourprojectpath /tmp/exportedprojecttar czf /tmp/yourproject.tgz /tmp/exportedprojectscp /tmp/myproject.tgz youruser@remotemachine:~/dev## Remote commands are in the right hand side, between ''ssh youruser@remotemachine 'tar xzf ~/dev/yourproject.tgz'ssh youruser@remotemachine 'qsub ~/dev/yourproject/script.py'For this to work, you need of course to have a passwordless login, based on public/private keys, set up between your local and the remote machine.If you need more than this, you can think of using Python's Fabric or the higher-level cuisine.";;;
736;1;2014-07-14T13:53:28.437;Skewed multi-class data;I have a dataset which contains ~100,000 samples of 50 classes. I have been using SVM with an RBF kernel to train and predict new data. The problem though is the dataset is skewed towards different classes. For example, Class 1 - 30 (~3% each), Class 31 - 45 (~0.6% each), Class 46 - 50 (~0.2% each)I see that the model tends to very rarely predict the classes which occur less frequent in the training set, even though the test set has the same class distribution as the training set. I am aware that there are technique such as 'undersampling' where the majority class is scaled down to the minor class. However, is this applicable here where there are so many different classes? Are there other methods to help handle this case?;[education, open-source];329;2
737;2;2014-07-14T15:58:42.517;;I have faced this problem many times while using SVM with Rbf kernel. Using Linear kernel instead of Rbf kernel solved my problem, but I dealt with lesser number of classes. The results were less skewed and more accurate with the linear kernel. Hope this solves your problem.;;;
739;1;2014-07-14T19:02:01.670;Starting my career as Data Scientist, is Software Engineering experience required?;I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.;[education, open-source];1337;3
740;2;2014-07-14T19:06:44.090;;I would suggest you to use libsvm, which already has adjustable class weights implemented in it. Rather than replicating the training samples, one modifies the C parameter for different classes in the SVM optimization. For example if your data has 2 classes, and the first class is only 10% of the data, you would choose class weights to be 10 and 1 for class 1 and 2 respectively. Therefore, margin violations of the first class would cost 10 times more than the margin violations for second class, and per-class accuracies would be more balanced.;;;
741;2;2014-07-15T04:39:11.670;;Absolutely. Keep your software skills sharp. You can do this in an academic program if you simply implement by yourself all the algorithms you learn about. Good selection of courses, btw. Consider getting an internship too.;;;
742;2;2014-07-15T06:19:31.820;;1) I think that there's no need to question whether your background is adequate for a career in data science. CS degree IMHO is more than enough for data scientist from software engineering point of view. Having said that, theoretical knowledge is not very helpful without matching practical experience, so I would definitely try to enrich my experience through participating in additional school projects, internships or open source projects (maybe ones, focused on data science / machine learning / artificial intelligence).2) I believe your concern about focusing on data science too early is unfounded, as long as you will be practicing software engineering either as a part of your data science job, or additionally in your spare time.3) I find the following definition of a data scientist rather accurate and hope it will be helpful in your future career success: A data scientist is someone who is better at statistics than any  software engineer and better at software engineering than any  statistician.P.S. Today's enormous number of various resources on data science topics is mind-blowing, but this open source curriculum for learning data science might fill some gaps between your BSc/MSc respective curricula and reality of the data science career (or, at least, provide some direction for further research and maybe answer some of your concerns): http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go.;;;
743;2;2014-07-15T09:30:02.183;;From the job ads I have seen, the answer depends: There are jobs which are more technical in nature (designing big data projects, doing some analysis) or the exact opposite (doing analysis, storage etc. is someone elses job).So I would say that SOME software design skills are extremely useful , but you don't need the abillity to build a huge program in C# / Java or whatever.Why I like some SW skills is simply that your code probably looks way better than code from someone who never programmed for the sake of programming. Most of the time, the latter code is very hard do understand / debug for outsiders. Also, sometimes your analysis needs to be integrated in a bigger program,an understand of the needs of the programms certainly helps.;;;
744;1;2014-07-15T21:30:11.600;Cosine similarity versus dot product as distance metrics;It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?;[education, open-source];1802;1
745;1;2014-07-16T00:06:02.160;ARMA/ARIMA on energy forecasts timeseries: strange prediction;"I'm trying to use ARMA/ARIMA with the statsmodel Python package, in order to predict the gas consumption. I tried with a dataset of this format:Using only the gas column.from pandas.tseries.offsets import *arma_mod20 = sm.tsa.ARMA(januaryFeb[['gas [m3]']], (5,3)).fit()predict_sunspots = arma_mod20.predict('2012-01-13', '2012-01-14', dynamic=True)ax = januaryFeb.ix['2012-01-13 00:00:00':'2012-01-15 22:00:00']['gas [m3]'].plot(figsize=(12,8))ax = predict_sunspots.plot(ax=ax, style='r--', label='Dynamic Prediction');ax.legend();Why is the prediction so bad?";[education, open-source];538;2
746;2;2014-07-16T03:32:16.270;;I'm not an expert on time series, but I have a general advice: may I suggest you to try other packages (and various parameters) to see, if there are any differences in results.Also, unless you have to use Python, I'd recommend to take a look at the R's extensive ecosystem for time series analysis: see http://www.statmethods.net/advstats/timeseries.html and http://cran.r-project.org/web/views/TimeSeries.html.In particular, you may want to check the standard stats package (including functions arima() and arima0), as well as some other packages: FitARMA (http://cran.r-project.org/web/packages/FitARMA), forecast (http://cran.r-project.org/web/packages/forecast) and education-focused fArma (cran.r-project.org/web/packages/fArma), to mention just a few. I hope this is helpful.;;;
747;2;2014-07-16T06:45:36.740;;"About automatic cleaning: You really cannot clean data automatically, because the number of errors and the definition of an error is often dependent on the data. E.g.: Your column ""Income"" might contain negative values, which are an error - you have to do something about the cases. On the other hand a column ""monthly savings"" could reasonably contain negative values. Such errors are highly domain dependent - so to find them, you must have domain knowledge, something at which humans excel, automated processes not so much.Where you can and should automate is repeated projects. E.g. a report which has to produced monthly. If you spot errors, you should place some automated process which can spot  these kinds of errors in subsequent months, freeing your time. ";;;
748;1;2014-07-16T07:47:48.603;Why use mean revenue in a split test?;"I asked a data science question regarding how to decide on the best variation of a split test on the Statistics section of StackExchange. I hope I will have better luck here. The question is basically, ""Why is mean revenue per user the best metric to make your decision on in a split test?""The original question is here: http://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-meanSince it was not well received/understood I simplified the problem to a discrete set of purchases and phrased it as a classical probability problem. That question is here: http://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urnThe mean may be the best metric for such a decision but I am not convinced. We often have a lot of prior information so a Bayesian method would likely improve our estimates. I realize that this is a difficult question but Data Scientists are doing such split tests everyday. ";[education, open-source];61;
749;1;2014-07-16T09:24:51.780;Meaning of latent features?;I'm learning about matrix factorization for recommending systems and I'm seeing the term latent features occurring too frequently but I'm unable to understand what it means. I know what a feature is but I don't understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?;[education, open-source];187;2
750;1;2014-07-16T09:49:15.933;How to increase accuracy of classifiers?;I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:UCI letter recognition:RTrees - 5.3% Boost -  13% MLP -    7.9% kNN(k=3) -   6.5%  Bayes -  11.5%  SVM -    3.3%Parameters used:RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,min_sample_count=1Boost -  boost_type=REAL, weak_count=200, weight_trim_rate=0.95,max_depth=7MLP -    method=BACKPROP, param=0.001, max_iter=300 (default values - tooslow to experiment) kNN(k=3) -   k=3Bayes -  noneSVM -    RBF kernel, C=10, gamma=0.01After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):Digits:RTrees - 5.1%Boost -  23.4%MLP -    4.3%kNN(k=3) -   7.3%Bayes -  17.7%SVM -    4.2%MNIST:RTrees - 1.4%Boost -  out of memoryMLP -    1.0%kNN(k=3) -   1.2%Bayes -  34.33%SVM -    0.6%I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?;[education, open-source];718;
751;2;2014-07-16T10:42:55.793;;"You are right, cosine similarity has a lot of common with dot product of vectors. Indeed, it is a dot product, scaled by magnitude. And because of scaling it is normalized between 0 and 1. CS is preferable because it takes into account variability of data and features' relative frequencies. On the other hand, plain dot product is a little bit ""cheaper"" (in terms of complexity and implementation).";;;
752;2;2014-07-16T11:07:56.467;;"It seems to me that latent features is a term used to describe criteria for classifying entities by their structure, in other words, by features (traits) they contain, instead of classes they belong to. Meaning of the word ""latent"" here is most likely similar to its meaning in social sciences, where very popular term latent variable (http://en.wikipedia.org/wiki/Latent_variable) means unobservable variable (concept).Section ""Introduction"" in the following paper provides a good explanation of latent features' meaning and use in modeling of social sciences phenomena: http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf.";;;
753;2;2014-07-16T13:05:41.123;;"I could try to explain you with words, but these slides explain it very well with pictures. Hope it helps.http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/phrase-model.pdfNote this slides correspond to the chapter 5 of ""Statistical Machine Translation"" by Philipp Koehn, highly recommended if you are working on machine translation, and it is easy to read.";;;
754;2;2014-07-16T14:22:38.677;;Most of the recent Frequent Pattern approaches that I've seen in the literature are based on optimizing FPGrowth. I have to admit, I haven't seen many developments within the literature in FPM in many years.This wikibook highlights many of the variants on FPGrowth that are out there.;;;
755;2;2014-07-16T17:34:24.880;;I expected more from random trees:With random forests, typically for N features, sqrt(N) features are used for each decision tree construction. Since in your case N=20, you could try setting max_depth (the number of sub-features to construct each decision tree) to 5.Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes. This might improve your accuracy.On MNIST kNN gives better accuracy, any ideas how to get it higher?Try with a higher value of K (say 5 or 7). A higher value of K would give you more supportive evidence about the class label of a point.You could run PCA or Fisher's Linear Discriminant Analysis before running k-nearest neighbour. By this you could potentially get rid of correlated features while computing distances between the points, and hence your k neighbours would be more robust.Try different K values for different points based on the variance in the distances between the K neighbours.;;;
756;2;2014-07-16T18:15:42.343;;At the expense of over-simplication, latent features are 'hidden' features to distinguish them from observed features. Latent features are computed from observed features using matrix factorization. An example would be text document analysis. 'words' extracted from the documents are features. If you factorize the data of words you can find 'topics', where 'topic' is a group of words with semantic relevance. Low-rank matrix factorization maps several rows (observed features) to a smaller set of rows (latent features).To elaborate, the document could have observed features (words) like [sail-boat, schooner, yatch, steamer, cruiser] which would 'factorize' to latent feature (topic) like 'ship' and 'boat'. [sail-boat, schooner, yatch, steamer, cruiser, ...] -> [ship, boat]The underlying idea is that latent features are semantically relevant 'aggregates' of observered features. When you have large-scale, high-dimensional, and noisy observered features, it makes sense to build your classifier on latent features.This is a of course a simplified description to elucidate the concept. You can read the details on Latent Dirichlet Allocation (LDA) or probabilistic Latent Semantic Analysis (pLSA) models for an accurate description.;;;
757;2;2014-07-16T18:26:54.233;;Gas usage has a daily cycle but there are also secondary weekly and annual cycles that the ARIMA may not be able to capture.There is a very noticeable difference between the weekday and Saturday data. Try creating a subset of the data for each day of the week or splitting the data into weekday and weekend and applying the model.If you can obtain temperature data for the same period check if there is a correlation between the temperature and gas usage.As @Aleksandr Blekh said R does have good packages for ARIMA models;;;
758;1;2014-07-16T20:09:08.640;Tools and protocol for reproducible data science using Python;I am working on a data science project using Python.The project has several stages.Each stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.I store the code in git, so that part is covered.I would like to hear about:Tools for data version control.Tools enabling to reproduce stages and experiments.Protocol and suggested directory structure for such a project.Automated build/run tools.;[education, open-source];1587;19
759;2;2014-07-17T06:02:04.813;;"The topic of reproducible research (RR) is very popular today and, consequently, is huge, but I hope that my answer will be comprehensive enough as an answer and will provide enough information for further research, should you decide to do so.While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more universal tools (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.1) Tools for data version control. Unless you plan to work with (very) big data, I guess, it would make sense to use the same git, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.2) Tools for managing RR workflows and experiments. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):Taverna Workflow Management System (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal myExperiment (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with Taverna is called Taverna Online, but it is being developed and maintained by totally different organization in Russia (referred there to as OnlineHPC: http://onlinehpc.com).The Kepler Project (https://kepler-project.org)VisTrails (http://vistrails.org)Madagascar (http://www.reproducibility.org)EXAMPLE. Here's an interesting article on scientific workflows with an example of the real workflow design and data analysis, based on using Kepler and myExperiment projects: http://f1000research.com/articles/3-110/v1.There are many RR tools that implement literate programming paradigm, exemplified by LaTeX software family. Tools that help in report generation and presentation is also a large category, where Sweave and knitr are probably the most well-known ones. Sweave is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that knitr might be a better option, as it's modern, has extensive support by popular tools (such as RStudio) and is language-neutral (http://yihui.name/knitr/demo/engines).3) Protocol and suggested directory structure. If I understood correctly what you implied by using term protocol (workflow), generally I think that standard RR data analysis workflow consists of the following sequential phases: data collection => data preparation (cleaning, transformation, merging, sampling) => data analysis => presentation of results (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.For sample directory structure, you may take a look at documentation for R package ProjectTemplate (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:4) Automated build/run tools. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is make. Read the following article for some reasons to use make as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other similar tools, which either improve some aspects of make, or add some additional features. For example: ant (officially, Apache Ant: http://ant.apache.org), Maven (""next generation ant"": http://maven.apache.org), rake (https://github.com/ruby/rake), Makepp (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.";;;
760;1;2014-07-17T09:26:11.833;Given time series data, how to model the frequency of someone changes his job?;I am given a time series data vector (ordered by months and years),which contains only 0s and 1s. 1 s represent a person changes his job at a particular a month. Questions: What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.  A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ? Would love to gather more information on how to model something like this. Thanks;[education, open-source];105;
761;1;2014-07-17T09:50:41.437;Clustering geo location coordinates (lat,long pairs);"What is the right approach and clustering algorithm for geo location clustering?I'm using the following code to cluster geolocation coordinates :import numpy as npimport matplotlib.pyplot as pltfrom scipy.cluster.vq import kmeans2, whitencoordinates= np.array([           [lat, long],           [lat, long],            ...           [lat, long]           ])x, y = kmeans2(whiten(coordinates), 3, iter = 20)  plt.scatter(coordinates[:,0], coordinates[:,1], c=y);plt.show()Is it right to use Kmeans for location clustering, as it uses Euclidean distance and not Haversine formula as a distance function?";[education, open-source];1993;3
762;1;2014-07-17T10:04:29.797;t-SNE Python implementation: Kullback-Leibler divergence;t-SNE, as in [1], works by progressively reducing the Kullback-Leibler (KL) divergence, until a certain condition is met.The creators of t-SNE suggests to use KL divergence as a performance criterion for the visualizations: you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence [2]I tried two implementations of t-SNE:python: sklearn.manifold.TSNE().R: tsne, from library(tsne).Both these implementations, when verbosity is set, print the error (Kullback-Leibler divergence) for each iteration. However, they don't allow the user to get this information, which looks a bit strange to me.For example, the code:import numpy as npfrom sklearn.manifold import TSNEX = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])model = TSNE(n_components=2, verbose=2, n_iter=200)t = model.fit_transform(X)produces:[t-SNE] Computing pairwise distances...[t-SNE] Computed conditional probabilities for sample 4 / 4[t-SNE] Mean sigma: 1125899906842624.000000[t-SNE] Iteration 10: error = 6.7213750, gradient norm = 0.0012028[t-SNE] Iteration 20: error = 6.7192064, gradient norm = 0.0012062[t-SNE] Iteration 30: error = 6.7178683, gradient norm = 0.0012114...[t-SNE] Error after 200 iterations: 0.270186Now, as far as I understand, 0.270186 should be the KL divergence. However i cannot get this information, neither from model nor from t (which is a simple numpy.ndarray).To solve this problem I could: i) Calculate KL divergence by my self, ii) Do something nasty in python for capturing and parsing TSNE() function's output [3]. However: i) would be quite stupid to re-calculate KL divergence, when TSNE() has already computed it, ii) would be a bit unusual in terms of code.Do you have any other suggestion? Is there a standard way to get this information using this library?I mentioned I tried R's tsne library, but I'd prefer the answers to focus on the python sklearn implementation.References[1] http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb[2] http://homepage.tudelft.nl/19j49/t-SNE.html[3] http://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call;[education, open-source];680;1
763;2;2014-07-17T10:26:45.547;;A simple and perhaps somewhat naive approach would be to assume that a person changes jobs at a constant rate and that previous job changes have no influence on future ones. Under these assumptions you could model the job changes as a Poisson process and estimate the rate parameter using MLE (http://en.wikipedia.org/wiki/Poisson_process and http://en.wikipedia.org/wiki/Poisson_distribution).Of course one should explore how well these assumptions hold in the data. To do this, you could study whether or not job changes are independent of one another by computing the correlation between events at various lags (http://en.wikipedia.org/wiki/Correlation). You could also plot the distribution of time between job change events. If the process is Poisson-like then you should observe little to no correlation between events at any number of lags and the distribution of time between job change events should be exponentially distributed (http://en.wikipedia.org/wiki/Exponential_distribution).;;;
764;2;2014-07-17T12:34:11.397;;K-means should be right in this case. Since k-means tries to group based solely on euclidean distance between objects you will get back clusters of locations that are close to each other. To find the optimal number of clusters you can try making an 'elbow' plot of the within group sum of square distance. This may be helpful (http://nbviewer.ipython.org/github/nborwankar/LearnDataScience/blob/master/notebooks/D3.%20K-Means%20Clustering%20Analysis.ipynb);;;
765;2;2014-07-17T13:19:46.817;;I've seen the same thing: My target variables are non-negative, but sklearn's GBR sometimes makes negative predictions.  To grossly oversimplify, GBR is just using averages of subsets of the target variable to make predictions, so if the target variables are non-negative, then the predictions should be non-negative. Also, I can provide an example, if anyone's interested.I'd make this a comment, but I don't have enough reputation.;;;
766;2;2014-07-17T14:07:12.643;;The TSNE source in scikit-learn is in pure Python. Fit fit_transform() method is actually calling a private _fit() function which then calls a private _tsne() function. That _tsne() function has a local variable error which is printed out at the end of the fit. Seems like you could pretty easily change one or two lines of source code to have that value returned to fit_transform().;;;
767;2;2014-07-17T14:28:38.207;;"The best reproducibility tool is to make a log of your actions, something like this:experiment/input ; expected ; observation/output ; current hypothesis and if supported or rejectedexp1 ; expected1 ; obs1 ; some fancy hypothesis, supportedThis can be written down on a paper, but, if your experiments fit in a computational framework, you can use computational tools to partly or completely automate that logging process (particularly by helping you track the input datasets which can be huge, and the output figures).A great reproducibility tool for Python with a low learning curve is of course IPython Notebook (don't forget the %logon and %logstart magics).Git is also awesome, and it's not tied to Python. It will help you not only to keep a history of all your experiments, code, datasets, figures, etc. but also provide you with tools to maintain (git pickaxe), collaborate (blame) and debug (git-bisect) using a scientific method of debugging (called delta debugging). Here's a story of a fictional researcher trying to make his own experiments logging system, until it ends up being a facsimile of Git.Another general tool working with any language (with a Python API on pypi) is Sumatra, which is specifically designed to help you do replicable research (replicable aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium, which is a lot harder and time consuming and not automatable).Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a ""save game state"" often found in videogames. More precisely, it will will save:all the parameters you provided;the exact sourcecode state of your whole experimental application and config files;the output/plots/results and also any file produced by your experimental application.It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime).In the end, you can use either Git or Sumatra, they will provide you with about the same replicability power, but Sumatra is specifically tailored for scientific research so it provides a few fancy tools like a web GUI to crawl your results, while Git is more tailored towards code maintenance (but it has debugging tools like git-bisect so if your experiments involve codes, it may actually be better). Or of course you can use both!/EDIT: dsign touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a full list of the libraries and compilers you used along with their exact versions and the details of your platform.Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as Anaconda (with the great package manager conda), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a virtualenv, or to package everything using the commercial Docker application as cited by dsign or the opensource Vagrant (with for example pylearn2-in-a-box which use Vagrant to produce an easily redistributable virtual environment package)./EDIT2: Here's a great video summarizing (for debugging but this can also be applied to research) what is fundamental to do reproducible research: logging your experiments and each other steps of the scientific method, a sort of ""explicit experimenting"".";;;
768;2;2014-07-17T15:19:32.237;;"Since I started doing research in academia I was constantly looking for a satisfactory workflow.I think that I finally found something I am happy with:1) Put everything under version control, e.g., Git:For hobby research projects I use GitHub, for research at work I use the private GitLab server that is provided by our university. I also keep my datasets there.2) I do most of my analyses along with the documentation on IPython notebooks. It is very organized (for me) to have the code, the plots, and the discussion/conclusion all in one documentIf I am running larger scripts, I would usually put them into separate script .py files, but I would still execute them from the IPython notebook via the %run magic to add information about the purpose, outcome, and other parameters.I have written a small cell-magic extension for IPython and IPython notebooks, called ""watermark"" that I use to conveniently create time stamps and keep track of the different package versions I used and also Git hashsFor example%watermark29/06/2014 01:19:10CPython 3.4.1IPython 2.1.0compiler   : GCC 4.2.1 (Apple Inc. build 5577)system     : Darwinrelease    : 13.2.0machine    : x86_64processor  : i386CPU cores  : 2interpreter: 64bit%watermark -d -t29/06/2014 01:19:11 %watermark -v -m -p numpy,scipyCPython 3.4.1IPython 2.1.0numpy 1.8.1scipy 0.14.0compiler   : GCC 4.2.1 (Apple Inc. build 5577)system     : Darwinrelease    : 13.2.0machine    : x86_64processor  : i386CPU cores  : 2interpreter: 64bitFor more info, see the documentation here.";;;
769;2;2014-07-17T15:28:25.940;;"Dimensionality ReductionAnother important procedure is to compare the error rates on training and test dataset to see if you are overfitting (due to the ""curse of dimensionality""). E.g., if your error rate on the test dataset is much larger than the error on the training data set, this would be one indicator.In this case, you could try dimensionality reduction techniques, such as PCA or LDA.If you are interested, I have written about PCA, LDA and some other techniques here: http://sebastianraschka.com/index.html#machine_learning and in my GitHub repo here: https://github.com/rasbt/pattern_classificationCross validationAlso you may want to take a look at cross-validation techniques in order to evaluate the performance of your classifiers in a more objective manner";;;
770;2;2014-07-17T15:35:17.487;;"I am not an export in using SVMs, but usually (if you are using a machine learning library like Python's scikit-learn or R's libsvm, there is the class_weight parameter, or class.weights, respectively. Or if you'd use a Bayes classifier, you would take this ""skew"" into account via the ""prior (class) probabilities"" P(ωj)";;;
771;2;2014-07-17T18:25:44.683;;"From what I heard, Pylearn2 might be currently the library of choice for most people. This reminds me of a recent blog post a few month ago that lists all the different machine learning libraries with a short explanationhttps://www.cbinsights.com/blog/python-tools-machine-learningThe section you might be interested in here would be ""Deep Learning"". About Pylearn2, he writes PyLearn2 There is another library built on top of Theano, called PyLearn2 which  brings modularity and configurability to Theano where you could create  your neural network through different configuration files so that it  would be easier to experiment different parameters. Arguably, it  provides more modularity by separating the parameters and properties  of neural network to the configuration file.";;;
773;2;2014-07-17T20:02:11.227;;"Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a ""distance metric"".";;;
774;2;2014-07-18T04:50:58.287;;The following general answer is my uneducated guess, so take it with grain of salt. Hopefully, it makes sense. I think that the best way to describe or analyze experiments (as any other systems, in general) is to build their statistical (multivariate) models and evaluate them. Depending on whether environments for your set of experiments are represented by the same model or different, I see the following approaches:1) Single model approach. Define experiments' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints). Analyze it (most likely, using regression analysis). Compare results across variables, which determine (influence) different environments.2) Multiple models approach. The same steps as previous case, but compare results across models, corresponding to different environments.;;;
775;2;2014-07-18T07:43:56.823;;"Be sure to check out docker! And in general, all the other good things that software engineering has created along decades for ensuring isolation and reproductibility. I would like to stress that it is not enough to have just reproducible workflows, but also easy to reproduce workflows. Let me show what I mean. Suppose that your project uses Python, a database X and Scipy. Most surely you will be using a specific library to connect to your database from Python, and Scipy will be in turn using some sparse algebraic routines. This is by all means a very simple setup, but not entirely simple to setup, pun intended. If somebody wants to execute your scripts, she will have to install all the dependencies. Or worse, she might have incompatible versions of it already installed. Fixing those things takes time. It will also take time to you if you at some moment need to move your computations to a cluster, to a different cluster, or to some cloud servers. Here is where I find docker useful. Docker is a way to formalize and compile recipes for binary environments. You can write the following in a dockerfile (I'm using here plain English instead of the Dockerfile syntax):Start with a basic binary environment, like Ubuntu'sInstall libsparse-dev(Pip) Install numpy and scipyInstall XInstall libX-dev(Pip) Install python-XInstall IPython-NotebookCopy my python scripts/notebooks to my binary environment, these datafiles, and these configurations to do other miscellaneous things. To ensure reproductibility, copy them from a named url instead of a local file. Maybe run IPython-Notebook.Some of the lines will be installing things in Python using pip, since pip can do a very clean work in selecting specific package versions. Check it out too!And that's it. If after you create your Dockerfile it can be built, then it can be built anywhere, by anybody (provided they also have access to your project-specific files, e.g. because you put them in a public url referenced from the Dockerfile). What is best, you can upload the resulting environment (called an ""image"") to a public or private server (called a ""register"") for other people to use. So, when you publish your workflow, you have both a fully reproducible recipe in the form of a Dockerfile, and an easy way for you or other people to reproduce what you do:docker run dockerregistery.thewheezylab.org/nowyouwillbelievemeOr if they want to poke around in your scripts and so forth:docker run -i -t dockerregistery.thewheezylab.org/nowyouwillbelieveme /bin/bash";;;
776;2;2014-07-18T15:24:38.007;;I am far from an expert, but my understanding of the subject tells me that R (superb in statistics) and e.g. Python (superb in several of those things where R is lacking) complements each other quite well (as pointed out by previous posts). ;;;
777;1;2014-07-18T22:29:05.017;Getting GitHub repository information by different criteria;New to the Data Science forum, and first poster here!This may be kind of a specific question (hopefully not too much so), but one I'd imagine others might be interested in.I'm looking for a way to basically query GitHub with something like this:Give me a collection of all of the public repositories that have more than 10 stars, atleast two forks, and more than three committers.The result could take any viable form: a JSON data dump, a URL to the web page, etc. It more than likely will consist of information from 10,000 repos or something large.Is this sort of thing possible using the API or some other pre-built way, or am I going to have to build out my own custom solution where I try to scrape every page? If so, how feasible is this and how might I approach it?;[education, open-source];59;
778;1;2014-07-18T22:34:48.080;Is Python suitable for big data;I read in this post Is the R language suitable for Big Data that big data constitutes 5TB, and while it does a good job of providing information about the feasibility of working with this type of data in R it provides very little information about Python. I was wondering if Python can work with this much data as well. ;[education, open-source];757;5
779;2;2014-07-18T22:59:39.100;;I believe the language itself has little to do with performance capabilities, when it comes to large data. What matters is:How large is the data actuallyWhat processing are you going to perform on itWhat hardware are you going to useWhich are the specific libraries that you plan to useAnyway, Python is well adopted in data science communities.;;;
780;2;2014-07-19T02:19:46.530;;"Some good answers here. I would like to join the discussion by adding the following two notes:1) The question's emphasis on the volume of data while referring to Big Data is certainly understandable and valid, especially considering the problem of data volume growth outpacing technological capacities' exponential growth per Moore's Law (http://en.wikipedia.org/wiki/Moore%27s_law).2) Having said that, it is important to remember about other aspects of big data concept, based on Gartner's definition (emphasis mine - AB): ""Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."" (usually referred to as the ""3Vs model""). I mention this, because it forces data scientists and other analysts to look for and use R packages that focus on other than volume aspects of big data (enabled by the richness of enormous R ecosystem).3) While existing answers mention some R packages, related to big data, for a more comprehensive coverage, I'd recommend to refer to CRAN Task View ""High-Performance and Parallel Computing with R"" (http://cran.r-project.org/web/views/HighPerformanceComputing.html), in particular, sections ""Parallel computing: Hadoop"" and ""Large memory and out-of-memory data"".";;;
781;2;2014-07-19T03:29:02.647;;To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent python in this particular case.Let me just start by saying that regardless of your data size, python shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets:Reading data into memory - This is by far the most common issue faced in the world of big data. Basically, you can't read in more data than you have memory (RAM) for. The best way to fix this is by making atomic operations on your data instead of trying to read everything in at once.Storing data - This is actually just another form of the earlier issue, by the time to get up to about 1TB, you start having to look elsewhere for storage. AWS S3 is the most common resource, and python has the fantastic boto library to facilitate leading with large pieces of data.Network latency - Moving data around between different services is going to be your bottleneck. There's not a huge amount you can do to fix this, other than trying to pick co-located resources and plugging into the wall.;;;
782;2;2014-07-19T03:42:34.433;;"My limited understanding, based on brief browsing GitHub API documentation, is that currently there is NO single API request that supports all your listed criteria at once. However, I think that you could use the following sequence in order to achieve the goal from your example (at least, I would use this approach):1) Request information on all public repositories (API returns summary representations only): https://developer.github.com/v3/repos/#list-all-public-repositories;2) Loop through the list of all public repositories retrieved in step 1, requesting individual resources, and save it as new (detailed) list (this returns detailed representations, in other words, all attributes): https://developer.github.com/v3/repos/#get;3) Loop through the detailed list of all repositories, filtering corresponding fields by your criteria. For your example request, you'd be interested in the following attributes of the parent object: stargazers_count, forks_count. In order to filter the repositories by number of committers, you could use a separate API: https://developer.github.com/v3/repos/#list-contributors.Updates or comments from people more familiar with GitHub API are welcome!";;;
783;1;2014-07-19T05:27:22.773;Data visualization for pattern analysis (language-independent, but R preferred);I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I'm almost sure this pattern was created by a disk testing program, but I'd like to reverse-engineer it anyway.I already know that the pattern is aligned, with a periodicity of 256 characters.I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel's color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).This is a snapshot of the pattern (you can see more than one), seen through xxd (32x16):Either way, I am trying to find a way of visualizing this information. This probably isn't hard for anyone into signal analysis, but I can't seem to find a way using open-source software.I'd like to avoid Matlab or Mathematica and I'd prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.Update, 2014-07-25: given Emre's answer below, this is what the pattern looks like, given the first 30MB of the pattern, aligned at 512 instead of 256 (this alignment looks better):Any further ideas are welcome!;[education, open-source];252;1
784;1;2014-07-19T18:31:36.573;Why should I care about seasonal data when I forecast?;I have a timeseries with hourly gas consumption. I want to use ARMA/ARIMA to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with Seasonal and Trend decomposition using Loess (STL)?)?;[education, open-source];218;2
785;2;2014-07-19T18:47:38.787;;I know almost nothing about signal analysis, but 2-dimensional visualization could be easily done using R. Particularly you will need reshape2 and ggplot2 packages. Assuming your data is wide (e.g. [n X 256] size), first you need to transform it to long format using melt() function from reshape2 package. Then use geom_tile geometry from ggplot2. Here is a nice recipe with gist.;;;
786;1;2014-07-19T20:46:52.740;NASDAQ Trade Data;I am trying to find stock data to practice with, is there a good resource for this? I found this: ftp://emi.nasdaq.com/ITCH/ but it only has the current year.I already have a way of parsing the protocol, but would like to have some more data to compare with. It doesn't have to be in the same format, as long as it has price, trades, and date statistics.;[education, open-source];201;
787;1;2014-07-19T20:51:58.527;Hadoop Resource Manager Won't Start;"I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. I cleared up some ""shared library"" problems by adding this to yarn-env.sh and hadoop-env.sh: export HADOOP_HOME=""/usr/local/hadoop"" export HADOOP_OPTS=""-Djava.library.path=$HADOOP_HOME/lib""I also added this to hadoop-env.sh: export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/nativebased on the advice of this post at horton works http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/ That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this: starting yarn daemons starting resourcemanager,   logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out localhost: starting nodemanager,   logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.outThe only problem is, JPS says that the Resource Manager isn't running. What's going on here?";[education, open-source];612;
788;2;2014-07-20T21:37:27.857;;Check your version of JPS and make sure it's the same as the version of java that you are running.  Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced.Run ps -ef |grep java and look for the resource manager threads.  Maybe it's actually running.  If it is, try update-alternatives --config jps to see what binary jps is pointing at and compare it with the java binary that you are using.If the resource manager is not running, it's time to do some basic linux troubleshooting.  Check log files and barring that check actual command output.On the system I'm looking at now, the log files for resource manager are placed in the hadoop-install/logs directory in yarn-username-resourcemanager-hostname.log and yarn-user-resourcemanager-hostname.out.  Your configuration may place them in /var/logs or what have you.  Also, have a look at the syslog.If the logs don't yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with echo), and then trying to run the command directly to see the output as it comes out.I have actually run into this problem before, but I can't remember the specific issue.  I'm sure the same result can manifest itself from a variety of problems.  Considering that you are as far as you are in the process of getting set up, I believe it's likely to be a minor configuration issue.;;;
789;2;2014-07-20T23:54:09.070;;You can pull stock data very easyly in python and R (probably other languages as well) with the following packages:In python with: https://pypi.python.org/pypi/ystockquoteThis is also a really nice tutorial in iPython which shows you how to pull the stock data and play with it: http://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynbIn R with: http://www.quantmod.com/HTH. ;;;
790;2;2014-07-21T00:35:13.440;;Absolutely. When you're working with data at that scale it's common to use a big data framework, in which case python or whatever language you're using is merely an interface. See for example Spark's Python Programming Guide. What kind of data do you have and what do you want to do with it?;;;
791;1;2014-07-21T09:00:04.917;Job title similarity;I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words  senior, primary, lead, head, vp, director, stuff, principal, chief, or the words  analyst, expert, modeler, researcher, scientist, developer, engineer, architect.How can I get all such possible words with their distance ?;[education, open-source];194;
792;2;2014-07-21T11:59:49.203;;To handle such amount of data, programming language is not the main concern but the programming framework is. Frameworks such as MapReduce or Spark have bindings to many languages including Python. These frameworks certainly have many ready-to-use packages for data analysis tasks. But in the end it all comes to your requirement, i.e., what is your task? People have different definitions of data analysis tasks, some of them can be easily solved with relational databases. In that case, SQL is much better than all other alternatives.;;;
793;1;2014-07-21T13:41:13.427;Uses of NoSQL database in data science;How can NoSQL databases like MongoDB be used for data analysis? What are the features in them that can make data analysis faster and powerful?;[education, open-source];1199;1
794;2;2014-07-21T15:35:59.540;;Pylearn2 seems to be the library of choice, however I find their YAML configuration files off-putting.Python itself was designed to be an easy language for prototyping, why would you not use it to define the network properties themselves? We have great editors with autocompletion that would make your life much easier and Python is not like C++ where you have to wait for long builds to finish before you can run your code.YAML files on the other hand you have to edit using a standard text editor with no assistance whatsoever and this makes the learning curve even steeper. I may be missing the big picture but I still don't understand what were they thinking, I don't think prototyping in code would be much slower. For that reason I'm considering Theanets or using Theano directly.;;;
795;2;2014-07-21T16:15:07.010;;"Not sure if this is exactly what you're looking for, but r-base has a function called ""adist"" which creates a distance matrix of approximate string distances (according to the Levenshtein distance). Type '?adist' for more.words = c(""senior"", ""primary"", ""lead"", ""head"", ""vp"", ""director"", ""stuff"", ""principal"", ""chief"")adist(words)      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,]    0    6    5    5    6    5    5    7    5 [2,]    6    0    6    6    7    7    7    6    6 [3,]    5    6    0    1    4    7    5    8    5 [4,]    5    6    1    0    4    7    5    8    4 [5,]    6    7    4    4    0    8    5    8    5 [6,]    5    7    7    7    8    0    8    8    7 [7,]    5    7    5    5    5    8    0    9    4 [8,]    7    6    8    8    8    8    9    0    8 [9,]    5    6    5    4    5    7    4    8    0Also, if R isn't an option, the Levenshtein distance algorithm is implemented in many languages here:http://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance";;;
796;2;2014-07-21T16:58:22.660;;There are couple off things you need to understand when dealing with Big data - What is Big data?You might be aware of famous V's of Big data - Volume, Velocity, Variety... So, Python may not be suitable for all. And it goes with all data science tools available. You need to know which tool is good for what purpose.If dealing with large Volume of data:Pig/Hive/Shark - Data cleaning and ETL workHadoop/Spark - Distributed parallel computingMahout/ML-Lib - Machine LearningNow, you can use R/Python in intermediate stages but you'll realize that they become bottleneck in your entire process.If dealing with Velocity of data:Kafka/Storm - High throughput systemPeople are trying to R/Python here but again it depends on kind of parallelism you want and your model complexity.What sort of analysis you wish to do?If your model demands the entire data to be first brought into memory then your model should not be complex because if the intermediate data is large then the code will break. And if you think of writing it into disk then you'll face additional delay because disk read/write is slow as compared to RAM.ConclusionYou can definitely use Python in Big data space (Definitely, since people are trying with R, why not Python) but know your data and business requirement first. There may be better tools available for same and always remember: Your tools shouldn’t determine how you answer questions. Your questions should determine what tools you use.;;;
797;2;2014-07-21T19:06:43.223;;To be perfectly honest, most NoSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of MongoDB compared to a relational database like MySQL is significantly is poor enough to warrant staying away from something like MongoDB entirely.With that said, there are a couple of really useful properties of NoSQL databases that certainly work in your favor when you're working with large data sets, though the chance of those benefits outweighing the generally poor performance of NoSQL compared to SQL for read-intensive operations (most similar to typical big data use cases) is low.No Schema - If you're working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. NoSQL databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something an SQL database will support.JSON - If you happen to be working with JSON-style documents instead of with CSV files, then you'll see a lot of advantage in using something like MongoDB for a database-layer. Generally the workflow savings don't outweigh the increased query-times though.Ease of Use - I'm not saying that SQL databases are always hard to use, or that Cassandra is the easiest thing in the world to set up, but in general NoSQL databases are easier to set up and use than SQL databases. MongoDB is a particularly strong example of this, known for being one of the easiest database layers to use (outside of SQLite). SQL also deals with a lot of normalization and there's a large legacy of SQL best practices that just generally bogs down the development process.Personally I might suggest you also check out graph databases such as Neo4j that show really good performance for certain types of queries if you're looking into picking out a backend for your data science applications.;;;
798;2;2014-07-21T20:42:00.143;;"If I understand your question, you can look at the co-occurrence matrix formed using the terms following the title; e.g., senior FOO, primary BAR, etc. Then you can compute the similarity between any pair of terms, such as ""senior"" and ""primary"", using a suitable metric; e.g., the cosine similarity.";;;
799;2;2014-07-21T21:17:07.303;;"I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:import os, numpy, matplotlib.pyplot as plt%matplotlib inlinedef read_in_chunks(infile, chunk_size=256):    while True:        chunk = infile.read(chunk_size)        if chunk:            yield chunk        else:            # The chunk was empty, which means we're at the end            # of the file            returnfname = 'enter something here'srcfile = open(fname, 'rb')height = 1 + os.path.getsize(fname)/256data = numpy.zeros((height, 256), dtype=numpy.uint8)    for i, line in enumerate(read_in_chunks(srcfile)):    vals = list(map(int, line))    data[i,:len(vals)] = valsplt.imshow(data, aspect=1e-2);This is what a PDF looks like:A 256 byte periodic pattern would have manifested itself as vertical lines. Except for the header and tail it looks pretty noisy.";;;
800;2;2014-07-21T21:29:26.270;;One benefit of the schema-free NoSQL approach is that you don't commit prematurely and you can apply the right schema at query time using an appropriate tool like Apache Drill. See this presentation for details. MySQL wouldn't be my first choice in a big data setting.;;;
801;2;2014-07-21T21:32:12.963;;That's an interesting problem, thanks for bring out here on stack.I think this problem is similar to when we apply LSA(Latent Semantic Analysis) in sentiment analysis to find list of positive and negative words with polarity with respect to some predefined positive and negative words.Good reads:Learning Word Vectors for Sentiment AnalysisUnsupervised Learning of Semantic Orientation from a Hundred-Billion-Word CorpusSo, according to me LSA is your best approach to begin with in this situation as it learns the underlying relation between the words from the corpus and probably that's what you are looking for.;;;
802;1;2014-07-21T23:53:11.120;Efficient database model for storing data indexed by n-grams;I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale.Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale.;[education, open-source];338;2
803;2;2014-07-22T00:06:10.500;;I haven't done this before but it sounds like a job for a graph database given the functionality you want. Here's a demo in neo4j.;;;
804;1;2014-07-22T01:12:03.860;What's the difference between data products and intelligent systems?;Basically, both are software systems that are based on data and algorithms.;[education, open-source];32;
805;2;2014-07-22T02:27:43.710;;"This is a very vague question. However, I will try to make sense of it. Considering rules of logic as well as your statement that both entities are ""software systems that are based on data and algorithms"", it appears that data products are intelligent systems and intelligent systems are, to some degree, data products. Therefore, it can be argued that the difference between the terms ""data products"" and ""intelligent systems"" is purely in the focus (source of information or purpose of system dimensions) of each type of systems (data vs. intelligence/algorithms).";;;
806;1;2014-07-22T03:43:20.327;Advantages of AUC vs standard accuracy;I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I've found that some claim its advantage is mostly marginal in that it is best for catching 'lucky' models with high standard accuracy measurements and low AUC.So should I avoid relying on AUC for validating models or would a combination be best? Thanks for all your help.;[education, open-source];611;2
807;2;2014-07-22T04:10:18.353;;Really great question, and one that I find that most people don't really understand on an intuitive level. AUC is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what AUC is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how AUC works.AUC stands for Area Under the Curve, which curve you ask? Well that would be the ROC curve. ROC stands for Receiver Operating Characteristic, which is actually slightly non-intuitive. The implicit goal of AUC is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate (TPR) and the false positive rate (FPR):Now in this graph, TPR is specifically the ratio of true positive to all positives, and FPR is the ratio of false positives to all negatives. (Keep in mind, this is only for binary classification.) On a graph like this, it should be pretty straightforward to figure out that a prediction of all 0's or all 1's will result in the points of (0,0) and (1,1) respectively. If you draw a line through these lines you get something like this:Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the AUC of such a model would be 0.5 (height and base are both 1). Similarly, if you predict a random assortment of 0's and 1's, let's say 90% 1's, you could get the point (0.9, 0.9), which again falls along that diagonal line.Now comes the interesting part. What if we weren't only predicting 0's and 1's? What if instead we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0's and all 1's (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the 1x1 graph that contains your ROC. In practice you get something like this:So basically, what you're actually getting when you do an AUC over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy.;;;
808;1;2014-07-22T08:39:33.810;Statistics + Computer Science = Data Science?;i want to become a data scientist. I studied applied statistics (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in Computer Science focus in Intelligent Systems.Here is my study plan:Machine learningAdvanced machine learningData miningFuzzy logicRecommendation SystemsDistributed Data SystemsCloud ComputingKnowledge discoveryBusiness IntelligenceInformation retrievalText miningAt the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?Thanks for the answers.;[education, open-source];487;2
809;2;2014-07-22T09:03:49.270;;I think that you're on the right track toward becoming an expert data scientist. Recently I have answered related question here on Data Science StackExchange: http://datascience.stackexchange.com/a/742/2452 (pay attention to the definition I mention there, as it essentially answers your question by itself, as well as to aspects of practicing software engineering and applying knowledge to solving real-world problems). I hope that you will find all that useful. Good luck in your career!;;;
810;1;2014-07-22T12:29:10.050;Should I go for a 'balanced' dataset or a 'representative' dataset?;My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should go with the similar kind of data for training my models as well. But then I did come across a research paper or two (in my area of work) which have used a balanced data to train models, implying equal number of instances of benign and malicious traffic.In general, if I am building ML models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?Can someone shed more light on the pros and cons of both the choices, and how to decide which one to go for?;[education, open-source];466;6
811;2;2014-07-22T14:22:49.137;;I would say the answer depends on your use case. Based on my experience:If you're trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data.If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains.If you're building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training.;;;
812;1;2014-07-22T15:17:04.810;Visualizing a graph with a million vertices;What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.I am thinking about writing a program to generate a svg. Any other suggestions?  ;[education, open-source];1462;4
813;2;2014-07-22T15:44:04.160;;"https://gephi.github.io/ says it can handle a million edges. If your graph has 1000000 vertices and only 50000 edges then most of your vertices won't have any edges anyway.In fact the Gephi spec is the dual of your example: ""Networks up to 50,000 nodes and 1,000,000 edges""";;;
814;2;2014-07-22T16:40:17.733;;"I also suggest Gephi software (https://gephi.github.io), which seems to be quite powerful. Some additional information on using Gephi with large networks can be found here and, more generally, here. Cytoscape (http://www.cytoscape.org) is an alternative to Gephi, being an another popular platform for complex network analysis and visualization.If you'd like to work with networks programmatically (including visualization) in R, Python or C/C++, you can check igraph collection of libraries. Speaking of R, you may find interesting the following blog posts: on using R with Cytoscape (http://www.vesnam.com/Rblog/viznets1) and on using R with Gephi (http://www.vesnam.com/Rblog/viznets2).For extensive lists of network analysis and visualization software, including some comparison and reviews, you might want to check the following pages: 1) http://wiki.cytoscape.org/Network_analysis_links; 2) http://www.kdnuggets.com/software/social-network-analysis.html; 3) http://www.activatenetworks.net/social-network-analysis-sna-software-review.";;;
815;1;2014-07-22T19:20:47.580;is this a good case for NOSQL?;I'm currently facing a project that I could solve with a relational database in a relatively painful way. Having heard so much about NOSQL, I'm wondering if there is not a more appropriate way of tackling it:Suppose we are tracking a group of animals in a forest (n ~ 500) and would like to keep a record of a set of observations (this is a fictional scenario).We would like to store the following information in a database:a unique identifier for each animala description of the animal with structured fields: Species, Genus, Family, ...a free text field with additional informationeach time-point at which it was detected close to a reference pointa picture of the animalan indication whether two given animals are siblingsAnd:there might be additional features appearing later as more data comes inWe would like to be able to execute the following types of queries:return all the animals spotted between in a given time intervalreturn all the animals of a given Species or Familyperform a text search on the free text fieldWhich particular database system would you recommend ? Is there any tutorial / examples that I could use as a starting point ?;[education, open-source];84;
816;2;2014-07-22T21:12:08.333;;"Well it depends on what kind of ""Data Science"" you wish to get in to.  For basic analytics and reporting statistics will certainly help, but for Machine Learning and Artificial Intelligence then you'll want a few more skillsProbability theory - you must have a solid background in pure probability so that you can decompose any problem, whether seen before or not, into probabilistic principles.  Statistics helps a lot for already solved problems, but new and unsolved problems require a deep understanding of probability so that you can design appropriate techniques.Information Theory - this (relative to statistics) is quite a new field (though still decades old), the most important work was by Shannon, but even more important and often neglected note in literature is work by Hobson that proved that Kullback-Leibler Divergence is the only mathematical definition that truly captures the notion of a ""measure of information"". Now fundamental to artificial intellgence is being able to quantify information.  Suggest reading ""Concepts in Statistical Mechanics"" - Arthur Hobson (very expensive book, only available in academic libraries).Complexity Theory - A big problem many Data Scientists face that do not have a solid complexity theory background is that their algorithms do not scale, or just take an extremely long time to run on large data.  Take PCA for example, many peoples favourite answer to the interview question ""how do you reduce the number of features in our dataset"", but even if you tell the candidate ""the data set is really really really large"" they still propose various forms of PCA that are O(n^3).  If you want to stand out, you want to be able to solve each problem on it's own, NOT throw some text book solution at it designed a long time ago before Big Data was such a hip thing.  For that you need to understand how long things take to run, not only theoretically, but practically - so how to use a cluster of computers to distribute an algorithm, or which data structures take up less memory.Communication Skills - A huge part of Data Science is understanding business.  Whether it's inventing a product driven by data science, or giving business insight driven by data science, being able to communicate well with both the Project and Product Managers, the tech teams, and your fellow data scientists is very important.  You can have an amazing idea, say an awesome AI solution, but if you cannot effectively (a) communicate WHY that will make the business money, (b) convince your collegues it will work and (c) explain to tech people how you need their help to build it, then it wont get done.";;;
817;2;2014-07-22T21:24:33.820;;"Data scientist (to me) a big umbrella term. I would see a data scientist as a person who can proficiently use techniques from the fields of data mining, machine learning, pattern classification, and statistics. However, those terms are intertwined to: machine learning is tied together with pattern classification, and also data mining overlaps when it comes finding patterns in data. And all techniques have their underlying statistical principles. I always picture this as a Venn diagram with a huge intersection. Computer sciences is related to all those fields too. I would say that you need ""data science"" techniques to do computer-scientific research, but computer science knowledge is not necessarily implied in ""data science"". However, programming skills - I see programming and computer science as different professions, where programming is more the tool in order solve problems - are also important to work with the data and to conduct data analysis.You have a really nice study plan, and it all makes sense. But I am not sure if you ""want"" to call yourself just ""data scientist"", I have the impression that ""data scientist"" is such a ambiguous term that can mean everything or nothing. What I want to convey is that you will end up being something more - more ""specialized"" - than ""just"" a data scientist.";;;
818;2;2014-07-23T05:29:06.903;;I think, that Gephi could face with lack-of-memory issues, you will need at least 8Gb of RAM. Though number of edges is not extremely huge. Possibly, more appropriate tool in this case would be GraphViz. It's a command line tool for network visualizations, and presumably would be more tolerant to graph size. Moreover, as I remember, in GraphViz it is possible to use precomputed coordinates to facilitate computations.I've tried to find a real-world examples of using GraphViz with huge graphs, but didn't succeed. Though I found similar discussion on Computational Science.;;;
819;2;2014-07-23T07:01:01.140;;"Three tables: animal, observation, and sibling. The observation has an animal_id column which links to the animal table, and the sibling table has animal_1_id and animal_2_id columns that indicates two animals are siblings for each row.Even with 5000 animals and 100000 observations I don't think query time will be a problem for something like PostgreSQL for most reasonable queries (obviously you can construct unreasonable queries but you can do that in any system). So I don't see how this is ""relatively painful"". Relative to what? The only complexity is the sibling table. In NOSQL you might store the full list of siblings in the record for each animal, but then when you add a sibling relationship you have to add it to both sibling's animal records. With the relational table approach I've outlined, it only exists once, but at the expense of having to test against both columns to find an animal's siblings. I'd use PostgreSQL, and that gives you the option of using PostGIS if you have location data - this is a geospatial extension to PostgreSQL that lets you do spatial queries (point in polygon, points near a point etc) which might be something for you. I really don't think the properties of NOSQL databases are a problem here for you - you aren't changing your schema every ten minutes, you probably do care that your database is ACID-compliant, and you don't need something web-scale.http://www.mongodb-is-web-scale.com/ [warning: strong language]";;;
820;1;2014-07-23T08:06:21.417;How can we effectively measure the impact of our data decisions;Apologies if this is very broad question, what I would like to know is how effective is A/B testing (or other methods) of effectively measuring the effects of a design decision.For instance we can analyse user interactions or click results, purchase/ browse decisions and then modify/tailor the results presented to the user.We could then test the effectiveness of this design change by subjecting 10% of users to the alternative model randomly but then how objective is this?How do we avoid influencing the user by the model change, for instance we could decided that search queries for 'David Beckham' are probably about football so search results become biased towards this but we could equally say that his lifestyle is just as relevant but this never makes it into the top 10 results that are returned.I am curious how this is dealt with and how to measure this effectively.My thoughts are that you could be in danger of pushing a model that you think is correct and the user obliges and this becomes a self-fulfilling prophecy.I've read an article on this: http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/ and also the book: http://shop.oreilly.com/product/0636920028529.do which discussed this so it piqued my interest.;[education, open-source];71;
821;2;2014-07-23T12:57:37.073;;"""Because its there"".The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise.But I think you've screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you've not told the function that your ""seasonality"" is a 24-hour cycle rather than an annual one. But hey you haven't given us any code or data so we don't really have a clue what you've done, do we? What do you think ""seasonality"" even means here? Do you have any idea?Your data seems the have three peaks every 24 hours. Really? Is this 'gas'='gasoline'='petrol' or gas in some heating/electric generating system? Either way if you know a priori there's an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you put that in your model.Actually you don't even say what your x-axis is so maybe its days and then I'd fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I'd not expect a model to do well on both sides of that.With statistics (which is what this is, sorry to disappoint you but you're not a data scientist yet) you don't just robotically go ""And.. Now.. I.. Fit.. An... S TL model...."". You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that.";;;
822;2;2014-07-23T13:49:01.423;;In A/B testing, bias is handled very well by ensuring visitors are randomly assigned to either version A or version B of the site.  This creates independent samples drawn from the same population.  Because the groups are independent and, on average, only differ in the version of the site seen, the test measures the effect of the design decision.Slight aside: Now you might argue that the A group or B group may differ in some demographic.  That commonly happens by random chance.  To a certain degree this can be taken care of by covariate adjusted randomization.  It can also be taken care of by adding covariates to the model that tests the effect of the design decision.  It should be noted that there is still some discussion about the proper way to do this within the statistics community.  Essentially A/B testing is an application of a Randomized Control Trial to website design.  Some people disagree with adding covariates to the test.  Others, such as Frank Harrel (see Regression Modeling Strategies) argue for the use of covariates in such models.I would offer the following suggestions:Design the study in advance so as to take care of as much sources of bias and variation as possible.   Let the data speak for itself.  As you get more data (like about searches for David Beckham), let it dominate your assumptions about how the data should be (as how the posterior dominates the prior in Bayesian analysis when the sample size becomes large).    Make sure your data matches the assumptions of the model.;;;
823;1;2014-07-23T14:04:31.057;How should ethics be applied in data science;There was a recent furore with facebook experimenting on their users to see if they could alter user's emotions and now okcupid.Whilst I am not a professional data scientist I read about data science ethics from Cathy O'Neill's book 'Doing Data Science' and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science accidentally.Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.The article discusses a code of practice and mentions the Data Science Association's Code of conduct, is this something that is in use? Rule 7 is of particular interest (quoted from their website): (a) A person who consults with a data scientist about the possibility  of forming a client-data scientist relationship with respect to a  matter is a prospective client. (b) Even when no client-data scientist relationship ensues, a data  scientist who has learned information from a prospective client shall  not use or reveal that information. (c) A data scientist subject to paragraph (b) shall not provide  professional data science services for a client with interests  materially adverse to those of a prospective client in the same or a  substantially related industry if the data scientist received  information from the prospective client that could be significantly  harmful to that person in the matterIs this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a senator to react to such things before something happens.By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally.;[education, open-source];153;
824;2;2014-07-23T15:46:48.977;;"(too long for a comment)Basically, @Emre's answer is correct: simple correlation matrix and cosine distance should work well*. There's one subtlety, though - job titles are too short to carry important context. Let me explain this. Imagine LinkedIn profiles (which is pretty good source for data). Normally, they contain 4-10 sentences describing person's skills and qualifications. It's pretty likely that you find phrases like ""lead data scientist"" and ""professional knowledge of Matlab and R"" in a same profile, but it's very unlikely to also see ""junior Java developer"" in it. So we may say that ""lead"" and ""professional"" (as well as ""data scientist"" and ""Matlab"" and ""R"") often occur in same contexts, but they are rarely found together with ""junior"" and ""Java"". Co-occurrence matrix shows exactly this. The more 2 words occur in same context, the more similar their vectors in the matrix will look like. And cosine distance is just a good way to measure this similarity. But what about job titles? Normally they are much shorter and don't actually create enough context to catch similarities. Luckily, you don't need source data to be titles themselves - you need to find similarities between skills in general, not specifically in titles. So you can simply build co-occurrence matrix from (long) profiles and then use it to measure similarity of titles. * - in fact, it's already worked for me on a similar project. ";;;
825;1;2014-07-24T00:01:40.760;Are there any annotators or Named Entity Recognition for license plate numbers?;Most vehicle license/number plate extractors I've found involve reading a plate from an image (OCR) but I'm interested in something that could tag instances of license plates in a body of text. Are there any such annotators out there?;[education, open-source];59;
826;2;2014-07-24T00:34:41.130;;This can be done using regular expressions. 2 letters followed by a number (\d denotes digits) would be[A-Z]{2} \d*2 or 3 letters followed by a number is[A-Z]{2,3} \d*;;;
827;2;2014-07-24T02:23:23.703;;There are a lot of pretty decent tools out there for text annotation in general, and given the broad nature of the task you're approaching (license plates are about as general as words), the annotation tools you are looking at should probably come from the more classical tools for annotation.There was actually a pretty good discussion about annotation tools on this question, which should actually apply to this problem. The most relied-upon thing in annotation right now is probably brat. You can learn more about brat here.Hope that helps! Let me know if you've got any more questions.;;;
828;2;2014-07-24T03:31:32.087;;Reporting back: I ended up coding graphml and using yEd for visualization (just because I am familiar with this combination. I bet gephi or graphviz would work fine and might even be better). Since I computed the location of all nodes, memory was not such big of an issue. Coding graphml is a little easier comparing to coding svg, since I don't have to explicitly specify the placement of edges.;;;
829;1;2014-07-24T06:26:07.290;Pig script code error?;"While running the below pig script I am getting error in line4: If it is GROUP then I am getting error.If I change from 'GROUP' TO 'group' in line4, then the script is running.What is the difference between group and GROUP.LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;WORDSGROUPED = GROUP WORDS BY ZIPS;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;DUMP WORDSSORT;";[education, open-source];50;
830;2;2014-07-24T07:00:13.187;;'group' in strictly lower case in the FOREACH is the thing you are looping/grouping over.http://squarecog.wordpress.com/2010/05/11/group-operator-in-apache-pig/ says: When you group a relation, the result is a new relation with two  columns: “group” and the name of the original relation.Column names are case sensitive, so you have to use lower-case 'group' in your FOREACH.'GROUP' in upper case is the grouping operator. You can't mix them. So don't do that.;;;
831;1;2014-07-24T07:38:07.163;Data Science Association?;"Have you heard of the ""Data Science Association""? URL: http://www.datascienceassn.org/Do you expect it to become a professional body like the Actuaries Institute?If yes, then why?If no, then why not and do you see anyone else becoming the professional body?Lastly, is this question ""on-topic"" ?";[education, open-source];62;
832;1;2014-07-24T14:04:09.533;How do you plot overlapping durations?;My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I'm currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.My question is, how should I adjust the data so that this is able to be plotted?The times for an action can be between 2 seconds and a minute. And, at any given time I would estimate there could be about 100 actions taking place. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.I'm open to any advice on how to think about this issue.Thanks in advance!;[education, open-source];97;1
833;2;2014-07-24T21:14:45.793;;"This can be done in R using ggplot. Based on this question, it could be done with this code where limits is the date range of the plot.tasks <- c(""Task1"", ""Task2"")dfr <- data.frame(name        = factor(tasks, levels = tasks),start.date  = c(""2014-08-07 09:03:25.815"", ""2014-08-07 09:03:25.956""),end.date    = c(""2014-08-07 09:03:28.300"", ""2014-08-07 09:03:30.409""))mdfr <- melt(dfr, measure.vars = c(""start.date"", ""end.date""))mdfr$time<-as.POSIXct(mdfr$value)ggplot(mdfr, aes(time,name)) + geom_line(size = 6) +xlab("""") + ylab("""") +theme_bw()+scale_x_datetime(breaks=date_breaks(""2 sec""),limits = as.POSIXct(c('2014-08-07 09:03:24','2014-08-07 09:03:29')))";;;
834;1;2014-07-25T00:58:12.253;Recommending movies with additional features using collaborative filtering;I am trying to build a recommendation engine using collaborative filtering. I have the usual [user, movie, rating] information. I would like to incorporate an additional feature like 'language' or 'duration of movie'. I am not sure what techniques I could use for such a problem. Please suggest references or packages in python/R. ;[education, open-source];2488;5
835;2;2014-07-25T03:05:38.107;;"Consider, try, and perhaps even use multiple databases. It's not just a ""performance"" issue at play here. It's really going to come down to your requirements. How much data are you talking about? what kind of data? how fast do you need it? Are you more read heavy or write heavy?Here's one thing you can't do in a SQL database: Calculate sentiment. http://www.slideshare.net/shift8/mongodb-machine-learningOf course the speed in that case may not be fast enough for your needs, but it is something that's possible. With some caching of specific aggregate values, it was quite acceptable even. Why would you do this? Convenience.Convenience really is something that you're going to be persuaded by. That's exactly why (in my opinion) NoSQL databases were created. Performance too of course, but I'm trying to discount benchmarks and focus more on other concerns.MongoDB (and some other NoSQL) databases have some very powerful features such as built-in map/reduce. This could result in a savings both in cost and time over using something like Hadoop. Or it could provide a prototype or MVP to launch a larger business.What about graph databases? They're ""NoSQL"" too. Look at databases like OrientDB. If you want to argue performance ...I don't think you're gonna show me a SQL database that's faster there =) ...and graph databases have some really amazing application based on what you need to do.Rule of technology (and the internet) don't get too comfortable with one thing. You're gonna be limited and set yourself up for failure.";;;
836;2;2014-07-25T04:54:07.787;;"Here some resources that might be helpful:Recommenderlab - a framework and open source software for developing and testing recommendation algorithms: http://lyle.smu.edu/IDA/recommenderlab. Corresponding R package recommenderlab: http://cran.r-project.org/package=recommenderlab.The following blog post illustrates the use of recommenderlab package (which IMHO can be generalized for any open source recommendation engine) for building movie recommendation application, based on collaborative filtering: http://scn.sap.com/community/developer-center/hana/blog/2013/11/06/movie-recommendation-by-leveraging-r.Research on recommender systems - a nice webpage with resources on the topic, maintained by Recommenderlab's lead developer Michael Hahsler: http://michael.hahsler.net/research/recommender.Mortar Recommendation Engine - an open source customizable recommendation engine for Hadoop and Pig, written in Python and Java: https://github.com/mortardata/mortar-recsys. Company, sponsoring the development of this project, Mortar Data, offers general commercial cloud platform for development and hosting data science software projects, including ones based on the Mortar Recommendation Engine (development and hosting of public projects are free): http://www.mortardata.com. Mortar Data provides help in form of public Q&A forum (https://answers.mortardata.com) as well as a comprehensive tutorial on building recommendation engine using open technologies (http://help.mortardata.com/data_apps/recommendation_engine).""Introduction to Recommender Systems"" - a relevant Coursera course (MOOC), which content and description provide additional resources on the topic: https://www.coursera.org/course/recsys.PredictionIO - an open source machine learning server software, which allows building data science applications, including recommendation systems: http://prediction.io (source code is available on GitHub: https://github.com/PredictionIO). PredictionIO includes a built-in recommendation engine (http://docs.prediction.io/current/engines/itemrec/index.html) and supports a wide range of programming languages and frameworks via RESTful APIs as well as SDKs/plug-ins. PredictionIO maintains an Amazon Machine Image on AWS Marketplace for deploying applications on the AWS infrastructure: https://aws.amazon.com/marketplace/pp/B00ECGJYGE.Additional open source software projects, relevant to the topic (discovered via MLOSS website on machine learning open source software: http://www.mloss.org):Jubatus: http://jubat.us/enMyMediaLite: http://mymedialite.netTBEEF: https://github.com/ChrisRackauckas/TBEEFPREA: http://prea.gatech.eduCofiRank: http://www.cofirank.orgThe following relevant R blog posts are also interesting:""Simple tools for building a recommendation engine"" (http://blog.revolutionanalytics.com/2012/04/simple-tools-for-building-a-recommendation-engine.html)""Recommendation System in R"" (http://blog.yhathq.com/posts/recommender-system-in-r.html)";;;
837;2;2014-07-25T12:59:00.850;;I would look at the raster package for this, which can read in raw binary data and present it as NxM grids. It can even extract subsets of large binary grids without having to read in the whole file (the R raster object itself is just a proxy to the data, not the data itself).;;;
838;1;2014-07-24T06:34:50.083;Pig latin code error;"While running the below pig script I am getting error in line4: If it is GROUP then I am getting error. If I change from GROUP TO group in line4, then the script is running. What is the difference between group and GROUP.LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;WORDSGROUPED = GROUP WORDS BY ZIPS;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;DUMP WORDSSORT;";[education, open-source];33;
839;2;2014-07-25T09:05:09.493;;"When we do grouping of the data, Pig creates a new key named ""group"" and puts all the tuple matching that key into a bag and associates the bag with the key. So after the group operation schema of the grouped data will be something like raw = load '$input' using PigStorage('\u0001') as (id1:int, name:chararray);groupdata1 = group raw by (id1,name);  describe groupdata1;{group: (id1: int,name: chararray),raw: {(id1: int,name: chararray)}}The 'GROUP' in line 4 you are trying to access is one of the attribute of the schema from the last statement. These attribute name are case sensitive. It will produce the error saying it doesn't exist in the schema. So you need to use 'group' only to access it.";;;
840;1;2014-07-25T17:18:21.393;How to fix similarity matrix in Pandas returning all NaNs?;"I've been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.The calculation completes in IPython but inspection shows the results all come back as NaN.I've also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.Can anyone suggest the best approach to tackle the problem?EDIT: Here's my code thus far:path = ""../../reuters.db""%pylab inlineimport pandas as pdimport numpy as npimport pandas.io.sql as psqlimport sqlite3 as litecon = lite.connect(path)with con:    sql = ""SELECT * FROM Frequency""    df = psql.frame_query(sql, con)    print df.shapedf = df.rename(columns={""term"":""term_id"", ""count"":""count_id""})pivoted = df.pivot('docid', 'term_id', 'count_id')pivoted.to_sparse()similarity_matrix = pivoted.dot(pivoted.T)";[education, open-source];77;
841;2;2014-07-25T18:12:49.847;;Instead of collaborative filtering I would use the matrix factorization approach, wherein users and movies alike a represented by vectors of latent features whose dot products yield the ratings. Normally one merely selects the rank (number of features) without regard to what the features represent, and the algorithm does the rest. Like PCA, the result is not immediately interpretable but it yields good results. What you want to do is extend the movie matrix to include the additional features you mentioned and make sure that they stay fixed as the algorithm estimates the two matrices using regularizastion. The corresponding entries in the user matrix will be initialized randomly, then estimated by the matrix factorization algorithm. It's a versatile and performant approach but it takes some understanding of machine learning, or linear algebra at least.I saw a nice ipython notebook a while back but I can't find it right now, so I'll refer you to another one which, while not as nice, still clarifies some of the maths.;;;
842;1;2014-07-25T18:36:31.340;Data Science Project Ideas;I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.A mix of Data Science and Machine learning would be great.A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.;[education, open-source];8804;7
843;2;2014-07-25T20:50:14.540;;I would try to analyze and solve one or more of the problems published on Kaggle Competitions (https://www.kaggle.com/competitions). Note that the competitions are grouped by their expected complexity, from 101 (bottom of the list) to Research and Featured (top of the list). A color-coded vertical band is a visual guideline for grouping. You can assess time you could spend on a project by adjusting the expected length of corresponding competition, based on your skills and experience.A number of data science project ideas can be found by browsing the following Coursolve webpage: https://www.coursolve.org/browse-needs?query=Data%20Science.If you have skills and desire to work on a real data science project, focused on social impacts, visit DataKind projects page: http://www.datakind.org/projects. More projects with social impacts focus can be found at Data Science for Social Good fellowship webpage: http://dssg.io/projects.Science Project Ideas page at My NASA Data site looks like another place to visit for inspiration: http://mynasadata.larc.nasa.gov/804-2.If you would like to use open data, this long list of applications on Data.gov can provide you with some interesting data science project ideas: http://www.data.gov/applications.;;;
844;1;2014-07-25T21:03:44.663;Using Apache Spark to do ML. Keep getting serializing errors;"so I'm using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.PySpark worker failed with exception:Traceback (most recent call last):  File ""/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py"", line 77, in main    serializer.dump_stream(func(split_index, iterator), outfile)  File ""/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py"", line 191, in dump_stream    self.serializer.dump_stream(self._batched(iterator), stream)  File ""/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py"", line 123, in dump_stream    for obj in iterator:  File ""/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py"", line 180, in _batched    for item in iterator:TypeError: __init__() takes exactly 3 arguments (2 given)and the code for serializers is available hereand my code is here";[education, open-source];512;1
845;2;2014-07-26T00:11:03.637;;"Most often serialization error in (Py)Spark means that some part of your distributed code (e.g. functions passed to map) has dependencies on non-serializable data. Consider following example: rdd = sc.parallelize(range(5))rdd = rdd.map(lambda x: x + 1)rdd.collect()Here you have distributed collection and lambda function to send to all workers. Lambda is completely self-containing, so it's easy to copy its binary representation to other nodes without any worries. Now let's make things a bit more interesting: f = open(""/etc/hosts"")rdd = sc.parallelize(range(100))rdd = rdd.map(lambda x: f.read())rdd.collect()f.close()Boom! Strange error in serialization module! What just happened is that we had attempted to pass f, which is a file object, to workers. Obviously, file object is a handle to local data and thus cannot be sent to other machines. So what's happening in your specific code? Without actual data and knowing record format, I cannot debug it completely, but I guess that problem goes from this line: def vectorizer(text, vocab=vocab_dict):In Python, keyword arguments are initialized when function is called for the first time. When you call sc.parallelize(...).map(vectorizer) just after its definition, vocab_dict is available locally, but remote workers know absolutely nothing about it. Thus function is called with fewer parameters than it expects which results in __init__() takes exactly 3 arguments (2 given) error. Also note, that you follow very bad pattern of  sc.parallelize(...)...collect() calls. First you spread your collection to entire cluster, do some computations, and then pull the result. But sending data back and forth is pretty pointless here. Instead, you can just do these computations locally, and run Spark's parallel processes only when you work with really big datasets (like you main amazon_dataset, I guess).";;;
846;2;2014-07-26T01:12:08.167;;Take something from your everyday life. Create predictor of traffic jams in your region, craft personalised music recommender, analyse car market, etc. Choose real problem that you want to solve - this will not only keep you motivated, but also make you go through the whole development circle from data collection to hypothesis testing. ;;;
847;2;2014-07-26T08:37:52.823;;Introduction to Data Science course that is being run on Coursera now includes real-world project assignment where companies post their problems and students are encouraged to solve them. This is done via coursolve.com (already mentioned here).More information here (you have to be enrolled in the course to see that link);;;
848;2;2014-07-26T16:04:25.363;;"K-means is not the most appropriate algorithm here.The reason is that k-means is designed to minimize variance. This is, of course, appearling from a statistical and signal procssing point of view, but your data is not ""linear"".Since your data is in latitude, longitude format, you should use an algorithm that can handle arbitrary distance functions, in particular geodetic distance functions. Hierarchical clustering, PAM, CLARA, and DBSCAN are popular examples of this.The problems of k-means are easy to see when you consider points close to the +-180 degrees wrap-around. Even if you hacked k-means to use Haversine distance, in the update step when it recomputes the mean the result will be badly screwed. Worst case is, k-means will never converge!";;;
849;2;2014-07-26T21:03:13.470;;Since you want to show so much data, I think that your best choice is going interactive. Check out this demo, it is close to what you want but not quite. It is very difficult to show a lot of data in a single diagram, together with the finest details and the bird-eyes view. But you can let the user interact and look for the details. To show counts, one option is to use color-coding. Take a look at this image (code here): .Here rgb channels have been used to encode (the logarithm of) the number of active events (red), events starting (green) and events ending (blue) for windows of different size. The X axis is time, and the Y axis represents window size, that is, duration. Thus, a point with coordinates (10, 4) represents the interval of time that goes from 10 to 14. To make a lot of data more detailed, it could be a good idea to make the diagram zoomable (like in the demo before), and to give the user the possibility of visualizing just one channel/magnitude.;;;
850;2;2014-07-27T03:58:22.907;;If you've got prior information then you should certainly not use simple mean in a split test. I assume you're trying to just predict which group will produce the greatest amount of revenue overall, by trying to emulate the underlying distribution.Firstly, it's worth noting that any metrics you choose will actually reduce to mean in a pretty trivial way. Eventually mean will necessarily work out, though using a standard bayesian method to estimate the mean is probably your best bet.If you've got a prior then using a standard bayesian approach to update the prior on your mean revenue is probably the best way to do it. Basically, just take the individual results you get and update a multinomial distribution representing your prior in each case.If you want some more full background on multinomial distributions as bayesian priors are pretty well, this Microsoft paper does a pretty good job of outlining it. In general, I wouldn't care so much about the fact that your distribution is technically discrete, as a multinomial distribution will effectively interpolate across your solution space, giving you a continuous distribution that is a very good approximation of your discrete space.;;;
851;1;2014-07-27T21:54:05.003;Computing Image Similarity based on Color Distribution;Image Similarity based on Color Palette DistributionI am trying to compute similarity between two images based on their color palette distribution, let's say I have two sets of key value pairs as follows,Img1: {'Brown': 14, 'White': 13, 'Black': 40, 'Gray': 31}Img2: {'Pink': 82, 'Brown': 8, 'White': 7}Where the numbers denote the % of that color present in the image. What would be the best way to compute similarity on a scale of 0-100 between the two images?;[education, open-source];119;
853;1;2014-07-28T07:19:49.877;Unsupervised Feature Learning for NER;I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.Now I want to make the same NER for different language. The problem here is that I can't use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.My question is, if it's possible to use methods for unsupervised feature learning with CRF algorithm. Did anyone try this and got any good result? Is there any article or tutorial about this matter.I still don't completely understand this way of feature creation so I don't want to spend to much time for something that won't work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.Thank you in advance.;[education, open-source];275;2
854;2;2014-07-28T08:36:53.633;;I like @Kallestad answer very much, but I would like to add a meta-step: Make sure that you understand how the data where collected, and what types of constraints there are. I think it is very common to think that there where no non-obvious steps when the data where collected, but this is not the case: Most of the time, some process or indivudal did somethink with the data, and these steps can and will influence the shape of the data.Two examples:I had a study recently where the data where collected by various contractors worldwide. I was not at the briefing, so that was opaque to me. Unfortunately, the measurements where off for some parts of france: People all liked ice cram, but we expected a random distribution. There was no obvious reason for this uniformity, so I began to hunt the errors. When I queried the contractors, one had misunderstood the briefing and selected only ice-cream lovers from his database.The second error was more challenging: When doing some geographic analysis, I found that a lot of people had extremely large movement patterns, which suggested that a lot of them traveled from Munich to Hamburg in minutes. When I spoke with ppeople upstream, they found a subtle bug in their data aggregation software, which was unnoticed before.Conclusions:Do not assume that your data was collected by perfect processes /humans.Do try to understand the limits of your data providers.Look at individual patterns / values and try to determine if they are logical (easy for movement / geographic data);;;
855;2;2014-07-28T12:07:25.573;;Below you can find a copy of my answer to a related (however, focused on data cleaning aspect) question here on Data Science StackExchange (http://datascience.stackexchange.com/a/722/2452), provided in its entirety for readers' convenience. I believe that it partially answers your question as well and hope it is helpful. While the answer is focused on R ecosystem, similar packages and/or libraries can be found for other data analysis environments. Moreover, while the two cited papers on data preparation also contain examples in R, these papers present general workflow (framework) and best practices that are applicable to any data analysis environment.R contains some standard functions for data manipulation, which can be used for data cleaning, in its base package (gsub, transform, etc.), as well as in various third-party packages, such as stringr, reshape, reshape2, and plyr. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.Additionally, R offers some packages specifically focused on data cleaning and transformation:editrules (http://cran.r-project.org/web/packages/editrules/index.html)deducorrect (http://cran.r-project.org/web/packages/deducorrect/index.html)StatMatch (http://cran.r-project.org/web/packages/StatMatch/index.html)MatchIt (http://cran.r-project.org/web/packages/MatchIt/index.html)DataCombine (http://cran.r-project.org/web/packages/DataCombine)A comprehensive and coherent approach to data cleaning in R, including examples and use of editrules and deducorrect packages, as well as a description of workflow (framework) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.;;;
858;2;2014-07-28T14:48:37.743;;"Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using word2vec features as inputs to your CRF.Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.  These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.For example, vector(""Paris"") - vector(""France"") + vector(""Italy"") yields a vector that is quite similar to vector(""Rome"").  At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information. For that matter, LDA and LSA are also valid options for unsupervised feature learning -- both attempt to represent words as combinations of ""topics"" and output dense word representations.  For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you'll have to train your own model.";;;
859;1;2014-07-28T16:17:49.823;Data science and MapReduce programming model of Hadoop;What are the different classes of data science problems that can be solved using mapreduce programming model?;[education, open-source];403;
860;2;2014-07-28T16:23:39.600;;I'll add one thing- if possible, do a reasonableness check by comparing you data against some other source. It seems that whenever I fail to do this, I get burnt:(;;;
861;2;2014-07-28T16:39:45.703;;"Data Science has many different sub-areas as described in my post). Nearly for each area, scientists and developer has significant contributions. To learn more about what can be done, please look at following websites:Data Mining Algorithms & Machine Learning -> Apache MahoutStatistics -> RHadoopData Warehousing & Database Querying -> SQL-MapReduceSocial Network Analysis -> ArticleBio-informatics -> Article - 1 , Article - 2Also, there are some work on MapReduce + Excel + Cloud combination but I have not found the link. What are the different classes of Data Science problems ...Each ""classes"" is not purely homogeneous problem domain, i.e. some problem cannot be solved via map and reduce approach due to its communication cost, or algorithm behavior. What I mean by behavior is that some problem wants to have control on all data sets instead of chunks. Thus, I refuse to list type of problem ""classes"".Do not forget that knowing what MapReduce can do is not enough for Data Science. You should also aware of What MapReduce can't do, too.";;;
862;2;2014-07-28T17:33:43.280;;"I think that there is no universal technique for ""cleaning"" data before doing actual research. On the other hand, I'm aiming for doing as much reproducible research as possible. By doing reproducible research, if you used cleaning techniques with bugs or with poor parameters/assumptions it could be spot by others.There is nice R package knitr which helps a lot in reproducible research.Of course, not all research could be fully reproduced (for example live Twitter data) , but at least you can document cleaning, formating and preprocessing steps easily.You can check my assessment prepared for Reproducible Research course at Coursera. ";;;
863;2;2014-07-29T13:43:39.060;;Let's first split it into parts. Data Science is about making knowledge from raw data. It uses machine learning, statistics and other fields to simplify (or even automate) decision making. Data science techniques may work with any data size, but more data means better predictions and thus more precise decisions. Hadoop is a common name for a set of tools intended to work with large amounts of data. Two most important components in Hadoop are HDFS and MapReduce.HDFS, or Hadoop Distributed File System, is a special distributed storage capable of holding really large data amounts. Large files on HDFS are split into blocks, and for each block HDFS API exposes its location. MapReduce is framework for running computations on nodes with data. MapReduce heavily uses data locality exposed by HDFS: when possible, data is not transferred between nodes, but instead code is copied to the nodes with data. So basically any problem (including data science tasks) that doesn't break data locality principle may be efficiently implemented using MapReduce (and a number of other problems may be solved not that efficiently, but still simply enough).Let's take some examples. Very often analyst only needs some simple statistics over his tabular data. In this case Hive, which is basically SQL engine over MapReduce, works pretty well (there are also Impala, Shark and others, but they don't use Hadoop's MapReduce, so more on them later). In other cases analyst (or developer) may want to work with previously unstructured data. Pure MapReduce is pretty good for transforming and standardizing data.Some people are used to exploratory statistics and visualization using tools like R. It's possible to apply this approach to big data amounts using RHadoop package. And when it comes to MapReduce-based machine learning Apache Mahout is the first to mention. There's, however, one type of algorithms that work pretty slowly on Hadoop even in presence of data locality, namely, iterative algorithms. Iterative algorithms tend to have multiple Map and Reduce stages. Hadoop's MR framework reads and writes data to disk on each stage (and sometimes in between), which makes iterative (as well as any multi-stage) tasks terribly slow. Fortunately, there are alternative frameworks that can both - use data locality and keep data in memory between stages. Probably, the most notable of them is Apache Spark. Spark is complete replacement for Hadoop's MapReduce that uses its own runtime and exposes pretty rich API for manipulating your distributed dataset. Spark has several sub-projects, closely related to data science: Shark and Spark SQL  provide alternative SQL-like interfaces to data stored on HDFSSpark Streaming makes it easy to work with continuous data streams (e.g. Twitter feed)MLlib implements a number of machine learning algorithms with a pretty simple and flexible APIGraphX enables large-scale graph processingSo there's pretty large set of data science problems that you can solve with Hadoop and related projects. ;;;
864;2;2014-07-30T07:49:14.467;;"map/reduce is most appropriate for parallelizable offline computations. To be more precise, it works best when the result can be found from the result of some function of a partition of the input. Averaging is a trivial example; you can do this with map/reduce by summing each partition, returning the sum and the number of elements in the partition, then computing the overall mean using these intermediate results. It is less appropriate when the intermediate steps depend on the state of the other partitions.";;;
865;1;2014-07-30T10:24:54.180;How to build parse tree with BNF;I need to build parse tree for some source code (on Python or any program language that describe by CFG).So, I have source code on some programming language and BNF this language.Can anybody give some advice how can I build parse tree in this case?Preferably, with tools for Python.;[education, open-source];250;
866;1;2014-07-30T11:45:08.313;Predicting next medical condition from past conditions in claims data;"I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).My goals are to:Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;Do the same as 1 and 2, but with procedures and/or diagnoses.Preferably, the results would be interpretable by a doctorI have looked at things like the Heritage Health Prize Milestone papers and have learned a lot from them, but they are focused on predicting hospitalizations.I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it's been an amazing learning experience.  I have not been able to decide on what ""works"" or ""doesn't work,"" if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?EDIT #2 to add plaintext table:CKD is the target condition, ""chronic kidney disease"", "".any"" denotes that they have acquired that condition at any time, "".isbefore.ckd"" means they had that condition before their frist diagnosis of CKD.  The other abbreviations correspond with other conditions identified by ICD-9CM code groupings.  This grouping occurs in SQL during the import process. Each variable, with the exception of patient_age, is binary.  gender patient_age anx.any art.any ast.any bpa.any can.any cer.any chf.any ckd.any dep.any dia.any end.any flu.any hrt.any hyp.any inf.any men.any ren.any sdp.any1   Male          31       1       0       1       1       0       0       0       0       1       0       1       1       1       1       0       1       0       02 Female          29       1       0       1       1       0       0       0       0       1       0       1       0       0       0       0       0       0       03 Female          31       0       1       1       1       0       0       0       0       0       1       0       0       0       0       0       1       0       04 Female          53       1       1       1       1       1       0       0       0       1       0       1       0       1       0       1       0       0       05   Male          47       0       1       0       0       0       0       0       0       0       0       0       0       0       1       0       1       0       06 Female          48       0       1       1       1       1       0       0       0       0       0       1       0       1       0       1       0       0       0  skn.any tra.any anx.isbefore.ckd art.isbefore.ckd ast.isbefore.ckd bpa.isbefore.ckd can.isbefore.ckd cer.isbefore.ckd chf.isbefore.ckd ckd.isbefore.ckd1       1       1                0                0                0                0                0                0                0                02       0       1                0                0                0                0                0                0                0                03       0       0                0                0                0                0                0                0                0                04       1       0                0                0                0                0                0                0                0                05       0       1                0                0                0                0                0                0                0                06       1       0                0                0                0                0                0                0                0                0  dep.isbefore.ckd dia.isbefore.ckd end.isbefore.ckd flu.isbefore.ckd hrt.isbefore.ckd hyp.isbefore.ckd inf.isbefore.ckd men.isbefore.ckd ren.isbefore.ckd1                0                0                0                0                0                0                0                0                02                0                0                0                0                0                0                0                0                03                0                0                0                0                0                0                0                0                04                0                0                0                0                0                0                0                0                05                0                0                0                0                0                0                0                0                06                0                0                0                0                0                0                0                0                0  sdp.isbefore.ckd skn.isbefore.ckd tra.isbefore.ckd1                0                0                02                0                0                03                0                0                04                0                0                05                0                0                06                0                0                0EDIT to add sample data frame:structure(list(gender = structure(c(1L, 2L, 2L, 2L, 1L, 2L), .Label = c(""Male"",         ""Female""), class = ""factor""), patient_age = c(31, 29, 31, 53, 47, 48), anx.any = c(1, 1, 0, 1, 0, 0), art.any = c(0, 0, 1, 1, 1, 1), ast.any = c(1, 1, 1, 1, 0, 1), bpa.any = c(1, 1, 1, 1, 0, 1), can.any = c(0, 0, 0, 1, 0, 1), cer.any = c(0, 0, 0, 0, 0, 0), chf.any = c(0, 0, 0, 0, 0, 0), ckd.any = c(0, 0, 0, 0, 0, 0), dep.any = c(1, 1, 0, 1, 0, 0), dia.any = c(0, 0, 1, 0, 0, 0), end.any = c(1, 1, 0, 1, 0, 1), flu.any = c(1, 0, 0, 0, 0, 0), hrt.any = c(1, 0, 0, 1, 0, 1), hyp.any = c(1, 0, 0, 0, 1, 0), inf.any = c(0, 0, 0, 1, 0, 1), men.any = c(1, 0, 1, 0, 1, 0), ren.any = c(0, 0, 0, 0, 0, 0), sdp.any = c(0, 0, 0, 0, 0, 0), skn.any = c(1, 0, 0, 1, 0, 1), tra.any = c(1, 1, 0, 0, 1, 0), anx.isbefore.ckd = c(0, 0, 0, 0, 0, 0), art.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ast.isbefore.ckd = c(0, 0, 0, 0, 0, 0), bpa.isbefore.ckd = c(0, 0, 0, 0, 0, 0), can.isbefore.ckd = c(0, 0, 0, 0, 0, 0), cer.isbefore.ckd = c(0, 0, 0, 0, 0, 0), chf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ckd.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dep.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dia.isbefore.ckd = c(0, 0, 0, 0, 0, 0), end.isbefore.ckd = c(0, 0, 0, 0, 0, 0), flu.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hrt.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hyp.isbefore.ckd = c(0, 0, 0, 0, 0, 0), inf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), men.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ren.isbefore.ckd = c(0, 0, 0, 0, 0, 0), sdp.isbefore.ckd = c(0, 0, 0, 0, 0, 0), skn.isbefore.ckd = c(0, 0, 0, 0, 0, 0), tra.isbefore.ckd = c(0, 0, 0, 0, 0, 0)), .Names = c(""gender"", ""patient_age"", ""anx.any"", ""art.any"", ""ast.any"", ""bpa.any"", ""can.any"", ""cer.any"", ""chf.any"", ""ckd.any"", ""dep.any"", ""dia.any"", ""end.any"", ""flu.any"", ""hrt.any"", ""hyp.any"", ""inf.any"", ""men.any"", ""ren.any"", ""sdp.any"", ""skn.any"", ""tra.any"", ""anx.isbefore.ckd"", ""art.isbefore.ckd"", ""ast.isbefore.ckd"", ""bpa.isbefore.ckd"", ""can.isbefore.ckd"", ""cer.isbefore.ckd"", ""chf.isbefore.ckd"", ""ckd.isbefore.ckd"", ""dep.isbefore.ckd"", ""dia.isbefore.ckd"", ""end.isbefore.ckd"", ""flu.isbefore.ckd"", ""hrt.isbefore.ckd"", ""hyp.isbefore.ckd"", ""inf.isbefore.ckd"", ""men.isbefore.ckd"", ""ren.isbefore.ckd"", ""sdp.isbefore.ckd"", ""skn.isbefore.ckd"", ""tra.isbefore.ckd""), row.names = c(NA, 6L), class = ""data.frame"")";[education, open-source];213;1
867;1;2014-07-30T13:55:36.187;Relative merits of different open source natural language generators;Does anyone know what (from your experience) is the best open source natural language generators (NLG) out there? What are the relative merits of each?I'm looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries.;[education, open-source];185;
868;2;2014-07-30T15:32:44.557;;There is a paper you should look into:MapReduce: Distributed Computing for Machine LearningThey distinguish 3 classes of machine-learning problems that are reasonable to address with MapReduce:Single pass algorithmsIterative algorithmsQuery based algorithmsThey also give examples for each class.;;;
869;1;2014-07-30T16:27:45.177;Neural Network parse string data?;"So, I'm just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I've seen how an artificial neural network can parse image data and categorize the images (demo with convnetjs), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.However, I'm trying to wrap my head around if this is possible to be done with string inputs? The use-case I've got is a ""recommendation engine"" for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine ""downsampling"" the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I'd need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I've seen don't allow adding new neurons after the system has been trained?Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what's a better tool for pattern-matching in string data)?";[education, open-source];495;
870;2;2014-07-30T17:42:09.797;;"This is not a problem about neural networks per se, but about representing textual data in machine learning. You can represent the movies, cast, and theme as categorical variables. The plot is more complicated; you'd probably want a topic model for that, but I'd leave that out until you get the hang of things. It does precisely that textual ""downsampling"" you mentioned.Take a look at this tutorial to learn how to encode categorical variables for neural networks. And good luck!";;;
871;2;2014-07-30T17:53:40.330;;Using a neural network for prediction on natural language data can be a tricky task, but there are tried and true methods for making it possible.  In the Natural Language Processing (NLP) field, text is often represented using the bag of words model.  In other words, you have a vector of length n, where n is the number of words in your vocabulary, and each word corresponds to an element in the vector.  In order to convert text to numeric data, you simply count the number of occurrences of each word and place that value at the index of the vector that corresponds to the word. Wikipedia does an excellent job of describing this conversion process.  Because the length of the vector is fixed, its difficult to deal with new words that don't map to an index, but there are ways to help mitigate this problem (lookup feature hashing).  This method of representation has many disadvantages -- it does not preserve the relationship between adjacent words, and results in very sparse vectors.  Looking at n-grams helps to fix the problem of preserving word relationships, but for now let's focus on the second problem: sparsity.  It's difficult to deal directly with these sparse vectors (many linear algebra libraries do a poor job of handling sparse inputs), so often the next step is dimensionality reduction. For that we can refer to the field of topic modeling:  Techniques like Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) allow the compression of these sparse vectors into dense vectors by representing a document as a combination of topics.  You can fix the number of topics used, and in doing so fix the size of the output vector producted by LDA or LSA. This dimensionality reduction process drastically reduces the size of the input vector while attempting to lose a minimal amount of information.  Finally, after all of these conversions, you can feed the outputs of the topic modeling process into the inputs of your neural network.     ;;;
872;1;2014-07-30T18:45:13.790;R error using package tm (text-mining);"I am attempting to use the tm package to convert a vector of text strings to a corpus element.My code looks something like thisCorpus(d1$Yes)where d1$Yes is a factor with 124 levels, each containing a text string.For example, d1$Yes[246] = ""So we can get the boat out!""I'm receiving the following error:""Error: inherits(x, ""Source"") is not TRUE""I'm not sure how to remedy this.";[education, open-source];618;1
873;2;2014-07-30T19:15:09.383;;You have to tell Corpus what kind of source you are using.  Try:Corpus(VectorSource(d1$Yes));;;
874;2;2014-07-30T21:53:13.790;;"I have some thoughts about your question. I hope it may help you to solve your problem. I'm planning to run some experiments with very large data sets, and I'd like to distribute the computation.In one of my posts, I have done research on topic of evaluation methods of Data Science. With Learning Curve, you can evaluate your experiments learning ability. To talk a bit more, you will fix commodity configuration, and then will run same experiment on the same number of machines with different size of data set you have (from starting from small chunk in size, incrementally increase the size until you reach the whole data set). To point on, you should avoid having power distribution for result of performance test being run with different size of data sets. To avoid, you should carefully choose step size (step size = amount of increments). I have about ten machines available, each with 200 GB of free space on hard disk. However, I would like to perform experiments on a greater number of nodes, to measure scalability more precisely.For this type question, I have intuitively searched and read materials; afterwards, published as a blog post. At the end of the post, I have briefly talked about how to test your hypothesis on real complex system. If you let me, I want to briefly talk about;First of all, base requirement should be formed in order to run data set as a whole. The minimum requirement will build your baseline evaluation score which is calculated with one/combination of evaluation metrics you have chosen, or with one/combination of methods being used to calculate Running Time = Computation complexity + Communication cost + Synchronization cost. After those steps, with an evaluation strategy, add new elements, e.g. new node, to the system you have doing scalability test; meanwhile, for each addition, measure performance w.r.t new system configuration. Just to note, evaluation strategy must be planned along with considerations of default behavior of parallel and distributed systems. For example, what I mean by behavior is that adding just more cores will, after some point, automatically drop performance of the system not due to your algorithm characteristics. It is because more cores need more RAMs, more hard driver, or etc. In other words, there is a N-Way relationship between hardware components. As a second example, adding more nodes to the distributed system will punished you with more communication and synchronization costs.As a last step, you will sketch two different graphs with your evaluation results via data analysis program or language (As a recommendation, use GNU Plot or R programming language). Print out and put those results at your desktop, and start to examine them, carefully. According to your investigation, modify/erase + rebuild evaluation strategy and re-do the performance test. Are there commodity services which would grant me that only my application would be running at a given time? Has anyone used such services yet?I have no much experiment on commodity services, but I can easily say whether it grants or not depends on your configuration of services. If you configured say Hadoop to your node as an only service, Hadoop will grant your code will be only running at any time.";;;
875;2;2014-07-30T23:40:54.700;;I suggest you use ANTLR, which is a very powerful parser generator. It has a good GUI for entering your BNF. It has a Python target capability.;;;
876;2;2014-07-31T16:39:37.127;;"""Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease""I'm not sure that it's possible to ID the most influential conditions; I think it will depend on what model you're using. Just yesterday I fit a random forest and a boosted regression tree to the same data, and the order and relative importance each model gave for the variables were quite different.";;;
877;1;2014-07-31T18:52:56.307;What is the use of user data collection besides serving ads?;Well this looks like the most suited place for this question.   Every website collect data of the user, some just for usability and personalization, but the majority like social networks track every move on the web, some free apps on your phone scan text messages, call history and so on.All this data siphoning is just for selling your profile for advertisers?;[education, open-source];141;1
878;2;2014-07-31T23:03:33.767;;A couple of days ago developers from one product company asked me how they can understand why new users were leaving their website. My first question to them was what these users' profiles looked like and how they were different from those who stayed. Advertising is only top of an iceberg. User profiles (either filled by users themselves or computed from users' behaviour) hold information about: user categories, i.e. what kind of people tend to use your website/productpaying client portraits, i.e. who is more likely to use your paid servicesUX component performance, e.g. how long it takes people to find the button they needaction performance comparison, e.g. what was more efficient - lower price for a weekend or propose gifts with each buy, etc. So it's more about improving product and making better user experience rather than selling this data to advertisers. ;;;
879;2;2014-08-01T06:03:47.700;;Most companies won't sell the data, not on any small scale anyways.  Most will use it internally.User tracking data is important for understanding a lot of things.  There's basic A/B testing where you provide different experiences to see which is more effective.  There is understanding how your UI is utilized.  Categorizing your end users in different ways for a variety of reasons.  Figuring out where your end user base is, and within that group where the end users that matter are.  Correlating user experiences with social network updates.  Figuring out what will draw people to your product and what drives them away.  The list of potential for data mining and analysis projects could go on for days.  Data storage is cheap.  If you track everything out of the gate, you can figure out what you want to do with that data later.  Scanning text messages is sketchy territory when there isn't a good reason for it.  Even when there is a good reason it's sketchy territory.  I'd love to say that nobody does it, but there have been instances where big companies have done it and there are a lot of cases where no-name apps at least require access to that kind of data for installation.  I generally frown on that kind of thing myself as a consumer, but the data analyst in me would love to see if I could build anything useful from a set of information like that.;;;
880;2;2014-08-01T12:27:41.580;;There always is the solution to try both approaches and keep the one that maximizes the expected performances. In your case, I would assume you prefer minimizing false negatives at the cost of some false positive, so you want to bias your classifier against the strong negative prior, and address the imbalance by reducing the number of negative examples in your training set.Then compute the precision/recall, or sensitivity/specificity, or whatever criterion suits you on the full, imbalanced, dataset to make sure you haven't ignored a significant pattern present in the real data while building the model on the reduced data.;;;
881;2;2014-08-01T14:08:13.043;;"I've never worked with medical data, but from general reasoning I'd say that relations between variables in healthcare are pretty complicated. Different models, such as random forests, regression, etc. could capture only part of relations and ignore others. In such circumstances it makes sense to use general statistical exploration and modelling. For example, the very first thing I would do is finding out correlations between possible precursor conditions and diagnoses. E.g. in what percent of cases chronic kidney disease was preceded by long flu? If it is high, it doesn't always mean causality, but gives pretty good food for thought and helps to better understand relations between different conditions. Another important step is data visualisation. Does CKD happens in males more often than in females? What about their place of residence? What is distribution of CKD cases by age? It's hard to grasp large dataset as a set of numbers, plotting them out makes it much easier. When you have an idea of what's going on, perform hypothesis testing to check your assumption. If you reject null hypothesis (basic assumption) in favour of alternative one, congratulations, you've made ""something real"". Finally, when you have a good understanding of your data, try to create complete model. It may be something general like PGM (e.g. manually-crafted Bayesian network), or something more specific like linear regression or SVM, or anything. But in any way you will already know how this model corresponds to your data and how you can measure its efficiency. As a good starting resource for learning statistical approach I would recommend Intro to Statistics course by Sebastian Thrun. While it's pretty basic and doesn't include advanced topics, it describes most important concepts and gives systematic understanding of probability theory and statistics. ";;;
882;2;2014-08-01T14:10:38.267;;Here's a practical example of using web data for something other than advertising. Distil Networks (disclaimer, I work there) uses network traffic to determine whether page accesses are from humans or bots - scrapers, click fraud, form spam, etc.Another example is some of the work that Webtrends is doing. They allow site users to build a model for each visitor to predict whether they'll leave, buy, add to cart, etc. Then based on the probability of each action you can change the users experience (e.g. if they're about to leave, give them a coupon). Here's the slides from a talk by them: http://www.oscon.com/oscon2014/public/schedule/detail/34809;;;
883;1;2014-08-01T18:13:06.063;R aggregate() with dates;"I am working on a data set that has multiple traffic speed measurements per day. My data is from the city of chicago, and it is taken every minute for about six months. I wanted to consolidate this data into days only, so this is what I did:traffic <- read.csv(""path.csv"",header=TRUE)traffic2 <- aggregate(SPEED~DATE, data=traffic, FUN=MEAN)this was perfect because it took all of my data and averaged it by date. For example, my original data looked something like this:DATE        SPEED  12/31/2012   2212/31/2012   2512/31/2012   23...and the final looked like this: DATE        SPEED 10/1/2012    2210/2/2012    2310/3/2012    22...The only problem, is my data is supposed to start at 9/1/2012. I plotted this data, and it turns out the data goes from 10/1/2012-12/31/2012 and then 9/1/2012-9/30/2012.What in the world is going on here?";[education, open-source];255;
884;2;2014-08-01T18:16:57.543;;"So I was never able to find the error by looking through my logs. I ended up reinstalling it with CDH5 (which was MUCH easier than installing ""poor"" Hadoop)Now everything runs fine! I'm still having trouble getting things to save to the hdfs, but thats a question for another day... ";;;
886;2;2014-08-01T23:21:34.360;;"I am going to agree with @user1683454's comment. After importing, your DATE column is of either character, or factor class (depending on your settings for stringsAsFactors). Therefore, I think that you can solve this issue in at least several ways, as follows:1) Convert data to correct type during import. To do this, just use the following options of read.csv(): stringsAsFactors (or as.is) and colClasses. By default, you can specify conversion to Date or POSIXct classes. If you need a non-standard format, you have two options. First, if you have a single Date column, you can use as.Date.character() to pass the desired format to colClasses. Second, if you have multiple Date columns, you can write a function for that and pass it to colClasses via setAs(). Both options are discussed here: http://stackoverflow.com/questions/13022299/specify-date-format-for-colclasses-argument-in-read-table-read-csv.2) Convert data to correct format after import. Thus, after calling read.csv(), you would have to execute the following code: dateColumn <- as.Date(dateColumn, ""%m/%d/%Y"") or dateColumn <- strptime(dateColumn, ""%m/%d/%Y"") (adjust the format to whatever Date format you need).";;;
887;1;2014-08-02T00:07:09.267;Modelling on one Population and Evaluating on another Population;I am currently on a project that will build a model (train and test) on Client-side Web data, but evaluate this model on Sever-side Web data.  Unfortunately building the model on Server-side data is not an option, nor is it an option to evaluate this model on Client-side data.This model will be based on metrics collected on specific visitors.  This is a real time system that will be calculating a likelihood based on metrics collected while visitors browse the website.I am looking for approaches to ensure the highest possible accuracy on the model evaluation.So far I have the following ideas,Clean the Server-side data by removing webpages that are never seen Client-side.Collect additional data Server-side data to make the Server-side data more closely resemble Client-side data.Collect data on the Client and send this data to the Server.  This is possible and may be the best solution, but is currently undesirable. Build one or more models that estimate Client-side Visitor metrics from Server-side Visitor metrics and use these estimates in the Likelihood model.Any other thoughts on evaluating over one Population while training (and testing) on another Population?;[education, open-source];37;
888;2;2014-08-02T04:21:49.927;;I'm not an expert on this, so take my advice with a grain of salt. It's not clear for me what is the relationship between server-side and client-side data. Are they both representative of the same population? If Yes, I think it's OK to use different data sets for testing/training and evaluating your models. If No, I think it might be a good idea to use some resampling technique, such as bootstrapping, jackknifing or cross-validation.;;;
889;2;2014-08-02T04:54:14.757;;If the users who you are getting client-side data from are from the same population of users who you would get server-side data from. If that is true, then you aren't really training on one population and applying to another. The main difference is that the client side data happened in the past (by necessity unless you are constantly refitting your model) and the server side data will come in the future.Let's reformulate the question in terms of models rather than web clients and servers.You are fitting a model on one dataset and applying it to another. That is the classic use of predictive modeling/machine learning. Models use features from the data to make estimates of some parameter or parameters. Once you have a fitted (and tested) model, all that you need is the same set of features to feed into the model to get your estimates.Just make sure to model on a set of features (aka variables) that are available on the client-side and server-side. If that isn't possible, ask that question separately.;;;
890;2;2014-08-02T14:55:37.347;;If your work is parallelizable enough for a distributed network of cpus to make a difference, why not try to run it on the gpu instead? That will require rather less investment than a large network of cpus with individual software licenses and still provide parallel processing which you can do runtime tracking on yourself.;;;
891;2;2014-08-02T15:02:42.437;;"As others have pointed out, these are not distance ""metrics"", because they do not satisfy the metric criteria. Say instead ""distance measure"".Anyway, what are you measuring and why? That information will help us give a more useful answer for your situation.";;;
893;1;2014-08-03T13:07:24.143;How to get correlation between two categorical variable and a categorical variable and continuous variable?;I am building a regression model and I need to calculate the below to check for correlationsCorrelation between 2 Multi level categorical variablesCorrelation between a Multi level categorical variable andcontinuous variable VIF(variance inflation factor) for a Multilevel categorical variablesI believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. Please answer the below questionsWhich correlation coefficient works best for the above cases ? VIF calculation only works for continuous data so what is thealternative? What are the assumptions I need to check before I use the correlation coefficient you suggest? How to implement them in SAS & R?;[education, open-source];3356;4
895;1;2014-08-03T16:09:35.750;Facebook's Huge Database;I assume that each person on Facebook is represented as a node (of a Graph) in Facebook, and relationship/friendship between each person(node) is represented as an edge between the involved nodes.Given that there are millions of people on Facebook, how is the Graph stored?;[education, open-source];125;1
896;1;2014-08-03T16:38:38.853;How to implement Brown Clustering Algorithm in O(|V|k^2);I am trying to implement the Brown Clustering Algorithm (link to paper)The algorithm is supposed to in O(|V|k^2) where |V| is the size of the vocabulary and k is the number of clusters. I am unable to implement it this efficiently. In fact, the best I can manage is O(|V|k^3) which is too slow. My current implementation for the main part of the algorithm is as follows:for w = number of clusters + 1 to |V|{   word = next most frequent word in the corpus   assign word to a new cluster    initialize MaxQuality to 0   initialize ArgMax vector to (0,0)   for i = 0 to number of clusters - 1    {      for j = i to number of clusters      {         Quality = Mutual Information if we merge cluster i and cluster j         if Quality > MaxQuality         {            MaxQuality = Quality             ArgMax = (i,j)          }      }   }} I compute quality as follows:1. Before entering the second loop compute the pre-merge quality i.e. quality before doing any merges.2. Every time a cluster-pair merge step is considered:    i. assign quality := pre-merge quality   ii. quality = quality - any terms in the mutual information equation that contain cluster i or cluster j (pre-merge)  iii. quality = quality + any terms in the mutual information equation that contain (cluster i U cluster j)  (post-merge)In my implementation, the first loop has approx |V| iterations, the second and third loop approx k iterations each. To compute quality at each step requires approx a further k iterations. In total it runs in (|V|k^3) time.How do you get it to run in (|V|k^2)?;[education, open-source];254;
897;2;2014-08-03T20:21:08.150;;Having worked with Facebook data a bit (harvested from Facebook users) we stored it just as a pair of values: USER_ID, FRIEND_USER_ID.But I guess your questions is a bit deeper? You can store it in different ways, depending on your research question. One interesting option is triads for example - http://mypersonality.org/wiki/doku.php?id=list_of_variables_available#triads;;;
898;2;2014-08-04T09:42:03.590;;"Two Categorical VariablesChecking if two categorical variables are independent can be done with Chi-Squared test of independence. This is a typical Chi-Square test: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are.There also exists a Crammer's V that is a measure of correlation that follows from this testExampleSuppose we have two variablesgender: male and femalecity: Blois and ToursWe observed the following data: Are gender and city independent? Let's perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way. Under the Null hypothesis, we assume uniform distribution. So our expected values are the followingSo we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.To compute Crammer's V we first find the normalizing factor chi-squared-max which is typically the size of the sample, divide the chi-square by it and take a square root Rtbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))chi2 = chisq.test(tbl, correct=F)c(chi2$statistic, chi2$p.value)Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the ""correlation"" here is 0.08We also compute V: sqrt(chi2$statistic / sum(tbl))And get 0.14 (the smaller v, the lower the correlation) Consider another dataset     GenderCity  M  F   B 51 49   T 24 26For this, it would give the followingtbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))chi2 = chisq.test(tbl, correct=F)c(chi2$statistic, chi2$p.value)sqrt(chi2$statistic / sum(tbl))The p-value is 0.72 which is far closer to 1, and v is 0.03 - very close to 0Categorical vs Numerical VariablesFor this type we typically perform One-way ANOVA test: we calculate in-group variance and intra-group variance and then compare them.ExampleWe want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from here)Is there any dependence between the variables?For that we conduct ANOVA test and see that the p-value is just 0.007 - there's no correlation between these variables. Rt1 = c(164, 172, 168, 177, 156, 195)t2 = c(178, 191, 197, 182, 185, 177)t3 = c(175, 193, 178, 171, 163, 176)t4 = c(155, 166, 149, 164, 170, 168)val = c(t1, t2, t3, t4)fac = gl(n=4, k=6, labels=c('type1', 'type2', 'type3', 'type4'))aov1 = aov(val ~ fac)summary(aov1)Output is             Df Sum Sq Mean Sq F value  Pr(>F)   fac          3   1636   545.5   5.406 0.00688 **Residuals   20   2018   100.9                   ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1So we can take the p-value as the measure of correlation here as well.Referenceshttps://en.wikipedia.org/wiki/Chi-square_testhttp://0agr.ru/wiki/index.php/Chi-square_Test_of_Independencehttp://courses.statistics.com/software/R/R1way.htmhttp://0agr.ru/wiki/index.php/One-Way_ANOVA_F-Testhttp://0agr.ru/wiki/index.php/Crammer%27s_Coefficient";;;
899;2;2014-08-04T09:48:09.660;;"When I worked with social network data, we stoted the ""friendship"" relation in a database in the table Friends(friend_a, friend_b, ...) with a B-Tree index on (friend_a, friend_b) plus also some partitioning.In our case it was a little bit different since the graph was directed, so it wasn't really ""friendship"", but rather ""following/follower"" relationship. But for friendship I would just store two edges: both (friend_a, friend_b) and (friend_b, friend_a)We used MySQL to store the data, if it matters, but I guess it shouldn't.";;;
900;2;2014-08-04T15:48:29.757;;I think hierarchical clustering would be more time efficient in your case (with a single dimension).Depending on your task, you may implement something like this:Having N datapoints di with their 1-dimension value xi:Sort datapoints based on their xi value. Calculate distances between adjacent datapoints (N-1 distances). Each distance must be assigned a pair of original datapoints (di, dj).Sort distances in descending order to generate list of datapoint pairs (di, dj), starting from the closest one.Iteratively unite datapoints (di, dj) into clusters, starting from beginning of the list (the closest pair). (Depending on current state of di and dj, uniting them means: (a) creating new cluster for two unclustered datapoints, (b) adding a datapoint to existing cluster and (c) uniting two clusters.)Stop uniting, if the distance is over some threshold.Create singleton clusters for datapoints which did not get into clusters.This algorithm implements single linkage clustering. It can be tuned easily to implement average linkage. Complete linkage will be less efficient, but maybe easier ones will give good results depending on your data and task.I believe for 200K datapoints it must take under second, if you use proper data structures for above operations.;;;
901;2;2014-08-04T15:59:12.573;;Strange as it sounds, graphs and graph databases are typically implemented as linked lists. As alluded to here, even the most popular/performant graph database out there (neo4j), is secretly using something akin to a doubly-linked list.Representing a graph this way has a number of significant benefits, but also a few drawbacks. Firstly, representing a graph this way means that you can do edge-based insertions in near-constant time. Secondly, this means that traversing the graph can happen extremely quickly, if we're only looking to either step up or down a linked list.The biggest drawback of this though comes from something sometimes called The Justin Bieber Effect, where nodes with a large number of connections tend to be extremely slow to evaluate. Imagine having to traverse a million semi-redundant links every time someone linked to Justin Bieber.I know that the awesome folks over at Neo4j are working on the second problem, but I'm not sure how they're going about it, or how much success they've had.;;;
902;1;2014-08-04T19:10:57.187;When is there enough data for generalization?;Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.Edit:  In addition to generalizing about a population, given a sample, I'm interested in generalizing about an individual's overall search behavior, given a time slice.  Theory and paper references are a plus!;[education, open-source];117;
903;1;2014-08-04T22:27:10.837;Analyzing A/B test results which are not normally distributed, using independent t-test;I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution. In fact the distribution resembles more closely the Landau Distribution.I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.But my question is: At what point can one say that the t-test is not a good method of significance testing?Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?;[education, open-source];197;2
904;1;2014-08-04T19:21:45.067;What do you use to generate a dashboard in R?;I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don't require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: but applied to web analytics. Any suggestions on what packages to use creating these dashboard reports? My first intuition is to use R markdown and knitr, but perhaps you've found a better solution. I can't seem to find rich examples of dashboards generated from R. ;[education, open-source];2775;8
905;2;2014-08-04T19:28:38.173;;"Shiny is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with knitr, and Shiny development is fully integrated into RStudio. There's even a free service called shinyapps.io for hosting Shiny apps, the shiny package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There's plenty more info in the Tutorial section of the site.Since it essentially ""compiles"" the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted.edit: I just realized you don't need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There's also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF.";;;
906;2;2014-08-04T23:37:27.877;;PajekXXL is designed to handle enormous networks. But Pajek is also kind of a bizarre program with an unintuitive interface.;;;
907;2;2014-08-05T07:17:34.750;;"I think that Shiny is an overkill in this situation and doesn't match your requirement of dashboard reports to be static. I guess, that your use of the term ""dashboard"" is a bit confusing, as some people might consider that it has more emphasis of interactivity (real-time dashboards), rather than information layout, as is my understanding (confirmed by the ""static"" requirement).My recommendation to you is to use R Markdown and knitr, especially since these packages have much lower learning curve than Shiny. Moreover, I have recently run across an R package, which, in my view, ideally suits your requirement of embedding small charts/plots in a report, as presented on your picture above. This package generates static or dynamic graphical tables and is called sparkTable (http://cran.r-project.org/web/packages/sparkTable). Its vignette is available here (there is no link to it on the package's home page): http://publik.tuwien.ac.at/files/PubDat_228663.pdf. Should you ever need some interactivity, sparkTable provides some via its simple interface to Shiny.";;;
908;2;2014-08-05T08:09:17.240;;It is my understanding that random sampling is a mandatory condition for making any generalization statements. IMHO, other parameters, such as sample size, just affect probability level (confidence) of generalization. Furthermore, clarifying the @ffriend's comment, I believe that you have to calculate needed sample size, based on desired values of confidence interval, effect size, statistical power and number of predictors (this is based on Cohen's work - see References section at the following link). For multiple regression, you can use the following calculator: http://www.danielsoper.com/statcalc3/calc.aspx?id=1.More information on how to select, calculate and interpret effect sizes can be found in the following nice and comprehensive paper, which is freely available: http://jpepsy.oxfordjournals.org/content/34/9/917.full.If you're using R (and even, if you don't), you may find the following Web page on confidence intervals and R interesting and useful: http://osc.centerforopenscience.org/static/CIs_in_r.html.Finally, the following comprehensive guide to survey sampling can be helpful, even if you're not using survey research designs. In my opinion, it contains a wealth of useful information on sampling methods, sampling size determination (including calculator) and much more: http://home.ubalt.edu/ntsbarsh/stat-data/Surveys.htm.;;;
909;2;2014-08-05T08:12:15.647;;The distribution of your data doesn't need to be normal, it's the Sampling Distribution that has to be nearly normal. If your sample size is big enough, then the sampling distribution of means from Landau Distribution should to be nearly normal, due to the Central Limit Theorem. So it means you should be able to safely use t-test with your data.ExampleLet's consider this example: suppose we have a population with Lognormal distribution with mu=0 and sd=0.5 (it looks a bit similar to Landau)So we sample 30 observations 5000 times from this distribution each time calculating the mean of the sample And this is what we get Looks quite normal, doesn't it? If we increase the sample size, it's even more apparentR codex = seq(0, 4, 0.05)y = dlnorm(x, mean=0, sd=0.5)plot(x, y, type='l', bty='n')n = 30m = 1000set.seed(0)samp = rep(NA, m)for (i in 1:m) {  samp[i] = mean(rlnorm(n, mean=0, sd=0.5))}hist(samp, col='orange', probability=T, breaks=25, main='sample size = 30')x = seq(0.5, 1.5, 0.01)lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))n = 300samp = rep(NA, m)for (i in 1:m) {  samp[i] = mean(rlnorm(n, mean=0, sd=0.5))}hist(samp, col='orange', probability=T, breaks=25, main='sample size = 300')x = seq(1, 1.25, 0.005)lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)));;;
910;2;2014-08-05T09:07:42.393;;"Here are the basic Natural Language Processing capabilities (or annotators) that are usually necessary to extract language units from textual data for sake of search and other applications:Sentence breaker - to split text (usually, text paragraphs) to sentences. Even in English it can be hard for some cases like ""Mr. and Mrs. Brown stay in room no. 20.""Tokenizer - to split text or sentences to words or word-level units. This task is not trivial for languages with no spaces and no stable understanding of word boundaries (e.g. Chinese, Japanese)Part-of-speech Tagger - to guess part of speech of each word in the context of sentence; usually each word is assigned a so-called POS-tag from a tagset developed in advance to serve your final task (for example, parsing).Lemmatizer - to convert a given word into its canonical form (lemma). Usually you need to know the word's POS-tag. For example, word ""heating"" as gerund must be converted to ""heat"", but as noun it must be left unchanged.Parser - to perform syntactic analysis of the sentence and build a syntactic tree or graph. There're two main ways to represent syntactic structure of sentence: via constituency or dependency.Summarizer - to generate a short summary of the text by selecting a set of top informative sentences of the document, representing its main idea. However can be done in more intelligent manner than just selecting the sentences from existing ones.Named Entity Recognition - to extract so-called named entities from the text. Named entities are the chunks of words from text, which refer to an entity of certain type. The types may include: geographic locations (countries, cities, rivers, ...), person names, organization names etc. Before going into NER task you must understand what do you want to get and, possible, predefine a taxonomy of named entity types to resolve.Coreference Resolution - to group named entities (or, depending on your task, any other text units) into clusters corresponding to a single real object/meaning. For example, ""B. Gates"", ""William Gates"", ""Founder of Microsoft"" etc. in one text may mean the same person, referenced by using different expressions.There're many other interesting NLP applications/annotators (see NLP tasks category), sentiment analysis, machine translation etc.). There're many books on this, the classical book: ""Speech and Language Processing"" by Daniel Jurafsky and James H. Martin., but it can be too detailed for you.";;;
911;2;2014-08-05T10:15:33.713;;Basically an independent t-test or a 2 sample t-test is used to check if the averages of the two samples are significantly different. Or, to put in another words, if there is a significant difference between the means of the two samples. Now, the means of those 2 samples are two statistics, which according with CLT, have a normal distribution, if provided enough samples. Note that CLT works no matter of the distribution from which the mean statistic is built. Normally one can use a z-test, but if the variances are estimated from the sample (because it is unknown), some additional uncertainty is introduced, which is incorporated in t distribution. That's why 2-sample t-test applies here.  ;;;
912;2;2014-08-05T12:29:05.300;;There is an excellent comparison of the common inner-product-based similarity metrics here.In particular, Cosine Similarity is normalized to lie within [0,1], unlike the dot product which can be any real number, but, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that's a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can't imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward.;;;
913;2;2014-08-05T12:58:16.000;;"There are two rules for generalizability:The sample must be representative. In expectation, at least, the distribution of features in your sample must match the distribution of features in the population. When you are fitting a model with a response variable, this includes features that you do not observe, but that affect any response variables in your model. Since it is, in many cases, impossible to know what you do not observe, random sampling is used.The idea with randomization is that a random sample, up to sampling error, must accurately reflect the distribution of all features in the population, observed and otherwise. This is why randomization is the ""gold standard,"" but if sample control is available by some other technique, or it is defensible to argue that there are no omitted features, then it isn't always necessary.Your sample must be large enough that the effect of sampling error on the feature distribution is relatively small. This is, again, to ensure representativeness. But deciding who to sample is different from deciding how many people to sample.Since it sounds like you're fitting a model, there's the additional consideration that certain important combinations of features could be relatively rare in the population. This is not an issue for generalizability, but it bears heavily on your considerations for sample size. For instance, I'm working on a project now with (non-big) data that was originally collected to understand the experiences of minorities in college. As such, it was critically important to ensure that statistical power was high specifically in the minority subpopulation. For this reason, blacks and Latinos were deliberately oversampled. However, the proportion by which they were oversampled was also recorded. These are used to compute survey weights. These can be used to  re-weight the sample so as to reflect the estimated population proportions, in the event that a representative sample is required.An additional consideration arises if your model is hierarchical. A canonical use for a hierarchical model is one of children's behavior in schools. Children are ""grouped"" by school and share school-level traits. Therefore a representative sample of schools is required, and within each school a representative sample of children is required. This leads to stratified sampling. This and some other sampling designs are reviewed in surprising depth on Wikipedia.";;;
915;1;2014-08-05T18:36:12.753;How do various statistical techniques (regression, PCA, etc) scale with sample size and dimension?;Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc.;[education, open-source];143;1
916;2;2014-08-05T20:24:09.200;;Most of the efficient (and non trivial) statistic algorithms are iterative in nature so that the worst case analysis O() is irrelevant as the worst case is 'it fails to converge'.Nevertheless, when you have a lot of data, even the linear algorithms (O(n)) can be slow and you then need to focus on the constant 'hidden' behind the notation. For instance, computing the variance of a single variate is naively done scanning the data twice (once for computing an estimate of the mean, and then once to estimate the variance). But it also can be done in one pass.For iterative algorithms, what is more important is convergence rate and number of parameters as a function of the data dimensionality, an element that greatly influences convergence. Many models/algorithm grow a number of parameters that is exponential with the number of variables (e.g. splines) while some other grow linearly (e.g. support vector machines, random forests, ...) ;;;
917;1;2014-08-05T20:45:01.383;Solving a system of equations with sparse data;I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value. My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:   y    x1  x2 x3 x4 x5 x6 ... x4087169   14  0  1  0  0  2  ... 0 46449   0   0  4  0  1  4  ... 12846449  0   0  0  0  0  3  ... 0....I am currently using a Genetic Algorithm to solve this and the results are coming out with roughly a factor of two difference between observed and expected. Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.;[education, open-source];203;2
918;2;2014-08-05T22:34:04.550;;"If I understand you correctly, this is the case of multiple linear regression with sparse data (sparse regression). Assuming that, I hope you will find the following resources useful.1) NCSU lecture slides on sparse regression with overview of algorithms, notes, formulas, graphics and references to literature: http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf2) R ecosystem offers many packages, useful for sparse regression analysis, including:Matrix (http://cran.r-project.org/web/packages/Matrix)SparseM (http://cran.r-project.org/web/packages/SparseM)MatrixModels (http://cran.r-project.org/web/packages/MatrixModels)glmnet (http://cran.r-project.org/web/packages/glmnet)flare (http://cran.r-project.org/web/packages/flare)3) A blog post with an example of sparse regression solution, based on SparseM: http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html4) A blog post on using sparse matrices in R, which includes a primer on using glmnet: http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r5) More examples and some discussion on the topic can be found on StackOverflow: http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrixUPDATE (based on your comment):If you're trying to solve an LP problem with constraints, you may find this theoretical paper useful: http://web.stanford.edu/group/SOL/papers/gmsw84.pdf.Also, check R package limSolve: http://cran.r-project.org/web/packages/limSolve. And, in general, check packages in CRAN Task View ""Optimization and Mathematical Programming"": http://cran.r-project.org/web/views/Optimization.html.Finally, check the book ""Using R for Numerical Analysis in Science and Engineering"" (by Victor A. Bloomfield). It has a section on solving systems of equations, represented by sparse matrices (section 5.7, pages 99-104), which includes examples, based on some of the above-mentioned packages: http://books.google.com/books?id=9ph_AwAAQBAJ&pg=PA99&lpg=PA99&dq=r+limsolve+sparse+matrix&source=bl&ots=PHDE8nXljQ&sig=sPi4n5Wk0M02ywkubq7R7KD_b04&hl=en&sa=X&ei=FZjiU-ioIcjmsATGkYDAAg&ved=0CDUQ6AEwAw#v=onepage&q=r%20limsolve%20sparse%20matrix&f=false. ";;;
919;1;2014-08-06T08:41:44.967;Which cross-validation type best suits to binary classification problem;Data set looks like:25000 observationsup to 15 predictors of different types: numeric, multi-class categorical, binarytarget variable is binaryWhich cross validation method is typical for this type of problems?By default I'm using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...);[education, open-source];135;
920;1;2014-08-06T09:02:01.033;SUMMARIST: Automated Text Summarization;There is a text summarization project called SUMMARIST. Apparently it is able to perform abstractive text summarization. I want to give it a try but unfortunately the demo links on the website do not work. Does anybody have any information regarding this? How can I test this tool?http://www.isi.edu/natural-language/projects/SUMMARIST.htmlRegards,PasMod;[education, open-source];71;1
921;1;2014-08-06T09:03:20.857;Avoid iterations while calculating average model accuracy;I am fitting a model in R.use createFolds method to create several k folds from the data setloop through the folds, repeating the following on each iteration:train the model on k-1 foldspredict the outcomes for the i-th foldcalculate prediction accuracyaverage the accuracyDoes R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?;[education, open-source];66;
922;1;2014-08-06T09:08:08.113;Can I classify set of documents using classifying method using limited number of concepts ?;I have  set of documents and I want classify them to true and false My question is I have to take the whole words in the documents then I classify them depend on the similarity words in these documents or I can take only some words that I interested in then I compare it with the documents. Which one is more efficient in classify document and can work with SVM.       ;[education, open-source];32;1
923;2;2014-08-06T09:08:12.117;;I think in your case a 10-fold CV will be O.K. I think it is more important to randomize the cross validation process than selecting the ideal value for k.So repeat the CV process several times randomly and compute the variance of your classification result to determine if the results are realiable or not.;;;
924;2;2014-08-06T12:16:22.850;;"Yes, you can do all this using the Caret (http://caret.r-forge.r-project.org/training.html) package in R. For example,fitControl <- trainControl(## 10-fold CV                           method = ""repeatedcv"",                           number = 10,                           ## repeated ten times                           repeats = 10)gbmFit1 <- train(Class ~ ., data = training,                 method = ""gbm"",                 trControl = fitControl,                ## This last option is actually one                ## for gbm() that passes through                verbose = FALSE)gbmFit1which will give the outputStochastic Gradient Boosting 157 samples 60 predictors  2 classes: 'M', 'R' No pre-processingResampling: Cross-Validated (10 fold, repeated 10 times) Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... Resampling results across tuning parameters:  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD  1                  50       0.8       0.5    0.1          0.2       1                  100      0.8       0.6    0.1          0.2       1                  200      0.8       0.6    0.09         0.2       2                  50       0.8       0.6    0.1          0.2       2                  100      0.8       0.6    0.09         0.2       2                  200      0.8       0.6    0.1          0.2       3                  50       0.8       0.6    0.09         0.2       3                  100      0.8       0.6    0.09         0.2       3                  200      0.8       0.6    0.08         0.2     Tuning parameter 'shrinkage' was held constant at a value of 0.1Accuracy was used to select the optimal model using  the largest value.The final values used for the model were n.trees = 150, interaction.depth = 3     and shrinkage = 0.1.Caret offers many other options as well so should be able to suit your needs. ";;;
925;2;2014-08-06T12:27:31.277;;Both methods work. However, if you retain all words in documents you would essentially be working with high dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document.;;;
926;2;2014-08-06T13:42:54.083;;"Aleksandr's answer is completely correct.However, the way the question is posed implies that this is a straightforward ordinary least squares regression question: minimizing the sum of squared residuals between a dependent variable and a linear combination of predictors.Now, while there may be many zeros in your design matrix, your system as such is not overly large: 300 observations on 40 predictors is no more than medium-sized. You can run such a regression using R without any special efforts for sparse data. Just use the lm() command (for ""linear model""). Use ?lm to see the help page. And note that lm will by default silently add a constant column of ones to your design matrix (the intercept) - include a -1 on the right hand side of your formula to suppress this. Overall, assuming all your data (and nothing else) is in a data.frame called foo, you can do this:model <- lm(y~.-1,data=foo)And then you can look at parameter estimates etc. like this:summary(model)residuals(model)If your system is much larger, say on the order of 10,000 observations and hundreds of predictors, looking at specialized sparse solvers as per Aleksandr's answer may start to make sense.Finally, in your comment to Aleksandr's answer, you mention constraints on your equation. If that is actually your key issue, there are ways to calculate constrained least squares in R. I personally like pcls() in the mgcv package. Perhaps you want to edit your question to include the type of constraints (box constraints, nonnegativity constraints, integrality constraints, linear constraints, ...) you face?";;;
927;1;2014-08-06T15:07:07.457;how to impute missing values on numpy array created by train_test_split from pandas.DataFrame?;I'm working on the dataset with lots of NA values with sklearn and pandas.DataFrame. I implemented different imputation strategies for different columns of the dataFrame based column names. For example NAs predictor 'var1' I impute with 0's and for 'var2' with mean.When I try to cross validate my model using train_test_split it returns me a nparray which does not have column names. How can I impute missing values in this nparray?P.S. I do not impute missing values in the original data set before splitting on purpose so I keep test and validation sets separately.;[education, open-source];219;
928;2;2014-08-06T15:10:59.600;;K-Fold should do just fine for binary classification problem. Depending on the time it is taking to train your model and predict the outcome I would use 10-20 folds.However sometimes a single fold takes several minutes, in this case I use 3-5 folds but not less than 3. Hope it helps.;;;
929;2;2014-08-06T17:07:17.520;;Can you just cast your nparray from train_test_split back into a pandas dataFrame so you can carry out your same strategy. This is very common to what I do when dealing with pandas and scikit. For example, a = train_test_split new_df = pd.DataFrame(a);;;
930;2;2014-08-06T18:17:29.700;;From the link you mentioned in the comment, the train and test sets should be in the form of a  dataframe if you followed the first explanation.In that case, you could do something like this:df[variable] = df[variable].fillna(df[variable].median())You have options on what to fill the N/A values with, check out the link.http://pandas.pydata.org/pandas-docs/stable/missing_data.htmlIf you followed the second explanation, using sklearn's cross-validation, you could implement  mike1886's suggestion of transforming the arrays into dataframes and then use the fillna option.;;;
931;2;2014-08-06T23:37:15.293;;To answer a simpler, but related question, namely 'How well can my model generalize on the data that I have?' the method of learning curves might be applicable. This is a lecture given by Andrew Ng about them.The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully 'understand' your data, at some point the complexity of the model will be high enough that performance on the training set will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.This analysis tells you two main things, I think. The first is an upper limit on performance. It's pretty unlikely that you'll do better on data that you haven't seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your training data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that. ;;;
932;2;2014-08-07T03:15:09.933;;I have managed to resolve this. There is an excellent and thorough explanation of the optimization steps in the following thesis: Semi-Supervised Learning for Natural Language by Percy Liang.My mistake was trying to update the quality for all potential clusters pairs. Instead, you should initialize a table with the quality changes of doing each merge. Use this table to find the best merge, and the update the relevant terms that make up the table entries.;;;
933;2;2014-08-07T09:33:50.817;;"It dates back to 1998, so most likely has been abandoned, or ""acquired"" by microsoft as the creator currently works there and has done since publishing that research.see http://research.microsoft.com/en-us/people/cyl/ists97.pdfand http://research.microsoft.com/en-us/people/cyl for the author. Maybe you could try to contact him.";;;
934;1;2014-08-07T10:45:38.557;Handling huge dataset imbalance (2 class values) and appropriate ML algorithm;I have train and test sets of chronological data consisting of 305000 instances and 70000,appropriately. There are 15 features in each instance and only 2 possible class values ( NEW,OLD). The problem is that there are only 725 OLD instances in the train set and 95 in the test. The only algorithm which succeeds for me to handle imbalance is NaiveBayes in Weka (0.02 precision for OLD class), others (trees) classify each instance as NEW.What is the best approach to handle the imbalance and the appropriate algorithm in such a case?Thank you in advance.;[education, open-source];95;
935;2;2014-08-07T12:46:25.123;;"I'm not allowed to comment, but I have more a suggestion: you could try to implement some ""Over-sampling Techniques"" like SMOTE:http://scholar.google.com/scholar?q=oversampling+minority+classes";;;
936;1;2014-08-07T15:12:48.617;R error using Knitr;"I am attempting to compile code using Knitr in R.My code below is returning the following error, and causes errors in the rest of the document.miss<-sample$sensor_glucose[!is.na(sample$sensor_glucose)]# Error: ""## Warning: is.na() applied to non-(list or vector) of type 'NULL'""str(miss)# int [1:103] 213 113 46 268 186 196 187 153 43 175 ...Does anyone know how to remedy this problem?Thanks in advance!";[education, open-source];79;
937;1;2014-08-07T15:33:43.793;Does scikit-learn have forward selection/stepwise regression algorithm?;I'm working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.However, I was wondering does scikit-learn have forward selection/stepwise regression algorithm?;[education, open-source];767;
938;2;2014-08-07T17:49:05.640;;You can apply a clustering algorithm to the instances in the majority class and train a classifier with the centroids/medoids offered by the cluster algorithm. This is subsampling the majority class, the converse of oversampling the minority class. ;;;
939;2;2014-08-07T17:53:06.970;;You will have best results if you care to build the folds so that each variable (and most importantly the target variable) is approximately identically distributed in each fold. This is called, when applied to the target variable, stratified k-fold. One approach is to cluster the inputs and make sure each fold contains the same number of instances from each cluster proportional to their size.;;;
940;1;2014-08-07T22:30:52.913;Database for a trie, or other appropriate structure for recommendation engine;"We are storing the information about our users showing interest in our items. Based on this information, we would like to create a simple recommendation engine that will take the items I1, I2, I3 etc of the current user, search for all other users that had shown interest in those items, and then output the items I4, I5, I6 etc of the other users, sorted by their decreasing popularity. So, basically, the standard ""other buyer were also interested in..."" functionality.I'm asking myself what kind of a database is suitable for a realtime recommendation engine like this. My current idea is to build a trie of item IDs, then sort the item IDs of the current user (as the order of items is irrelevant) and to go down the trie; the children of the last trie node will build the needed output.The problem is that we have 2 million items so that according to our estimation the trie will have at least 1E12 nodes, so that we probably need a distributed sharded database to store it. Before we reinvent the wheel, are there any ready-to-use databases or generally, non-cloud solutions for recommendation engines out there?";[education, open-source];118;
941;2;2014-08-08T00:15:05.527;;I agree with @ssdecontrol that a minimal reproducible example would be the most helpful. However, looking at your code (pay attention to the sequence Error: ... Warning: ...), I believe that the issue you are experiencing is due to an inappropriate setting of R's global warn option. It appears that your current setting is likely 2, which refers to converting warnings to errors, whereas, you, most likely want the setting 1, which is to treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately:options(warn=1)  # print warnings as they occuroptions(warn=2)  # treat warnings as errorsNote for moderators/administrators: This question seems not to be a data science question, but purely an R question. Therefore, I think it should be moved to StackOverflow, where it belongs.;;;
942;2;2014-08-08T00:25:04.210;;You mentioned regression and PCA in the title, and there is a definite answer for each of those.The asymptotic complexity of linear regression reduces to O(P^2 * N) if N > P, where P is the number of features and N is the number of observations. More detail in Computational complexity of least square regression operation.Vanilla PCA is O(P^2 * N + P ^ 3), as in Fastest PCA algorithm for high-dimensional data. However fast algorithms exist for very large matrices, explained in that answer and Best PCA Algorithm For Huge Number of Features?.However I don't think anyone's compiled a single lit review or reference or  book on the subject. Might not be a bad project for my free time...;;;
944;2;2014-08-08T07:12:52.680;;In addition to undersampling the majority class (i.e. taking only a few NEW), you may consider oversampling the minority class (in essence, duplicating your OLDs, but there are other smarter ways to do that)Note that oversampling may lead to overfitting, so pay special attention to testing your classifiers Check also this answer on CV: http://stats.stackexchange.com/a/108325/49130;;;
945;1;2014-08-08T10:01:25.400;Classifying Java exceptions;"We have a classification algorithm to categorize Java exceptions in Production.This algorithm is based on hierarchical human defined rules so when a bunch of text forming an exception comes up, it determines what kind of exception is (development, availability, configuration, etc.) and the responsible component (the most inner component responsible of the exception). In Java an exception can have several causing exceptions, and the whole must be analyzed.For example, given the following example exception:com.myapp.CustomException: Error printing ...... (stack)Caused by: com.foo.webservice.RemoteException: Unable to communicate ...... (stack)Caused by: com.acme.PrintException: PrintServer002: Timeout ....... (stack)First of all, our algorithm splits the whole stack in three isolated exceptions. Afterwards it starts analyzing these exceptions starting from the most inner one. In this case, it determines that this exception (the second caused by) is of type Availability and that the responsible component is a ""print server"". This is because there is a rule that matches containing the word Timeout associated to the Availability type. There is also a rule that matches com.acme.PrintException and determines that the responsible component is a print server. As all the information needed is determined using only the most inner exception, the upper exceptions are ignored, but this is not always the case.As you can see this kind of approximation is very complex (and chaotic) as a human have to create new rules as new exceptions appear. Besides, the new rules have to be compatible with the current ones because a new rule for classifying a new exception must not change the classification of any of the already classified exceptions.We are thinking about using Machine Learning to automate this process. Obviously, I am not asking for a solution here as I know the complexity but I'd really appreciate some advice to achieve our goal.";[education, open-source];86;
946;1;2014-08-08T13:44:52.803;Analysis of Split (A/B) tests using Poisson and/or Binomial Distribution;"Cross posting this from Cross Validated:I've seen this question asked before, but I have yet to come across a definitive source answering the specific questions:What's the most appropriate statistical test to apply to a small A/B test?What's the R code and interpretation to analyze a small A/B test?I'm running a small test to figure out which ads perform better. I have the following results:Position 1:variation,impressions,clicksrow-1,753,26row-3,767 7Position 2:variation,impressions,clicksrow-1,753,16row-3,767 13Position 3:variation,impressions,clicksrow-1,753,2row-3,767 7I think it's safe to say these numbers are small and likely to be not normally distributed. Also, it's click data so there's a binary outcome of clicked or not and the trials are independent.Appropriate testIn analyzing each position for significance, I think comparison with a binomial or Poisson distribution makes the most sense. According to the OpenIntro Stats (and other sources) book, a variable follows a Poisson distribution ""... if the event being considered is rare, the population is large, and the events occur independently of each other.""The same source classifies a binomial variable approximately the same way adding that the probability of success is the same and the number of trials is fixed.I appreciate this is not an either/or decision and analysis can be done using both distributions.Given A/B (split) testing is a science that has been practiced for several years, I imagine that there is a canonical test. However, looking around the internet, I mostly come across analysis that uses the standard normal distribution. That just seems wrong :)Is there a canonical test to use for A/B tests with small #'s of clicks?Interpretation and R codeI've used the following R code to test significance for each position:Position 1:binom.test(7, 767, p=(26/753))Exact binomial testdata:  7 and 767number of successes = 7, number of trials = 767, p-value = 1.077e-05alternative hypothesis: true probability of success is not equal to 0.0345285595 percent confidence interval: 0.003676962 0.018713125sample estimates:probability of success            0.009126467I interpret this result to mean: The probability of success in the test group is indeed different than the control group with a 95% confidence interval that the success probability is between .368% and 1.87%ppois(((26-1)/753), lambda=(7/767), lower.tail = F)[1] 0.009084947I interpret this result to mean: Given a Poisson distribution with a click rate of 7 per 767 trials, there is a 0.9% chance of having a click rate of 26 or more per 753 trials in the same distribution. Contextualized in the ad example,there is a .1% chance that the control ad actually performs the same as the test ad.Is the above interpretation correct? Does the test and interpretation change with the different positions (i.e. are the results of the Poisson test more appropriate for Position 3 given the small numbers)?";[education, open-source];178;1
947;2;2014-08-08T13:53:36.847;;"First of all, some basics of classification (and in general any supervised ML tasks), just to make sure we have same set of concepts in mind. Any supervised ML algorithm consists of at least 2 components: Dataset to train and test on.Algorithm(s) to handle these data.Training dataset consists of a set of pairs (x, y), where x is a vector of features and y is predicted variable. Predicted variable is just what you want to know, i.e. in your case it is exception type. Features are more tricky. You cannot just throw raw text into an algorithm, you need to extract meaningful parts of it and organize them as feature vectors first. You've already mentioned a couple of useful features - exception class name (e.g. com.acme.PrintException) and contained words (""Timeout""). All you need is to translate your row exceptions (and human-categorized exception types) into suitable dataset, e.g.: ex_class                  contains_timeout  ...   | ex_type-----------------------------------------------------------[com.acme.PrintException, 1                , ...] | Availability[java.lang.Exception    , 0                , ...] | Network ...This representation is already much better for ML algorithms. But which one to take? Taking into account nature of the task and your current approach natural choice is to use decision trees. This class of algorithms will compute optimal decision criteria for all your exception types and print out resulting tree. This is especially useful, because you will have possibility to manually inspect how decision is made and see how much it corresponds to your manually-crafted rules. There's, however, possibility that some exceptions with exactly the same features will belong to different exception types. In this case probabilistic approach may work well. Despite its name, Naive Bayes classifier works pretty well in most cases. There's one issue with NB and our dataset representation, though: dataset contains categorical variables, and Naive Bayes can work with numerical attributes only*. Standard way to overcome this problem is to use dummy variables. In short, dummy variables are binary variables that simply indicate whether specific category presents or not. For example, single variable ex_class with values {com.acme.PrintException, java.lang.Exception, ...}, etc. may be split into several variables ex_class_printexception, ex_class_exception, etc. with values {0, 1}:ex_class_printexception  ex_class_exception  contains_timeout | ex_type-----------------------------------------------------------------------[1,                    , 0                 , 1              ] | Availability[0,                    , 1                 , 0              ] | NetworkOne last algorithm to try is Support Vector Machines (SVM). It neither provides helpful visualisation, nor is probabilistic, but often gives superior results. * - in fact, neither Bayes theorem, nor Naive Bayes itself state anything about variable type, but most software packages that come to mind rely on numerical features. ";;;
948;1;2014-08-08T16:49:13.850;Any clear winner for Data Science in Scala?;I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).Some of the tools I've found are (in no particular order):ScaldingBreezeSparkSaddleH2OSpireMahoutHadoopMongoDBIf I need to be more specific to make the question answerable: I'm not particularly interested in clusters and Big Data at this moment, but I'm interested in sizable data (up to 100 GB) for information integration and predictive analytics.;[education, open-source];161;
949;1;2014-08-08T17:32:48.377;Pig Rank function not generating rank in output;"I am facing this bizarre issue while using Apache Pig rank utility. I am executing the following code:email_id_ranked = rank email_id;store email_id_ranked into '/tmp/';So, basically I am trying to get the following result1,email12,email23,email3... Issue is sometime pig dumps the above result but sometimes it dumps only the emails without the rank. Also when I dump the data on screen using dump function pig returns both the columns. I don't know where the issue is. Kindly advice.Please let me know if you need any more information. Thanks in advance.Pig version: Apache Pig version 0.11.0-cdh4.6.0";[education, open-source];177;
950;2;2014-08-08T19:23:33.637;;The approximation Binomial(k,n,p) ~= Poisson(k,s) (where s = n*p) can be shown under the assumptions: 1) n >> k (to say that n!/(n-k)! ~= n^k), 2) p <<1 (to say that (1-p)^(n-k) ~= (1-p)^n). It's up to you whether those are sufficiently satisfied. If the exact calculation can be done quickly, in my opinion, it's nice to stay with that.Also since, if the probability of row 3 sample is different from the row 1 sample, it would almost certainly be on the lower side. It would probably best for you to use binom.test(7, 767, p=(26/753), alternative='less')the final option indicating that the alternative to your null hypothesis is that the probability is less than 26/753, not equal to. Of course, that's simply just the sum of Binomial probabilities from 0 to 7 (you can check yourself), the interpretation being that this is the probability of having gotten at most 7 rolls from random chance, if the probability truly was 26/753.Keep in mind the interpretation of that last sentence. These kinds of tests are generally used when we know what the inherent probability is that we're comparing to (e.g. to see if the set of coin flips has a probability significantly different from 1/2 which is what we expect from a fair coin). In this case, we don't know what the probability is that we're comparing to, we're just making the very crude guess that the 26/753 outcome of row 1 reflects the true probability. It's better than a regular Normal t-test in this case, but don't put too much stock in it unless you have a much higher sample size for row 1.;;;
951;2;2014-08-08T20:46:58.493;;Not sure anybody have worked with all these tools, so I'm going to share my experience with some of them and let others share their experience with the others. MongoDB addresses problems that involve heterogeneous and nested objects, while data mining mostly works with simple tabular data. MongoDB is neither fast with this type of data, nor provide any advanced tools for analysis (correct me if you know any). So I can think of a very few applications for Mongo in data mining. Hadoop is a large ecosystem, containing dozens of different tools. I will assume that you mean core Hadoop features - HDFS and MapReduce. HDFS provides flexible way to store large amounts of data, while MapReduce gives basis for processing them. It has its clear advantages for processing multi-terabyte datasets, but it also has significant drawbacks. In particular, because of intensive disk IO during MapReduce tasks (that slows down computations a lot) it is terrible for interactive development, iterative algorithms and working with not-that-big datasets. For more details see my earlier answer.Many algorithms in Hadoop require multiple MapReduce jobs with complicated data flow. This is where Scalding gets shiny. Scalding (and underlying Java's Cascading) provides much simpler API, but at the moment uses same MapReduce as its runtime and thus holds all the same issues.  Spark addresses exactly these issues. It drops Hadoop's MapReduce and offers completely new computational framework based on distributed in-memory collections and delayed evaluations. Its API is somewhat similar to Scalding's with all MR complexity removed, so it's really easy to get started with it. Spark is also the first in this list that comes with data mining library - MLlib. But Spark doesn't reinvent things like basic linear algebra. For this purpose it uses Breeze. To my opinion, Breeze is far in quality from scientific packages like SciPy, Octave or Julia, but it is still good enough for most practical use cases. Mahout relies on Hadoop's MapReduce and thus is terrible for iterative algorithms. Spire and Saddle look cute and probably have their niche, but seem to be much less well-known than Breeze. I couldn't find much information about H2O, so it doesn't look like a big player here (comments from people who used it are welcome).Some quick summary. Spark seems to be the most simple, flexible and fast-growing project for large-scale data processing. It facilitates a number of new projects (e.g. Shark or Spark SQL) and penetrates into existing (including Cascading and Mahout). Spark knows how to utilize HDFS API, and thus scales to terabytes of data easily. And for data mining downshifters who don't want to bother with a cluster setup there's always pure Breeze. ;;;
954;1;2014-08-11T07:59:22.780;OpenNLP Coreference Resolution (German);I need to do coreference resolution for German texts and I plan to use OpenNLP to perform this task.As far as I know OpenNLP coreference resolution does not support the German language.Which components/data do I need to adapt the code such that it is possible to perform coreference resolution for German texts?;[education, open-source];145;1
955;1;2014-08-11T12:25:47.700;Coreference Resolution for German Texts;does anybody know a libarary for performing coreference resolution on German texts?As far as I know OpenNLP and Standord NLP are not able to perform coreference resolution for German Texts.The only tool that I know is CorZu which is a python library.;[education, open-source];76;
956;2;2014-08-11T12:48:52.320;;I had to create a web based dashboard. My main charting tool was d3js. But I needed to use ggplot2 to generate few charts. Through d3js's ggplot2 extension, I could create the same. If your charts can be generated through existing extension, then web has better alternatives. Later you can export them to PDF for distribution. ;;;
957;1;2014-08-11T15:19:02.047;Multiple seasonality with ARIMA?;"I know that ARIMA can't detect multiple seasonality, but it is possible to use fourier functions to add a second seasonality.I need to forecast gas consumption composed by a daily, weekly (week days-weekend), yearly seasonality. Does it make sense to apply three times the STL decomposition by LOESS? The reason is that I applied the fourier method and I have bad results but I don't know if it is only because I applied it wrong.I'm interested in the theoretical explanation, but here you find also the code:ARIMA + 2 STL:b <- ts(drop(coredata(dat.ts)), deltat=1/12/30/24, start=1)fit <- stl(b, s.window=""periodic"")b <- seasadj(fit)dat.ts <- xts(b, index(dat.ts))# The weekdays are extracteddat.weekdays <- dat.ts[.indexwday(dat.ts) %in% 1:5]dat.weekdaysTS <- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)fit <- stl(dat.weekdaysTS, s.window=""periodic"")dat.weekdaysTS <- seasadj(fit)arima <- Arima(dat.weekdaysTS, order=c(3,0,5))With fourier:dat.weekdays <- dat.ts[.indexwday(dat.ts) %in% 1:5]dat.weekdaysTS <- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)z <- fourier(ts(dat.weekdaysTS, frequency=365.25), K=5)arima <- Arima(dat.weekdaysTS, order=c(3,0,5),xreg=z)";[education, open-source];103;
958;2;2014-08-11T16:05:02.197;;Have a look at Apache Mahout. Last version features also user-item-based recommenders.;;;
959;2;2014-08-11T18:33:37.023;;While I am not a data scientist, I am an epidemiologist working in a clinical setting. Your research question did not specify a time period (ie odds of developing CKD in 1 year, 10 years, lifetime?).Generally, I would go through a number of steps before even thinking about modeling (univariate analysis, bivariate analysis, colinearity checks, etc). However, the most commonly used method for trying to predict a binary event (using continuous OR binary variables) is logistic regression. If you wanted to look at CKD as a lab value (urine albumin, eGFR) you would use linear regression (continuous outcome).While the methods used should be informed by your data and questions, clinicians are used to seeing odds ratios and risk ratios as these the most commonly reported measures of association in medical journals such as NEJM and JAMA. If you are working on this problem from a human health perspective (as opposed to Business Intelligence) this Steyerberg's Clinical Prediction Models is an excellent resource. ;;;
960;2;2014-08-11T19:21:50.137;;"I have to agree that k-fold should do ""just"" fine. However, there is a nice article about the ""Bootstrap .632+"" method (basically a smoothened cross validation) that is supposed to be superior (however, they did the comparisons on not-binary data as far as I can tell)Maybe you want to check out this article here: http://www.jstor.org/stable/2965703";;;
961;1;2014-08-11T21:13:36.230;Why RBM tends to learn very similar weights?;These are 4 different weight matrices that I got after training RBM with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though none of weights are exactly the same. I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. My question is: what is the most likely reason for weights to get very similar values? Do they all just get to some local minimum? Or is it a sign of overfitting? I currently use a kind of Gaussian-Bernoulli RBM, code may be found here. UPD. My dataset is based on CK+, which contains > 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this:  When training RBM, I take only non-zero pixels, so outer black region is ignored. ;[education, open-source];116;
962;1;2014-08-12T03:50:52.303;what is difference between text classification and topic models?;I know the difference between cluster and classification in machine learning. But I don't know what is difference between text classification and topic models in documentsAlso can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ?  ;[education, open-source];1042;1
963;1;2014-08-12T07:57:03.283;Sentiment data for Emoji;"For experimenting we'd like to use the Emoji embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?(Note that SentiWordNet does allow for ambiguous meanings, too. Consider e.g. funny, which is not just positive: ""this tastes funny"" is probably not positive... same will hold for ;-) for example. But I don't think this is harder for Emoji than it is for regular words...)Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.";[education, open-source];419;1
964;2;2014-08-12T07:59:13.060;;"Topic models are usually unsupervised. There are ""supervised topic models"", too; but even then they try to model topics within a classes.E.g. you may have a class ""football"", but there may be topics inside this class that relate to particular matches or teams.The challenge with topics is that they change over time; consider the matches example above. Such topics may emerge, and disappear again.";;;
965;1;2014-08-12T08:54:34.903;Tool for finding correlations between data after some event;I'm looking for a product that allows us to take in a collection of datastreams, and then after some event, will find any data that changes or correlates with that event. (For example, having a headache, and identifying that I drank too much beer last night and didn't drink enough water);[education, open-source];34;
966;2;2014-08-12T09:52:01.153;;"Text ClassificationI give you a bunch of documents, each of which has a label attached. I ask you to learn why you think the contents of the documents have been given these labels based on their words. Then I give you new documents and ask what you think the label for each one should be. The labels have meaning to me, not to you necessarily.Topic ModelingI give you a bunch of documents, without labels. I ask you to explain why the documents have the words they do by identifying some topics that each is ""about"". You tell me the topics, by telling me how much of each is in each document, and I decide what the topics ""mean"" if anything.You'd have to clarify what you me by ""identify one topic"" or ""classify the text"".";;;
967;2;2014-08-12T12:40:44.907;;"Total of 972 emoji is not really that big not to be able to label them manually, but I doubt that they will work as a good ground truth. Sources like Twitter are full of irony, sarcasm and other tricky settings where emotional symbols (such as emoji or emoticon) mean something different from normal interpretation. For example, someone may write ""xxx cheated their clients, and now they are cheated themselves! ha ha ha! :D"". This is definitely negative comment, but author is glad to see xxx company in trouble and thus adds positive emoticon. These cases are not that frequent, but definitely not suitable for ground truth. Much more common approach is to use emoticon as a seed for collecting actual data set. For example, in this paper authors use emoticon and emotional hash tags to grab lexicon of words useful for further classification. ";;;
968;1;2014-08-12T17:11:45.447;Understanding output stepAIC;I am using the stepAIC function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means. The output is:          Df     Sum of Sq    RSS       AIC<none>                        350.71   -5406.0- aaa      1     0.283        350.99   -5405.9- bbb      1     0.339        351.05   -5405.4- ccc      1     0.982        351.69   -5400.5- ddd      1     0.989        351.70   -5400.5Question Are the values listed under Df, Sum of Sq, RSS, and AIC the values for a model where only one variable would be considered as the independent variable (i.e. y ~aaa, y ~ bbb, etc.)? ;[education, open-source];164;
969;2;2014-08-13T14:18:58.917;;After consulting with someone I found out that the  corresponds to a model that would include all the variables, in other words none of the variables were removed. So consider the line in the output for the variable aaa. The listed RSS and AIC are the values for a model that would include all variables but aaa and we see an increase in the RSS and AIC. The other listed results can be considered in the same fashion. The best model is then the one where none are removed since this has the smallest AIC. ;;;
972;1;2014-08-13T20:30:18.143;Subgraph isomorphism and Anti-monotone property;While finding frequent subgraphs in single large graph, subgraph isomorphism (test) is not considered because its not anti-monotone. How and why subgraph isomorphism is not anti-monotone ?;[education, open-source];16;
973;1;2014-08-13T20:53:15.443;feature weights in structured support vector machine;I like to find the weight vector for input-space features in a structured SVM. The idea is to identify the most important set of input-space features (based on the magnitude of their corresponding weights). I know that in a binary SVM the weight vector can be written as a linear combination of examples, and the magnitude of those weights represents how much they were effective for the prediction problem at hand. But how do you compute the same for an SSVM?;[education, open-source];36;0
974;1;2014-08-14T00:50:51.103;Nearest neighbors search for very high dimensional data;I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I'm exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I'm considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)Using locality-sensitive hashing (LSH)Reducing first the dimensionality of the problem with a PCAI'd appreciate any thoughts or advices about possible other ways in which I could tackle this problem.;[education, open-source];428;4
975;2;2014-08-14T04:28:36.933;;"I hope that the following resources might get you additional ideas toward solving the problem:1) Research paper ""Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data"": http://arxiv.org/abs/1011.28072) Class project paper ""Recommendation System Based on Collaborative Filtering"" (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf3) Project for the Netflix Prize Competition (k-NN-based): http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html4) Research paper ""Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data"" on the curse of dimensionality phenomenon and its relation to machine learning, in general, and k-NN algorithm, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf5) Software for sparse k-NN classification (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html6) Several discussion threads on StackOverflow:http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-pythonhttp://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matriceshttp://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipyhttp://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knnhttp://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to Python, this one refers to R ecosystem)7) Pay attention to GraphLab, an open source parallel framework for machine learning (http://select.cs.cmu.edu/code/graphlab), which supports parallel clustering via MapReduce model: http://select.cs.cmu.edu/code/graphlab/clustering.htmlYou might also check my answer here on Data Science StackExchange on sparse regression for links to relevant R packages and CRAN Task View pages: http://datascience.stackexchange.com/a/918/2452.";;;
976;2;2014-08-14T17:28:40.453;;I gave a very limited partial answer for the confirmatory factor analysis package that I developed for Stata in this Stata Journal article based on timing the actual simulations. Confirmatory factor analysis was implemented as a maximum likelihood estimation technique, and I could see very easily how the computation time grew with each dimension (sample size n, number of variables p, number of factors k). As it is heavily dependent on how Stata thinks about the data (optimized to compute across columns/observations rather than rows), I found performance to be O(n^{0.68} (k+p)^{2.4}) where 2.4 is the fastest matrix inversion asymptotics (and there's hell of a lot of that in confirmatory factor analysis iterative maximization). I did not give a reference for the latter, but I think I got this from Wikipedia.Note that there is also a matrix inversion step in OLS. However, for reasons of numerical accuracy, no one would really brute-force inverse the X'X matrix, and would rather use sweep operators and identify the dangerously collinear variables to deal with precision issues. If you add up $10^8$ numbers that originally were in double precision, you will likely end up with a number that only has a single precision. Numerical computing issues may become a forgotten corner of big data calculations as you start optimizing for speed.;;;
977;1;2014-08-14T19:09:29.523;Solutions for Continuous Online Cluster Identification?;"Let me show you an example of a hypothetical online clustering application:At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster. Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.This finally leads to my questions:Does this ""problem"" have a name that it can be referred to?Are there ""standard"" solutions to this and ...... is there maybe even an R package for that?Reasonable Inheritance of Cluster Identities in Repetitive Clustering";[education, open-source];151;1
978;2;2014-08-14T21:38:57.630;; But I don't know what is difference between text classification and topic models in documentsText classification is a form of supervised learning -- the set of possible classes are known/defined in advance and don't change.Topic modeling is a form of unsupervised learning (akin to clustering) -- the set of possible topics are unknown apriori. They're defined as part of generating the topic models. With a non-deterministic algorithm like LDA, you'll get different topics each time you run the algorithm.Text classification often involves mutually-exclusive classes -- think of these as buckets. But it doesn't have to -- given the right kind of labeled input data, you can set of a series of non-mutually-exclusive binary classifiers.Topic modeling is generally not mutually-exclusive -- the same document can have its probability distribution spread across many topics. In addition, there are also hierarchical topic modeling methods, etc. Also can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ? If you're asking whether you can take all of the documents assigned to one topic by a topic modeling algorithm and then apply a classifier to that collection, then yes, you certainly can do that. I'm not sure it makes much sense, though -- at a minimum, you'd need to pick a threshold for the topic probability distribution above which you'll include documents in your collection (typically 0.05-0.1). Can you elaborate on your use case?By the way, there's a great tutorial on topic modeling using the MALLET library for Java available here: Getting Started with Topic Modeling and MALLET;;;
979;1;2014-08-15T13:10:20.937;Algorithms for text clustering;I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?I'm trying now the simplest features with just list of words and distance between sentences as next    A and B are corresponding sets of words in sentence A and B. Does it make sense at all? I'm trying to apply Mean-Shift algorithm from scikit library to this distance as it does not require number of clusters in advance.If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.;[education, open-source];1104;5
980;2;2014-08-15T14:00:14.927;;Check the Stanford NLP Group's open source software (http://www-nlp.stanford.edu/software), in particular, Stanford Classifier (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in Java, which will likely delight you, but also has bindings for some other languages. Note, the licensing - if you plan to use their code in commercial products, you have to acquire commercial license.Another interesting set of open source libraries, IMHO suitable for this task and much more, is parallel framework for machine learning GraphLab (http://select.cs.cmu.edu/code/graphlab), which includes clustering library, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for very large volume of data (like you have), as it implements MapReduce model and, thus, supports multicore and multiprocessor parallel processing.You most likely are aware of the following, but I will mention it just in case. Natural Language Toolkit (NLTK) for Python (http://www.nltk.org) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the NLTK Book: http://www.nltk.org/book/ch06.html.UPDATE:Speaking of algorithms, it seems that you've tried most of the ones from scikit-learn, such as illustrated in this topic extraction example: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html. However, you may find useful other libraries, which implement a wide variety of clustering algorithms, including Non-Negative Matrix Factorization (NMF). One of such libraries is Python Matrix Factorization (PyMF) with home page at https://code.google.com/p/pymf and source code at https://github.com/nils-werner/pymf. Another, even more interesting, library, also Python-based, is NIMFA, which implements various NMF algorithms: http://nimfa.biolab.si. Here's a research paper, describing NIMFA: http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf. Here's an example from its documentation, which presents the solution for very similar text processing problem of topic clustering: http://nimfa.biolab.si/nimfa.examples.documents.html.;;;
981;2;2014-08-15T20:22:11.750;;Basically for this task you can efficiently use any SQL database with good support of B+tree based indexes (MySQL will suite you needs just perfect). Create 3 tables:Documents table, columns: id/documentN-grams table: n_gram_id/n_gramMapping between n-grams and documents: document_id/n_gram_idCreate indexes on N-gram table/n_gram string and Mapping table/n_gram_id, also primary keys will be indexed by default well.Your operations will be efficient:Insertion of document: just extract all n-grams and insert into document table and N-grams tableLookup for in_gram will be quick with support of indexQuerying for all n-grams that contain a sub-n-gram: in 2 steps - just query based on index all n-grams which contain sub-n-gram from 2nd table. Then - retrieve all corresponding documents for each of these n-grams.You don't even need to use joins to achieve all these operations so indexes will help a lot. Also if data will not suite in one machine - you can implement sharding scheme, like storing n_grams started from a-n on one server and o-z on another or other suitable scheme. Also you can use MongoDB, but I'm not sure how exactly you need to implement indexing scheme. For MongoDB you will get sharding scheme for free as it is already built-in.;;;
982;2;2014-08-16T10:25:19.687;;"See Lucene NGramTokenizerAre you sure you can't just use lucene or similar indexing techniques?Inverted indexes will store the n-gram only once, then just the document ids that contain the ngram; they don't store this as highly redundant raw text.As for finding ngrams that contain your query sub-n-gram, I would build an index on the observed ngrams, e.g. using a second lucene index, or any other substring index such as a trie or suffix tree. If your data is dynamic, probably lucene is a reasonable choice, using phrase queries to find your n-grams.";;;
984;1;2014-08-16T10:42:58.337;Can anyone provide the 24 hour challenge dataset from Climate Corporation?;I have no knowledge about the climate or soil. And I just want to find out more about these kind of dataset. I heard that Climate Corporation asked its candidates to perform statistical analysis on various climate dataset. This is why I am asking this question. Please do not get me wrong. I am not trying to get the dataset to prepare myself for an interview, as I know they give out different dataset to people from different background. I know that Climate Corporation only hires PHD, which I am not. I only want to play around with their dataset such that I can learn and implement time series analysis. That's it. So, if anyone does not mind sharing their dataset. Please post the link them below. Thank you very much. ;[education, open-source];127;
985;1;2014-08-16T11:05:43.353;Can I use unsupervised learning followed by supervised learning?;I have a question about classifying documents using supervised learning and unsupervised learning.For example: - I have a bunch of documents talking about football.As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.  My goal is to create more accurate categorizations because if I want to test a new document I want know if this document can be related to these categorizations or not.  ;[education, open-source];343;1
986;2;2014-08-17T14:17:41.307;;OK, so here's your data. dd <- data.frame(position=rep(1:3, each=2),                  variation=rep(c(1,3), 3),                  impressions=rep(c(753, 767), 3),                  clicks=c(26,7,16,13,2,7))which is  position variation impressions clicks1        1         1         753     262        1         3         767      73        2         1         753     164        2         3         767     135        3         1         753      26        3         3         767      7The two model assumptions you're thinking about are Binomialmod.bin <- glm(cbind(clicks, impressions-clicks) ~ variation + position,               family=binomial, data=dd)where the dependent variable is constructed to have the count of the event of interest in the first column, and the Poissonmd.pois <- glm(clicks ~ variation + position + offset(log(impressions)),                family=poisson, data=dd)where the log(impressions) offset is necessary whenever the number of trials differs across observations.  This means coefficients are interpretable in terms of change in rate not change in count, which is what you want.  The first model generalises the binom.test to a setting with covariates, which is what you have.  That gets you a more direct answer to your question, and better (if not perfect) measurement of the relevant uncertainty.NotesBoth models assume no interaction between variation and position ('independent effects').  This may or may not be reasonable.  You'd want more replications to investigate that properly.  Swap the + for a * to do so.In this data summary confirms that the two models give rather similar results, so concerns about Poisson vs Binomial don't seem to matter much.In the wild, count data is usually overdispersed, that is: more variable than you'd expect from a Poisson with a constant rate or a Binomial with constant click probability, often due to unmodeled determinants of click rate / probability.  If that's the case then prediction intervals from these models will be too narrow.;;;
987;1;2014-08-17T17:29:44.123;Text categorization: combining different kind of features;The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams. I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren't garbage. Scaling the features didn't help as well. My current thinking is that these kind of features don't mix well with the (sparse) bag of words features.So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?;[education, open-source];355;3
988;2;2014-08-18T08:49:11.933;;Here is a couple of tools that may be worth a look:Bart, an open source tool that have been used for several languages, including German. Available from the websiteSucre is a tool developed at the University of Stuttgart. I don't know if it's available easily. You can see this paper about it.;;;
989;1;2014-08-18T10:46:57.360;SVM using scikit learn runs endlessly and never completes execution;I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized.I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn't seem to be the issue.I have also tried changing the 'C' parameter value from 1 to 1e3.I am facing similar issues with all svm implementations using scikit.Am I not waiting enough for it to complete ?How much time should this execution take ?From my experience it shouldn't require over a few minutes.Here is my system configuration:Ubuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor;[education, open-source];1037;1
990;2;2014-08-18T11:33:32.617;;This makes sense. IIUC, the speed of execution of support vector operations is bound by number of samples, not dimensionality. In other words, it is capped by CPU time and not RAM. I'm not sure exactly how much time this should take, but I'm running some benchmarks to find out.;;;
991;2;2014-08-18T13:35:28.000;;Linear models simply add their features multiplied by corresponding weights. If, for example, you have 1000 sparse features only 3 or 4 of which are active in each instance (and the others are zeros) and 20 dense features that are all non-zeros, then it's pretty likely that dense features will make most of the impact while sparse features will add only a little value. You can check this by looking at feature weights for a few instances and how they influence resulting sum.One way to fix it is to go away from additive model. Here's a couple of candidate models. SVM is based on separating hyperplanes. Though hyperplane is linear model itself, SVM doesn't sum up its parameters, but instead tries to split feature space in an optimal way. Given the number of features, I'd say that linear SVM should work fine while more complicated kernels may tend to overfit the data. Despite its name, Naive Bayes is pretty powerful statistical model that showed good results for text classification. It's also flexible enough to capture imbalance in frequency of sparse and dense features, so you should definitely give it a try. Finally, random forests may work as a good ensemble method in this case. Randomization will ensure that different kinds of features (sparse/dense) will be used as primary decision nodes in different trees. RF/decision trees are also good for inspecting features themselves, so it's worth to note their structure anyway.Note that all of these methods have their drawbacks that may turn them into a garbage in your case. Combing sparse and dense features isn't really well-studied task, so let us know what of these approaches works best for your case. ;;;
992;1;2014-08-18T14:56:13.800;Why might several types of models give almost identical results?;I've been analyzing a data set of ~400k records and 9 variables The dependent variable is binary. I've fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.Why is this so? I'm guessing that it's because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? ;[education, open-source];203;
993;2;2014-08-18T16:28:02.147;; I'm guessing that it's because my observations to variable ratio is so high.I think this explanation makes perfect sense. If this is correct, at what observation to variable ratio will different models start to give different results? This will probably depend very much on your specific data (for instance, even whether your nine variables are continuous, factors, ordinary or binary), as well as any tuning decisions you made while fitting your model.But you can play around with the observation-to-variable ratio - not by increasing the number of variables, but by decreasing the number of observations. Randomly draw 100 observations, fit models and see whether different models yield different results. (I guess they will.) Do this multiple times with different samples drawn from your total number of observations. Then look at subsamples of 1,000 observations... 10,000 observations... and so forth.;;;
994;2;2014-08-18T17:05:19.957;;its worth also looking at the training errors. basically I disagree with your  analysis. if logistic regression etc are all giving the same results it would suggest that the 'best model' is a very simple one (that all models can fit equally well - eg basically linear).So then the question might be why is the best model a simple model?:It might suggest that your variables are not very predictive. Its of course hard to analyse without knowing the data. ;;;
995;2;2014-08-18T17:25:27.007;;"As @seanv507 suggested, the similar performance may simply be due to the data being best separated by a linear model. But in general, the statement that it is because the ""observations to variable ratio is so high"" is incorrect. Even as your ratio of sample size to number of variables goes to infinity, you should not expect different models to perform nearly identically, unless they all provide the same predictive bias.";;;
996;2;2014-08-19T00:56:40.890;;Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of O(n_features x n_observations^2). The storage of the distances is a burden on memory, so they're recomputed on the fly. Thankfully, only the points nearest the decision boundary are needed most of the time. Frequently computed distances are stored in a cache. If the cache is getting thrashed then the running time blows up to O(n_features x n_observations^3). (Seriously, no LaTeX?)You can increase this cache by invoking SVR asmodel = SVR(cache_size=7000)In general, this is not going to work. But all is not lost. You can subsample the data and use the rest as a validation set, or you can pick a different model. Above the 200,000 observation range, it's wise to choose linear learners.Kernel SVM can be approximated, by approximating the kernel matrix and feeding it to a linear SVM. This allows you to trade off between accuracy and performance in linear time.A popular means of achieving this is to use 100 or so cluster centers found by kmeans/kmeans++ as the basis of your kernel function. The new derived features are then fed into a linear model. This works very well in practice. Tools like sophia-ml and vowpal wabbit are how Google, Yahoo and Microsoft do this. Input/output becomes the dominating cost for simple linear learners.In the abundance of data, nonparametric models perform roughly the same for most problems. The exceptions being structured inputs, like text, images, time series, audio.Further reading:How to implement this.How to train an ngram neural network with dropout that scales linearlyKernel ApproximationsA formal paper on using kmeans to approximate kernel machines;;;
997;1;2014-08-19T03:41:24.207;Where can I find free spatio-temporal dataset for download?;Where can I find free spatio-temporal dataset for download so that I can play with it in R ? Thanks;[education, open-source];543;1
998;2;2014-08-19T13:33:32.453;;No, sklearn doesn't seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the documentation here:http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html;;;
999;2;2014-08-19T14:13:07.147;;"This results means that whatever method you use, you are able to get reasonably close to the optimal decision rule (aka Bayes rule). The underlying reasons have been explained in Hastie, Tibshirani and Friedman's ""Elements of Statistical Learning"". They demonstrated how the different methods perform by comparing Figs. 2.1, 2.2, 2.3, 5.11 (in my first edition -- in section on multidimensional splines), 12.2, 12.3 (support vector machines), and probably some others. If you have not read that book, you need to drop everything RIGHT NOW and read it up. (I mean, it isn't worth losing your job, but it is worth missing a homework or two if you are a student.)I don't think that observations to variable ratio is the explanation. In light of my rationale offered above, it is the relatively simple form of the boundary separating your classes in the multidimensional space that all of the methods you tried have been able to identify.";;;
1000;2;2014-08-19T15:21:57.833;;You can get some documented, publicly available EEG data from the HeadIT database at UCSD.http://headit-beta.ucsd.edu/studiesThe data itself appears to be in Biosemi Data Format (.bdf) files, described here: http://www.biosemi.com/faq/file_format.htmBiosemi provides links to several open-source methods to access and import .bdf files on their website, including several functions for importing into Matlab, as well as into Python (BioSig) and C/C++ libraries:http://www.biosemi.com/download.htmJust as a forewarning, EEG data can be a bit of a bear to work with, due to it's inherently low signal/noise ratio.;;;
1001;2;2014-08-19T15:47:29.413;;First thing that came to mind would be one's personal workout data from running or biking apps.Otherwise there is a dataset around NYC's taxi trip data. Quick Googling brought me this: http://www.andresmh.com/nyctaxitrips/. Variables include time and location for both pickups and dropoffs.Another dataset comes from Chicago's bikesharing service. It can be found here: https://www.divvybikes.com/datachallenge.;;;
1002;1;2014-08-19T17:50:52.583;Making sense of large data sets;Caveat: I am a complete beginner when it comes to machine learning, but eager to learn.I have a large dataset and I'm trying to find pattern in it. There may / may not be correlation across the data, either with known variables, or variables that are contained in the data but which I haven't yet realised are actually variables / relevant.I'm guessing this would be a familiar problem in the world of data analysis, so I have a few questions:The 'silver bullet' would be to throw this all this data into a stats / data analysis program and for it to crunch the data looking for known / unknown patterns trying to find relations. Is SPSS suitable, or are there other applications which may be better suited.Should I learn a language like R, and figure out how to manually process the data. Wouldn't this comprimise finding relations as I would have to manually specify what and how to analyse the data?How would a professional data miner approach this problem and what steps would s/he take?;[education, open-source];310;1
1003;1;2014-08-19T18:59:03.013;Query similarity: how much data is used in practice?;I recently read Similarity Measures for Short Segments of Text (Metzler et al.).  It describes basic methods for measuring query similarity, and in the paper, the data consists of queries and their top results. Results are lists of page urls, page titles, and short page snippets.  In the paper, the authors collect 200 results per query.When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.  There's a substantial difference between 10 and 200.  Hence, how much data is commonly used in practice to measure query similarity (e.g., how many results per query)?References are a plus!;[education, open-source];175;
1005;2;2014-08-20T03:06:01.753;;Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package (http://www.r-bloggers.com/the-openstreetmap-package-opens-up), with census data (population census data, such as the US data: http://www.census.gov/data/data-tools.html, as well as census data in other categories: http://national.census.okfn.org) to analyze temporal patterns of geosocial trends. HTH.;;;
1006;2;2014-08-20T05:43:08.610;;"I will try to answer your questions, but before I'd like to note that using term ""large dataset"" is misleading, as ""large"" is a relative concept. You have to provide more details. If you're dealing with bid data, then this fact will most likely affect selection of preferred tools, approaches and algorithms for your data analysis. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general data analysis workflow, at least, how I understand it.1) Firstly, I think that you need to have at least some kind of conceptual model in mind (or, better, on paper). This model should guide you in your exploratory data analysis (EDA). A presence of a dependent variable (DV) in the model means that in your machine learning (ML) phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.2) Secondly, EDA is a crucial part. IMHO, EDA should include multiple iterations of producing descriptive statistics and data visualization, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - data cleaning and transformation. Just throwing your raw data into a statistical software package won't give much - for any valid statistical analysis, data should be clean, correct and consistent. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: http://vita.had.co.nz/papers/tidy-data.pdf (by Hadley Wickham) and http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf (by Edwin de Jonge and Mark van der Loo).3) Now, as you're hopefully done with EDA as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is exploratory factor analysis (EFA), which will allow you to extract the underlying structure of your data. For datasets with large number of variables, the positive side effect of EFA is dimensionality reduction. And, while in that sense EFA is similar to principal components analysis (PCA) and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data ""describe"", thus making sense of your datasets. Of course, in addition to EFA, you can/should perform regression analysis as well as apply machine learning techniques, based on your findings in previous phases.Finally, a note on software tools. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are constrained by them. However, if that is not the case, I would heartily recommend open source statistical software, based on your comfort with its specific programming language, learning curve and your career perspectives. My current platform of choice is R Project, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include Python, Julia and specific open source software for processing big data, such as Hadoop, Spark, NoSQL databases, WEKA. For more examples of open source software for data mining, which include general and specific statistical and ML software, see this section of a Wikipedia page: http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications.UPDATE: Forgot to mention Rattle (http://rattle.togaware.com), which is also a very popular open source R-oriented GUI software for data mining.";;;
1007;1;2014-08-20T14:12:03.870;Looking for Web scraping tool for unstructured data;I want to scrape some data from a website. I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web;[education, open-source];304;
1008;2;2014-08-20T15:34:00.830;;"Try BeautifulSoup - http://www.crummy.com/software/BeautifulSoup/From the website ""Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping.""I have no personally used it, but it often comes up in regards to a nice library for scraping. Here's a blog post on using it to scrape Craigslist http://www.gregreda.com/2014/07/27/scraping-craigslist-for-tickets/";;;
1009;2;2014-08-20T15:53:29.403;;"If you have R and the spacetime package then you are only data(package=""spacetime"") away from a list of space-time data sets bundled with the package:Data sets in package ‘spacetime’:DE_NUTS1 (air)          Air quality data, rural background PM10 in                        Germany, daily averages 1998-2009fires                   Northern Los Angeles County Firesrural (air)             Air quality data, rural background PM10 in                        Germany, daily averages 1998-2009then for example:> data(fires)> str(fires)'data.frame':   313 obs. of  3 variables: $ Time: int  5863 5870 6017 6018 6034 6060 6176 6364 6366 6372 ... $ X   : num  63.9 64.3 64.1 64 64.4 ... $ Y   : num  19.4 20.1 19.7 19.8 20.3 ...";;;
1010;2;2014-08-20T19:08:39.807;;You don't mention what language you're programming in (please consider adding it as a tag), so general help would be to seek out a HTML parser and use that to pull the data. Some web sites can have simply awful HTML code and can be very difficult to scrape, and just when you think you have it...A HTML parser will parse all the html and allow you to access it in a structured sort of way, whether that's from an array, an object etc.;;;
1013;1;2014-08-20T21:12:49.927;Using Heuristic Methods for AB Testing;I've just started reading about AB testing, as it pertains to optimizing website design.  I find it interesting that most of the methods assume that changes to the layout and appearance are independent of each other.  I understand that the most common method of optimization is the 'multi-armed bandit' procedure.  While I grasp the concept of it, it seems to ignore the fact that changes (changes to the website in this case) are not independent to each other.For example, if company is testing the placement and color of the logo on the website, they find the optimal color first then the optimal placement.  Not that I'm some expert on human psychology, but shouldn't these be related? Can the multi-armed bandit method be efficiently used in this case or more complicated cases?My first instinct is to say no.  On that note, why haven't people used heuristic algorithms to optimize over complicated AB testing sample spaces?  For an example, I thought someone might have used a genetic algorithm to optimize a website layout, but I can find no examples of something like this out there. This leads me to believe that I'm missing something important in my understanding of AB testing as it applies to website optimization.Why isn't heuristic optimization used on more complicated websites?;[education, open-source];52;
1015;1;2014-08-21T06:31:50.197;Making Factual drake work on Windows 7 64-bit;I have installed Drake on Windows 7 64-bit.I am using JDK 1.7.0_51.I tried both using the pre-compiled jar file andcompiling from the Clojure source using leiningen.The resulting Drake version is 0.1.6, the current development version.When running Drake, I get the current version number.Next, I tried to go through the tutorial. The command:java -jar drake.jar  -w .\workflow.dresults in the following Exception:java.lang.Exception: no input data found in locations: D:\tools\drake\in.csvEven though the file exists and has text inside it. The same scenario works in a similar installation on Ubuntu 12.04.Am I doing something wrong, or is this a Windows-specific bug?;[education, open-source];60;
1017;1;2014-08-21T10:13:54.130;How can I model open environment in reinforcement learning?;I'm studying reinforcement learning in order to implement a kind of time series pattern analyzer such as market.The most examples I have seen are based on the maze environment.But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.Another question is about buy-sell modeling.Let's assume that the agent randomly buy at time t and sell at time t + alpha.It's simple to calculate reward.The problem is how can I model Q matrix and how can I model signals between buy and sell actions.Can you share some source code or guidance for similar situation?Thanks in advance,;[education, open-source];34;
1019;2;2014-08-22T09:03:13.333;;I think it always depends on the scenario. Using a representative data set is not always the solution. Assume that your training set with 1000 negative examples and 20 positive examples. Without any modeification of the classifier, your algorithm will tend to classify all new examples as negative. In some scenarios this is O.K. But in many cases the costs of missing postive examples is high so you have to find a solution for it.In such cases you can use a cost sensitive machine learning algorithm. For example in the case of medical diagnosis data analysis.In summary: Classification erros do not have the same cost! ;;;
1020;1;2014-08-22T14:01:23.640;Available data about 'likers' as a page on Facebook;I am hoping to model the characteristics of the users of a specific page on Facebook, which has roughly 2 million likes. I have been looking at the Facebook SDK/API, but I can't really see if what I would like to do is possible. It seems that the users share quite different amounts of data so I probably discard a lot of users and only use the ones with a quite open public profile. I would like to have the following data:1) See the individuals that have 'liked' the page.2) See the list of friends for each person that have 'liked' the page.3) See gender for each person (optional)4) See other pages that each person has liked (optional)Could anyone tell me if it is possible to get this data? As mentioned earlier it is okay if I discard data for users that don't like to share this data.;[education, open-source];56;
1021;1;2014-08-22T15:59:07.097;Extract canonical string from a list of noisy strings;I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. They can be considered to be noisy variations of a canonical string. I am looking for an algorithm or a library that will convert each list into this canonical string.Here is one such list.Star Wars: Episode IV A New Hope | StarWars.comStar Wars Episode IV - A New Hope (1977)Star Wars: Episode IV - A New Hope - Rotten TomatoesWatch Star Wars: Episode IV - A New Hope Online FreeStar Wars (1977) - Greatest Films[REC] 4 poster promises death by outboard motor - SciFiNowFor this list, any string matching the regular expression ^Star Wars:? Episode IV (- )?A New Hope$ would be acceptable.I have looked at Andrew Ng's course on Machine Learning on Coursera, but I was not able to find a similar problem.;[education, open-source];119;1
1022;2;2014-08-22T18:27:04.377;;"Want to wish you good luck. Some time ago faced with the same problem, but didn't find any satisfying solution. First of all, there is no way to get list of users, who ""liked"" a particular page. Even, if you are an administrator of this page (I was). One only can get list of last 3 or 5 hundred users. Friendships data for most of the users is also inaccessible. Looks like gender is the only thing from your list, that you can get. Data about pages, that exact user ""likes"", should be available (as it's written in docs), but in reality, through API you can collect something only for friends and FoF. Even though this data is available through web interface. So the only way is to try dirty trick with parsing and scraping (but remember, that I didn't advise it ;) ). ";;;
1023;2;2014-08-23T09:19:08.577;;"As a naive solution I would suggest to first select the strings which contain the most frequent tokens inside the list. In this way you can get rid of irrelevant string.In the second phrase I would do a majority voting. Assuming the 3 sentences:Star Wars: Episode IV A New Hope | StarWars.comStar Wars Episode IV - A New Hope (1977)Star Wars: Episode IV - A New Hope - Rotten TomatoesI would go through the tokens one by one. We start by ""Star"". It wins as all the string start with it. ""Wars"" will also win. The next one is "":"". It will also win.All the tokens will ein in majority voting till ""Hope"". The next token after ""Hope"" will be either ""|"", or ""("" or ""-"". None of the will win in majority voting so I will stop here!Another solution would be probably to use Longest common subsequence.As I said I have not though about it much. So there might be much more better solutions to your problem :-)";;;
1024;1;2014-08-23T13:47:01.907;Extrapolating GLM coefficients for year a product was sold into future years?;I've fit a GLM (Poisson) to a data set where one of the variables is categorical for the year a customer bought a product from my company, ranging from 1999 to 2012. There's a linear trend of the coefficients for the values of the variable as the year of sale increases.Is there any problem with trying to improve predictions for 2013 and maybe 2014 by extrapolating to get the coefficients for those years?;[education, open-source];76;
1025;1;2014-08-23T13:56:43.813;implementing temporal difference in chess;"I have been developing a chess program which makes use of alpha-beta pruning algorithm and an evaluation function that evaluates positions using the following features namely material, kingsafety, mobility, pawn-structure and trapped pieces etc..... My evaluation function is derived from the f(p) = w1 * material + w2 * kingsafety + w3 * mobility + w4 * pawn-structure + w5 * trapped pieces , where ""w"" is the weight assigned to each feature. At this point i want to tune the weights of my evaluation function using temporal difference, where the agent plays against itself and in the process gather training data from its environment (which is a form of reinforcement learning). i have read some books and articles in order to have an insight on how to implement this in java but they seems to be theoretical rather than practical. please i need a detailed explanation and pseudo codes on how to automatically tune the weights of my evaluation function based on previous games. thanks in advances.";[education, open-source];70;
1026;2;2014-08-23T14:15:58.660;;I think there are two separate issues to consider: Training time, and prediction accuracy. Take a simple example : consider you have two classes, that have a multivariate normal distribution. Basically you need to estimate the respective class means and class covariances.  Now the first thing you care about is your estimate of the difference in the class means: but your performance is limited by the accuracy of the worst estimated mean: its no good estimating one mean to the 100th decimal place - if the other mean is only estimated to 1 decimal place.  So its a waste of computing resources to use all the data - you can instead undersample the  more common class AND reweight the classes appropriately. ( those computing resources can then be used exploring different input variables etc)Now the second issue is predictive accuracy: different algorithms use different error metrics, which may or may not agree with your own objectives. eg logistic regression will penalise overall probability error,  so if  most of your data is from one class, then it will tend to try to improve accurate probability estimates ( eg 90 vs 95% probability) of that one class rather than trying to identify the rare class. in that case you would definitely want to try to reweight to emphasize the rare class ( and subsequently adjust the estimate [by adjusting the bias term] to get the probability estimates realigned);;;
1027;2;2014-08-23T15:25:44.903;;A first remark, you should watch 'Wargames' to know what you're getting yourself into.What you want is f(p) such that f(p) is as close as possible to strength of position.A very simple solution using genetic algo would be to setup 10000 players with different weights and see which wins. Then keep the top 1000 winners' weight, copy them 10 times, alter them slightly to explore weight space, and run the simulation again. That's standard GA, given a functional form, what are the best coefficients for it.Another solution is to extract the positions, so you have a table '(material, kingsafety, mobility, pawn-structure, trappedpieces) -> goodness of position' where goodness of position is some objective factor (outcome win/lose computed using simulations above or known matches, depth of available tree, number of moves under the tree where one of the 5 factors gets better. You can then try different functional forms for your f(p), regression, svm.;;;
1028;1;2014-08-23T16:54:06.380;Do Random Forest overfit?;I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem.;[education, open-source];188;3
1029;1;2014-08-23T19:34:09.417;Which one will be the dominating programming language for next 5 years for analytics , machine learning . R or python or SAS;Which one will be the dominating programming language for next 5 years for analytics , machine learning . R verses python verses SAS. Advantage and disadvantage.;[education, open-source];345;
1030;2;2014-08-23T20:05:38.700;;I believe that this is a case for applying time series analysis, in particular time series forecasting (http://en.wikipedia.org/wiki/Time_series). Consider the following resources on time series regression:http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.htmlhttp://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf (especially section4.6)http://arxiv.org/abs/0802.0219 (Bayesian approach);;;
1031;2;2014-08-24T00:46:03.813;;There is a great survey published by O'Reilly collected at Strata.You can see that SAS is not widely popular, and there is no reason why that should change at this point. One can rule that out.R is barely ahead of Python, 43% vs 41%. You can find many blogs expressing the rise of Python in data science. I would go with Python in the near future.But 5 years is a very long time. I think Golang will steal a lot of developers from Python in general. This might spill over to data science usage as well. Code can be written to execute in parallel very easily, which makes it a perfect vehicle for Big Data processing. Julia's benchmarks for technical computing are even more impressive, and you can have iPython like stuff with iJulia. Hence Python is likely to lose some steam to both. But there are ways to call Julia functions from R and Python, so you can experiment using best sides of each.;;;
1032;2;2014-08-24T08:22:23.497;;You may want to check cross-validated - a stachexchange website for many things, including machine learning. In particular, this question (with exactly same title) has already been answered multiple times. Check these links: http://stats.stackexchange.com/search?q=random+forest+overfitBut I may give you the short answer to it: yes, it does overfit, and sometimes you need to control the complexity of the trees in your forest, or even prune when they grow too much - but this depends on the library you use for building the forest. E.g. in randomForest in R you can only control the complexity;;;
1033;2;2014-08-24T11:03:08.773;;"Due to the very Big increase in Big Data (pun intended) and the desire for robust stable scalable applications I actually believe it to be Scala.  Spark will inevitably become the main Big Data Machine Learning tool, and it's main API is in Scala.  Furthermore you simply cannot build a product with scripting languages like Python and R, one can only experiment with these languages.  What Scala brings is a way to BOTH experiment and produce a product.  More reasonsThink functionally - write faster code and more readable codeScala means the end of the two team development cycle. So better product ownership, more agile cross functional teams, and half as many employees required to make a product as we will no longer need both a ""research"" team and an engineering team, Data Scientists will be able to do both.  This is because Scala is;A production quality language - static typing, but with the flexibility of dynamic typing due to implicitsInteroperable with rest of Java world (so Apache Commons Math, Databases, Cassandra, HBase, HDFS, Akka, Storm, many many databases, more spark components (e.g. graphx, SparkStreaming)Step into Spark code easily and understand it, also helps with debuggingScala is awesome:Amazing IDE support due to static typingProperty based tests with ScalaCheck - insane unit testingVery concise languageSuits mathematicians perfectly (especially Pure Mathematicians)A little more efficient as compiled not interpretedPython Spark API sits on Scala API and therefore will always be behind Scala APIMuch easier to do Mathematics in Scala as it's a Scalable Language where one can easily define DSLs and due to being so functionalAkka - another way other than storm to do High VelocityPimp my library pattern makes adding methods to Spark RDDs really easy";;;
1034;1;2014-08-24T17:09:40.510;What features are generally used from Parse trees in classification process in NLP?;I am exploring different types of parse tree structures. The two widely known parse tree structures are a) Constituency based parse tree and b) Dependency based parse tree structures. I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task. For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?;[education, open-source];196;
1035;2;2014-08-24T22:02:01.050;;"First compute the edit distance between all pairs of strings.  See http://en.wikipedia.org/wiki/Edit_distance and http://web.stanford.edu/class/cs124/lec/med.pdf.  Then exclude any outliers strings based on some distance threshold.With remaining strings, you can use the distance matrix to identify the most central string.  Depending on the method you use, you might get ambiguous results for some data. No method is perfect for all possibilities.  For your purposes, all you need is some heuristic rules to resolve ambiguities -- i.e. pick two or more candidates.Maybe you don't want to pick ""most central"" from your list of strings, but instead want to generate a regular expression that captures the pattern common to all the non-outlier strings.  One way to do this is to synthesize a string that is equidistant from all the non-outlier strings.  You can work out the required edit distance from the matrix, and then you'd randomly generate regular using those distances as constraints.  Then you'd test candidate regular expressions and accept the first one that fits the constraints and also accepts all the strings in your non-outlier list.  (Start building regular expressions from longest common substring lists, because those are non-wildcard characters.)";;;
1036;1;2014-08-25T00:28:09.003;How to numerically estimate MLE estimators in python when gradients are very small far from the optimal solution?;I am exploring how to model a data set using normal distributions with both mean and variance defined as linear functions of independent variables.Something like N ~ (f(x), g(x)).I generate a random sample like this:def draw(x):    return norm(5 * x + 2, 3 *x + 4).rvs(1)[0]So I want to retrieve 5, 2 and 4 as the parameters for my distribution.I generate my sample:smp = np.zeros((100,2))for i in range(0, len(smp)):    smp[i][0] = i    smp[i][1] = draw(i)The likelihood function is:def lh(p):    p_loc_b0 = p[0]    p_loc_b1 = p[1]    p_scl_b0 = p[2]    p_scl_b1 = p[3]    l = 1    for i in range(0, len(smp)):        x = smp[i][0]        y = smp[i][1]        l = l * norm(p_loc_b0 + p_loc_b1 * x, p_scl_b0 + p_scl_b1 * x).pdf(y)    return -lSo the parameters for the linear functions used in the model are given in the p 4-variable vector.Using scipy.optimize, I can solve for the MLE parameters using an extremely low xtol, and already giving the solution as the starting point:fmin(lh, x0=[2,5,3,4], xtol=1e-35)Which does not work to well:Warning: Maximum number of function evaluations has been exceeded.array([ 3.27491346,  4.69237042,  5.70317719,  3.30395462])Raising the xtol to higher values does no good.So i try using a starting solution far from the real solution:>>> fmin(lh, x0=[1,1,1,1], xtol=1e-8)Optimization terminated successfully.         Current function value: -0.000000         Iterations: 24         Function evaluations: 143array([ 1.,  1.,  1.,  1.])Which makes me think:PDF are largely clustered around the mean, and have very low gradients only a few standard deviations away from the mean, which must be not too good for numerical methods.So how does one go about doing these kind of numerical estimation in functions where gradient is very near to zero away from the solution?;[education, open-source];162;
1037;2;2014-08-25T03:01:09.837;;If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:1) Feedback data is coming in too slow, say once a day, genetic algorithm might take a while to converge.2) In the process of testing genetic algorithm will probably come up with combinations that are 'strange' and that might not be the risk the company wants to take.;;;
1038;1;2014-08-25T09:53:13.823;Hive External table does not showing in Namenode (Cloudera-QuickstartVm);"I have created external table in Hive in the hdfs path 'hdfs://localhost.localdomain:8020/user/hive/training'. If I apply describe command I can find the table path as shown below. But when I browse through the namenode web page, the table name does not showing up in the path.hive> describe extended testtable4;OKfirstname   string  lastname    string  address string  city    string  state   string  country string      ***Detailed Table Information  Table(tableName:testtable4, dbName:default, owner:cloudera, createTime:1408765301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:firstname, type:string, comment:null), FieldSchema(name:lastname, type:string, comment:null), FieldSchema(name:address, type:string, comment:null), FieldSchema(name:city, type:string, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], location:hdfs://localhost.localdomain:8020/user/hive/training, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,, line.delim=        }), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1408765301}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)           Time taken: 0.7 seconds***";[education, open-source];167;
1039;2;2014-08-25T11:29:35.047;;"I think dependencies can be used to improve the accurary of your sentiment classifier. Consider the following examples:E1: Bill is not a scientistand assume that the token ""scientist"" has a positive sentiment in a specific domain.Knowing the dependency neg(scientist, not) we can see that the example above has a negative sentiment. Without knowing this dependency we would probably classify the sentence as positive.Another types of dependencies can be used probably in the same way to improve the accuracy of the classifiers.";;;
1041;2;2014-08-26T07:14:54.767;;If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical.Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates.Just extrapolating the parameters from a categorical variable is wrong for a number of reasons. The first one I can think of is that there may be more observations in some years than others, so any linear extrapolation needs to weight those year's estimates more. Just eyeballing a line - or even fitting a line to the coefficients - won't do this. ;;;
1042;1;2014-08-26T07:33:40.080;Studying social media platforms - key data analysis methods?;I am seeking a basic list of key data analysis methods used for studying social media platforms online. Are there such key methods, or does this process generally vary according to topic? And is there a standard order in which these methods are applied?(The particular context I'm interested in is how the news is impacting on social media);[education, open-source];79;
1044;1;2014-08-26T14:53:40.647;which programming language has a large library that can do machine learning algorithm, R, matlab or python;As what I described in the title, we are especially interested in those for dealing with big data----ts efficiency and stability, and used in industry not in experiment or university. Thanks!;[education, open-source];152;
1045;1;2014-08-26T21:37:12.107;Using Shark with Apache Spark;Have anyone used Shark as repository from resulting datasets from Apache Spark?I'm starting some tests with Spark and read about this database tecnology. Have anyone been using it?;[education, open-source];65;
1046;2;2014-08-26T21:51:07.247;;Try http://deeplearning4j.org/word2vec.html. This has an implementation of Word2Vec used instead of Bag of Words for NER and other NLP tasks.;;;
1047;2;2014-08-26T22:48:16.617;;"SPSS is a great tool, but you can accomplish a great deal with resources that you already have on your computer, like Excel, or that are free, like the R-project. Although these tools are powerful, and can help you identify patterns, you'll need to have a firm grasp of your data before running analyses (I'd recommend running descriptive statistics on your data, and exploring the data with graphs to make sure everything is looking normal). In other words, the tool that you use won't offer a ""silver bullet"", because the output will only be as valuable as the input (you know the saying... ""garbage in, garbage out""). Much of what I'm saying has already been stated in the reply by Aleksandr - spot on. R can be challenging for those of us who aren't savvy with coding, but the free resources associated with R and its packages are abundant. If you practice learning the program, you'll quickly gain traction. Again, you'll need to be familiar with your data and the analyses you want to run anyway, and that fact remains regardless of the statistical tools you utilize. I'd begin by getting super familiar with my data (follow the steps outlined in the reply from Aleksandr, for starters). You might consider picking up John Foreman's book called Data Smart. It's a hands-on book, as John provides datasets and you follow along with his examples (using Excel) to learn various ways of navigating and exploring data. For beginners, it's a great resource. ";;;
1048;2;2014-08-27T03:41:20.317;;I've making some researches last months and I could find more libraries, contente and active community with Python. Actually I'm using it to ETL processes, some minning jobs and to make map/reduce.;;;
1050;1;2014-08-27T13:01:14.643;Libraries for (label propagation algorithms/frequent subgraph mining) for graphs in R;"General description of the problemThere is a graph where some of the nodes have a certain type(There are about 3-4 types). For other nodes, type is not known.I want to predict, based on my graph, for the nodes with unknown type their ""most probable"" type.Possible frameworkI guess the general framework for such tasks is called label propagation algorithm.according to literature on the topic.Here are some examples: one, twoAnother often mentioned topic is Frequent Subgraph Mining, which includes algorithms like SUBDUE,SLEUTH, and gSpan.Found in RThe only label propagation algorithm I managed to find in R is label.propagation.community() from igraph library. However as the name suggests it is mostly for finding communities, not classification of nodes.There also seems to be several references to subgraphMining library, (here for example)but looks like it is missing from CRAN.QuestionIf someone could suggest libraries/frameworks for the task described, I would be very grateful.Made the question as specific as I could, hope that shall do)";[education, open-source];165;1
1051;2;2014-08-27T18:33:23.110;; When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.Here's how to get more than 10 results per query: https://support.google.com/customsearch/answer/1361951?hl=en Google Custom Search and Google Site Search return up to 10 results per query. If you want to display more than 10 results to the user, you can issue multiple requests (using the start=0, start=11 ... parameters) and display the results on a single page. In this case, Google will consider each request as a separate query, and if you are using Google Site Search, each query will count towards your limit.There are other search engine APIs as well (e.g., Bing);;;
1053;1;2014-08-28T01:36:40.540;Summarize and visualize a CSV in Java/Scala?;I would like to summarize (as in R) the contents of a CSV (possibly after loading it, or storing it somewhere, that's not a problem). The summary should contain the quartiles, mean, median, min and max of the data in a CSV file for each numeric (integer or real numbers) dimension. The standard deviation would be cool as well.I would also like to generate some plots to visualize the data, for example 3 plots for the 3 pairs of variables that are more correlated (correlation coefficient) and 3 plots for the 3 pairs of variables that are least correlated.R requires only a few lines to implement this. Are there any libraries (or tools) that would allow a similarly simple (and efficient if possible) implementation in Java or Scala?PD: This is a specific use case for a previous (too broad) question.;[education, open-source];428;1
1054;2;2014-08-28T06:49:05.210;;There are several reasons why you are getting erroneous results.First, you should consider using log likelihood instead of likelihood. There are numerical issues with multiplying many small numbers(imagine if you had millions of samples you had to multiply millions of small numbers for the lhd). Also taking gradients for optimization methods that require gradients is often easier when you are dealing with the log likelihood. In general, it is good to have an objective which is a sum rather than a product of variables when dealing with optimization problems.Second, fmin is using Nelder-Mead simplex algorithm which has no convergence guarantees according to scipy documentation. This means the convergence is totally random and you should not expect to find parameters close to the originals. To get around this, I would suggest you to use a gradient based method like stochastic gradient descent or BFGS. Since you know the generative model (rvs are Gaussian distributed) you can write the likelihood and log likelihood as:Where a,b,c and d are your model parameters 5,2,3 and 4 respectively.Then take the gradient with respect to [a,b,c,d] and feed that into prime input of fmin_bfgs. Note that due to varying variance what could be solved by just linear regression is now a nastier problem.Finally, you may also want to check Generalized least squares on http://en.wikipedia.org/wiki/Linear_regression#Least-squares_estimation_and_related_techniques and http://en.wikipedia.org/wiki/Heteroscedasticity, which talk about your problem and offer several available solutions.Good luck!;;;
1055;2;2014-08-28T11:24:38.743;;Scala is the only real language that has Big Data at it's core. You have MLLib that sits on Spark, and as Scala is Functional it makes parallel computing really natural.  R, Python and Matlab are not suitable for industry productization, well some would say Python's horrible dynamic typing can be handled a little using special build tools, but really its not type safe and there is no way to solve that problem.;;;
1056;2;2014-08-28T16:56:45.577;;"It sounds as if you want to use unsupervized learning to create a training set. Am I right? You use your cluster analysis to determine which docs come from UK, US or Oz -- or which docs are talking about Soccer, Football or Australian football respectively? Then feed those tagged docs into a supervized learning algorithm of some sort?How well this works will depend entirely on how well you can distinguish UK, US and OZ. I would have thought it would be fairly straightforward to find documents where national origin was known, so that you could build a supervized algorithm for detecting language variant. You wouldn't even need a corpus that talked about football, since dialectical differences show up in other ways that are subject matter independent. (For example, I am clearly from North America, since I just wrote ""in ways that are subject matter independent"" rather than ""Since dialectical differences do not depend on subject matter"").However, the answer to your question, ""can I use unsupervized learning and then supervized learning"" is No, if you are looking for supervized learning. If the results of an unsupervized learning algorithm are fed to a supervized learning algorithm, the net result is unsupervized --- there are still no grown-ups in the room. And the classification errors of the resulting process will contain error terms from both stages. You won't get the same performance as you would if you did a SVM with properly tagged training data. This doesn't mean you shouldn't use the method you propose ... it might still work well ... but it won't be a supervized learning algorithm.";;;
1057;2;2014-08-28T18:53:57.297;;You might want to try this book Mining the Social Web for an overview of different techniques. Obviously, the methods you need will depend on the use case. A lot of people do interesting things with graphs, displaying relationships between users, with respect to certain topics. Or you might simply to a timeline showing how a news topic builds in interest and wanes.;;;
1058;1;2014-08-29T01:36:36.320;How much data space is used by all scientific articles?;I was wondering if there is any research or study made to calculate the volume of space is used by all scientific articles. It could be in pdf, txt, compressed, or any other format. Is there even a way to measure it?Can some one point me towards realizing this study?Regards and thanks.;[education, open-source];76;
1059;1;2014-08-29T06:00:53.420;Forecasting Foreign Exchange with Neural Network - Lag in Prediction;I have a question regarding the use of neural network. I am currently working with R (neuralnet package) and I am facing the following issue.My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?Maybe something is wrong in my analysisI use the daily log return I normalise my data with the sigmoid function (sigma and mu computed on my whole set)I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?;[education, open-source];360;
1060;2;2014-08-29T10:42:14.957;;Checkout Breeze and apache commons math for the maths, and ScalaLab for some nice examples of how to plot things in Scala.I've managed to get an environment setup where this would just be a couple of lines. I dont actually use ScalaLab, rather borrow some of its code, I use Intellij worksheets instead.;;;
1061;1;2014-08-29T11:40:28.657;Apps to manage/host data sets;I'm looking for the best solution to manage and host datasets for journalistic pursuits. I am assessing https://www.documentcloud.org and http://datahub.io/.Can anyone explain the differences between them, or recommend a superior solution?;[education, open-source];19;
1062;2;2014-08-29T16:19:06.903;;"You can definitely try to first cluster your data, and then try to see if the cluster information helps your classification task.For example if your data looked like this (in 1D):AA A AA A A      BBB B B B BB BB BB      AA AA A A AAAthen it may be reasonable to run a clustering algorithm on each class, to obtain two different kinds of A, and learn two separate classifiers for A1 and A2, and just drop the cluster distinction for the final output.Other common unsupervised techniques used include PCA.As for your football example, the problem is that the unsupervised algorithm does not know what it should be looking for. Instead of learning to separate american football and soccer, it may just as well decide to cluster on international vs. national games. Or Europe vs. U.S.; which may look like it learned about american football and soccer at first, but it put american soccer into the same cluster as american football, and american football teams in Europe into the Europe cluster... because it does not have guidance on what structure you are interested in; and the continents are a valid structure, too!So usually, I would not blindly assume that unsupervised techniques yield a distrinction that matches your desired result. They can yield any kind of structure, and you will want to carefully inspect what they found before using it. If you use it blindly, make sure you spend enough time on evaluation (e.g. if the clustering improves your classifier performance, then it probably worked as intended ...)";;;
1063;2;2014-08-30T17:05:23.197;;"If your data is numeric, try loading it into ELKI (Java). With the NullAlgorithm it will give you scatterplots, histograms and parallel coordinate plots. It's fast in reading the data; only the current Apache Batik-based visualization is slooow because it's using SVG. :-( I'm mostly using it ""headless"".It also has classes for various statistics (including higher order moments on data streams), but I havn't seen them in the default UI yet.";;;
1064;2;2014-08-30T18:14:05.577;;"MapReduce is not used in searching. It was used a long time ago to build the index; but it is a batch processing framework, and most of the web does not change all the time, so the newer architectures are all incremental instead of batch oriented.Search in Google will largely work the same it works in Lucene and Elastic Search, except for a lot of fine tuned extra weighting and optimizations. But at the very heart, they will use some form of an inverted index. In other words, they do not search several terabytes when you enter a search query (even when it is not cached). They likely don't look at the actual documents at all. But they use a lookup table that lists which documents match your query term (with stemming, misspellings, synonyms etc. all preprocessed). They probably retrieve the list of the top 10000 documents for each word (10k integers - just a few kb!) and compute the best matches from that. Only if there aren't good matches in these lists, they expand to the next such blocks etc.Queries for common words can be easily cached; and via preprocessing you can build a list of the top 10k results and then rerank them according to the user profile. There is nothing to be gained by computing an ""exact"" answer, too. Looking at the top 10k results is likely enough; there is no correct answer; and if a better result somewhere at position 10001 is missed, nobody will know or notice (or care). It likely was already ranked down in preprocessing and would not have made it into the top 10 that is presented to the user at the end (or the top 3, the user actually looks at)Rare terms on the other hand aren't much of a challenge either - one of the lists only contains a few matching documents, and you can immediately discard all others.I recommend reading this article: The Anatomy of a Large-Scale Hypertextual Web Search Engine  Sergey Brin and Lawrence Page  Computer Science Department, Stanford University, Stanford, CA 94305 http://infolab.stanford.edu/~backrub/google.htmlAnd yes, that's the Google founders who wrote this. It's not the latest state, but it will already work at a pretty large scale.";;;
1065;2;2014-08-30T18:36:07.490;;"State of the art as in: used in practise or worked on in theory?APRIORI is used everywhere, except in developing new frequent itemset algorithms. It's easy to implement, and easy to reuse in very different domains. You'll find hundreds of APRIORI implementations of varying quality. And it's easy to get APRIORI wrong, actually.FPgrowth is much harder to implement, but also much more interesting. So from an academic point of view, everybody tries to improve FPgrowth - getting work based on APRIORI accepted will be very hard by now.If you have a good implementation, every algorithm has it's good and it's bad situations in my opinion. A good APRIORI implementation will only need to scan the database k times to find all frequent itemsets of length k. In particular if your data fits into main memory this is cheap. What can kill APRIORI is too many frequent 2-itemsets (in particular when you don't use a Trie and similar acceleration techniques etc.). It works best on large data with a low number of frequent itemsets.Eclat works on columns; but it needs to read each column much more often. There is some work on diffsets to reduce this work. If your data does not fit into main memory, Eclat suffers probably more than Apriori. By going depth first, it will also be able to return a first interesting result much earlier than Apriori, and you can use these results to adjust parameters; so you need less iterations to find good parameters. But by design, it cannot exploit pruning as neatly as Apriori did.FPGrowth compresses the data set into the tree. This works best when you have lots of duplicate records. You could probably reap of quite some gains for Apriori and Eclat too if you can presort your data and merge duplicates into weighted vectors. FPGrowth does this at an extreme level. The drawback is that the implementation is much harder; and once this tree does not fit into memory anymore it gets a mess to implement.As for performance results and benchmarks - don't trust them. There are so many things to implement incorrectly. Try 10 different implementations, and you get 10 very different performance results. In particular for APRIORI, I have the impression that most implementations are broken in the sense of missing some of the main contributions of APRIORI... and of those that have these parts right, the quality of optimizations varies a lot.There are actually even papers on how to implement these algorithms efficiently: Efficient Implementations of Apriori and Eclat. Christian BorgeltWorkshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).You may also want to read these surveys on this domain: Goethals, Bart. ""Survey on frequent pattern mining."" Univ. of Helsinki (2003). Ferenc Bodon, A Survey on Frequent Itemset Mining, Technical Report, Budapest University of Technology and Economic, 2006,  Frequent Item Set MiningChristian BorgeltWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(6):437-456. 2012";;;
1066;2;2014-09-01T00:01:58.917;;"Perhaps you are looking to quantify the amount of filespace used by a specific subset of data that we will label as ""academic publications."" Well, to estimate, you could find stats on how many publications are housed at all the leading libraries (JSTOR, EBSCO, AcademicHost, etc) and then get the mean average size of each. Multiply that by the number of articles and whamo, you've got yourself an estimate.Here's the problem, though: PDF files store the text from string s differently (in size) than, say, a text document stores that same string. Likewise, a compressed JPEG will store an amount of information i differently than a non-compressed JPEG. So you see we could have two of the same articles containing the same information i but taking up different amounts of memory m.Are you looking to get a wordcount on the amount of scientific literature?Are you looking to get an approximation of file system space used to store all academically published content in the world? ";;;
1067;2;2014-09-01T00:17:56.070;;It is likely to be very hard to draw any conclusion if you are training with only 10 input samples.  With more data, your diagnosis that the model is predicting lagged values would have more plausibility.  As it stands, it seems pretty likely that your model is just saying that the last observed value is pretty close to correct.  This isn't the same as a real lag model, but it is a very reasonable thing to guess if you haven't seen enough data.;;;
1068;2;2014-09-01T08:10:56.967;;I'd have a closer look at one of Apache Spark's modules: MLlib.;;;
1069;1;2014-09-02T03:24:12.793;Kappa From Combined Confusion Matrices;I am trying to evaluate and compare several different machine learning models built with different parameters (i.e. downsampling, outlier removal) and different classifiers (i.e. Bayes Net, SVM, Decision Tree).I am performing a type of cross validation where I randomly select 67% of the data for use in the training set and 33% of the data for use in the testing set. I perform this for several iterations, say, 20.Now, from each iteration I am able to generate a confusion matrix and compute a kappa. My question is, what are some ways to aggregate these across the iterations? I am also interested in aggregating accuracy and expected accuracy, among other things.For the kappa, accuracy, and expected accuracy, I have just been taking the average up to this point. One of the problems is that when I recompute kappa with the aggregated average and expected average, it is not the same with the aggregated kappa.For the confusion matrix, I have been first normalizing the confusion matrix from each iteration and then averaging them, in an attempt to avoid an issue of confusion matrices with different numbers of total cases (which is possible with my cross validation scheme).When I recompute the kappa from this aggregated confusion matrix, it is also different from the previous two.Which one is most correct? Is there another way of computing an average kappa that is more correct?Thanks, and if more concrete examples are in order to illustrate my question please let me know.;[education, open-source];46;
1070;1;2014-09-02T08:47:38.737;SAP HANA vs Exasol;"I am interested in knowing the differences in functionality between SAP HANA and Exasol. Since this is a bit of an open ended question let me be clear. I am not interested in people debating which is ""better"" or faster. I am only interested in what each was designed to do so please keep your opinions out of it. I suspect it is a bit like comparing HANA to Oracle Exalytics where there is some overlap but the functionality goals are different. ";[education, open-source];310;
1071;1;2014-09-02T09:48:08.150;How is the concept of data different for different disciplines?;"How is the concept of data different for different disciplines? Obviously, for physicists and sociologists, ""data"" is something different.";[education, open-source];32;
1072;2;2014-09-02T14:01:07.407;;"There's not an enormous difference between what you can do with the two databases, it's more a question of the focus and the way the functionality is implemented and that's where it becomes difficult to explain without using words like ""better"" and ""faster"" (and for sure words like ""cheaper"") EXASOL was designed for speed and ease of use with Analytical processing and is designed to run on clusters of commodity hardware. SAP is a more complex, aims to do more than ""just"" Analytical processing and runs only on a range of ""approved"" hardware.What type of differences did you have in mind ?";;;
1073;1;2014-09-02T19:17:43.210;Libraries for Online Machine Learning;I am looking for packages (either in python, R, or a standalone package) to perform online learning to predict stock data.I have found and read about Vowpal Wabbit (https://github.com/JohnLangford/vowpal_wabbit/wiki),which seems to be quite promising but I am wondering if there are any other packages out there.Thanks in advance.;[education, open-source];344;2
1074;1;2014-09-02T19:29:07.490;Polynomial Kernel Parameters in SVMs;In SVMs the polynomial kernel is defined as:(scale * crossprod(x, y) + offset)^degreeHow do the scale and offset parameters affect the model and what range should they be in? (intuitively please)Are the scale and offset for numeric stability only (that's what it looks like to me), or do they influence prediction accuracy as well?Can good values for scale and offset be calculated/estimated when the data is known or is a grid search required? The caret package always sets the offset to 1, but it does a grid search for scale. (Why) is an offset of 1 a good value?ThanksPS.: Wikipedia didn't really help my understanding: For degree-d polynomials, the polynomial kernel is defined as  where x and y are vectors in the input space, i.e. vectors of features  computed from training or test samples,  is a constant trading off the influence of higher-order versus lower-order terms in the  polynomial. When , the kernel is called homogeneous.(A further generalized polykernel divides  by a user-specified scalar parameter .)Neither did ?polydot's explanation in R's help system: scale: The scaling parameter of the polynomial and tangent kernel is a  convenient way of normalizing patterns (<-!?) without the need to modify the  data itself offset: The offset used in a polynomial or hyperbolic tangent kernel (<- lol thanks);[education, open-source];172;
1075;1;2014-09-04T18:13:57.343;Hadoop for grid computing;Background:I run a product that compares sets of data (data matching and data reconciliation).To get the result we need to compare each row in a data set with every N rows on the opposing data setNow however we get sets of up to 300 000 rows of data in each set to compare and are getting 90 Billion computations to handle.So my question is this:Even though we dont have the data volumes to use Hadoop, we have the computational need for something distributed. Is Hadoop a good choice for us?;[education, open-source];113;
1076;2;2014-09-05T06:17:28.533;;You could look at scikit-learn and orange module in python.Scikit-learn has a SGD classifier and regressor that could do a partial fit data in case of online learning.In R take a look at caret package ;;;
1077;2;2014-09-05T11:00:22.053;;Your job seems like a map-reduce job and hence might be good for Hadoop. Hadoop has a zoo of an ecosystem though. Hadoop is a distributed file system. It distributes data on a cluster and because this data is split up it can be analysed in parallel. Out of the box, Hadoop allows you to write map reduce jobs on the platform and this is why it might help with your problem.The following technologies work on Hadoop: If the data can be represented in a table format you might want to check out technologies like hive and impala. Impala uses all the distributed memory across a cluster and is very performant while it allows you to still work with a table structure. A more new, but promising alternative might also be spark which allows for more iterative procedures to be run on the cluster. Don't underestimate the amount of time setting up and the amount of time needed to understand Hadoop.;;;
1078;1;2014-09-05T12:08:11.347;Unstructured text classification;"I'm going to classify unstructured text documents, namely web sites of unknown structure. The number of categories is highly limited (at this point I believe, that there are not going to be more than three categories). Any suggestions where to start?I'm not sure if the ""bag of words"" approach would be feasible. Later, I can add another classification stage based on document structure (so finally decision trees are going to be utilised also).I can say, that I am somehow familiar with Mahout and Hadoop, therefore Java is preferred. If needed, I can switch to Scala and/or Spark engine (the ML library).";[education, open-source];1116;3
1079;1;2014-09-05T14:47:52.127;Data sets for evaluating text retrieval quality;I'm searching for data sets for evaluating text retrieval quality.TF-IDF is a popular similarity measure, but is it the best choice? And which variant is the best choice? Lucenes Scoring for example uses IDF^2, and IDF defined as 1+log(numdocs/(docFreq+1)). TF in lucene is defined as sqrt(frequency)...Many more variants exist, including Okapi BM25, which is used by the Xapian search engine...I'd like to study the different variants, and I'm looking for evaluation data sets. Thanks!;[education, open-source];132;
1080;1;2014-09-05T16:34:55.170;ANOVA RBF kernel returns very poor results;"I was curious about the ANOVA RBF kernel provided by kernlab package available in R.I tested it with a numeric dataSet of 34 input variables and one output variable. For each variable I have 700 different values. Comparing with other kernels, I got very bad results with this kernel.For example using the simple RBF kernel I could predict with 0,88 R2 however with the anova RBF I could only get 0,33 R2.I thought that ANOVA RBF would be a very good kernel. Any thoughts? ThanksThe code is as follows:set.seed(100) #use the same seed to train different modelssvrFitanovaacv <- train(R ~ .,                       data = trainSet,                       method = SVManova,                       preProc = c(""center"", ""scale""),                       trControl = ctrl, tuneLength = 10) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""define custom model in caret package:library(caret)#RBF ANOVA KERNELSVManova <- list(type = ""Regression"", library = ""kernlab"", loop = NULL)prmanova <- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""epsilon""),                     class = rep(""numeric"", 4),                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Epsilon""))SVManova$parameters <- prmanovasvmGridanova <- function(x, y, len = NULL) {library(kernlab)sigmas <- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,            C = 2^(-40:len), degree = 1:2) # len = tuneLength in train}SVManova$grid <- svmGridanovasvmFitanova <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {  ksvm(x = as.matrix(x), y = y,       kernel = ""anovadot"",       kpar = list(sigma = param$sigma, degree = param$degree),       C = param$C, epsilon = param$epsilon,       prob.model = classProbs,       ...) #default type = ""eps-svr""}SVManova$fit <- svmFitanovasvmPredanova <- function(modelFit, newdata, preProc = NULL, submodels = NULL)  predict(modelFit, newdata)SVManova$predict <- svmPredanovasvmProb <- function(modelFit, newdata, preProc = NULL, submodels = NULL)  predict(modelFit, newdata, type=""probabilities"")SVManova$prob <- svmProbsvmSortanova <- function(x) x[order(x$C), ]SVManova$sort <- svmSortanovaload data:dataA2<-read.csv(""C:/results/A2.txt"",header = TRUE,                              blank.lines.skip = TRUE,sep = "","")set.seed(1)inTrainSet <- createDataPartition(dataA2$R, p = 0.75, list = FALSE) #[[1]]trainSet <- dataA2[inTrainSet,]testSet <- dataA2[-inTrainSet,]#-----------------------------------------------------------------------------#K-folds resampling method for fitting svrctrl <- trainControl(method = ""repeatedcv"", number = 10, repeats = 10,                     allowParallel = TRUE) #10 separate 10-fold cross-validationslink to data:wuala.com/jpcgandre/Documents/Data%20SVR/?key=BOD9NTINzRHG";[education, open-source];62;
1081;2;2014-09-05T16:36:03.227;;"Data is, at it's most basic reduction, a raw element of something. Data is a raw ""thing"" that exists in any form from which we can analyze it and construct intelligence. When I was an Intelligence Analyst, we used to define data as ""anything and everything that could be used to construct a hypothesis."" Thus, data for any discipline is interchangeable; as a sociologist, I have a vector of discrete variables indicating ethnicity, as an economist I have a vector with housing prices, and as an anthropologist I have a vector of tablet names used in some long-gone civilization. Data is data.";;;
1082;2;2014-09-05T19:13:39.993;;Here are a couple of really great open source software packages for text classification that should help get you started:MALLET is a CPL-licensed Java-based machine learning toolkit built by UMass for working with text data. It includes implementations of several classification algorithms (e.g., naïve Bayes, maximum entropy, decision trees).The Stanford Classifier from the Stanford NLP Group is a GPL-licensed Java implementation of a maximum entropy classifier designed to work with text data.;;;
1084;2;2014-09-07T01:08:04.330;;"Let's work it out from the ground up. Classification (also known as categorization) is an example of supervised learning. In supervised learning you have: model - something that approximates internal structure in your data, enabling you to reason about it and make useful predictions (e.g. predict class of an object); normally model has parameters that you want to ""learn""training and testing datasets - sets of objects that you use for training your model (finding good values for parameters) and further evaluating training and classification algorithms - first describes how to learn model from training dataset, second shows how to derive class of a new object given trained modelNow let's take a simple case of spam classification. Your training dataset is a corpus of emails + corresponding labels - ""spam"" or ""not spam"". Testing dataset has the same structure, but made from some independent emails (normally one just splits his dataset and makes, say, 9/10 of it to be used for training and 1/10 - for testing). One way to model emails is to represent each of them as a set (bag) of words. If we assume that words are independent of each other, we can use Naive Bayes classifier, that is, calculate prior probabilities for each word and each class (training algorithm) and then apply Bayes theorem to find posterior probability of a new document to belong to particular class. So, basically we have: raw model + training set + training algorithm -> trained modeltrained model + classification algorithm + new object -> object labelNow note that we represented our objects (documents) as a bag of words. But is the only way? In fact, we can extract much more from raw text. For example, instead of words as is we can use their stems or lemmas, throw out noisy stop words, add POS tags of words, extract named entities or even explore HTML structure of the document. In fact, more general representation of a document (and, in general, any object) is a feature vector. E.g. for text: actor, analogue, bad, burn, ..., NOUN, VERB, ADJ, ..., in_bold, ... | label    0,        0,   1,    1, ...,    5,    7,   2, ...,       2, ... | not spam    0,        1,   0,    0, ...,    3,   12,  10, ...,       0, ... | spamHere first row is a list of possible features and subsequent rows show how many times that feature occurs in a document. E.g. in first document there's no occurrences of word ""actor"", 1 occurrence of word ""burn"", 5 nouns, 2 adjectives and 2 pieces of text in bold. Last column corresponds to a resulting class label. Using feature vector you can incorporate any properties of your texts. Though finding good set of features may take some time. And what about model and algorithms? Are we bound to Naive Bayes. Not at all. logistic regression, SVM, decision trees - just to mention few popular classifiers. (Note, that we say ""classifier"" in most cases we mean model + corresponding algorithms for training and classification). As for implementation, you can divide task into 2 parts: Features extraction - transforming raw texts into feature vectors. Object classification - building and applying model.First point is well worked out in many NLP libraries. Second is about machine learning, so, depending on your dataset, you can use either Weka, or MLlib. ";;;
1085;2;2014-09-07T08:02:39.670;;If I understand your description correctly, hadoop seems a huge overhead, for the wrong problem. basically you just need a standard distributed architecture: don't you just have to pass pairs of rows - eg mpi.... ipython.parallel, ...;;;
1086;1;2014-09-07T22:51:27.490;Document Ranking - Non Query based;"We currently have some ~500 bio-medical  documents each of some 1-2 MB . We want to use a non query based method to rank the documents in order of their unique content score. I'm calling it as ""unique content"" bcos our researchers want to know from which document to start reading . All the documents are of the same topic ,in bio medical world we know that there is always a lot of content overlap . So all we want to do is to arrange the documents in the order of their unique content.  Most Information Retrieval literature suggest query based ranking which dose not fit our need.  ";[education, open-source];83;2
1087;2;2014-09-08T00:47:21.590;;Here's a simple initial approach to try:Calculate the TF-IDF score of each word in each document.Sort the documents by the average TF-IDF score of their words.The higher the average TF-IDF score, the more unique a document is with respect to the rest of the collection.You might also try a clustering-based approach where you look for outliers, or perhaps something with the Jaccard index using a bag-of-words model.;;;
1088;2;2014-09-08T02:45:42.653;;"Topic Modeling would be a very appropriate method for your problem.  Topic Models are a form of unsupervised learning/discovery, where a specified (or discovered) number of topics are defined by a list of words that have a high probability of appearing together. In a separate step, you can label each topic using subject matter experts, but for your purposes this isn't necessary since you are only interested in getting to three clusters.You treat each document as a bag of words, and pre-process to remove stop words, etc.  With the simplest methods, you pre-specify the number of topics.  In your case, you could either specify ""3"", which is your fixed limit on categories, or pick a larger number of topics (between 10 and 100), and then in a separate step, form three clusters for documents with common emphasis on topics. K-means or other clustering methods could be used.  (I'd recommend the latter approach)You don't need to code topic modeling software from scratch. Here's a web page with many resources, including software libraries/packages: http://www.cs.princeton.edu/~blei/topicmodeling.htmlNone are in Java, but there are ways to run C++ and Python under Java.";;;
1089;2;2014-09-08T03:07:07.413;;"You could use Topic Modeling as described in this paper:http://faculty.chicagobooth.edu/workshops/orgs-markets/pdf/KaplanSwordWin2014.pdfThey performed Topic Modeling on abstracts of patents (limited to 150 words).  They identified papers as ""novel"" if they were the first to introduce a topic, and measured degree of novelty by how many papers in the following year used the same topic. (Read the paper for details).I suggest that you follow their lead and only process paper abstracts.  Processing the body of each paper might reveal some novelty that the abstract does not, but you also run the risk of having much more noise in your topic model (i.e. extraneous topics, extraneous words).While you say that all 500 papers are on the same ""topic"", it's probably safer to say that they are all on the same ""theme"" or in the same ""sub-category"" of Bio-medicine. Topic modeling permits decomposition of the ""theme"" into ""topics"".The good news is that there are plenty of good packages/libraries for Topic Modeling.  You still have to do preprocessing, but you don't have to code the algorithms yourself.  See this page for many resources:http://www.cs.princeton.edu/~blei/topicmodeling.html";;;
1090;1;2014-09-08T14:57:11.223;Freebase Related Models;"It may be unlikely that anyone knows this but I have a specific question about Freebase.  Here is the Freebase page from the Ford Taurus automotive model .  It has a property called ""Related Models"".  Does anyone know how this list of related models was compiled.  What is the similarity measure that they use?  I don't think it is only about other wikipedia pages that link to or from this page.  Alternatively, it may be that this is user generated.  Does anyone know for sure?";[education, open-source];21;
1091;1;2014-09-08T19:33:00.253;Creating Bag of words;What is the best technology to be used to create my custom bag of words with N-grams to apply to. I want to know a functionality that can be achieved over GUI. I cannot use spot fire as it is not available in the organization. Though i can get SAP Hana or R-hadoop. But R-hadoop is bit challenging, any suggessions.;[education, open-source];438;
1092;1;2014-09-08T21:25:26.183;Machine learning libraries for Ruby;Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's scikit-learn for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in this Stackoverflow post.Thanks in advance!;[education, open-source];441;2
1094;1;2014-09-09T06:33:00.730;Creating obligatory combinations of variables for drawing by random forest;"ProblemFor my machine learning task, I create a set of predictors.Predictors come in ""bundles"" - multi-dimensional measurements (3 or 4 - dimensional in my case).The hole ""bundle"" makes sense only if it has been measured, and taken all together.The problem is, different 'bundles' of predictors can be measured only for small part of the sample, and those parts don't necessary intersect for different 'bundles'. As parts are small, imputing leads to considerable decrease in accuracy(catastrophical to be more accurate)Possible solutionsI could create dummy variables that would mark whether the measurement has taken place for each variable. The problem is, when random forests draws random variables, it does so individually.So there are two basic ways to solve this problem:1) Combine each ""bundle"" into one predictor. That is possible, but it seems information will be lost. 2) Make random forest draw variables not individually, but by obligatory ""bundles"".Problem for random forestAs random forest draws variables randomly, it takes features that are useless (or much less useful) without other from their ""bundle"". I have a feeling that leads to a loss of accuracy.ExampleFor example I have variables a,a_measure, b,b_measure.The problem is, variables a_measure make sense only if variable a is present, same for b. So I either have to combine aand a_measure into one variable, or make random forest draw both, in case at least one of them is drawn.QuestionWhat are the best practice solutions for problems when different sets of predictors are measured for small parts of overall population, and these sets of predictors come in obligatory ""bundles""?Thank you!";[education, open-source];74;
1095;1;2014-09-09T12:44:16.967;Gini coefficient vs Gini impurity - decision trees;"The problem refers to decision trees building. According to Wikipedia 'Gini coefficient' should not be confused with 'Gini impurity'. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.1) 'Gini impurity' - it is a standard decision-tree splitting metric (see in the link above);2) 'Gini coefficient' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.";[education, open-source];3525;2
1096;2;2014-09-09T15:05:46.093;; create my custom bag of words with N-grams to apply toMy initial recommendation would be to use the NLTK library for Python. NLTK offers methods for easily extracting bigrams from text or ngrams of arbitrary length, as well as methods for analyzing the frequency distribution of those items. However, all of this requires a bit of programming. a functionality that can be achieved over GUIThat's tricky. Have you looked into GATE? I'm not exactly sure if/how GATE does what you want, but it does offer a GUI.;;;
1097;2;2014-09-09T16:58:10.537;;"I'll go ahead and post an answer for now; if someone has something better I'll accept theirs. At this point the most powerful option appears to be accessing WEKA using jRuby. We spent yesterday scouring the 'net, and this combination was even used by a talk at RailsConf 2012, so I would guess if there were a comparable pure ruby package, they would have used it.Note that if you know exactly what you need, there are plenty of individual libraries that either wrap standalone packages like libsvm or re-implement some individual algorithms like Naive Bayes in pure Ruby and will spare you from using jRuby.But for a general-purpose library, WEKA and jRuby seem to be the best bet at this time.";;;
1098;2;2014-09-10T07:33:06.470;;You can use SKlearn, It is a python library. It is simplest method which i like with minimal code. You can follow this link http://scikit-learn.org/stable/modules/feature_extraction.html ;;;
1099;2;2014-09-10T08:15:17.343;;No, despite their names they are not equivalent or even that similar. Gini impurity is a measure of misclassification, which applies in a multiclass classifier context. Gini coefficient applies to binary classification, and requires a classifier that can in some way rank examples according to likelihood of being in the positive class. Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in decision trees.;;;
1100;1;2014-09-10T14:50:13.720;Neural net for server monitoring;I'm looking at pybrain for taking server monitor alarms and determining the root cause of a problem. I'm happy with training it using supervised learning and curating the training data sets. The data is structured something like this:Server Type A #1Alarm type 1Alarm type 2Server Type A #2Alarm type 1Alarm type 2Server Type B #1Alarm type 99Alarm type 2So there are n servers, with x alarms that can be UP or DOWN. Both n and x are variable. If Server A1 has alarm 1 & 2 as DOWN, then we can say that service a is down on that server and is the cause of the problem.If alarm 1 is down on all servers, then we can say that service a is the cause.There can potentially be multiple options for the cause, so straight classification doesn't seem appropriate.I would also like to tie later sources of data to the net. Such as just scripts that ping some external service.All the appropriate alarms may not be triggered at once, due to serial service checks, so it can start with one server down and then another server down 5 minutes later.I'm trying to do some basic stuff at first:from pybrain.tools.shortcuts import buildNetworkfrom pybrain.datasets import SupervisedDataSetfrom pybrain.supervised.trainers import BackpropTrainerINPUTS = 2OUTPUTS = 1# Build network# 2 inputs, 3 hidden, 1 output neuronsnet = buildNetwork(INPUTS, 3, OUTPUTS)# Build dataset# Dataset with 2 inputs and 1 outputds = SupervisedDataSet(INPUTS, OUTPUTS)# Add one sample, iterable of inputs and iterable of outputsds.addSample((0, 0), (0,))# Train the network with the datasettrainer = BackpropTrainer(net, ds)# Train 1000 epochsfor x in xrange(10):    trainer.train()# Train infinite epochs until the error rate is lowtrainer.trainUntilConvergence()# Run an input over the networkresult = net.activate([2, 1])But I[m having a hard time mapping variable numbers of alarms to static numbers of inputs. For example, if we add an alarm to a server, or add a server, the whole net needs to be rebuilt. If that is something that needs to be done, I can do it, but want to know if there's a better way.Another option I'm trying to think of, is have a different net for each type of server, but I don't see how I can draw an environment-wide conclusion, since it will just make evaluations on a single host, instead of all hosts at once. Which type of algorithm should I use and how do I map the dataset to draw environment-wide conclusions as a whole with variable inputs?;[education, open-source];131;2
1101;2;2014-09-10T15:52:04.743;;I think they both represent the same concept. In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be:Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)In CART we perform binary splits. So The gini index will be computed as the weighted sum of the resulting partitions and we select the split with the smallest gini index.So the use of Gini Impurity (Gini Index) is not limited to binary situations.Another term for Gini Impurity is Gini Coefficient which is used normally as a measure of income distribution.;;;
1102;1;2014-09-11T17:09:52.313;Improve CoreNLP POS tagger and NER tagger?;"The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:""Oversaw car manufacturing"" gets tagged as NNP-NN-NNRather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).So what's the best way to accomplish accuracy improvement?Are there better models out there for POS/NER that can be incorporated into CoreNLP?Should I switch to other NLP tools?Or create training models with exception rules?";[education, open-source];214;
1103;2;2014-09-12T00:40:07.877;;Your best best is to train your own models on the kind of data you're going to be working with.;;;
1104;1;2014-09-12T04:22:38.823;Modeling Pipeline Budget;I have been tasked with creating a pipeline chart with the live data and the budgeted numbers.  I know what probability of each phase of reaching the next.  The problem is I have no Idea what to do about the pipeline budgeting with regards to time.  For instance what period of time should I have closed sales in the chart.  I have honestly been working on trying to figure it out.  Each successive revision gets me farther from the answer.;[education, open-source];22;
1105;1;2014-09-12T11:06:48.203;Logistic Regression implementation does not converge;"I am currently trying to implement logistic regression with iteratively reweightes LS, according to ""Pattern Recognition and Machine Learning"" by C. Bishop. In a first approach I tried to implement it in C#, where I used Gauss' algorithm to solve eq. 4.99. For a single feature it gave very promising (nearly exact) results, but whenever I tried to run it with more than one feature my system matrix became singular, and the weights did not converge. I first thought that it was my implementation, but when I implemented it in SciLab the results sustained. The SciLab (more concise due to matrix operators) code I used isphi = [1; 0; 1; 1];t = [1; 0; 0; 0];w= [1];w' * phi(1,:)'for in=1:100    y = [];    R = zeros(size(phi,1));    R_inv = zeros(size(phi,1));    for i=1:size(phi,1)        y(i) = 1/(1+ exp(-(w' * phi(i,:)')));        R(i,i) = y(i)*(1 - y(i));        R_inv(i,i) = 1/R(i,i);    end    z = phi * w - R_inv*(y - t)    w = inv(phi'*R*phi)*phi'*R*zendWith the values for phi (input/features) and t (output/classes), it yields a weight of  -0.6931472, which is pretty much 1/3, which seems fine to me, for there is 1/3 probability of beeing assigned to class 1, if feature 1 is present (please forgive me, if my terms do not comply with ML-language completely, for I am an software developer). If I now added an intercept feature, which would accord tophi = [1, 1; 1, 0; 1, 1; 1, 1];w = [1; 1];my R-matrix becomes singular and the last weights value isw  =  - 5.8151677    1.290D+30  which - to my reading - would mean, that the probability of belonging to class 1 would be close to 1 if feature 1 is present about 3% for the rest. There has got to be any error I made, but I do not get which one. For both implementations yield the same results I suspect that there is some point I've been missing or gotten wrong, but I do not understand which one.";[education, open-source];55;1
1106;1;2014-09-12T11:48:21.617;How to build a textual search engine?;I am having an HTML string and want to find out if a word I supply is relevant in that string.Relevancy could be measured based on frequency in the text.An example to illustrate my problem:this is an awesome bike storebikes can be purchased online.the bikes we own rock.check out our bike store nowNow I want to test a few other words:bike repairsdog poobike repairs should be marked as relevant whereas dog poo should not be marked as relevant.Questions:How could this be done?How to I filter out ambiguous words like in or orThanks for your ideas!I guess it's something Google does to figure out what keywords are relevant to a website. I am basically trying to reproduce their on-page rankings.;[education, open-source];129;
1107;1;2014-09-12T15:20:51.767;Quick guide into training highly imbalanced data sets;I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.Some good answers about sub-sampling and weighted random forest are given here: What are the implications for training a Tree Ensemble with highly biased datasets?Which classification methods besides RF can handle the problem in the best way?;[education, open-source];646;4
1108;1;2014-09-12T16:26:15.827;Kappa near to 60% in unbalanced (1:10) data set;As mentioned before, I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.I have trained a Generalized Boosted Regression model using gbm() from the gbm package in R and get the following output:  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD  1                  50       0.906     0.523  0.00978      0.0512    1                  100      0.91      0.561  0.0108       0.0517    1                  150      0.91      0.572  0.0104       0.0492    2                  50       0.908     0.569  0.0106       0.0484    2                  100      0.91      0.582  0.00965      0.0443    2                  150      0.91      0.584  0.00976      0.0437    3                  50       0.909     0.578  0.00996      0.0469    3                  100      0.91      0.583  0.00975      0.0447    3                  150      0.911     0.586  0.00962      0.0443  Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That's clear.And what is not transparent: how Kappa is calculated.What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? What do Accuracy SD and Kappa SD mean?;[education, open-source];262;
1109;2;2014-09-12T20:30:51.740;;Undersampling the majority class is usually the way to go in such situations.If you think that you have too few instances of the positive class, you may perform oversampling, for example, sample 5n instances with replacement from the dataset of size n.Caveats:Some methods may be sensitive to changes in the class distribution, e.g. for Naive Bayes - it affects the prior probabilities. Oversampling may lead to overfitting  ;;;
1110;1;2014-09-13T06:33:17.360;Binning long-tailed / pareto data before clustering;I want to cluster a set of long-tailed /pareto-alike data into several bins (Actually the bin number is not determined yet). Is there any algorithms or models I can use?;[education, open-source];78;1
1112;2;2014-09-13T15:36:19.867;;Max Kuhn covers this well in Ch16 of Applied Predictive Modeling.     As mentioned in the linked thread, imbalanced data is essentially a cost sensitive training problem. Thus any cost sensitive approach is applicable to imbalanced data.There are a large number of such approaches. Not all implemented in R: C50, weighted SVMs are options. Jous-boost. Rusboost I think is only available as Matlab code. I don't use Weka, but believe it has a large number of cost sensitive classifiers. Handling imbalanced datasets: A review: Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas'On the Class Imbalance Problem: Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guangtong Zhou;;;
1113;1;2014-09-13T17:13:23.373;General approahces for grouping a continuous variable based on text data?;I have a general methodological question. I have two columns of data, with one a column a numeric variable for age and another column a short character variable for text responses to a question. My goal is to group the age variable (that is, create cut points for the age variable), based on the text responses. I'm unfamiliar with any general approaches for doing this sort of analysis. What general approaches would you recommend? Ideally I'd like to categorize the age variable based on linguistic similarity of the text responses.;[education, open-source];85;2
1114;2;2014-09-13T18:17:33.253;;Gradient boosting is also a good choice here.  You can use the gradient boosting classifier in sci-kit learn for example.  Gradient boosting is a principled method of dealing with class imbalance by constructing successive training sets based on incorrectly classified examples.;;;
1118;2;2014-09-15T13:03:44.083;;There are several approaches. You can start from the second one. Equal-width (distance) partitioning:It divides the range into N intervals of equal size: uniform gridif A and B are the lowest and highest values of the attribute, the width of intervals will be: W = (B-A)/N.The most straightforward- Outliers may dominate presentation- Skewed data is not handled well. Equal-depth (frequency) partitioning:It divides the range into N intervals, each containing approximately same number of samplesGood data scalingManaging categorical attributes can be tricky.Other MethodsRank: The rank of a number is its size relative to other values of a numerical variable. First, we sort the list of values, then we assign the position of a value as its rank. Same values receive the same rank but the presence of duplicate values affects the ranks of subsequent values (e.g., 1,2,3,3,5). Rank is a solid binning method with one major drawback, values can have different ranks in different lists.Quantiles (median, quartiles, percentiles, ...): Quantiles are also very useful binning methods but like Rank, one value can have different quantile if the list of values changes.Math functions: For example, logarithmic binning is an effective method for the numerical variables with highly skewed distribution (e.g., income).Entropy-based BinningEntropy based method uses a split approach. The entropy (or the information content) is calculated based on the class label. Intuitively, it finds the best split so that the bins are as pure as possible that is the majority of the values in a bin correspond to have the same class label. Formally, it is characterized by finding the split with the maximal information gain. ;;;
1119;1;2014-09-15T13:14:40.797;Predictive modeling based on RFM scoring indicators;RFM - is a ranking model when all customers are ranked according to their purchasing F requency, R recency and M monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?Update:predicting which customer will most likely spend morewho is going to upgrade/renew subscribtion/refund etcUpdate2:I understand, this is simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?;[education, open-source];205;
1120;2;2014-09-15T18:14:16.180;;"This may provide some answer: http://cran.r-project.org/web/packages/caret/vignettes/caret.pdfYou may also check out Max Kuhn's ""Applied Predictive Modeling"" book. He talks about the caret package at length in this book, including the kappa statistics and how to use it. This may be of some help to you.";;;
1121;2;2014-09-15T20:29:16.620;;"The Kappa is Cohen's Kappa score for inter-rater agreement. It's a commonly-used metric for evaluating the performance of machine learning algorithms and human annotaters, particularly when dealing with text/linguistics. What it does is compare the level of agreement between the output of the (human or algorithmic) annotater and the ground truth labels, to the level of agreement that would occur through random chance. There's a very good overview of how to calculate Kappa and use it to evaluate a classifier in this stats.stackexchange.com answer here, and a more in-depth explanation of Kappa and how to interpret it in this paper, entitled ""Understanding Interobserver Agreement: The Kappa Statistic"" by Viera & Garrett (2005).The benefit of using Kappa, particularly in an unbalanced data set like yours, is that with a 90-10% imbalance between the classes, you can achieve 90% accuracy by simply labeling all of the data points with the label of the more commonly occurring class. The Kappa statistic is describing how well the classifier performs above that baseline level of performance.Kappa ranges from -1 to 1, with 0 indicating no agreement between the raters, 1 indicating a perfect agreement, and negative numbers indicating systematic disagreement. While interpretation is somewhat arbitrary (and very task-dependent), Landis & Koch (1977) defined the following interpretation system which can work as a general rule of thumb:Kappa Agreement< 0 Less than chance agreement0.01–0.20 Slight agreement0.21– 0.40 Fair agreement0.41–0.60 Moderate agreement0.61–0.80 Substantial agreement0.81–0.99 Almost perfect agreementWhich would indicate that your algorithm is performing moderately well. Accuracy SD and Kappa SD are the respective Standard Deviations of the Accuracy and Kappa scores. I hope this is helpful!";;;
1122;2;2014-09-15T22:43:37.583;;A Google search leads to many relevant resources that answer your question:USING RFM DATA TO OPTIMIZE DIRECT MARKETING CAMPAIGNS: A LINEAR PROGRAMMING APPROACH http://www.thefreelibrary.com/Using+RFM+data+to+optimize+direct+marketing+campaigns%3A+a+linear...-a0272246211Data Mining using RFM Analysis http://cdn.intechweb.org/pdfs/13162.pdfLibby on Recency, Frequency, Monetary Value (book) http://www.amazon.com/Recency-Frequency-Monetary-Century-Library/dp/1882222067From a data science point of view, there is nothing very special or unique about this problem. You have three independent variables and one dependent variable. Regression, clustering, and classification methods can be applied.;;;
1123;1;2014-09-16T08:01:35.997;Combine multiple classifiers to build a multi-modal classifier;Suppose I am interested in classifying a set of instances composed by different content types, e.g.:a piece of textan imageas relevant or non-relevant for a specific class C.In my classification process I perform the following steps:Given a sample, I subdivide it in text and imageA first SVM binary classifier (SVM-text), trained only on text, classifies the text as relevant/non-relevant for the class CA second SVM binary classifier (SVM-image), trained only on images, classifies the image as relevant/non-relevant for the class CBoth SVM-text and SVM-image produce an estimate of the probability of the analyzed content (text or image) of being relevant for the class C. Given this, I am able to state whether the text is relevant for C and the image is relevant for C.However, these estimates are valid for segments of the original sample (either the text or the image), while it is not clear how to obtain a general opinion on the whole original sample (text+image). How can I combine conveniently the opinions of the two classifiers, so as to obtain a classification for the whole original sample?;[education, open-source];68;
1124;1;2014-09-16T08:14:06.307;Difference between tf-idf and tf with Random Forests;I am working on a text classification problem using Random Forest as classifiers, and a bag-of-words approach. I am using the basic implementation of Random Forests (the one present in scikit), that creates a binary condition on a single variable at each split. Given this, is there a difference between using simple tf (term frequency) features. where each word has an associated weight that represents the number of occurrences in the document, or tf-idf (term frequency * inverse document frequency), where the term frequency is also multiplied by a value that represents the ratio between the total number of documents and the number of documents containing the word)?In my opinion, there should not be any difference between these two approaches, because the only difference is a scaling factor on each feature, but since the split is done at the level of single features this should not make a difference.Am I right in my reasoning? ;[education, open-source];133;
1125;1;2014-09-16T11:41:48.140;Named entity disambiguation contests;I am interested in the field of named entity disambiguation and want to learn more about it. I have heard that there are contests organised by various associations on these kind of research topics. These contests are very helpful as they give a practical experience in these fields. I found one such contest organised by Microsoft research here though the dates have already passed. Can anyone point me to any other such contests ? Also, is there a site which catalogues these contests so that one can just go there and know about all upcoming contests ?Thanks in advance.  ;[education, open-source];100;
1126;2;2014-09-16T12:32:10.130;;Basically, you can do one of two things: Combine features from both classifiers. I.e., instead of SVM-text and SVM-image you may train single SVM that uses both - textual and visual features. Use ensemble learning. If you already have probabilities from separate classifiers, you can simply use them as weights and compute weighted average. For more sophisticated cases there are Bayesian combiners (each classifier has its prior), boosting algorithms (e.g. see AdaBoost) and others. Note, that ensembles where initially created for combining different learners, not different sets if features. In this later case ensembles have advantage mostly in cases when different kinds of features just can't be combined in a single vector efficiently. But in general, combing features is simpler and more straightforward. ;;;
1127;2;2014-09-16T14:29:41.520;;"One class learningI wouldn't be too quick to throw out one-class classification methods (option 2) - the key is to model the positive (minority) class with the one-class model.  There has been research demonstrating cases where one-class classification out-performed other approaches like sampling for highly imbalanced data as often seen with protein classification tasks.I couldn't find the research I recalled, but I did find some other comparisons, showing using one-class classifiers (typically modeling the minority class) achieved as good or better performance than binary classification typically with sampled ""negatives"" from the large set of proteins not known to be positive.  Additionally this approach also gives the advantage of much improved run-time - since you only need to train the classifier on the smaller, positive set.  A couple papers:""Prediction of protein-protein interactions using one-class classification methods and integrating diverse biological data""""A One-Class Classification Approach for Protein Sequences and Structures""At the very least I would try some one-class methods and compare the performance using validation with your binary/multi-class classification approaches.  There are also open source implementations for many of these so it shouldn't be too costly to try them out, for example LibSVM has a one-class SVM implementation.  Additionally, it might prove valuable for use in an ensemble with binary classifiers, since there may be more disagreement in their predictions.Higher level representation embedding / clusteringAlong the lines of what you were thinking with (1) and the other post suggesting PCA, approaches like clustering, sparse coding, or even topic modeling - treating each protein as a document string and different protein families as different topics - could yield a representation that might make classifying the proteins straightforward.  I.e., you could identify which group/cluster a protein belongs to or classify the cluster memberships / embedded representations.  E.g., such embedding approaches as sparse coding can yield representations that reveal which cluster a protein belongs too - so that some sets of features are only active (non-zero) for proteins in the same cluster - which can make classifying them much easier. Additionally class labels or known cluster membership can be incorporated in the embedding process for most methods.EnsembleEnsembles of multiple classifiers tend to work best - especially when the classifiers are very diverse and can achieve comparable performance individually.  There are at least two ways use ensembles for this problem.You can build an ensemble of binary classifiers by sampling multiple different same-size negative sets and training a classifier on each.You can build an ensemble from different approaches, such as binary classifiers with different negative samples, combined with a one-class classification approach, combined with classification models trained on the embedded data. ";;;
1128;1;2014-09-16T15:47:17.430;Looking for algebras designed to transform time series;"I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.Edit... (Attempt to better explain what I meant by an algebraic system...)I was thinking about ""abstract algebras"" as discussed in Wikipedia:http://en.wikipedia.org/wiki/Algebra#Abstract_algebrahttp://en.wikipedia.org/wiki/Abstract_algebra#Basic_conceptsBoolean Algebras are (very simple) algebras that range over Boolean values.  A simple example of such an algebra would consist the values True and False and the operations AND, OR and NOT. One might argue this algebra is 'complete' as, from these two constants (free-variables) and three basic operations, arbitrary boolean functions can be constructed/described.I am interested to discover algebras where the values are (time-domain) time-series.  I'd like it to be possible to construct ""arbitrary"" functions, that map time-series to time-series, from a few operations which, individually, map time-series to time-series.  I am open to liberal interpretations of ""arbitrary"". I would be especially interested in examples of these algebras where the operations consist 'higher-order functions' - where such operations have been developed for a specific domain.";[education, open-source];246;1
1129;2;2014-09-16T16:37:33.357;;It's probably due to the effect coding of categorical predictors. Eg, the regression coefficient for CPR = 7 is not zero but -(sum of regression coefficients for the other 6 levels). I guess EM should have an option to switch it to reference coding, then your way of computing the predicted probability should work.;;;
1131;2;2014-09-17T00:44:19.967;;The most direct and obvious transformation is from time domain to frequency domain. Possible methods include Fourier transform and wavelet transform.  After the transform the signal is represented by a function of frequency-domain elements which can be operated on using ordinary algebra.It's also possible to model a time series as a trajectory of a dynamical system in a state space (see: http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1980.tb00300.x/abstract, and http://www3.stat.sinica.edu.tw/statistica/oldpdf/A2n16.pdf). Dynamical systems can be modeled symbolically at a course-grain level (see: http://en.wikipedia.org/wiki/Symbolic_dynamics and http://www.math.washington.edu/SymbolicDynamics/)  Symbolic dynamics draws on linear algebra.;;;
1132;1;2014-09-17T04:49:39.770;K-Means clustering for mixed numeric and categorical data implementation in C#;I am a research scholar in data mining. I'm interested in C# implementation of K-Means clustering algorithm for mixed numeric and categorical data.;[education, open-source];514;
1133;2;2014-09-17T10:16:07.100;;The solution is described here: K-Means clustering for mixed numeric and categorical dataC# implementation can be found in ALGLIB library, which I strongly recommend: http://www.alglib.net/translator/man/manual.csharp.html#gs_packages;;;
1135;1;2014-09-17T13:13:59.147;Statistical Commute Analysis in Java;"I have a rather large commute every day - it ranges between about an hour and about an hour and half of driving.I have been tracking my driving times, and want to continue to do so. I am capturing the date, my time of departure, my time of arrival, the route I took (there are two or three possible ones), weather conditions (wet/dry and clear/hazy/foggy), and whether I stopped (and if so, for what reason - fuel/toilet break/food break, and for how long) for every journey to and from work.I would like to create a system to analyse this data and suggest an optimal departure time (for the next journey) based on day of the week, weather conditions, and whether i need to stop.Anecdotally, I can see that Tuesday mornings are worse than other mornings, the earlier I leave the more likely I am to take a toilet break or a food break, and obviously that the journey takes longer on rainy or foggy days than on clear and dry days - but I would like the system to empirically tell me that!I assume this is a machine-learning and statistical analysis problem.However, I have absolutely no knowledge of machine-learning, or statistical methods.What statistical methods should I use to do this kind of analysis to the point where the data will lead to suggestions like ""tomorrow is Tuesday and it is going to rain, so you must leave home between 7.50 and 8.00, and take route XYZ, to get the optimal driving time. Oh and chances are you will need a toilet break - and I have factored that in""? (assume that I manually enter tomorrow’s weather forecast - I’ll look into integrating with a weather service later)Note that this is life-hacking for me, trying to optimise the hell out of a tedious process, and it is very personal - specific to me and my habits, specific to this route, and specific to the morning/evening commute times. Google Maps with Traffic, TomTom with IQ, and Waze do very well in the more open-ended situations of ad-hoc driving-time prediction. Even Apple is happy to tell me on my iPhone notification screen how long it will take me to get home if I leave right now.Also note, it appears to me that traffic is not a consideration - that is to say, I do not think I need to know the actual traffic conditions - traffic is a function of day of the week and weather. For example, there are more people on the roads on Monday and Tuesday mornings, and people drive more slowly, and more people are in cars (opting to drive instead of cycle or take public transport) when it rains.To what extent can I let the data do all the talking? I have a somewhat ambiguous hidden agenda which may not be apparent from the data;I should be at work at 9.30 (i.,e. 9.15 +/- 15 minutes) every day, but the occasional 10am arrival is OKI want to leave home as late as possible, and yet arrive at work as early as possibleI want to leave work as early as possible, and yet have done at least 8 hours’ workit is OK for me to, say, leave half an hour early on one day but stay late on another to compensateI think I can come up with a procedural formula that can encompass all of these rules, but my gut feeling is that statistical analysis can make it a lot smarter.Apart from the methods of analysis, the technology stack is not an issue. Java is my language of choice - I am quite familiar with programming in it, and in creating web applications.Assuming that it is possible, are there Java libraries that can provide the requisite methods?What limitations are there? I want to keep capturing more and more data every day, making the data set bigger, hopefully, making the prediction more accurate.What other ways are there to do it? Can I push this data into, say, Wolfram Programming Cloud, or maybe something Google provides to get the desired results?";[education, open-source];136;
1136;2;2014-09-18T01:39:30.063;;"You do want to model the traffic, at least over a work day, otherwise it wouldn't matter what time you traveled! Absent any data, I'd assume there isn't much variance over the working week, but that's one thing the data will quickly confirm or refute. If it is varying, you can use a different model for each day. You have two variables; the departure times from home and work, respectively. Let's call them t_h and t_w. Let's call the commute time T_c(t), where t is the time of day. You can estimate this function from the data, so I'll assume it is given.You want to maximize c t_h - (1-c) t_w subject to the constraints t_h + T_c(t_h) < 9.5 and t_w > t_h + T_c(t_h) + 8where c is a constant you can set to adjust the relative importance of leaving home early relative to leaving work early. You should be able to solve this numerical optimization problem with Mathematica, MATLAB, or something similar. I would not recommend Java; it's not meant for this. The only tricky part is estimating T_c. You know that it's a non-negative function, so you could use the standard trick of estimating it's logarithm (say, with kernels) and exponentiating. For implementation with Mathematica see Smoothing Data, Filling Missing Data, and Nonparametric Fitting and Constrained Optimization.";;;
1137;1;2014-09-18T06:22:13.940;Choosing a window size for DTW;I have time series data from mobile sensors for different motions such as walking, pushups, dumbellifts, rowing and so on. All these motions have different length of time series. For classifying them using Dynamic Time Warping (DTW), how do I choose an appropriate window size that will give good results?;[education, open-source];142;3
1138;2;2014-09-18T13:14:37.940;;Decision trees (and hence Random Forests) are insensitive to monotone transformations of input features. Since multiplying by the same factor is a monotone transformation, I'd assume that for Random Forests there indeed is no difference. However, you eventually may consider using other classifiers that do not have this property, so it may still make sense to use the entire TF * IDF. ;;;
1139;2;2014-09-18T13:51:58.473;;Some of the GREC shared task challenges included a named entity recognition & coreference resolution component (i.e., disambiguation), but I don't think they've run GREC since 2010...?https://sites.google.com/site/genchalrepository/reg-in-context/grec-ner;;;
1140;2;2014-09-18T19:56:25.827;;pre-process your documents (some of the steps may be skipped)tokenizeremove stop wordsstem or lemmatizedo normalization (e.g. U.S.A. -> USA, météo -> meteo, etc) and orthographic correction perform phonetic normalization (e.g. with Soundex)find equivalence classes (using thesauri, e.g. WordNet) use a Vector Space model to represent documents (you may use TF, aforementioned TF-IDF or other models)do the same with the query: preprocess and represent it in the vector spacefind the most similar documents by computing the vector similarity (e.g. using the cosine similarity)That's an outline of the Information Retrieval process Introduction to Information Retrieval by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze is a very good book to get started in IR.Or just use Apache Solr to get everything you need out of the box (or Apache Lucene, that is used by Solr, to build your own application);;;
1141;1;2014-09-19T02:34:04.093;Some suggestion for career in data science or predictive modeling;I am looking to choose my career in the area of decision science or predictive modeling and I am aware that this is kind of opinion based but I would like to have some suggestion from experts that I can use it to build my career in correct path. What are the tools should I know like R, SAS or any other. What are the thinks I should know to work in a data science or machine learning or predictive modeling. For me I am having problem in identifying steps that I should follow. Please suggest me some steps to follow.;[education, open-source];135;1
1142;2;2014-09-19T07:15:13.803;;"First of all I should say you question probably is an off-topic and will be closed soon.Discussed at this SE siteAnyway I can target you to similar questions discussed at this SE site already:Statistics + Computer Science = Data Science?Starting my career as Data Scientist, is Software Engineering experience required?Cross Validated SEA set of relevant questions at Cross Validated Stack Exchange:http://stats.stackexchange.com/questions/tagged/careersData scientist mapThis is good infographics of data science knowledge you might need to start a career (Link to image):Careers SEAlso, simple ""data scientist"" querying of Careers SE site http://careers.stackoverflow.com/jobs?searchTerm=data+scientist&location= will lead you to the following knowlege ""tags"":Rbigdatadata-visualizationhadoopmapreducescalapythonmatlabetc.";;;
1143;1;2014-09-19T09:08:55.180;Machine Learning Algorithms, Depth of Understanding Vs Number of algorithms,;"Recently i was introduced to the field of Data Science (its been 6 months approx),and i started the journey with Machine Learning Course by Andrew Ng and post that started working on the Data Science Specialization by JHU.On practical Application front i have been working on building a Predictive Model that would Predict attrition so far i have used glm,bayesglm,rf in an effort to learn and apply these methods,but i find a lot of gap in my understanding of these Algorithms.Dilemma:Whether ""i should focus more on learning the intricacies of a few algorithms or should i use the approach of knowing a lot of them as and when and as much as required""**Please guide me in the right direction,may be by suggesting books or articles or anything that you think would help. * i would be grateful if you would reply with an idea of guiding someone who has just started his career in the field of Data Science, and wants to be a person who solves practical issues for the business world.***I would read(as many as possible) resources(books,articles) suggested in this post and would provide a personal feed back on the pros and cons of the same so as to make this a helpful post for people who come across a similar question in future,and i think it would be great if people suggesting these books can do the same.";[education, open-source];368;1
1144;2;2014-09-19T11:38:36.150;;Arguably someone calling himself a data scientist ought to know more about the intricacies of the algorithms he uses—e.g. what affects the convergence rate of the Fisher scoring algorithm in GLM—than a common or garden statistician—who might be content just to know that the maximum-likelihood solution will be found (perhaps after he makes a cup of coffee). In any case understanding the general concepts of statistics & machine learning is important in addition to familiarity with the methods you do use—the theory behind them, the assumptions they make, what diagnostic checks you should perform, how to interpret the results. Avoid being this parody.You'd probably enjoy reading Hastie et al. (2009), The Elements of Statistical Learning.;;;
1145;2;2014-09-19T12:39:23.103;;I would recommend limiting yourself to a few tried and trusted algorithms. I would not recommend Elements of statistical learning ( as a first book).  It is too theoretical, aimed  at graduate students, with exercises asking how to prove X or Y...  I think ISL is more appropriate, with more practical advice ( in any case both books are free as pdf downloads).Besides statistics, I would make sure you are comfortable with experimental design/AB tests, and with Business Intelligence/Visualisation.;;;
1146;2;2014-09-19T22:51:43.883;;"Since it is general methodological question, let's assume we have only one text-based variable - total number of words in a sentence. First of all, it's worth to visualize your data. I will pretend I have following data: Here we see slight dependency between age and number of words in responses. We may assume that young people (approx. between 12 and 25) tend to use 1-4 words, while people of age 25-35 try to give longer answers. But how do we split these points? I would do it something like this: In 2D plot it looks pretty straightforward, and this is how it works most of the time in practise. However, you asked for splitting data by a single variable - age. That is, something like this: Is it a good split? I don't know. In fact, it depends on your actual needs and interpretation of the ""cut points"". That's why I asked about concrete task. Anyway, this interpretation is up to you. In practise, you will have much more text-based variables. E.g. you can use every word as a feature (don't forget to stem or lemmatize it first) with values from zero to a number of occurrences in the response. Visualizing high-dimensional data is not an easy task, so you need a way to discover groups of data without plotting them. Clustering is a general approach for this. Though clustering algorithms may work with data of arbitrary dimensionality, we still have only 2D to plot it, so let's come back to our example. With algorithm like k-means you can obtain 2 groups like this: Two dots - red and blue - show cluster centres, calculated by k-means. You can use coordinates of these points to split your data by any subset of axes, even if you have 10k dimensions. But again, the most important question here is: what linguistic features will provide reasonable grouping of ages. ";;;
1147;1;2014-09-20T02:22:07.510;Invariance Property of Vowpal Wabbit Updates - Explaination;"One of the discussed nice aspects of the procedure that Vowpal Wabbit uses for updates to sgd pdf is so-called weight invariance, described in the linked as:""Among these updates we mainly focus on a novelset of updates that satisfies an additional invarianceproperty: for all importance weights of h, the updateis equivalent to two updates with importance weighth/2. We call these updates importance invariant.""What does this mean and why is it useful?";[education, open-source];139;
1148;2;2014-09-21T06:01:41.097;;If you have enough data, use cross validation.If you don't have a lot of data, use cross validation on a similar dataset, and transfer the window size (the UCR archive has a bunch of similar dataset)Don't forget, that the best warping  window size depends on the amount of training data. As you get more data, you can have a smaller warping window, see fig 6 of http://www.cs.ucr.edu/~eamonn/DTW_myths.pdfeamonn;;;
1149;1;2014-09-22T15:48:32.697;Text Classification with mixed features in Random Forests;I am working on a text classification problem on tweets. At the moment I was only considering the content of the tweets as a source of information, and I was using a simple bag of words approach using term frequencies as features, using Random Forests (this is something I cannot change). Now my idea is to try to incorporate information present in the URLs used in tweets. Now, not all the tweets have URLs, and if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLs. For this reason, I suppose that having a single set of features containing both the tweet term frequencies and the URL term frequencies could be bad. Besides I'll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features. Do you have any suggestions regarding this issue? ;[education, open-source];150;
1150;2;2014-09-23T02:20:24.670;;This has been asked, and answered, on CrossValidated. See http://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression and http://stats.stackexchange.com/questions/45803/logistic-regression-in-r-resulted-in-hauck-donner-phenomenon-now-what for two related answers (and there are other related questions that you can explore there). That logistic regression can blow up is a known effect in computational statistics.Also, in situations where exp(-30) gives you roughly the relative accuracy of the double type, you would want to be extremely careful with accumulation of the round-off errors, as 1+exp(-30)=1: summing the likelihood contributions may run into numerical problems, especially when you start computing numerical derivatives and gradients. For a brief introduction into the issue in application to the typical problems in statistical computing and the specific problems it tends to encounter, see http://www.stata.com/meeting/nordic-and-baltic14/abstracts/materials/dk14_gould.pdf.;;;
1151;1;2014-09-23T03:34:35.750;Prerequisites for Data Science;I'm a java developer and I want to pursue career in Data Science and machine learning. Please advise me where and how to begin? What subjects like mathematical/statistical skills are required and so on.;[education, open-source];621;3
1152;2;2014-09-23T13:51:24.650;;Are you using raw term frequencies, or TF-IDF?Perhaps you could simply combine the terms in the tweet with the terms in the URL-linked pages (if any) into a single bag of words, calculate TF-IDF, and normalize to avoid bias towards longer documents (i.e., those tweets containing URL links). if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLsI don't understand what you mean here. Aren't your features the terms in your bag of words? So the number of features will be the size of your vocabulary, which I imagine won't change much whether you include URLs or not. Besides I'll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features.I don't understand this either. Term-document matrices are virtually always a sparse matrix, since most of the terms in your vocabulary won't appear in most of your documents. So, the vast majority of values in your TDM will be 0. I don't know where you're getting -1 from.;;;
1153;2;2014-09-23T20:39:29.810;;I remember a long time ago playing with Elastic Search (the website is very different now from what I remember). There is some stuff about dealing with human language here : http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/languages.htmlBe warned that Elastic search is like a big bazooka to your problem. If your problem is very simple, maybe you want to go from scratch. There is some docs in the web about it.;;;
1154;1;2014-09-23T20:58:01.027;Trouble representing a problem;I have a problem and I'm having trouble representing it - first I thought I should use graph theory (nodes and edges) and now I'm not sure.My data is some tanks names and it's volumes, those tanks are connected by pipelines which I have the names and length.------(pipeline 1)------.-----(pipeline 2)------.----(pipeline 3)---  |                     |    |                           |         |[R tank 1]        [S tank 1] [S tank 2]            (pipeline 4) [S tank 3]                                                         |                                                     [S tank 4]R tank is sink (receiver) and S tank is source (sender)Problem is the pipe names change doesn't occur where there is a tank - they change name because historical reasons, size or connections...So if I want to graphically show that S tank 2 is connected to pipeline 2 at point X and pipeline 2 connects to pipeline and the content goes to R tank 1, how should I do this? (I think the point X may not be relevant but if I had some way to get the distance travelled would be great).  ;[education, open-source];84;
1155;1;2014-09-24T16:25:48.803;Estimating Variance Reduction Resultant from Additional Data;"I couldn't quite think of how best to title this, so recommendations are welcome. Same goes for the tags (I don't have the reputation to use the tags that I thought were appropriate). The question is this:""Suppose you have N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -> y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?""The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.";[education, open-source];60;
1159;1;2014-09-25T08:40:59.467;How to do SVD and PCA with big data?;I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD then PCA to reduce the data dimension for efficiency. But MATLAB and Octave cannot load such a large dataset.What tools I can use to do SVD with such a large amount of data? ;[education, open-source];858;3
1160;2;2014-09-25T15:02:45.533;;If I understand you correctly, I would try a few featurization methods to transform the text column to a numeric value. Then you can proceed with analysis as usual. There is a great book on NLP called Taming Text that would give numerous ways to think about your text variables.;;;
1161;2;2014-09-25T15:20:18.660;;I would say that trying out the resources online such as the Explore Data Science course and Andrew Ng's ML Course on Coursera (mentioned in the notes above) are great. However, nothing beats true data that you understand inside and out. After picking up some skills from Explore Data Science, I propose you collect your fuel usage data continuously. Collect the basics: city, state, gallons, total cost, and your vehicles mileage. When you forget to enter it, then you have to learn how to work with missing data. As you feel more comfortable, enter in kaggle contests. The data science community is growing so quickly that the online resources are vast and will help you identify any areas where a text book is what you need. Good Luck!! Have fun!;;;
1162;2;2014-09-25T15:29:00.877;;"I think ethics in Data Science is important. There is a fundamental difference in using user data to better their experience and show relevant ads and using user data to trick people into clicking on ads for the sake of monetary profit. Personally I like ads that give me relevant information like deals on things I would buy anyway. However, showing me weight loss ads because I got dumped is creepy and unethical. As my friend Peter always says, ""don't be creepy with data"". ";;;
1163;2;2014-09-25T17:33:09.617;;Don't bother.First rule of programming- which also applies to data science: get everything working on a small test problem.so take a random sample of your data of say 100,000 rows. try different algorithms etc.  once you have got everything working to your satisfaction, you can try larger (and larger) data sets - and see how the test error reduces as you add more data.furthermore you do not want to apply svd to only 8 columns: you apply it when you have a lot of columns.;;;
1164;2;2014-09-25T18:48:12.303;;"Although you can probably find some tools that will let you do it on a single machine, you're getting into the range where it make sense to consider ""big data"" tools like Spark, especially if you think your data set might grow. Spark has a component called MLlib which supports PCA and SVD. The documentation has examples.";;;
1165;1;2014-09-25T20:18:46.880;Looking for a strong Phd Topic in Predictive Analytics in the context of Big Data;I'm going to start a Computer Science phd this year and for that I need a research topic. I am interested in Predictive Analytics in the context of Big Data. I am interested by the area of Education (MOOCs, Online courses...). In that field, what are the unexplored areas that can help me choose a strong topic? Thanks.;[education, open-source];581;
1166;1;2014-09-26T06:56:26.913;A tool like Matlab for NLP?;Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... I would like to know if there is such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenization, POS tagging, parsing, training, testing .... However I am new to NLP and I need a tool which let me experiment, get familiar and progress;[education, open-source];801;
1167;2;2014-09-26T07:47:45.600;;The correct model is binomial, both poisson and normal are just approximations. The binomial pdf is defined on the integers between zero and number of trials. The poisson is defined on the integers between 0 and infinity, and normal is on all real variables between +/- infinity. In other words: for a poisson there is a (possibly small) but non zero probability of having more clicks than impressions. For gaussian you can have even negative clicks. Of course, the particular parameters determine how big an impact this has... probably worth plotting the respective pdfs;;;
1168;2;2014-09-26T08:51:46.727;;Please take a look at this paper: Survey on Various Natural Language Processing ToolkitsIn this work several NLP tools are introduced. Examples are:Carabao Language ToolkitGATEStanford NLP toolkitNLTKJust google the title of the paper and you will find the .pdf file.;;;
1169;2;2014-09-26T11:39:07.313;;"First of all, dimensionality reduction is used when you have many covariated dimensions and want to reduce problem size by rotating data points into new orthogonal basis and taking only axes with largest variance. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In you concrete case it's more promising to take a look at online learning methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as ""mini-batches"") at a time and build a model incrementally. (I personally like to interpret word ""online"" as a reference to some infinitely long source of data from Internet like Twitter feed, where you just can't load the whole dataset at once). But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn't fit into a memory? Normally dataset is represented as a data matrix X of size n x m, where n is number of observations (rows) and m is a number of variables (columns). Typically problem with memory comes from only one of these two numbers. Too many observations (n >> m)When you have too many observations, but number of variables is from small to moderate, you can build covariance matrix incrementally. Indeed, typical PCA consists of constructing covariance matrix of size m x m and applying singular value decomposition to it. With m=1000 variables of type float64 covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build covariance matrix without loading entire dataset into memory - pretty tractable task. Alternatively, you can select small representative sample from your dataset and approximate covariance matrix. This matrix will have all the same properties as normal, just a little bit less accurate. Too many variables (n << m)On another hand, sometimes, when you have too many variables, covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in 703Gb covariance matrix! That's definitely not what you would like to keep in memory of your computer. Or even in memory of your cluster. So we need to reduce dimensions without building covariance matrix at all. My favourite method for doing it is Random Projection. In short, if you have dataset X of size n x m, you can multiply it by some sparse random matrix R of size m x k (with k << m) and obtain new matrix X' of a much smaller size n x k with approximately same properties as original one. Why it works? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first k of them. It turns out, that sparse random vectors are nearly orthogonal and thus may also be used as a new basis. And, of course, you don't have to multiply the whole dataset X by R - you can translate every observation x into new basis separately or in mini-batches.There's also somewhat similar algorithm called Random SVD. I don't have any real experience with it, but you can find example code with explanations here.As a bottom line, here's short check list for dimensionality reduction of big datasets: If you have not that many dimensions (variables), simply use online learning algorithms. If there are many observations, but moderate number of variables (covariance matrix fits into memory), construct matrix incrementally and use normal SVD. If number of variables is too high, use incremental algorithms. ";;;
1171;2;2014-09-26T16:15:05.580;;There's a NLP toolbox for MATLAB called MatlabNLP. It includes modules for tokenization, preprocessing (stop word removal, text cleaning, stemming), and learning algorithms (linear regression, decision trees, support vector machines and a Naïve Bayes). A module for POS tagging is coming soon.;;;
1172;1;2014-09-27T13:14:16.710;General programs/libraries for studying user search behavior?;"Are there any general open-source programs or libraries (e.g., a Python library) for analyzing user search behavior?  By ""search behavior"", I mean a user's interaction with a search engine, such as querying, clicking relevant results, and spending time on those results.  I'd like something with the following properties - it doesn't have to be all of them, but the more the merrier:Models individual user behavior (aggregate and time-based)Models group user behaviorSimulates individual user behavior, given a modelIs easily extensible (to accept data input formats, user models, document models, etc., that end-users define)Links are a plus!";[education, open-source];21;
1173;1;2014-09-27T21:58:42.477;Versatile data structure for combined statistics;Not sure if this is Math, Stats or Data Science, but I figured I would post it here to get the site used.As a programmer, when you have a system/component implemented, you might want to allow some performance monitoring. For example to query how often a function call was used, how long it took and so on. So typically you care about count, means/percentile, max/min and similiar statistics. This could be measurements since startup, but also a rolling average or window.I wonder if there is a good data structure which can be updated efficiently concurrently which can be used as the source for most of those queries. For example having a ringbuffer of rollup-metrics (count, sum, min, max) over increasing periods of time and a background aggregate process triggered regularly.The focus here (for me) is on in-memory data structures with limited memory consumption. (For other things I would use a RRD type of library).;[education, open-source];37;
1174;1;2014-09-28T12:51:43.823;How to classify and cluster this time series data;I have post already the question few months ago about my project that I'm starting to work on. This post can be see here: Human activity recognition using smartphone data set problemNow, I know this is based around multivariate time series analysis and tasks are to classify and cluster the data. I have gathered some materials (e-books, tutorials etc.) on this but still can't see a more detailed picture of how even I should start. Here's the tutorial that looks like it might be helpful but the thing is my data looks differently and I'm not really sure if this can be applied to my work.http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-componentsSo basically, my questions are:How I can start on some very basic analysis? How to read data so it any meaning for me.Any tips and advises will be much appreciated!Note: I'm just the beginner in data science.  ;[education, open-source];254;
1175;2;2014-09-29T00:20:24.093;;"Often different data samples have different weighting ( eg the costs of misclassification error  for one group of data  is higher than for other classes).Most error metrics are of the form $\sum_i e_i$ where e_i is the loss ( eg squared error) on data point $i$. Therefore weightings of the form $\sum_i w_i e_i$ are equivalent to duplicating the data w_i times (eg for w_i integer). One simple case is if you have repeated data - rather than keeping all the duplicated data points, you just ""weight"" your one repeated sample by the number of instances.Now whilst this is easy to do in a batch setting, it is hard in vowpal wabbits online big data setting: given that you have a large data set, you do not just want to represent the data n times to deal with the weighting ( because it increases your computational load). Similarly, just multiplying the gradient vector by the weighting - which is correct in batch gradient descent - will cause big problems for stochastic/online gradient descent: essentially you shoot off in one direction ( think of large integer weights) then you shoot off in the other - causing significant instability. SGD essentially relies on all the errors to be of roughly the same order ( so that the learning rate can be set appropriately). So what they propose is to ensure that the update for training sample x_i with weight n is equivalent to presenting training sample x_i n times consecutively.The idea being that presenting it consecutively reduces the problem because the error gradient (for that single example $x_i$) reduces for each consecutive presentation and update (as you get closer & closer to the minimum for that specific example). In other words the consecutive updates provides a kind of feedback control. To me it sounds like you would still have instabilities (you get to zero error on x_i, then you get to zero error on x_i+1,...). the learning rate will need to be adjusted to take into account the size of the weights.";;;
1176;2;2014-09-29T00:28:26.413;;PCA is usually implemented by computing SVD on the covariance matrix.Computing the covariance matrix is an embarrassingly parallel task, so it scales linear with the number of records, and is trivial to distribute on multiple machines!Just do one pass over your data to compute the means. Then a second pass to compute the covariance matrix. This can be done with map-reduce easily - essentially it's the same as computing the means again. Sum terms as in covariance are trivial to parallelize! You may only need to pay attention to numerics when summing a lot of values of similar magnitude.Things get different when you have a huge number of variables. But on an 8 GB system, you should be able to run PCA on up to 20.000 dimensions in-memory with the BLAS libraries. But then you may run into the problem that PCA isn't all that reliable anymore, because it has too many degrees of freedom. In other words: it overfits easily. I've seen the recommendation of having at least 10*d*d records (or was it d^3). So for 10000 dimensions, you should have at least a billion records (of 10000 dimensions... that is a lot!) for the result to be statistically reliable.;;;
1177;1;2014-09-29T14:52:48.130;Distance calculation/vector range significance;I'm trying to implement item based collaborative filtering. Do any distance calculations allow for weighting of certain ranges of values within each vector? For example, I would like to be able to say values 10..22 within each vector are more significant than values within the range 0..10. I've been experimenting with Pearson, Tanimoto and Euclidean algorithms, but they all seem to assume equal weighting for each value within the vector.Am I approaching this problem in the right way, and if not, how do others deal with this problem? ;[education, open-source];60;
1178;2;2014-09-29T17:46:58.617;;"The kinds of tools you will use will vary based on the problem you are trying to solve. Social media data is rich and therefore many questions can be asked - and many tools can be used.However, there is a general pattern you might keep in mind. Typically, you will have to use the platform's API to gather data. You will then have to normalized and store the data in a data warehouse. Finally, you will access and analyze the data with the tools you desire. Keeping the end goal in mind, you will have to strategically choose the best technologies for the job you are doing.For example, let's say you wanted to study the relationships between users on a social network - a question like ""Who are the mutual friends of the most popular individuals in this social network?"" In this case, you would gather data using the social media platform's API, normalize it into a CSV, import it into a Neo4j database, and then use Cypher to make queries. ";;;
1179;2;2014-09-29T20:31:49.387;;"I created a scoring system (""Thomas Scoring System"") to deal with this problem.  If you treat ""distance"" as a similarity score, this system should work for you. http://exploringpossibilityspace.blogspot.com/2014/02/thomas-scoring-system.html Here's Thomas Scoring System (TSS) in a nutshell: Treat each metric and their values as evidence that weigh for or against specific values in the performance index. This is an inference process and not an arithmetic calculation, as in the Usual Method. The output of TSS is an estimation of the Weight of Evidence for all index values.  The Weight of Evidence can be expressed as a probability distribution, where all the evidence weights sum to one.To use your example, each range is a condition: A) 10 <= x <= 22; and B) x < 10. If A is true, then it weighs in favor of certain similarity (distance) scores.  If B is true, then it weights in favor of other similarity (distance) scores, but would have less weight compared to other evidence.";;;
1180;2;2014-09-29T22:45:38.623;;You mention distance metrics but Pearson and Tanimoto are not. For Euclidean distance, simply scale the dimensions in question by some factor. For example doubling a dimension's values makes its contribution to distance larger and so makes the feature more important to the distance metric. On a related note you may wish to look at the Mahalanobis distance which kind of accounts for covariance or 'redundancy' across dimensions. ;;;
1181;1;2014-09-30T09:43:24.153;document clustering by semantic similarity based EMD(earth mover distance);I want the text-based semantic clustering EMD do.Is there a better way of using LDA to detect topics in text, there are so provide better results?I'm going to do my EMD on discovery topics.Thanks;[education, open-source];75;
1182;2;2014-09-30T14:29:06.307;;It sounds like you would like the Boost Accumulators library: Boost.Accumulators is both a library for incremental statistical  computation as well as an extensible framework for incremental  calculation in general. The library deals primarily with the concept  of an accumulator, which is a primitive computational entity that  accepts data one sample at a time and maintains some internal state.  These accumulators may offload some of their computations on other  accumulators, on which they depend. Accumulators are grouped within an  accumulator set. Boost.Accumulators resolves the inter-dependencies  between accumulators in a set and ensures that accumulators are  processed in the proper order.;;;
1183;1;2014-09-30T19:53:20.163;Discovering dis-associations between periods of time-series;I'm interested in discovering some kind of dis-associations between the periods of a time series based on its data, e.g., find some (unknown number of) periods where the data is not similar with the data from another period.Also I would like to compare the same data but over 2 years (something like DTW?).I get my data Excel as a two-column list:c1=date (one per each day of the year), c2=Data To AnalyzeSo, what algorithms could I use and in what software?Update/Later edit:I'm looking for dates as cut-off points from which the DataToAnalyze could be part of another cluster of consecutive dates. For example:2014-1-1 --> 2014-3-10are part of Cluster_1 based on DataToAnalyze. And:2014-3-11 --> 2014-5-2are part of Cluster_2 based on DataToAnalyze, and so on. So, clusters of consecutive dates should be automatically determined based on some algorithms, which is what I'm looking for. Which ones (or which software) would be applicable to this problem?;[education, open-source];22;
1184;2;2014-09-30T20:12:32.870;;I would reccomend python if you lazily evaluate the file you will have a miniscule memory footprint, and numpy/scipy give you access to all of the tools Octave/Matlab would. ;;;
1185;1;2014-10-01T10:32:32.960;What are some best papers on gradient descent for NN implementation?;"I'm trying to implement GD for standard task of NN training :) The best papers for practioneer I've founded so far are:1) ""Efficient BackProp"" by Yann LeCun et al.2) ""Stochastic Gradient Descent Tricks"" by Leon BottouAre there some other must read papers on this topic?Thank you!";[education, open-source];34;
1186;1;2014-10-01T13:26:52.037;How much time scikit classifier will take to classify?;I am planning to use scikit linear svc classifier for text classification.I have 1 million classified data. What I am planning to do is when user enters keyword ... first classifier will classify it in one catagory and search will happen in that catagory.Now how do I confirm that classification will not take much time ... as I don't want search to suffer for better catagory result.Is using scikit for website / web application is suitable ?Do anyone know how amazon or flipkart do classification on user query or they have completely different logic ?;[education, open-source];140;1
1187;1;2014-10-01T14:03:05.870;Which method works best for attribution models for the objectives?;I have a client that is managing several campaigns. However I'm not clear what percentage should be applied to each channel that bring traffic to my website, when assessing their participation in the objectives. For those interested, here I leave the link to my profile on linkedin. Specialist online markegin in Bogotá.;[education, open-source];13;
1188;2;2014-10-01T14:44:35.467;; How I can start on some very basic analysis?Take your labeled data and compute histograms of the values for each of the sets. Plot these and visually see if there's any differences. Also compute the mean and variance of each of the different labeled sets and see if there are differences. If it's timeseries data, take small (overlapping) windows of time and compute various metrics - min, max, variance, mean, for instance - and use that as input to a classifier.;;;
1189;1;2014-10-01T18:11:26.220;How to define confusion matrix for classification?;Below is the dataset where the response variable is play with two labels (yes, and no), No. outlook temperature humidity    windy   play1   sunny       hot     high        FALSE   no2   sunny       hot     high        TRUE    no3   overcast    hot     high        FALSE   yes4   rainy       mild    high        FALSE   yes5   rainy       cool    normal      FALSE   yes6   rainy       cool    normal      TRUE    no7   overcast    cool    normal      TRUE    yes8   sunny       mild    high        FALSE   no9   sunny       cool    normal      FALSE   yes10  rainy       mild    normal      FALSE   yes11  sunny       mild    normal      TRUE    yes12  overcast    mild    high        TRUE    yes13  overcast    hot     normal      FALSE   yes14  rainy       mild    high        TRUE    noHere are the decisions with their respective classifications: 1: (outlook,overcast) -> (play,yes) [Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]2: (humidity,normal), (windy,FALSE) -> (play,yes)[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]3: (outlook,sunny), (humidity,high) -> (play,no) [Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]4: (outlook,rainy), (windy,FALSE) -> (play,yes) [Support=0.21 , Confidence=1.00 , Correctly Classify= 4]5: (outlook,sunny), (humidity,normal) -> (play,yes) [Support=0.14 , Confidence=1.00 , Correctly Classify= 11]6: (outlook,rainy), (windy,TRUE) -> (play,no) [Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]Thanks. ;[education, open-source];74;
1190;1;2014-10-01T19:51:21.220;Type of regression with nominal, ordinal, interval and ratio data;Statement of problem: An ambulance is at the hospital dropping off a patient. The goal of the paramedic is to get released from the hospital as soon as possible. I am curious, what are the factors in how long an ambulance off loads a patient at the hospital? Can I predict how long an offload will take given certain variables. And how confident can I be in this model? The Dependent Variable is HospitalTime, it is a ratio type of data and is measured in seconds. The Independent Variables are:Hospital, a nominal type of data recoded into integers, 1 would standfor Lee Memorial.Ambulance, a nominal type of data recoded into integers, 9 wouldstand for ambulance #9PatientPriority is an ordinal type of data recoded into integers. A 1is a high priority, 2 is a medium priority and 3 is low acuity.MonthOfCall is an interval type of data recoded into integers. A 6would be June and 12 is December. A 12 (December) is not twice asmuch as a 6 (June) in this case.HourOfCall is an interval type of data recoded into integers. Onceagain, an offload happening at 10:00 pm is not more than somethinghappening at 10:00 am.Officer1 and Officer2 are nominal data and are integers representingan EMT and a paramedic.My question is this: Given this type of data and my goal to predict the off loading time at the hospital, what kind of regression model should I look into?I have looked at my statistics books from university days and they are all using ratio data. My data is mixed with nominal, ordinal, interval and ratio.I have as much data as you could ask for. I have at least 100,000 observations.Can you please push me in the right direction? What kind of model should I use with this type of data?Shown below are observations to give you a tiny peek at my data:IncidentID,HospitalTime,Hospital,Ambulance,PatientPriority,MonthOfCall,HourOfCall,Officer1,Officer2757620,1849,7,11,2,10,10,234,771,chr(10) 802611,2625,7,11,3,1,18,234,777,chr(10) 765597,1149,7,12,3,11,2,234,777,chr(10) 770926,1785,7,12,3,11,15,234,777,chr(10) 771689,3557,7,12,2,11,14,234,777,chr(10) 822758,1073,7,20,3,3,13,777,307,chr(10) 767249,2570,7,22,2,11,11,560,778,chr(10) 767326,1998,7,22,1,11,18,560,777,chr(10) 785903,1660,7,22,3,12,12,234,777,chr(10) 787644,2852,7,22,3,12,17,234,777,chr(10) 760294,1327,7,23,2,10,14,498,735,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) 759983,1342,5,11,2,10,8,474,777,chr(10)791243,1635,5,11,2,1,18,234,777,chr(10) 800796,1381,5,11,3,1,11,234,777,chr(10)P.S. This question is cross-posted in Stack-Overflow under the same title and author.;[education, open-source];139;
1191;1;2014-10-02T03:30:37.930;Using SVD for clustering;The dataset that I am experimenting with is in the form of a table with columns userid and itemid. If there is a row for a given user and a given item, that means the user accessed the item (like in an online store). I am trying to cluster similar items based on this data. If a pair of items is accessed together often, then the items are similar.Because this is a case of a high dimensionality (# of users and items will be in 10,000's) I think I am justified in trying to use SVD as a pre-clustering step and then do some classical clustering. When I tried doing this I got poor clustering results when compared with simple hierarchical clustering. Items that weren't very similar were being bucketed together in one dimension, while there were available dimensions that weren't used. The results weren't completely random, but they were definitely worse than the output from the hierarchical clustering. I attempted the SVD step with Mahaut and Octave and the results were similar. For the hierarchical clustering I used the Jaccard measure.At this point I am starting to doubt the notion of SVD as a way to reduce dimensionality. Do you think that SVD cannot be used effectively in this case (and why?) or do you think that I made some mistake along the way?;[education, open-source];132;
1192;1;2014-10-02T06:18:11.397;Amalgamating multiple datasets with different variables coding;I have several datasets with thousands of variables. This different datasets have different variables for the same thing. Is there a way to automatically/semi-automatically check compatible variables and make them consistent?If there is such thing, that would save me months of tedious work. The data is stored in SPSS format.;[education, open-source];15;
1193;2;2014-10-02T08:17:03.253;;The only reliable way to see how long it takes is to code it up and give it a shot.  Training will take more time, then you can save your model (pickle) to use later.;;;
1194;2;2014-10-02T08:58:18.340;;We are using Singular Value Decomposition in the much same manner as you, except rather than clustering similar items, we are using a reduced-rank matrix to power a recommendation engine based on a term-document matrix in Latent Semantic Indexing.From your brief description, your approach seems sound enough. However, I highly recommend reading Berry, Dumais & O'Brien's Using Linear Algebra for Intelligent Information Retrieval.Key to using SVD is selecting an acceptable rank-k approximation to the original sparse matrix. You should carry out some exploratory analysis to see how much variance can be explained, using the singular values in the diagonal matrix Sigma. This question was brought up in this question on CrossValidated.A lot of the papers I've read suggest anywhere a rank k from 200 to 300 singular values. In a proof-of-concept implementation, we had original sparse matrix of about 10000 rows (unique terms) to about 1000 columns (unique documents), and we were capturing just under 85% of the variance with only 300 singular values.However, that really hinges upon the nature of your data, so your mileage may vary.;;;
1195;1;2014-10-02T13:06:22.433;How to classify test objects?;"I'm coding a program that tests several classifiers over a database weather.arff, I found rules below, I want classify test objects.I do not understand how the classification, it is described:""In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects. ""How to classify test objects?No. outlook temperature humidity    windy   play1   sunny       hot     high        FALSE   no2   sunny       hot     high        TRUE    no3   overcast    hot     high        FALSE   yes4   rainy       mild    high        FALSE   yes5   rainy       cool    normal      FALSE   yes6   rainy       cool    normal      TRUE    no7   overcast    cool    normal      TRUE    yes8   sunny       mild    high        FALSE   no9   sunny       cool    normal      FALSE   yes10  rainy       mild    normal      FALSE   yes11  sunny       mild    normal      TRUE    yes12  overcast    mild    high        TRUE    yes13  overcast    hot     normal      FALSE   yes14  rainy       mild    high        TRUE    noRule found:1: (outlook,overcast) -> (play,yes) [Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]2: (humidity,normal), (windy,FALSE) -> (play,yes)[Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]3: (outlook,sunny), (humidity,high) -> (play,no) [Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]4: (outlook,rainy), (windy,FALSE) -> (play,yes) [Support=0.21 , Confidence=1.00 , Correctly Classify= 4]5: (outlook,sunny), (humidity,normal) -> (play,yes) [Support=0.14 , Confidence=1.00 , Correctly Classify= 11]6: (outlook,rainy), (windy,TRUE) -> (play,no) [Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]Thanks,Dung";[education, open-source];55;
1196;1;2014-10-02T17:32:42.003;Supervised Learning with Necessarily Missing Data;Many discussions of missing data in supervised (and unsupervised) learning deal with various methods of imputation, like mean values or EM. But in some cases the data will be missing as a necessary consequence of the data generation process.For instance, let's say I'm trying to predict students' grades, and one of the inputs I want to analyze is the average grades of the student's siblings. If a particular student is an only child, then that value will be missing, not because we failed to collect the data, but because logically there is no data to collect. This is distinct from cases where the student has siblings, but we can't find their grades. Other examples abound: say we're in college admissions and we want to include students' AP exam results, but not all students took AP exams. Or we're looking at social network data, but not all subjects have facebook and/or twitter accounts.These data are missing, but they're certainly not missing at random. And many algorithms, such as all supervised learning packages in scikit-learn, simply demand that there be no missing values at all in the data set.How have people dealt with this in the past, and what off-the-shelf solutions are there? For instance, I believe the gradient boosting algorithm in R uses trees with three possible branches: left, right, and missing. Any other alternatives out there?;[education, open-source];75;
1197;1;2014-10-02T23:15:27.050;How can I classify text considering word order, instead of just using a bag-of-words approach?;I'm sorry if this is a basic or repeated question. My searches are yielding little fruit. I've made a Naive Bayes classifier that uses the bag-of-words technique to classify spam posts on a messageboard. It works, but I think I could get much better results if my models considered the word orderings and phrases. (ex: 'girls' and 'live' may not trigger a high spam score, even though 'live girls' is most likely junk). How can I build a model that takes word ordering into account?I've considered storing n-grams (check-out-these, out-these-live, these-live-girls), but this seems to radically increase the size of the dictionary I keep score in and causes inconsistency as phrases with very similar wording but different order will slip through.I'm not tied to Bayesian classification, but I'd like something that someone without a strong background in statistics could grok and implement. ;[education, open-source];231;3
1198;2;2014-10-03T00:25:48.490;;As you mentioned, the API is the hard part, not the data.  Quandl seems to solve this problem by providing over 10 million publicly available data sets under one easy, RESTful API.  If programming isn't your strong suit, there is a free tool to make loading data into Excel very easy.  Additionally, if you do enjoy programming, there are several native libraries in R, Python, Java and more.;;;
1199;2;2014-10-03T11:22:55.273;;I solved a similar problem not so long ago. Let X be a numerical variable that has missing values. First, we assign the value 0 to those instances in which the value is missing. Then, we add a new categorical variable to our model, X_missing, which domain is {True,False}. We obtain a data model with mixed numerical/categorical variables. You can then apply gradient descent to train a regression model from these variables. ;;;
1200;2;2014-10-03T11:39:11.240;;"I think I'd try to model the rate at which the ambulances are released instead of the time it takes. That would let me use Poisson regression, which is the canonical type of GLM for rates (in R's GLM, set family = ""poisson"".) However, in order to use PR, the data needs to have its variance equal to its mean, or at least close to it. ";;;
1202;2;2014-10-03T16:07:30.327;;Try out some generative models like HMM. Just check the following link:http://stats.stackexchange.com/questions/91290/how-do-i-train-hmms-for-classification;;;
1203;2;2014-10-03T18:05:13.163;;"There is a very simple hack to incorporate word order in an existing bag-of-words model implementation. Treat some of the phrases, such as the frequently occurring bi-grams (e.g. New York) as a unit, i.e. a single word instead of treating them as separate entities. This will ensure that ""New York"" is different from ""York New"". You could also define higher order word shingles such as for n=3,4 etc.You could use the Lucene ShingleFilter to decompose your document text into shingles as a pre-processing step and then apply the classifier on this decomposed text.import java.io.*;import org.apache.lucene.analysis.core.*;import org.apache.lucene.analysis.*;import org.apache.lucene.analysis.shingle.ShingleFilter;import org.apache.lucene.analysis.standard.*;import org.apache.lucene.util.*;import org.apache.lucene.analysis.util.*;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.charfilter.*;import org.apache.lucene.analysis.core.WhitespaceTokenizer;class TestAnalyzer extends Analyzer {    TestAnalyzer() {        super();    }    protected TokenStreamComponents createComponents( String fieldName, Reader reader ) {        String token;        TokenStream result = null;        Tokenizer source = new WhitespaceTokenizer( Version.LUCENE_CURRENT, reader );        result = new ShingleFilter(source, 2, 2);        return new TokenStreamComponents( source, result );    }}public class LuceneTest {    public static void main(String[] args) throws Exception {        TestAnalyzer analyzer = new TestAnalyzer();        try {            TokenStream stream = analyzer.tokenStream(""field"", new StringReader(""This is a sample sentence.""));            CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);            stream.reset();            // print all tokens until stream is exhausted            while (stream.incrementToken()) {                System.out.println(termAtt.toString());            }            stream.end();            stream.close();         }         catch (Exception ex) {             ex.printStackTrace();         }    }}";;;
1204;1;2014-10-03T20:12:24.427;NLP lab, linux or windows and which programming languages?;I would like to do some data mining and NLP experiments to do some researchI have decided to use NLTK or related tools and softwareWhich environment or operating system do you suggest for my purpose? I mean doing research on NLPWindows or Linux? I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to LinuxWhat is your experience and your preferred OS?As NLTK is in Python I thought Python is a good language for my purpose, do you suggest Python too?;[education, open-source];266;2
1205;1;2014-10-04T07:50:11.633;Recommended Language/Framework for Building a New Recommendation Engine;Next week I'm going to begin prototyping a recommendation engine for work. I've implemented/completed the Netflix Challenge in Java before (for college) but have no real idea what to use for a production/enterprise level recommendation engine. Taking into consideration everything from a standalone programming language to things like Apache Mahout and Neo4j, does anyone have any advice on how to proceed?;[education, open-source];176;1
1206;2;2014-10-04T15:52:44.370;;"Suppose your test object is (sunny, hot, normal, TRUE). Look through the rules top to bottom and see if any of the conditions are matched. The first rule for example tests the outlook feature. The value doesn't match, so the rule isn't matched. Move on to the next rule. And so on. In this case, rule 5 matches the test case and the classification for the p lay variable is ""yes"".More generally, for any test case, look at the values its features take and find the first rule that those values satisfy. The implication of that rule will be its classification.";;;
1207;2;2014-10-04T19:37:51.523;;"There are a bunch of techniques. You have already mentioned n-gram, then there is word combination and others. But the main problem (at least from your point of view) is that as the feature becomes more complex (like n-gram) the feature count increases dramatically. This is manageable. Basically before classification you must score your features and then threshold at a certain score. this way the features (or in your case n-grams) that are scored below a certain level are omitted and the feature count becomes manageable.as for the scoring. There are numerous ways (which to select is dependent on your application) to score the features. You can begin with ""BiNormal separation"", ""chi square"", ""Information Gain"" and etc.I don't know if this answer helps you but if you are interested i can elaborate...I forgot, in word combination you put a window of size m on the text and extract each combination of n words. of course n";;;
1208;1;2014-10-04T19:49:53.543;What circumstances causes two different classifiers to classify data exactly like one another;"Okay, here is the background:I am doing text mining, and my basic flow is like this:extract feature (n-gram), reduce feature count, score (tf-idf) and classify. for my own sake i am doing comparison between SVM and neural network classifiers. here is the weird part (or am i wrong and this is reasonable?), if i use 2gram the classifiers' result (accuracy/precision) is different and the SVM is the better one; but when i use 3-gram the results are exactly the same. what causes this? is there any explanation? is it the case of very separable classes?";[education, open-source];38;
1209;2;2014-10-04T19:55:31.480;;"coursera should be a good start. i have seen machine learning on their course list if im not mistaken. once you get the hang of it, a machine learning classic text like ""machine learning and pattern recognition"" by bishop or other texts can familiarize you with different classes of algorithms.if you want to go in depth, mathematics is a must, but if you just want to get familiar and use the algorithms there's absolutely no need for it";;;
1210;2;2014-10-04T20:09:17.547;;i have seen many papers that are basically what you just described. there is nothing wrong with what you are doing, but there are severe limitations in how this can predict the market.let me give you an example: suppose that u have some data and you begin to predict. with each set of data, you predict the next datapoint. and then you feed this datapoint back to the system as input and do this on and on .... in most of the times the system would just continue the last trend and the timeseries won't break. this is not prediction, this is line continuation... only when the system sees the break in real data, will the prediction break, and this is the lag that you are talking about (if i understand your question right).The first thing that you can do to enhance this is to extract some market indicators from the price. this would really help;;;
1211;2;2014-10-04T20:25:30.527;;AAAAh. this was one of my course assignments. this is a simple dataset. almost all simple classifiers work well with this. try to reduce the features with PCA and then use a classifier like K-Nearest Neighbors or Neural Networks. you can skip the feature reduction part but that would probably hurt a little;;;
1212;2;2014-10-05T05:01:03.437;;"If you merely want to scale up a simple collaborative filter (low rank matrix factorization), I'd suggest looking at graphlab. Another graph-based (or should I say Giraph?) solution is Okapi. Spark's MLLib is another option (details), and it also supports implicit feedback out of the box. Mahout's behind the curve today; I wouldn't bother with it until it is migrated to Spark.If you want to do something that the libraries don't do, say with regularization, you'll have to roll your own solution in your general purpose programming language of choice. It's not hard to get a prototype running, but you might run into scale problems in production; that's why I recommended solutions that scale easily.There are also black box recommender systems from commercial vendors but I have no experience with those.";;;
1213;1;2014-10-05T05:12:13.633;machine learning algorithms for 2d data?;I'm looking for a supervised learning algorithm that can take 2d data for input and output. As an example of something similar to my data, consider a black image with some sparse white dots. Blur that image using a full range of grayscale. Then create a machine that can take the blurred image as input and produce the original sharp image as output. I could make some sample 1D data by taking a region/radius around the original sharp point, but I don't know the exact radius. It would be significant data duplication and a lot of guessing.Any good algorithm suggestions for this problem? Thanks for your time.;[education, open-source];129;1
1214;1;2014-10-05T06:25:27.303;Book keeping of experiment runs and results;I am a hands on researcher and I like testing out viable solutions, so I tend to run a lot of experiments. For example, if I am calculating a similarity score between documents, I might want to try out many measures. In fact, for each measure I might need to make several runs to test the effect of some parameters. So far, I've been tracking the runs inputs and their results by writing out the results into  files with as much info about the inputs. The problem is that retrieving a specific result becomes a challenge sometimes, even if I try to add the input info to th filename. I tried using a spreadsheet with links to results but this isn't making a huge difference.What tools/process do you use for the book keeping of your experiments?;[education, open-source];47;2
1215;2;2014-10-05T12:25:15.910;;I vastly prefer to use windows.Despite that preference, i would recommend that you build your lab in Linux. Installation of these types of tools is often much easier under Linux. The anaconda distribution (http://docs.continuum.io/anaconda/pkg-docs.html) contains nltk, and is available under Linux (http://docs.continuum.io/anaconda/install.html).As one example of an issue we've run into with windows, installing theano with cuda drivers is very difficult under windows, and quite simple under Linux.;;;
1216;1;2014-10-06T12:00:55.303;Career switch to Big Data Analytics;I am a 35 year old IT professional who is purely technical. I am good at programming, learning new technologies, understanding them and implementing. I did not like mathematics at school, so I didn't score well in mathematics. I am very much interested in pursuing a career in Big Data analytics. I am more interested in Analytics rather than Big Data technologies (Hadoop etc.), though I do not dislike it.  However, when I look around in the internet, I see that, people who are good in analytics (Data Scientists) are mainly Mathematics graduates  who have done their PHds and sound like intelligent creatures, who are far far ahead of  me. I get scared sometimes to think whether my decision is correct, because learning advance statistics on your own is very tough and requires a of hard work and time investment. I would like to know whether my decision is correct, or should I leave this piece of work to only intellectuals who have spend their life in studying in prestigious colleges and earned their degrees and PHDs.;[education, open-source];2221;3
1220;2;2014-10-06T19:38:00.207;;"Due to high demand, it is possible to start a career in data science without a formal degree. My experience is that having a degree is often a 'requirement' in job descriptions, but if the employer is desperate enough, then that won't matter. In general, it's harder to get into large corporations with formalized job application processes than smaller companies without them. ""Knowing people"" can get you a long way, in either case.Regardless of your education, no matter how high demand is, you must have the skills to do the job.You are correct in noting that advanced statistics and other mathematics are very hard to learn independently. It is a matter of how badly you want to make the career change. While some people do have 'natural talent' in mathematics, everybody does have to do the work to learn. Some may learn more quickly, but everybody has to take the time to learn. What it comes down to is your ability to show potential employers that you have a genuine interest in the field, and that you will be able to learn quickly on the job. The more knowledge you have, the more projects you can share in a portfolio, and the more work experience under your belt, the higher level jobs that will be available to you. You may have to start  in an entry level position first.I could suggest ways to study mathematics independently, but that isn't part of your question. For now, just know that it's hard, but possible if you are determined to make a career change. Strike while the iron is hot (while demand is high).";;;
1221;2;2014-10-06T19:54:57.197;;Python is easy to use and manage in Linux. The Python package manager PIP and the Python environment manager Virtualenv are both fully supported in Linux. See: http://docs.python-guide.org/en/latest/starting/install/linux/The PyCharm IDE from JetBrains is also available for free in Linux (community edition), and it runs well. This is a great tool for a Python developer.In addition, if you need to integrate other tools, they will most likely be best supported in Linux. Need a MapReduce in your app? Hadoop will run in Linux. Need to store data in MySQL? Linux. Most data science tools are open source, and most open source technologies are supported best in Linux.There is also no cost for running Linux.If you need to keep Windows for your primary work computer (like your laptop), then you can install Linux on a server and use Putty to SSH into the Linux machine. You would then perform your work on the Linux machine, though you would still be running Windows locally.;;;
1222;2;2014-10-06T23:38:17.523;;"You should look more into the infrastructure side of things if you don't like maths. The lower you go in the software stack, the further away you get from maths (of the data science sort). In other words, you could build the foundation that others will use to create the tools that will serve analysts. Think of companies like Cloudera, MapR, Databricks, etc. Skills that will come in handy are distributed systems and database design. You are not going to be become a data scientist without maths; that's a ridiculous notion!";;;
1223;1;2014-10-07T17:27:22.063;Scikit Learn Logistic Regression Memory Leak;"I'm curious if anyone else has run into this. I have a data set with about 350k samples, each with 4k sparse features. The sparse fill rate is about 0.5%. The data is stored in a scipy.sparse.csr.csr_matrix object, with dtype='numpy.float64'.I'm using this as an input to sklearn's Logistic Regression classifier. The documentation indicates that sparse CSR matrices are acceptable inputs to this classifier. However, when I train the classifier, I get extremely bad memory performance; the memory usage of my process explodes from ~150 MB to fill all the available memory and then everything grinds to a halt as memory swapping to disk takes over.Does anyone know why this classifier might expand the sparse matrix to a dense matrix? I'm using the default parameters for the classifier at the moment, within an updated anacoda distribution. Thanks!scipy.__version__ = '0.14.0'sklearn.__version__ = '0.15.2'";[education, open-source];319;1
1224;2;2014-10-07T18:09:19.433;;Ok, this ended up being an RTFM situation, although in this case it was RTF error message.While running this, I kept getting the following error:DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().I assumed that, since this had to do with the target vector, and since it was a warning only, that it would just silently change my target vector to 1-D.However, when I explicitly converted my target vector to 1-D, my memory problems went away. Apparently having the target vector in an incorrect form caused it to convert my input vectors into dense vectors from sparse vectors.Lesson learned: follow the recommendations when sklearn 'suggests' you do something.;;;
1225;1;2014-10-08T10:42:41.833;data processing, correlation calculation;I have product purchase count data which looks likes this:user item1 item2   a     2     4   b     1     3   c     5     6   ...   ...   ...These data are imported into python using numpy.genfromtxt. Now I want to process it to get the correlation between item1 purchase amount and item2 purchase amount -- basically for each value x of item1 I want to find all the users who bought item1 in x quantity then average the item2 over the same users. What is the best way to do this? I can do this by using for loops but I thought there might be something more efficient than that. Thanks!;[education, open-source];78;1
1226;2;2014-10-08T13:37:27.357;;If you want to do NLP stay away from Windows! ( No Offense) Python NLTK stands out in terms of resource and community support for NLP. Moreover in Linux you can always leverage the benefit of  awk,sed, grep which are pretty useful tools when it comes to pre-processing. And no worries of funny characters getting introduced when you use a file.;;;
1227;1;2014-10-08T16:17:11.663;Union grouping in bipartite graphs?;I'm trying to figure out a good (and fast) solution to the following problem:I have two models I'm working with, let's call them players and teams. A player can be on multiple teams and a team can have multiple players). I'm working on creating a UI element on a form that allows a user to select multiple teams (checkboxes). As the user is selecting (or deselecting) teams, I'd like to display the teams grouped by the players.So for examples:If the selected teams have no players that intersect, each team would have its own section. If the user selects two teams and they have the same players, there would be one section containing the names of the two teams and all the players.If TEAM_A has players [1, 2, 4, 5] and TEAM_B has players [1, 3, 5, 6]. There would be the following sections: SECTION_X = [TEAM_A, TEAM_B, 1, 5], SECTION_Y = [TEAM_A, 2, 3], SECTION _Z = [TEAM_B, 3, 5]I hope that's clear. Essentially, I want to find the teams that players have in common and group by that. I was thinking maybe there is a way to do this by navigating a bipartite graph? Not exactly sure how though and I might be overthinking it. I was hoping to do this by creating some type of data structure on the server and using it on the client. I would love to hear your suggestions and I appreciate any help you can give!;[education, open-source];50;
1228;2;2014-10-08T22:12:31.823;;Use one of Pandas' built in functions:http://pandas.pydata.org/pandas-docs/stable/computation.html#correlation;;;
1229;1;2014-10-09T01:41:11.797;Cosine Similarity for Ratings Recommendations? Why use it?;Lets say I have a database of users who rate different products on a scale of 1-5. Our recommendation engine recommends products to users based on the preferences of other users who are highly similar. My first approach to finding similar users was to use Cosine Similarity, and just treat user ratings as vector components. The main problem with this approach is that it just measures vector angles and doesn't take rating scale or magnitude into consideration.My question is this:Are there any drawbacks to just using the percentage difference between the vector components of two vectors as a measure of similarity? What disadvantages, if any, would I encounter if I used that method, instead of Cosine Similarity or Euclidean Distance?For Example, why not just do this:n = 5 starsa = (1,4,4)b = (2,3,4)similarity(a,b) = 1 - ( (|1-2|/5) + (|4-3|/5) + (|4-4|/5) ) / 3 = .86667Instead of Cosine Similarity :a = (1,4,4)b = (2,3,4)CosSimilarity(a,b) = (1*2)+(4*3)+(4*4) / sqrt( (1^2)+(4^2)+(4^2) ) * sqrt( (2^2)+(3^2)+(4^2) ) = .9697;[education, open-source];454;
1230;2;2014-10-09T01:57:41.010;;For ratings, I think you would need to use Spearman's rank correlation for your similarity metric.Cosine similarity is often used when comparing documents, and perhaps would not be a good fit for rank variables. Euclidean distance is fine for lower dimensions, but comparison of rank variables normally call for Spearman.Here's a question on CrossValidated regarding Spearman (vs Pearson), which might shed more light for you.;;;
1231;2;2014-10-09T02:24:07.640;;Rating bias and scale can easily be accounted for by standardization. The point of using  Euclidean similarity metrics in vector space co-embeddings is that it reduces the recommendation problem to one of finding the nearest neighbors, which can be done efficiently both exactly and approximately. What you don't want to do in real-life settings is to have to compare every item/user pair and sort them according to some expensive metric. That just doesn't scale.One trick is to use an approximation to cull the herd to a managable size of tentative recommendations, then to run your expensive ranking on top of that.edit: Microsoft Research is presenting a paper that covers this very topic at RecSys right now: Speeding Up the Xbox Recommender System Using aEuclidean Transformation for Inner-Product Spaces;;;
1232;2;2014-10-09T02:35:24.533;;nDCG is a ranking metric and RMSE is not. In the context of recommender systems, you would use a ranking metric when your ratings are implicit (e.g., item skipped vs. item consumed) rather than explicit (the user provides an actual number, a la Netflix).;;;
1233;2;2014-10-09T10:22:47.857;;Not online learning, but take a look at this post, this may help you to get a starthttp://francescopochetti.com/stock-market-prediction-part-introduction/;;;
1234;2;2014-10-09T10:50:39.197;;I am developing a recommendation engine for stack overflow (personal project). Check it on http://recommender.im . It is still a working in progress, but I have a quite functional website working. I am putting there most of the code I used through python notebooks.Basically I used:- Frontend: angularJS- Website backend: flask + scikit-learn- machine learning and data preparation: python, pandas, scikit-learnI really like python for data science as the community and libraries are really good.;;;
1235;2;2014-10-09T10:58:02.557;;Pandas is the best thing since sliced bread (for data science, at least). an example:import pdIn [22]: df = pd.read_csv('yourexample.csv')In [23]: dfOut[23]:   user   item1   item20     a        2      41     b        1      32     c        5      6In [24]: df.columnsOut[24]: Index([u'user ', u'item1 ', u'item2'], dtype='object')In [25]: df.corr()Out[25]:          item1      item2item1   1.000000  0.995871item2   0.995871  1.000000In [26]: df.cov()Out[26]:          item1      item2item1   4.333333  3.166667item2   3.166667  2.333333Bingo!;;;
1236;1;2014-10-09T12:26:01.643;how to get the Polysemes of a word in wordnet or any other api?;how to get the Polysemes of a word in wordnet or any other api. I am looking for any api. with java any idea is appreciated?;[education, open-source];26;
1240;1;2014-10-10T00:59:46.703;Methods for standardizing / normalizing different rank scales;I know there is the normal subtract the mean and divide by the standard deviation for standardizing your data, but I'm interested to know if there are more appropriate methods for this kind of discrete data. Consider the following case.I have 5 items that have been ranked by customers. First 2 items were ranked on a 1-10 scale. Others are 1-100 and 1-5. To transform everything to a 1 to 10 scale, is there another method better suited for this case?If the data has a central tendency, then the standard would work fine, but what about when you have more of a halo effect, or some more exponential distribution?;[education, open-source];225;1
1241;1;2014-10-10T01:12:19.163;Collaborative Social Network Visualization;I do movement building work for Effective Altruism (http://en.m.wikipedia.org/wiki/Effective_altruism), and would like to level up our growth strategy. It occurred to me that a social network visualization tool which allowed us to strategically find and recruit new influencers/donors would be mega useful. I'd love to find something (preferably free), similar to InMaps, which would allow us to:Combine all of our social media connections into a single mapEasily see who the superconnectors areWeight each person by their degree of social influence (perhaps some function of things like Klout score * amount of social media connections * number of Google mentions, etc) Does such a thing exist? If not, is anyone interested in pro bono work for an amazing cause? =)Disclaimer: I am a data science noob, so preferably the solution would be one with a nice GUI and minimal involvement of R or Python.;[education, open-source];134;
1242;2;2014-10-10T03:33:45.353;;I think Gephi, an open-source visualization tool, would help you a lot. Actually, as I know, the InMaps and its community detection algorithm are same as the Gephi's.;;;
1243;1;2014-10-10T03:45:09.343;Can we quantify how position within search results is related to click-through probability?;Suppose, for example, that the first search result on a page of Google search results is swapped with the second result. How much would this change the click-through probabilities of the two results? How much would its click-through probability drop if the fifth search result was swapped with the sixth? Can we say something, with some level of assurance, about how expected click-through probabilities change if we do these types of pairwise swaps within pages of search results?What we seek is a measure of the contribution to click-through rates made specifically by position bias.Likely, how position ranking would affect the sales in Amazon or other online shopping website? If we cast the sales into two parts, the product quality and its ranking effect.sales = alpha*quality + beta*position + epsilonHow can we quantify the beta?;[education, open-source];49;2
1244;1;2014-10-10T03:48:54.660;Hashing Trick - what actually happens;When ML algorithms, e.g. Vowpal Wabbit or some of the factorization machines winning click through rate competitions (Kaggle), mention that features are 'hashed', what does that actually mean for the model? Lets say there is a variable that represents the ID of an internet add, which takes on values such as '236BG231'. Then I understand that this feature is hashed to a random integer. But, my question is:Is the integer now used in the model, as an integer (numeric) ORis the hashed value actually still treated like a categorical variable and one-hot-encoded? Thus the hashing trick is just to save space somehow with large data?;[education, open-source];1259;4
1246;1;2014-10-10T13:34:11.543;Stochastic gradient descent based on vector operations?;"let's assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or ""epoch"", I use each training sample exactly once after randomly reordering the whole training set.My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.Does anybody know any vectorized implementation of stochastic gradient descent? EDIT: I've been asked why would I like to use online gradient descent if the size of my dataset is fixed. From [1], one can see that online gradient descent converges slower than batch gradient descent to the minimum of the empirical cost. However, it converges faster to the minimum of the expected cost, which measures generalization performance. I'd like to test the impact of these theoretical results in my particular problem, by means of cross validation. Without a vectorized implementation, my online gradient descent code is much slower than the batch gradient descent one. That remarkably increases the time it takes to the cross validation process to be completed.EDIT: I include here the pseudocode of my on-line gradient descent implementation, as requested by ffriend. I am solving a regression problem.Method: on-line gradient descent (regression)Input: X (nxp matrix; each line contains a training sample, represented as a length-p vector), Y (length-n vector; output of the training samples)Output: A (length-p+1 vector of coefficients)Initialize coefficients (assign value 0 to all coefficients)Calculate outputs Fprev_error = inferror = sum((F-Y)^2)/nit = 0while abs(error - prev_error)>ERROR_THRESHOLD and it<=MAX_ITERATIONS:    Randomly shuffle training samples    for each training sample i:        Compute error for training sample i        Update coefficients based on the error above    prev_error = error    Calculate outputs F    error = sum((F-Y)^2)/n    it = it + 1[1] ""Large Scale Online Learning"", L. Bottou, Y. Le Cunn, NIPS 2003.";[education, open-source];1251;
1247;2;2014-10-10T14:35:11.323;;I am new to this forum. Chiming in late on this question. I have been maintaining (I am a co-founder of) a catalog of publicly available data portals. There is over 1000 now listed and cover portals at international, federal, state, municipal and academic levels across the globe. http://www.opengeocode.org/opendata/;;;
1248;2;2014-10-10T14:53:22.500;;I am new to this forum. Data cleansing of address data is an area I work in. I agree with the other posters that you should not modify the original data, but add fields for corrected values. I developed a technique in our systems (opengeocode.org) we call 'reduced to common form'. In this method, addresses and geographic names are analyzed for reduction into an unambiguous short form, which is then used for record matching (vs. the original values). For example, the method I use for matching US postal addresses is based on the US Post Office's published method for matching addresses.For geographic names, the method will reduce to short gazetteer form in Romanized script.The link below is an article I wrote a couple of years ago that explains how the street address reduction works:http://www.nwstartups.com/api/doc/middleware.php#streetR;;;
1249;2;2014-10-10T16:54:45.047;;There are several third-party Java APIs for WordNet listed here: http://wordnet.princeton.edu/wordnet/related-projects/#JavaIn the past, I've used JWNL the most: http://sourceforge.net/projects/jwordnet/The documentation for JWNL isn't great, but it should provide the functionality you need.;;;
1250;2;2014-10-10T18:10:14.507;;"I recently ran into a similar problem: How to manage extracting a variety of features from a large dataset, without knowing up front what all of them would be. (Even calculating mean values repeatedly would be computationally expensive.) Further, how would I manage predictions based on different feature sets? Meaning, if I added a new feature, how would I know which models to train on new features? It could quickly snowball into a huge mess.My current solution is to track it all in a local NoSQL database (MongoDB). For example, I might have a collection features, each entry of which has a name, a description of how the feature was was calculated, the python file that ran the extraction, etc.Likewise, a collection models includes models run on the data. Each entry might have a name, a list of features that were used to train the model, its eventual parameters, predicted values on a held-out test set, metrics for how the model performed, etc. From my vantage point, this has a number of benefits:By saving predictions, I can use them later in ensemble predictions.Because I keep track of which features were used, I know which ones need retraining as I extract more features.By saving model descriptions, I ensure that I always know what I've tried. I never have to wonder, ""Have I tried LASSO with regularization parameters set by grid-search CV?"" I can always look it up, and see how successful it was.From your question, it sounds like you could adapt this approach to your problem's workflow. Install Mongo or another database of choice, and then save each experimental run, its inputs, its results, and anything else you might wish to track over the course of the project. This should be much easier to query than a spreadsheet, at the least.";;;
1253;1;2014-10-11T10:24:01.393;Why are NLP and Machine Learning communities interested in deep learning?;I hope you can help me, as I have some questions on this topic. I'm new in the field of deep learning, and while I did some tutorials, I can't relate or distinguish concepts from one another.;[education, open-source];483;7
1255;2;2014-10-11T19:48:20.583;;The a second bullet is the value in feature hashing.  Hashing and one hot encoding to sparse data saves space.  Depending on the hash algo you can have varying degrees of collisions which acts as a kind of dimensionality reduction.Also, in the specific case of Kaggle feature hashing and one hot encoding help with feature expansion/engineering by taking all possible tuples (usually just second order but sometimes third) of features that are then hashed with collisions that explicitly create interactions that are often predictive whereas the individual features are not.In most cases this technique combined with feature selection and elastic net regularization in LR acts very similar to a one hidden layer NN so it performs quite well in competitions.;;;
2255;1;2014-10-11T23:26:53.197;Single Layer Perceptron with three classes;I need some help with a single layered perceptron with multiple classes.What I need to do is classify a dataset with three different classes, by now I just learnt how to do it with two classes, so I have no really a good clue how to do it with three.The dataset have three different classes: Iris-setosa, Iris-versicolor and Iris-versicolor.The url with the dataset and the information is in : http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data.I really appreciate any help anyone can give to me.Thanks a lot!;[education, open-source];249;
2256;2;2014-10-12T02:53:51.650;;I don't think there is a way to build your graph from raw data without using at least basic programming skills. I'm not aware of a drag-and-drop interface for importing and displaying data. Graphs are just a bit too complex. Imagine trying to find the profit of selling a product if all you had was CSVs of receipts dropped into Excel. You'd need labels of the columns, some basic calculations, and so on before you had anything intelligible. Graphs are similar in this regard.Thankfully, there are open source solutions, with some elbow grease and a few days of work, you can probably get a nice visualization.Cypher queries are relatively simple to write. Using Neo4j and Cypher, you can create a basic visualization of your graph, which is displayed using D3.jsGraphAlchemist recently open-sourced their project Alchemy.js which specializes in graph visualization. https://github.com/GraphAlchemist/Alchemy;;;
2257;1;2014-10-12T05:27:17.687;How to do this complicated data extrapolation, prediction modeling?;I have some very complicated data about some movie sales online, first for each data entry, I have a key which is a combination of five keys, which are territory, day, etc, and then, for each key I have the sales for a period of time, and other information, like the movie's box office and genre. For each day, there is a delay for the data loading to the database, around ten hours, I try to fill the gap, do some data extrapolations.For each movie we sell, there is some decay of selling since the new release of the movie, i.e. usually for each movie, it follows some sales decay pattern.For a recent day, I pulled some data, and I found that some decay pattern:And for that day, the sales for each key can range from around $150000 to $0. The pic is as follow: In the picture, the 15000 means there are around 15000 keys for each day. found this article,http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&Pyo.pdfI am trying to predict for each key, the sales amount, like for a movie, territory, day etc combination, the sales amount, how much dollars, means for that movie, that territory, that day, how much money we get from selling online. I tried ARIMA time series model, but there is some concerns for that model, seen from the pics, there is some seasonal thing, and decay thing for the movie, so the sales prediction can not be always flat, there may be a pump after a going down, it may happens on a weekend, since there is seasonal thing, and the decay trend, etc, how to capture these things. Thank you for your reply!I am not sure whether can be applied, and how to be applied here.Thanks a lot in advance. ;[education, open-source];109;
2258;1;2014-10-12T11:23:26.493;Where to start on neural networks;First of all I know the question may be not suitable for the website but I'd really appreciate it if you just gave me some pointers.I'm a 16 years old programmer, I've had experience with many different programming languages, a while ago I started a course at Coursera, titled introduction to machine learning and since that moment i got very motivated to learn about AI, I started reading about neural networks and I made a working perceptron using Java and it was really fun but when i started to do something a little more challenging (building a digit recognition software), I found out that I have to learn a lot of math, I love math but the schools here don't teach us much, now I happen to know someone who is a math teacher do you think learning math (specifically calculus) is necessary for me to learn AI or should I wait until I learn those stuff at school?Also what other things would be helpful in the path of me learning AI and machine learning? do other techniques (like SVM) also require strong math?Sorry if my question is long, I'd really appreciate if you could share with me any experience you have had with learning AI.;[education, open-source];220;2
2259;2;2014-10-12T11:49:04.480;;Well , When it comes to AI I am an absolute beginner but here is my answer to your question based on my understandings :a perceptron has only one activation function, therefore it can return only the values of true and false (in most cases true=0 and false=1), so because of that, I don't think that you will be able to accomplish your goal using only one perceptron but you can absolutely do it using multiple perceptrons which essentially is a neural networks, of course training the network would be a lot harder than calculating the changes of weights as you do in perceptrons, You are gonna have to take advantage of a training algorithm such as backpropagation and a sigmoid activation function.I hope my answer was helpful.;;;
2260;2;2014-10-12T16:35:21.260;;The standard way to do this is called 'one versus all'... you train three perceptrons. First 1 target = is class a?, 2nd perceptron target = is class b? 3rd = is class c. You just train each perceptron separately, and then take max of the three perceptrons to decide class;;;
2261;2;2014-10-12T19:36:33.027;;"No, you should go ahead and learn the maths on your own. You will ""only"" need to learn calculus, statistics, and linear algebra (like the rest of machine learning). The theory of neural networks is pretty primitive at this point -- it more of an art than a science -- so I think you can understand it if you try. Ipso facto, there are a lot of tricks that you need practical experience to learn. There are lot of complicated extensions, but you can worry about them once you get that far.Once you can understand the Coursera classes on ML and neural networks (Hinton's), I suggest getting some practice. You might like this introduction.";;;
2262;2;2014-10-12T19:52:55.430;;you might want to look at http://deeplearning.net/software/jobman/intro.htmlit was designed for deep learning (I guess), but it is application agnostic. It is effectively an API version of SeanEasters approach;;;
2263;1;2014-10-13T11:23:23.483;"""Hadoop"" formats for user database: online advertising";I was wondering if someone could point me to suitable database formats for building up a user database:basically I am collecting logs of impressions data, and I want to compile a user database which sites user visits, country/gender/..? and other categorisations with the aim of a) doing searches: give me all users visiting games sites from france...b) machine learning: eg clustering users by the sites they visitso I am interested in storing info about 100's of millions of userswith indexes? on user, sites, geo-locationand the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)what are suitable database systems. Can someone suggest suitable reading material? I was imagining Hbase might be suitable...;[education, open-source];57;
2264;2;2014-10-13T13:35:09.443;;Perceptrons, strictly speaking, are binary classifiers.To make a multi-class classifier, you should switch to a standard feed-forward neural net with a softmax output layer. Without any hidden layers this is equivalent to multinomial logistic regression.You can also do the one-vs-all trick as @seanv507 suggests. This often works well in practice but there's no strong basis for it in theory, and the true multi-class version is easier conceptually and practically in this case.;;;
2265;2;2014-10-13T13:50:07.080;;"Check out the partial_fit method of scikit's SGD classifier. You have control over what you call with it: you can do it ""true"" online learning by passing an instance at a time, or you can batch up instances into mini-batches if all your data are available in an array. If they are, you can slice the array to provide the minibatches.";;;
2266;2;2014-10-13T13:53:36.027;;"It's just logistic regression. Get a bunch of data about presentations of search results, along with whether an item was clicked on. An instance is a search result item, with possible features being rank, ""quality"" (not sure what you mean by this) etc. The what you're asking about is a question of inference on the parameter related to rank.";;;
2267;2;2014-10-13T14:46:54.403;;you might want to look at this paperPredicting Clicks: Estimating the Click-Through Rate for New Ads Whenever an ad is displayed on the search results page, it has   some chance of being viewed by the user. The farther down the   page an ad is displayed, the less likely it is to be viewed. As a   simplification, we consider the probability that an ad is clicked on   to be dependent on two factors: a) the probability that it is viewed,   and b) the probability that it is clicked on, given that it is viewed:;;;
2268;1;2014-10-13T19:02:24.670;What is the state of the art in the field of NLP?;I am new to Natural Language Processing, I think NLP is a challenging field, the syntax and semantic ambiguities could cause a lot of problems. For example I think for these problems machine translation is a hard task.Therefore there are probably many approaches and methods that have been applied to this field. But what are the latest and most promising approaches and methods in the field of NLP?Are these techniques highly dependent on the target language?;[education, open-source];185;
2269;1;2014-10-13T21:13:48.447;Any Online R console?;I am looking for an online console for the language R. Like I write the code and the server should execute and provide me with the output.Similar to the website Datacamp.;[education, open-source];1350;4
2270;2;2014-10-13T22:07:51.180;;Yes. I believe this is what you are looking for.http://www.compileonline.com/execute_r_online.php;;;
2271;2;2014-10-14T01:19:37.067;;You are probably aware that deep learning is all the rage these days, and it has touched NLP too. There is a tutorial on it from a recent conference: Deep Learning for Natural Language Processing (without Magic) by Richard Socher and Christopher Manning, who are from Stanford.;;;
2272;2;2014-10-14T07:02:47.500;;"R-fiddle provides an easy-to-use interactive R console that you can run from your browser. R On Cloud provides also a browser-embedded R-consoleDataJoy provides easy to use, online data processing with Python and R (sign-up is required; the free basic plan has the following restrictions: low CPU priority, 256MB memory, 1 minute script timeout). Jupyter.org evolved from the IPython Project (the language-agnostic parts of IPython); supports Python 3, Julia, R, Haskell, Ruby, etc.  ";;;
2273;1;2014-10-14T08:50:53.907;What is the best Data Mining algorithm for prediction based on a single variable?;"I have a variable whose value I would like to predict, and I would like to use only one variable as predictor. For instance, predict traffic density based on weather.Initially, I thought about using Self-Organizing Maps (SOM), which performs unsupervised clustering + regression. However, since it has an important component of dimensionality reduction, I see it as more appropriated for a large number of variables.Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this simple case: I used ""Data Mining"" instead of ""machine learning"" in the title of my question, because I think maybe a linear regression could do the job...";[education, open-source];168;1
2274;2;2014-10-14T13:10:39.773;;"Common rule in machine learning is to try simple things first. For predicting continuous variables there's nothing more basic than simple linear regression. ""Simple"" in the name means that there's only one predictor variable used (+ intercept, of course): y = b0 + x*b1where b0 is an intercept and b1 is a slope. For example, you may want to predict lemonade consumption in a park based on temperature:cons = b0 + temp * b1Temperature is in well-defined continuous variable. But if we talk about something more abstract like ""weather"", then it's harder to understand how we measure and encode it. It's ok if we say that the weather takes values {terrible, bad, normal, good, excellent} and assign values numbers from -2 to +2 (implying that ""excellent"" weather is twice as good as ""good""). But what if the weather is given by words {shiny, rainy, cool, ...}? We can't give an order to these variables. We call such variables categorical. Since there's no natural order between different categories, we can't encode them as a single numerical variable (and linear regression expects numbers only), but we can use so-called dummy encoding:  instead of a single variable weather we use 3 variables - [weather_shiny, weather_rainy, weather_cool], only one of which can take value 1, and others should take value 0. In fact, we will have to drop one variable because of collinearity. So model for predicting traffic from weather may look like this: traffic = b0 + weather_shiny * b1 + weather_rainy * b2  # weather_cool droppedwhere either b1 or b2 is 1, or both are 0. Note that you can also encounter non-linear dependency between predictor and predicted variables (you can easily check it by plotting (x,y) pairs). Simplest way to deal with it without refusing linear model is to use polynomial features - simply add polynomials of your feature as new features. E.g. for temperature example (for dummy variables it doesn't make sense, cause 1^n and 0^n are still 1 and 0 for any n):traffic = b0 + temp * b1 + temp^2 * b2 [+ temp^3 * b3 + ...]";;;
2275;1;2014-10-14T15:26:01.460;what are the parameter estimation methods for the two methods in this paper;"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635in the above paper, page 4, two models, non-adaptive regression model, adaptive regression model, the non-adaptive regression model's parameter estimation method is ""least squares"", what is the parameter estimation for the adaptive regression model? is there any package in R to do parameter estimation for this kind of adaptive regression model? If I add more predictors in the adaptive regression model, can R still solve it? and how?Thank you!";[education, open-source];17;
2276;2;2014-10-14T17:47:02.663;;The kind of data you store and analyze is very much dependent upon the kind of data you can gather. So, without knowing what your 'impression data' looks like, it is very hard to suggest how to normalize and store it.Furthermore, the way you store data is also dependent upon how you wish to analyze it. For example, if you want to perform basic analytics like page view counts, how many pages a user visits per session, etc (SQL). . . data needs to be stored differently than if you want to build recommendations based on traffic patterns (graph database).Please edit your question to include more detail. Apologies that I cannot simply leave a comment.;;;
2277;2;2014-10-14T18:24:36.880;;I am more of an expert on data ETL and combining/aggregating than on the forumulas themselves. I work frequently with weather data. I like to give some suggestions on using weather data in analysis.Two types of data are reported in US/Canada:A. MeasurementsB. Weather TypeAs far as weather type (sunny, rainy, severe thunderstorm) they are either going to already be reflected in measurements (e.g., sunny, rainy) and are redundant or they are inclement weather conditions and are not necessarily reflected in the measurements. For inclement weather types, I would have separate formulae.For measurements, there are 7 standard daily measurements for Weather Station reporting in North America.Temp Min/MaxPrecipitationAverage Wind SpeedAverage Cloudiness (percentage)Total sunlight (minutes)SnowfallSnow DepthNot all stations report all 7 daily measurements. Some report only Temp and Precipitation. So you may want to have one formula for Temp/Precipitation and an expanded formulae when all seven measurements are available.The two links below are NOAA/NWS weather terms used in their datasets:This document is the vocabulary for the annual summaries:http://www1.ncdc.noaa.gov/pub/data/cdo/documentation/ANNUAL_documentation.pdfThis document is the vocabulary for the daily summaries http://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf;;;
2278;1;2014-10-15T02:22:09.983;Adaptive regression parameter estimation in R;How would I do parameter estimation and prediction for the adaptive regression model using R, as in the 4th page of the paper linked below?http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635Could anyone clarify this for me?If you know adaptive regression models very well, share some useful link, or describe the model/parameter estimation/prediction, that would be very helpful.Thank you so much!;[education, open-source];52;
2279;2;2014-10-15T03:08:59.263;;Here is some notes for an R package 'earth' for adaptive regression. This may be useful.http://cran.r-project.org/web/packages/earth/vignettes/earth-notes.pdf;;;
2280;2;2014-10-15T11:37:47.090;;In the case you mention, I recommend to keep the changes as a dictionary, for instance in a .csv file. Write a script that replaces the values in the original data based on the translation in your dictionary. That way, you separate the corrections from the script itself.;;;
2281;2;2014-10-15T20:37:40.633;;I would say... it really depends. You may need to: use machine learning algorithms: this will be useful for specific applications you may have. In this situation what you need is some programming skills and the taste for testing (practicing will make you strong). Here maths are not so much required I would say. be able to modify existing algorithms. Your specific application may be reticent to regular algorithms, so you may need to adapt them to get maximum efficiency. Here maths come into play. understand the theory behind algorithms. Here maths are necessary, and will help you increase your knowledge of the field of machine learning, develop your own algorithms, speak the same langage as your peers... NN theory may be primitive as said by @Emre, but for instance this is not the case for SVM (the theory behind SVM requires e.g. to understand reproducing kernel Hilbert spaces). On the mid term for sure you will need strong maths. But you don't need to wait for them to come to you, you can start right now with linear algebra, which is beautiful and useful for everything. And in case you encounter (possibly temporary) difficulties of any sort with maths, keep on practicing the way you already do (many people can talk about the perceptron but are not able to make a perceptron in Java), this is very valuable. ;;;
2284;1;2014-10-16T05:44:40.183;Is it possible to identify different queries/questions in sentence?;"I want to identifies different queries in sentences. Like - Who is Bill Gates and where he was born? or Who is Bill Gates, where he was born? contains two queries Who is Bill Gates?Where Bill Gates was bornI worked on Coreference resolution, so I can identify that he points to Bill Gates so resolved sentence is ""Who is Bill Gates, where Bill Gates was born""Like wiseMGandhi is good guys, Where he was born?single querywho is MGandhi and where was he born?2 querieswho is MGandhi, where he was born and died?3 quriesIndia won world cup against Australia, when?1 query (when India won WC against Auz)I can perform Coreference resolution (Identifying and converting he to Gandhi) but not getting how can I distinguish queries in it. How to do this? I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it. I tried to find ""Sentence disambiguation"" like ""word sense disambiguation"", but nothing exist like that.Any help or suggestion would be much appreciable. ";[education, open-source];105;1
2285;1;2014-10-16T06:37:09.887;what is buyer classification problem?;Recently in a data analytic job interview for an e-commerce site, they asked me, do i have some knowledge of buyer classification problem. Unfortunately i heard this term for the first time.After interview i tried to search a lot about it over google but didn't find something meaningful. Please any one let me know if you have heard this term before and paste some links explaining this concept. Thanks   ;[education, open-source];75;
2286;2;2014-10-16T08:03:15.593;;The basic thing, you can do in that situation, is to split your query into N simple sentences each of which should be processed in order to receive YES/NO answer considering if the sentence is a query. That way you will receive following results:Input: Gandhi is good guys, Where he was born?->Gandhi is good guys - not queryWhere he was born?  - query===1 queryInput: who is MGandhi and where was he born?->who is MGandhi     - querywhere was he born? - query===2 queriesThis approach will require anaphora resolution (in order to convert he into Gandhi in first example) and a parser to correctly divide complex sentence into simple ones.;;;
2287;1;2014-10-16T12:15:32.017;Regression Model for explained model(Details inside);I am kind of a newbie on machine learning and I would like to ask some questions based on a problem I have .Let's say I have x y z as variable and I have values of these variables as time progresses like :t0  = x0 y0 z0  t1  = x1 y1 z1  tn  = xn yn zn  Now I want a model that when it's given 3 values of x , y , z I want a prediction of them like:Input : x_test y_test z_test Output : x_prediction y_prediction z_predictionThese values are float numbers. What is the best model for this kind of problem? Thanks in advance for all the answers.More details:Ok so let me give some more details about the problems so as to be more specific.I have run certain benchmarks and taken values of performance counters from the cores of a system per interval.The performance counters are the x , y , z in the above example.They are dependent to each other.Simple example is x = IPC , y  =  Cache misses , z  = Energy at Core.So I got this dataset of all these performance counters per interval .What I want to do is create a model that after learning from the training dataset , it will be given a certain state of the core ( the performance counters) and predict the performance counters that the core will have in the next interval.;[education, open-source];227;
2288;2;2014-10-16T14:34:33.180;;"AFAIK if you want to predict the value of one variable, you need to have one or more variables as predictors; i.e.: you assume the behaviour of one variable can be explained by the behaviour of other variables.In your case you have three independent variables whose value you want to predict, and since you don't mention any other variables, I assume that each variable depends on the others. In that case you could fit three models (for instance, regression models), each of which would predict the value of one variable, based on the others. As an example, to predict x:x_prediction=int+cy*y_test+cz*z_test, where int is the intercept and cy, cz, the coefficients of the linear regression. Likewise, in order to predict y and z:y_prediction=int+cx*x_test+cx*z_testz_prediction=int+cx*x_test+cy*y_test";;;
2290;2;2014-10-16T21:12:09.490;;Buyer classification is used to categorize users who purchase groups of items. Buyers are categorized in order to be targeted for advertising. When users buy similar items, they are more likely to buy similar items in the future. This is useful information when pricing items on a website.A clear example, a website selling nutritional supplements may want to target different buyers by category. Men in their 20s are unlikely to purchase menopause supplements, and women in their 50s are also unlikely to buy creatine. So, by analyzing user purchase history and categorizing types of buyers, the site can send intelligent promotions - males in their 20s get ads for creatine while women in their 50s get ads for menopause supplements.Also, if I want to run a sale to attract customers, I don't want to offer lower prices on items which are often purchased together. I'd rathe price one low to attract the buyer, and then hope they buy the complimentary item at full price. Categorizing buyers also helps with this problem.You may want to read up on shopping cart analysis, which is not a new problem. Department stores have been analyzing shopping carts and classifying buyers long before online shopping was popularized. That's why you have to use membership cards to get the 'special' prices.Fine tuning these details can increase revenue substantially.;;;
2293;1;2014-10-17T10:47:13.197;Scalable Outlier/Anomaly Detection;I am trying to setup a Big Data Infra-Structure using Hadoop, Hive, Elastic Search (amongst others) and I would like to run some algorithms over these datasets. I would like the algorithms themselves, to be scalable, so that excludes using tools such as Weka, R, or even RHadoop.The Apache Mahout Library seems to be a good option, and it features algorithms for regression and clustering tasks. What I am struggling to find is a solution for anomaly or outlier detection.Since Mahout features Hidden Markov Models and a variety of clustering techniques (including K-Means) I was wondering if it would be possible to build a model to detect outliers in time-series, using any of this. I would be grateful if somebody experienced on this could advice me a) if it is possible, and in case it is b) how-to do it, plus c) an estimation of the effort involved and d) accuracy/problems of this approach.;[education, open-source];821;7
2294;2;2014-10-17T10:55:04.147;;"It strongly depends on the environment/company you are working with. In my eyes there is a ""big data"" hype at the moment and a lot of companies try to enter the field with hadoop based solutions - what makes hadoop also a buzzword but its not always the best solution. In my mind, a good Data Scientist should be able to ask the right questions and keep on asking again until its clear whats really needed. Than a good DataScientist - of course - needs to know how to address the problem (or at least know someone who can). Otherwise your stakeholder could be frustrated :-) So, i would say its not absolutely necessary to learn Hadoop.";;;
2296;2;2014-10-17T12:50:56.243;;I would take a look at t-digest algorithm. It's been merged into mahout and also a part of some other libraries (github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/quantile/TDigest.java) for big data streaming. You can get more about this algorithm particularly and big data anomaly detection in general in next resources:Practical machine learning anomaly detection book.(info.mapr.com/rs/mapr/images/Practical_Machine_Learning_Anomaly_Detection.pdf)Webinar: Anomaly Detection When You Don't Know What You Need to Find (youtube.com/watch?v=i-mSV63Q9rA#t=757)Anomaly Detection in Elasticsearch (info.prelert.com/anomaly-detection-in-elasticsearch)Beating Billion Dollar Fraud Using Anomaly Detection: A Signal Processing Approach using Argyle Data on the Hortonworks Data Platform with Accumulo (oreilly.com/pub/e/3211);;;
2297;1;2014-10-17T13:34:48.540;Masters thesis topics in Applied probability and Probabilistic models in Machine Learning;I'm looking for a topic for my masters thesis. Machine learning is my primary domain and I want to work on probabilistic models and applied probability in Machine Learning. Please suggest some exciting new topics that would make for a good masters thesis subject.Anything related to Markov chains Monte Carlo, Bayesian methods, Probabilistic graphical models, Markov models and so on in context of machine learning would be great!;[education, open-source];121;1
2298;1;2014-10-17T13:58:39.353;Differences in scoring from PMML model on different platforms;I've built a toy Random Forest model in R (using the German Credit dataset from the caret package), exported it in PMML 4.0 and deployed onto Hadoop, using the Cascading Pattern library.I've run into an issue where Cascading Pattern scores the same data differently (in a binary classification problem) than the same model in R. Out of 200 observations, 2 are scored differently.Why is this? Could it be due to a difference in the implementation of Random Forests? ;[education, open-source];72;1
2299;2;2014-10-18T03:22:19.060;;One of real challenges, I faced with R is different packages compatible with different versions.. quite a lot R packages are not available for latest version of R.. And R quite a few time gives error  due to library or package was written for older version.. ;;;
2302;1;2014-10-18T04:58:45.100;R Script to generate random dataset in 2d space;I want to analyze the effectiveness and efficiency of kernel methods for which I would require 3 different data-set in 2 dimensional space for each of the following cases:BAD_kmeans: The data set for which the kmeans clustering algorithmwill not perform well.BAD_pca: The data set for which the Principal Component Analysis(PCA) dimension reduction method upon projection of the originalpoints into 1-dimensional space (i.e., the first eigenvector) willnot perform well.BAD_svm: The data set for which the linear Support Vector Machine(SVM) supervised classification method using two classes of points(positive and negative)  will not perform well.Which packages can I use in R to generate the random 2d data-set for each of the above cases ? A sample script in R would help in understanding;[education, open-source];494;
2303;1;2014-10-18T08:08:25.513;Python and R good tutorials?;I would like to learn both Python and R for usage in data science projects. I am currently unemployed, fresh out of university, scouting around for jobs and thought it would be good if I get some Kaggle projects under my profile.However, I have very little knowledge in either language. Have used Matlab and C/C++ in the past. But I haven't produced production quality code or developed an application or software in either language. It has been dirty coding for academic usage all along.I have used a little bit of Python, in a university project, but I dont know the fundamentals like what is a package , etc etc. ie havent read the intricacies of the language using a standard Python Textbook etc..Have done some amount of coding in C/C++ way back (3-4 years back then switched over to Matlab/Octave).I would like to get started in Python Numpy Scipy scikit-learn and pandas etc. but just reading up Wikipedia articles or Python textbooks is going to be infeasible for me.And same goes with R, except that I have zero knowledge of R.Does anyone have any suggestions?;[education, open-source];804;3
2304;1;2014-10-18T08:19:00.607;Energy consumption time series forcasting;Is there a good java library for doing time series energy consumption forecasting based on weather data and other variables?;[education, open-source];230;2
2305;2;2014-10-18T17:32:52.583;;"I can only recommend Advanced R by Hadley Wickham. I think it is at the same time incredibly rich in content and easy to read. You say you have zero knowledge in R, but I believe since you already have programming skills in other languages this book can complement very fruitfully any classical ""R beginner manual"" (for the latter see here). ";;;
2306;1;2014-10-18T18:05:26.930;Treebank conversion from dependency to constituency?;Most Treebank conversion which I found in the web are from constituency treebank to dependency treebank, I wonder why there is little jobs in the opposite direction?;[education, open-source];27;
2307;2;2014-10-19T03:24:22.877;;None of the algorithms you mention are good with data that has uniform distribution.size <- 20             #length of random number vectorsset.seed(1) x <- runif(size)          # generate samples from uniform distribution (0.0, 1.0)y <-runif(size) df <-data.frame(x,y)# other distributions: rpois, rmvnorm, rnbinom, rbinom, rbeta, rchisq, rexp, rgamma, rlogis, rstab, rt, rgeom, rhyper, rwilcox, rweibull.See this page for tutorial on generating random samples from distributions.For specific set of randomized data sets that are 'hard' for these methods (e.r. linearly inseparable n-classes XOR patterns), see this blog post (incl. R code): http://tjo-en.hatenablog.com/entry/2014/01/06/234155.;;;
2308;1;2014-10-19T11:02:44.397;Masters thesis topics in big data;I am looking for a thesis to complete my master M2, I will work on a topic in the big data's field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some topics or project that would make for a good masters thesis subject.I add that I have bases in data warehouses, databases, data mining, good skills in programming, system administration and cryptography ... Thanks ;[education, open-source];1472;
2311;2;2014-10-19T19:12:32.940;;"First we need to understand why we need Deep learning. To build models ML need Test Data with Labels (supervised or unsupervised). In many domains as the data grows maintaining the data with labels is hard. Deep learning networks doesn't need labeled data. The Deep learning algorithms can figure out the labels. So this obviates the need for domain experts to come out with labels for the data which is very important in the areas of speech recognition, computer vision, and language understanding. Google Cat image recognition is a very interesting experiment. Also it is interesting to know ""Geoffrey hinton"" the professor who was hired by Google.http://www.wired.com/2014/01/geoffrey-hinton-deep-learning/You may get more insight as you explore in this framework.";;;
2313;1;2014-10-20T06:38:16.490;Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?;Where is the difference between one-class, binary-class and multinominal-class classification?If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. Couldn't I just use all the methods that I mentioned above to reach my goal?e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).Is unlabeled-data C1 -> 1 or 0Is unlabeled-data C2 -> 1 or 0... and so on ...For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...But where is the difference between this two methods? (except of that I have to use different algorithms)And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =);[education, open-source];322;3
2314;1;2014-10-20T08:48:42.167;On-line random forests by adding more single Decisions Trees;A Random Forest (RF) is created by an ensemble of Decision Trees's (DT). By using bagging, each DT is trained in a different data subset. Hence, is there any way of implementing an on-line random forest by adding more decision tress on new data?For example, we have 10K samples and train 10 DT's. Then we get 1K samples, and instead of training again the full RF, we add a new DT. The prediction is done now by the Bayesian average of 10+1 DT's.In addition, if we keep all the previous data, the new DT's can be trained mainly in the new data, where the probability of picking a sample is weighted depending how many times have been already picked.;[education, open-source];166;2
2315;2;2014-10-20T09:31:18.650;;Every ML algorithm with high complexity can overfit. However, the OP is asking wether an RF will not overfit when increasing the number of trees in the forest.In general, ensemble methods reduces the prediction variance to almost nothing, improving the accuracy of the ensemble. If we define the variance of the expected generalization error of an individual randomized model as:From here, the variance of the expected generalization error of an ensemble corresponds to:where p(x) is the Pearson’s correlation coefficient between the predictions of two randomized models trained on the same data from two independent seeds. If we increase the number of DT's in the RF, larger M, the variance of the ensemble decreases when ρ(x)<1. Therefore, the variance of an ensemble is strictly smaller than the variance of an individual model. In a nutshell, increasing the number of individual randomized models in an ensemble will never increase the generalization error.;;;
2316;1;2014-10-19T21:03:35.390;Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?;Where is the difference between one-class, binary-class and multinominal-class classification?If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. Couldn't I just use all the methods that I mentioned above to reach my goal?e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).Is unlabeled-data C1 -> 1 or 0Is unlabeled-data C2 -> 1 or 0... and so on ...For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...But where is the difference between this two methods? (except of that I have to use different algorithms)And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =);[education, open-source];27;
2317;2;2014-10-20T15:32:58.097;;The Art of R Programming by Normal Matloff is a great way to find your way towards being an R user.  I've recommended this book to several people navigating the tutorial / book universe and to my knowledge they've all stuck with it. ;;;
2318;2;2014-10-20T16:12:45.427;;"There is an online data science ""game"" that takes you from learning how to use Python for loading a csv and using scikit to machine learning algorithms such as support vector machines. Here is a blog post with a demo video and the actual site is Explore Data Science. Personally, I think its genius. ";;;
2320;2;2014-10-20T19:40:34.193;;"To me Big Data is primarily about the tools (after all, that's where it started); a ""big"" dataset is one that's too big to be handled with conventional tools - in particular, big enough to demand storage and processing on a cluster rather than a single machine. This rules out a conventional RDBMS, and demands new techniques for processing; in particular, various Hadoop-like frameworks make it easy to distribute a computation over a cluster, at the cost of restricting the form of this computation. I'll second the reference to http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html ; Big Data techniques are a last resort for datasets which are simply too big to handle any other way. I'd say any dataset for any purpose could qualify if it was big enough - though if the shape of the problem is such that existing ""big data"" tools aren't appropriate, then it would probably be better to come up with a new name.Of course there is some overlap; when I (briefly) worked at last.fm, we worked on the same 50TB dataset using Hadoop and also in an SQL database on a fairly ridiculous server (I remember it had 1TB RAM, and this is a few years ago). Which in a sense meant it both was and wasn't big data, depending on which job you were working on. But I think that's an accurate characterization; the people who worked on the Hadoop jobs found it useful to go to Big Data conferences and websites, while the people who worked on the SQL jobs didn't.";;;
2321;1;2014-10-20T22:13:32.420;Bayes Optimal Decision Boundaries for Gaussian Data with Equal Covariance;"I am drawing samples from two classes in the two-dimensional Cartesian space, each of which has the same covariance matrix $[2, 0; 0, 2]$.  One class has a mean of $[1.5, 1]$ and the other has a mean of $[1, 1.5]$.  If the priors are $4/7$ for the former and $3/7$ for the later, how would I derive the equation for the ideal decision boundary?If it turns out that misclassifying the second class is twice as expensive as the first class, and the objective is to minimize the expected cost, what equation would I use for the best decision boundary?";[education, open-source];91;
2322;1;2014-10-20T22:22:09.660;Distributed Scalable Decision Trees;Are there any good sources that explain how decision trees can be implemented in a scalable way on a distributed computing system.  Where in a given source is this explained?;[education, open-source];134;
2323;1;2014-10-21T00:09:40.597;Field Aware Factorization Machines;"Can anyone explain how field aware factorization machines (FM) compare to standard FM?Standard:http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf""Field Aware"":http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf";[education, open-source];948;
2324;2;2014-10-21T01:34:13.407;;"Standard factorization machines have fields too. The ""novelty"" here seems to be the use of GBDT features and the application of the hashing tricks. Not to great effect, it seems: check out the minute range in performance on the last slide.";;;
2325;2;2014-10-21T02:43:26.790;;There's a recent paper on this subject (On-line Random Forests), coming from computer vision. Here's an implementation, and a presentation: Online random forests in 10 minutes;;;
2326;2;2014-10-21T06:12:51.887;;Apache Spark can do it, using the new MLLib library. Here's a presentation, and here are some benchmarks. Bindings are available for python, scala, and java.;;;
2327;2;2014-10-21T07:06:21.580;;I have found the video tutorial/IPython notebook format really helped me get into the python ecosystem.There were two tutorials at SciPy 2013 that cover sklearn (part 1 of 1st tutorial, github repo for notebooks). Similar tutorials, from PyCon2012 and PyData2012, are out there for pandas but I don't have the rep to link searching for pandas tutorial on youtube should allow you to find them. Since you mention Kaggle, I guess you will have seen their getting started with python tutorial for the titanic passenger dataset (I don't have the rep here to provide a link but searching for Getting Started with Python: Kaggle's Titanic Competition should get you there).;;;
2328;1;2014-10-21T09:42:51.610;Which packages or functions can I use in R to plot 3D data like this?;"There're many data points, each of which is associated with two coordinates and a numeral value, or three coordinates. And I wish it is coloured.I checked packages ""scatterplot3d"" and ""plot3D"" but I couldn't find one like the example I give. It is like it has a fitting surface.My data is basically like the following. In this way I think this kind of plot is gonna be perfectly suitble for this data:    ki,kt,Top10AverageF1Score    360,41,0.09371256716549396    324,41,0.09539634212851525    360,123,0.09473510831594467    36,164,0.09773486852645874    ...But I also may have one more additional variable, which makes it like:    NeighborhoodSize,ki,kt,Top10AverageF1Score    10,360,41,0.09371256716549396    15,324,41,0.09539634212851525    15,360,123,0.09473510831594467    20,36,164,0.09773486852645874    ...Do you also have any good idea for visualizing the second case? What kind of plot and which packages and functions, etc.";[education, open-source];93;
2329;2;2014-10-21T10:22:21.390;;"You could use the wireframe function from the lattice package:library(""lattice"")wireframe(volcano[1:30, 1:30], shade=TRUE, zlab="""")";;;
2332;2;2014-10-22T08:30:00.990;;"Since it's a master's thesis, how about writing something regarding decision trees, and their ""upgrades"": boosting and Random Forests? And then integrate that with Map/Reduce, together with showing how to scale a Random Forest on Hadoop using M/R?";;;
2334;1;2014-10-22T14:53:42.127;MovieLens data set;I want to analyze MovieLens data set and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according 'userId' and 'Time' columns. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):In [36]: df.head(10)Out[36]:         userId  movieId  Rating       Time                         movieName  \40034        1      150       5  978301777                  Apollo 13 (1995)   77615        1     1028       5  978301777               Mary Poppins (1964)   550485       1     2018       4  978301777                      Bambi (1942)   400889       1     1962       4  978301753         Driving Miss Daisy (1989)   787274       1     1035       5  978301753        Sound of Music, The (1965)   128308       1      938       4  978301752                       Gigi (1958)   497972       1     3105       5  978301713                 Awakenings (1990)   28417        1     2028       5  978301619        Saving Private Ryan (1998)   6551         1     1961       5  978301590                   Rain Man (1988)   35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)                               genre  40034                       Drama  77615   Children's|Comedy|Musical  550485       Animation|Children's  400889                      Drama  787274                    Musical  128308                    Musical  497972                      Drama  28417            Action|Drama|War  6551                        Drama  35492        Action|Crime|Romance  [10 rows x 6 columns]I can not understand that the same user with user Id 1 see or rated the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id:2018) exactly at the same time (up to the milleseconds). If somebody works already with this data set, please, clear this situation.;[education, open-source];124;
2335;2;2014-10-22T15:50:43.230;;"When you enter ratings on movie lens, you get pages with 10 movies or so. You set all the ratings, then submit by clicking ""next page"" or something. So I guess all the ratings for the same page are received at the same time, when you submit the page.";;;
2337;1;2014-10-23T14:51:57.160;Clustering strings inside strings?;I am not sure whether I formulated the question correctly. Basically, what I want to do is:Let's suppose I have a list of 1000 strings which look like this:cvzxcvzxstringcvzcxvzotortorotrstringgrptprtvmvmvmeopstring2vmrprpvccermpqpstring2rowermproorororstring3potrprtmprto2435string3famerpaeretc.I'd like to extract these reoccuring strings that occur on the list. What solution should I use? Does anyone know about algorithm that could do this?;[education, open-source];69;
2338;2;2014-10-23T15:50:09.820;;In my experience, the answer depends on the project at hand. For pure research, I prefer R for two reasons: 1) broad variety of libraries and 2) much of the data science literature includes R samples.If the project requires an interactive interface to be used by laypersons, I've found R to be too constrained. Shiny is a great start, but it's not flexible enough yet. In these cases, I'll start to look at porting my R work over to Python or js.;;;
2339;2;2014-10-23T17:10:40.890;;"Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product if using the sphere). Assume random initialization, and use a gradient-based optimization method by taking the Jacobian of the error.Now you have a Hilbert space embedding, so cluster using your algorithm of choice!Response to deleted comment asking how to cluster multiple substrings: The bulk of the complexity lies in the first stage; the calculation of the LCS, so it depends on efficiently you do that. I've had luck with genetic algorithms. Anyway, what you'd do in this case is define a similarity vector rather than a scalar, whose elements are the k-longest pair-wise LCS; see this discussion for algorithms. Then I would define the error by the sum of the errors corresponding to each substring.Something I did not address is how to choose the dimensionality of the embedding. The word2vec paper might provide some heuristics; see this discussion. I recall they used pretty big spaces, on the order of a 1000 dimensions, but they were optimizing something more complicated, so I suggest you start at R^2 and work your way up. Of course, you will want to use a higher dimensionality for the multiple LCS case.";;;
2340;2;2014-10-23T20:11:18.750;;"If you just want to get a forecast, use automatic forecasting software like Autobox, ForecastPro, SCA, etc.  Comparison of different software: http://stats.stackexchange.com/questions/68253/expert-forecasting-software-evaluationWorked example: http://people.duke.edu/~rnau/autocomp.htm If you are doing research on forecasting techniques, there are some Java packages, for example:JMotif http://code.google.com/p/jmotif/JTSA http://sourceforge.net/projects/jtsa/However, Java is not very widely used for this; if you must use Java you will likely have a feeling of being out in the cold.   R and Python would have much more packages, examples, etc.  A tiny sample of what is available in R:http://www.amazon.com/Introductory-Time-Series-Paul-Cowpertwait/dp/0387886974/http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/http://www.r-bloggers.com/time-series-analysis-and-mining-with-r/";;;
2341;2;2014-10-23T21:12:27.350;;I would think about starting with a power analysis: i.e. how many data points do you need to measure the effect (or parameter) that you are interested to a specified level of confidence, ceteris paribus? Then, you estimate a cost. ;;;
2342;2;2014-10-23T21:59:32.713;;You should read the paper from Google on PLANET, which was their distributed MapReduce-based implementation of random decision forests: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36296.pdfYou may or may not like the architecture but there are a number of interesting ideas about scaling up here.;;;
2343;1;2014-10-24T23:13:07.803;Small project ideas for Machine Learning;I need some serious help. I am supposed to implement a project (Non-Existing as of now) for my Machine Learning course. I have no basics in AI or Data mining or Machine learning. I have been searching for a while and unable to find something that i can finish implementing in 3-4 weeks time. It carries a huge chunk of my final marks and no matter how much i try i am unable to understand how it works!Can the machine learning masters please help me out with this. I need a project suggestion to start with. And i want to know how to proceed after gathering the data set. I am totally blank and running out of time for my graduation :(Appreciate your suggestions! Thanks in advance.;[education, open-source];1034;
2344;1;2014-10-25T01:36:27.483;pandas dataframes memory;"I have a question about memory usage. I want to do 4 things:1) make a dataframe from one of several columns from a datasource, say a json string2) make the third column of the original dataset the index to the dataframe3) change the name of another column4) change the series i've created to a dataframeMy question is about memory efficiency. It seems that for step 1), I am first loading a whole dataframe, then run a concat command to concatenate the columns I want. For step 2, I again need to resave the new dataframe as another object.For step 3, it seems to stick so nothing there.Please advise on a more efficient way to go about this, if that exists.Command:   df = pd.DataFrame(jsonobject)   df = df.set_index(""columnC"")   df.index.names= [""foo""]   df1 = df[""foo""].map(lambda x:x[""id""])   df2 = pd.DataFrame(df1)";[education, open-source];157;
2345;2;2014-10-25T08:40:14.613;;Gather million twitter user accounts. Then try to guess their gender based on their avatar, tweets and who they follow.;;;
2346;1;2014-10-25T22:37:18.523;Business exception reporting;"I've looked at all the other sections on stackexchange and haven't found a better fit for this question. Sorry if it's misplaced. If it is, feel free to downvote, but could you please also drop a recommendation where the question would be better suited? Thanks!I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.I want to change this to a model of daily exception reporting and weekly trend reporting. Features might include: User report consolidation (so there's only one daily email)report ordering based upon level of variance from past performance (see the most important stuff first)HTML email support (with my audience, pretty counts)web interface to allow preference changes, including LDAP support (make administration easier)Unsubscribe feature at the report levelHere's what I'd like to knowWhat are the practical problems I might run into?What is the best way to display the new reports?How should  I define an ""exception""? How can I know if my definition is a good one?I assume I'd be using a mix of Python, SQL, and powershell. Anything else I should consider, e.g. R? What are some good resources?Thanks,Brandon ";[education, open-source];98;
2347;2;2014-10-25T23:07:15.843;;"Here's some practical advice from my own experiences-The first thing to do is to convince management the the change will be for the better. A mock-up of a sample report can be very useful here.Even if management agrees, they'll still want to know why variances have occurred, and hence you'll need to be able to supply more data.Although exception reporting is best, management will want to see everything anyway, as doing so makes them feel as though they are doing something useful.Don't change everything at once- too big a change at once can cause resistance.For how to best present the data, read Edward Tufte's books, at the very least his first one, ""The Visual Display of Quantitative Data"".Defining what's an exception can be hard, because the recipients will each have their own  ideas. Using say, a 95% confidence interval is good, but it won't be universally liked. Some people will consider any change above $X significant, and others will want to see everything that's more than X% different from the prior period. Have fun with this part :(";;;
2348;1;2014-10-25T23:46:14.493;How to calculate classification accuracy with confusion matrix?;I have Train and Test data, how to calculate classification accuracy with confusion matrix ? Thanks@attribute outlook {sunny, overcast, rainy}@attribute temperature {hot, mild, cool}@attribute humidity {high, normal}@attribute windy {TRUE, FALSE}@attribute play {yes, no}Train:1   sunny       hot     high    FALSE   no2   sunny       hot     high    TRUE    no3   overcast    hot     high    FALSE   yes4   rainy       mild    high    FALSE   yes5   rainy       cool    normal  FALSE   yes6   rainy       cool    normal  TRUE    no7   sunny       cool    normal  FALSE   yes8   rainy       mild    normal  FALSE   yes9   sunny       mild    normal  TRUE    yes10  overcast    mild    high    TRUE    yes11  overcast    hot     normal  FALSE   yes12  rainy       mild    high    TRUE    noTest:overcast    cool    normal  TRUE    yessunny       mild    high    FALSE   noRules found:(humidity,normal), (windy,FALSE) -> (play,yes) [Support=0.33 , Confidence=1.00 , Correctly Classify= 4, 8, 9, 12](outlook,overcast) -> (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 2, 11](outlook,rainy), (windy,FALSE) -> (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 3](outlook,sunny), (temperature,hot) -> (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 0, 1](outlook,sunny), (humidity,normal) -> (play,yes) [Support=0.17 , Confidence=1.00 , Correctly Classify= 10](outlook,rainy), (windy,TRUE) -> (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 5, 13];[education, open-source];168;
2349;1;2014-10-26T13:12:23.597;Rough vs Fuzzy vs Granular Computing;For my Computational Intelligence class, I'm working on classifying short text. One of the papers that I've found makes a lot of use of granular computing, but I'm struggling to find a decent explanation of what exactly it is.From what I can gather from the paper, it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I'm asking about rough sets as well, because I'm curious about them and how they relate to fuzzy sets. If at all.Edit: Here is the paper I'm referencing.;[education, open-source];72;1
2350;2;2014-10-26T19:36:07.113;;"""Granularity"" refers to the resolution of the variables under analysis.  If you are analyzing height of people, you could use course-grained variables that have only a few possible values -- e.g. ""above-average, average, below-average"" -- or a fine-grained variable, with many or an infinite number of values -- e.g. integer values or real number values.A measure is ""fuzzy"" if the distinction between alternative values is not crisp.  In the course-grained variable for height, a ""crisp"" measure would mean that any given individual could only be assigned one value -- e.g. a tall-ish person is either ""above-average"", or ""average"".  In contrast, a ""fuzzy"" measure allows for degrees of membership for each value, with ""membership"" taking values from 0 to 1.0.  Thus, a tall-ish person could be a value of ""0.5 above-average"", ""0.5 average"", ""0.0 below-average"".Finally, a measure is ""rough"" when two values are given: upper and lower bounds as an estimate of the ""crisp"" measure.  In our example of a tall-ish person, the rough measure would be {UPPER = above-average, LOWER = average}.Why use granular, fuzzy, or rough measures at all, you might ask?  Why not measure everything in nice, precise real numbers?  Because many real-world phenomena don't have a good, reliable intrinsic measure and measurement procedure that results in a real number.  If you ask married couples to rate the quality of their marriage on a scale from 1 to 10, or 1.00 to 10.00, they might give you a number (or range of numbers), but how reliable are those reports? Using a course-grained measure (e.g. ""happy"", ""neutral/mixed"", ""unhappy""), or fuzzy measure, or rough measure can be more reliable and more credible in your analysis.  Generally, it's much better to use rough/crude measures well than to use precise/fine-grained measures poorly.";;;
2351;2;2014-10-26T20:10:06.923;;I think the most likely explanation is that the two libraries do not quite support TreeModel in PMML in the same way. Perhaps one only supports a subset of features, and ignores ones it does not understand. This could cause different scoring.I'd also double check that upstream parsing code is the same in both cases. Maybe a missing value is treated differently upstream.;;;
2352;1;2014-10-26T20:11:22.317;making logical inference from a simuation generated data;I have data collected from a computer simulation of football games which seem to have recurring patterns of the following form.if madrid plays arsernal and the match ends under 3 goal, then on their next match against each others, madrid will win. if madrid happens to loose and then plays against chelsea next, they will win 90% of the time.how do I find such inferences from simulation generated data like this. There are other forms of hidden patterns that I believe exists in the dataset.;[education, open-source];25;
2353;1;2014-10-26T20:35:39.767;MATLAB Perceptron;"I cant seem to figure out why I have a high percentage error. I'm trying to get a perceptron between X1 and X2 which are Gaussian distributed data sets with distinct means and identical covariances. My code:N=200;C= [2 1; 1 2]; %Covariancem1=[0 2];m2=[1.5 0];%meanX1 = mvnrnd(m1, C, N/2);X2 = mvnrnd(m2, C, N/2);X = [X1; X2];X = [X ones(N,1)]; %biasy = [-1*ones(N/2,1); ones(N/2,1)]; %classification%Split data into training and test ii = randperm(N);Xtr = X(ii(1:N/2),:);ytr = X(ii(1:N/2),:);Xts = X(ii(N/2+1:N),:);yts = y(ii(N/2+1:N),:);Nts = N/2;w = randn(3,1);eta = 0.001;%learn from training setfor iter=1:500 j = ceil(rand*N/2);if( ytr(j)*Xtr(j,:)*w < 0)w = w + eta*Xtr(j,:)'; endend%apply what you have learnt to test setyhts = Xts * w;disp([yts yhts])PercentageError = 100*sum(yts .*yhts < 0)/Nts;Any help would be appreciated. Thank you";[education, open-source];159;1
2354;2;2014-10-27T04:15:00.747;;I think your best bet would be rosetta. I'm finding it extremely useful and easy. Check its pandas methods.You can get it by pip.;;;
2355;1;2014-10-27T09:55:36.887;Graphlab, plots are invisible;Sorry, if this topic is not connected direct to the Data Science...I try to understand how the Graphlab tool works. Firstly I want to execute the toy examples from Gallery site. When I try to execute the example code, everything is ok excepted one command: I can not see the graphlab plot after show(). The command show() return me some kind of object in IPython and nothing in IPython Notebook.If the example code has the plot, which depends direct from matplotlib module, I can produce the real plots and save it on my machine. Consequently I suppose the main error is depended from the graphlab (or object from  its class). If somebody already used this tool and got the plot, can he/she tell me, how I can execute the plots command.In [8]: import graphlabIn [9]: from IPython.display import display        from IPython.display import Image        graphlab.canvas.set_target('ipynb')In [10]:import urllib        url = 'https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv'        urllib.urlretrieve(url, filename='freebase_performances.csv')  # downloads an 8MB file to the working directoryOut[10]: ('freebase_performances.csv', <httplib.HTTPMessage instance at 0x7f44e153cf38>)In [11]: data = graphlab.SFrame.read_csv('remote://freebase_performances.csv', column_type_hints={'year': int}).........In [15]:data.show()No plot after this line......In [19]:print data.show()<IPython.core.display.Javascript object at 0x7f44e14c0850>The object of graphlab (?) after print command;[education, open-source];207;
2356;2;2014-10-27T15:43:00.067;;"OK, so values at time t-1 predict values at time t. That makes sense. First you should decide whether you think these values are independent or not. Do the x predict the y or z at all? And, do you think just the previous 1 value is predictive, or the previous n?Either way you could model this as a simple regression problem. What technique is best really depends on what you expect the relationship to be, and what these variables are; I am not sure that's given here.For example if they're sensor values read fairly rapidly, and the sensor changes slowly, you'd expect some simple model like a moving average to do well. For other types of values this would not be predictive at all.This looks like the Markov chain model, so you may look into that, but somehow I think it's over-general for what I think the problem is.";;;
2357;2;2014-10-27T16:13:21.233;;A confusion matrix is a cross tabulation of your predicted values against the true observed values, and (test) accuracy is emperical rate of correct predictions. So in this case you'll need to Predict the 'play' attribute for your test set.  (Currently you don't have a method to predict your second test case, so for the sake of argument let's assume your model would predict yes for the sunny example. The following method of keeping track of your predictions is reffered to as a confusion matrix.  The top labels are prediced               Predicted         +----------------+         ¦     ¦ yes ¦ no ¦Oserved  ¦ yes ¦ 1   ¦ 1  ¦         ¦ no  ¦ 0   ¦ 0  ¦         +----------------+Here the first 1 is from your first test case and the second 1 is from the misclassified second test case.Calculate accuracy,Accuracy = (# correct predictions)/(# total predictions) = 1 / 2 = .50.;;;
2358;2;2014-10-27T17:39:18.997;;"It is classify test objects: ""In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects.""Suppose 1 test case is (overcast, cool, normal, TRUE). Look through the rules top to bottom and see if any of the conditions are matched. The first rule for example tests the outlook feature. The value doesn't match, so the rule isn't matched. Move on to the next rule. And so on. In this case, rule 2 matches the test case and the classification for the play variable is ""yes"". The second test case is misclassified.Thanks";;;
2359;1;2014-10-27T22:17:31.497;Ranked tag recommendation for university courses;"Our system allows an admin to manage a database of university courses. These courses have multiple fields, like the department, a title, and a description.I am adding the ability to add learning objectives to a course. To simplify the problem, let's say that learning objectives are just tags. Courses can have more than one learning objective associated with them. So a course like CHEM 101 might have ""chemistry"", ""technology"", ""science"", and several others.Assuming I can reduce a course to a set of features, (using keywords/stemming/nlp, I suppose?), what kind of problem is this and what algorithm would you suggest? It seems very similar to a classification problem, but I want to provide a sorted list of suggestions with the most relevant at the top.";[education, open-source];54;
2360;2;2014-10-27T23:24:46.673;;For item-ratings type of data with the restriction that an item's rating should be between 1 and 10 after transformation, I would suggest using a simple re-scaling, such that the item's transformed rating $x_t$ is given by:$$x_t = 9\left(\frac{x_i - x_{min}}{x_{max} - x_{min}}\right) + 1$$where $x_{min}$ and $x_{max}$ are the minimum and maximum possible rating in the specific scale for the item, and $x_i$ is the item rating. In the case of the above scaling, the transformation applied is independent of the data. However, in the normalization, the transformation applied is dependent on the data (through mean and standard deviation), and might change as more data becomes available.Section 4.3 on page 30 of this document shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved. ;;;
2361;2;2014-10-27T23:56:11.853;;This will most likely turn out to be multiple binary classification problems. Instead of building just one classifier, you would have to build as many classifiers as there are tags. Each classifier's task would be to predict whether a certain tag exists or not. A tag specific probability threshold can then be learnt by tuning over the precision-curve where course wise precision recall can be calculated. The percentile of the predicted probability for each tag can be used to obtain a ordered (sorted) list of tag suggestions. This way of converting a multi-class problem to multiple binary classification problems is known as binary relevance. While it is very efficient, it assumes that the labels themselves are independent. However, picking the probability threshold for each tag in order to optimize the course wise precision recall curve can lead to modeling dependence in the tags. ;;;
2362;1;2014-10-28T06:00:49.643;Neural Networks getting stuck at local optima;"I'm training a NN with 8 features and 8000 training examples with a single output (0, 1) using the scipy.optimise CG algorithm and the results are somewhat inconsistent. The goal is to get the NN to be as 'precise' as possible (recall doesn't really matter too much) so I've set the threshold for y value quite high (0.75). Most of the time it gets a precision of around 80%, however sometimes it fails (using exactly the same parameters, lambda etc..) to generate any outputs which are above the 0.75 threshold, meaning the precision equals 0.I've successfully trained NNs before without these unusual results (albeit the goal was a somewhat more conventional multi-class classifier with many more features).I'm wondering if the training NNs with fewer features increases the chances of it getting stuck at a local optima; or getting stuck at local optima has a more significant impact on NNs with fewer features?Any thoughts on what's going on!?";[education, open-source];159;
2363;2;2014-10-28T06:27:40.090;;The difference was, it appears, due to the different implementation of Random Forests in R and Cascading Pattern (as well as openscoring which I tried later) with respect to ties in the tree voting - i.e. when an even number of trees are built (say, 500) and exactly half classify an application as Good, and the other half as Bad, the handling of those situations differs. Solved it by growing and odd (501) number of trees.;;;
2364;2;2014-10-28T07:00:01.593;;Using unsupervised learning to reduce the dimensionality and then using supervised learning to obtain an accurate predictive model is commonly used. See for example Bhat and Zaelit, 2012 where they first use PCA to reduce the dimension of a problem from 87 to 35. Then, they use L1 regression to obtain the best predictive model. This method beats non-linear tree based models built on the entire dataset and also its subset. If your goal is to create more accurate classification of data into clusters, then a commonly used technique is to use supervised learning as a method to accurately pick the number of clusters see Pan et al, 2013 for a recent example. The basic approach here is to choose the number of clusters such that a supervised multi-class method can learn these clusters and predict the clusters with the highest out of sample accuracy. This is one way to convince yourself that the clusters are both meaningful and predictable. Another approach, if your goal is to classify documents as being from US/ USA/ Australia or for that matter discussing, soccer/ American football/ Australian football could be to solve three binary classification problems that independently predict if the document talks about soccer, American football or Australian football. Combining the results from these three classifiers (known as binary relevance), you could also have the ability of tagging a document as both soccer or American football or any combination of the above three tags. ;;;
2365;2;2014-10-28T07:14:06.873;;Ok all sorted - Bit embarrassing but forgot to normalise the data!;;;
2366;2;2014-10-28T17:07:32.723;;"You are altering weights in the wrong direction for the negative cases.The linew = w + eta*Xtr(j,:)';should bew = w + eta*Xtr(j,:)'*ytr(j);With that change I got 12% error.";;;
2367;2;2014-10-29T00:52:06.097;;"Instead of ""recursive neural nets with back propagation"" you might consider the approach used by Frantzi, et. al. at National Centre for Text Mining (NaCTeM) at University of Manchester for Termine (see: http://www.nactem.ac.uk/index.php and http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf) Instead of deep neural nets, they ""combine linguistic and statistical information"".";;;
2368;1;2014-10-29T05:25:55.603;Machine learning - features engineering from date/time data;"What are the common/best practices to handle time data for machine learning application?For example, if in data set there is a column with timestamp of event, such as ""2014-05-05"", how you can extract useful features from this column if any?Thanks in advance!";[education, open-source];462;6
2369;2;2014-10-29T07:52:41.060;;In several cases data and events inside a time series are seasonal. In such cases the month and the year of the event matters alot. Hence in such scenarios you can use binary variables to represent if the event is during a given month/year or not.Hope this answers your question. If not kindly be a little more specific on what exactly are you trying to achieve  ;;;
2370;2;2014-10-29T13:54:23.793;;I would start by graphing the time variable vs other variables and looking for trends.  For exampleIn this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:day_of_weekabsolute_timeIn generalThere are several common time frames that trends occur over:absolute_timeday_of_yearday_of_weekmonth_of_yearhour_of_dayminute_of_hourLook for trends in all of these.  Weird trendsLook for weird trends too.  For example you may see rare but persistent time based trends:is_easteris_superbowlis_national_emergencyetc.These often require that you cross reference your data against some external source that maps events to time.  Why graph?There are two reasons that I think graphing is so important.Weird trendsWhile the general trends can be automated pretty easily (just add themevery time), weird trends will often require a human eye and knowledgeof the world to find.  This is one reason that graphing is soimportant.Data errorsAll too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data. ;;;
2371;2;2014-10-29T13:58:55.653;;I have found some solution and will post it here, because somebody, who works with graphlab, can have the same question.We can look at the example here: Six degrees of Kevin BaconAt te beginning of the program execution you need to run next command:graphlab.canvas.set_target('ipynb')Exactly this is a key of the whole problem (at least by me:-)At the beginning it is important to know, which parameter of set_target() command you want to execute. You can use two options for argument of this command:  'ipynb' (which is executed direct in iPython Notebook, like in example) or 'browser' (which open the new window with the plots)On my machine 64-bit, Ubuntu, I can not use the command 'ipynb'. Only the 'browser'-command get me the plot back. I don't think, it is necessary to change https to http, but you can do it anyway. We have also the other machine by us (32-bit, Ubuntu) and it executes the other command 'ipynb', but not 'browser' (without to change https to http);;;
2372;2;2014-10-29T14:37:00.910;;Decision Trees are unstable learners and very sensitive to changes in the input parameters. ;;;
2373;1;2014-10-29T21:57:15.603;Detecting Spam using Machine Learning;The most online tutorials like to use a simple example to introduce to machine learning by classify unknown text in spam or not spam. They say that this is a binary-class problem. But why is this a binary-class problem? I think it is a one-class problem! I do only need positive samples of my inbox to learn what is not spam. If I do take a bunch of not spam textes as positiv samples and a bunch of spam-mails as negativ samples, then of course it's possible to train a binary-classifier and make predictions from unlabeled data, but where is the difference to the onc-class-approach? There I would just define a training-set of all non spam examples and train some one-class classifier. What do you think?;[education, open-source];103;1
2374;2;2014-10-29T22:10:53.570;;Your training data needs to be one set of data with samples of all the categories, because you are trying (I think) to create a model that will be fed such data.Have you given any thoughts as to what model(s) you might be using? I'm asking because pure classification models will achieve a better fit if the amount of data in each class is pretty uniform in the training data. However, regression models need the data type proportions to match the expected input.;;;
2375;2;2014-10-30T07:31:03.903;;"The problem arises if you want to classify a new example as either spam or not spam. A one-class method will only give you some score of how well a new instance fits to this class, but how do you turn this into a binary prediction without knowing how big the score would be for the other class?If you look at the Naive Bayes classifier it essentially trains a ""one-class"" model for each class and arrives at a prediction by choosing the class with the highest score. But this requires you to have training examples for all classes.";;;
2376;2;2014-10-30T14:48:34.907;;"Strictly speaking, ""one class classification"" does not make sense as an idea. If there is only one possible state of a predicted value, then there is no prediction problem. The answer is always the single class.Concretely, if you only have spam examples, you would always achieve 100% accuracy by classifying all email as spam. This is clearly wrong, and the only way to know how it is wrong is to know where the classification is wrong -- where emails are not in the spam class.So-called one-class classification techniques are really anomaly detection approaches. They have an implicit assumption that things unlike the examples are not part of the single class, but, this is just an assumption about data being probably not within the class. There's a binary classification problem lurking in there.What is wrong with a binary classifier?";;;
2377;2;2014-10-30T14:59:13.243;;Basically your machine-learning problem is:given [day-of-week, weather, departure-time, route], predict arrival time, then obtain travel time. Once your model is solid, you can then predict your travel time for each potential departure time and route, and choose the lowest.You don't really need to factor in the pit stops, if you think they're a function of the rest, or a random variable altogether.If you want a tool that can do the machine-learning aspect 'out of the box' you should try Weka.You'll have to encode the data in the specific format it expects, but other than that you won't have to do any coding (i.e. you won't need to code any of the actual machine-learning algorithms).I would discretize your departure time to make sure you have enough data.Weka will let you try out the different algorithms, see which one is best.Note that it's a regression problem as opposed to a classification problem, so only a subset of the algorithms apply. Once you've got the Weka part figured out, you can also call it programmatically, which will allow you to code the interface you want, and potentially include the automatic weather retrieval. ;;;
2378;1;2014-10-30T18:22:18.907;Matrix properties and machine learning/data mining;I'm doing some data analysis in a Statistical Pattern Recognition course using PRML. We analyzed a lot of matrix properties, like eigenvalues, column independence, positive semi-definite matrix, etc. When we are doing, for example, linear regression, we need to calculate some of those properties, and fit them into the equation.So my question is, my question is about the intuition behind these matrix properties, and their implications in the ML/DM literature.If anyone could answer, can you teach me what is the importance of eigenvalue, positive semi-definite matrix, and column independence for ML/DM. And possibly, other important matrix properties you think important in study the dataset, and why.I'd be really appreciated if someone can answer this question.;[education, open-source];160;
2379;2;2014-10-31T01:38:57.483;;"A few things where the knowledge of Linear Algebra might be helpful in the context of Machine Learning: Dimensionality Reduction: There are lots of problems where PCA (a special case of an SVD) followed by a simple Machine Learning method applied on the reduced dataset produces much better results than a non parametric model on the full (non-reduced dataset). For an example see Bhat and Zaelit, 2012 where PCA followed by linear regression performs better than more involved non-parametric models. It also suggests reasons why dimensionality reduction performs better in these cases.  Visualizing data: Higher dimensional data is complicated to visualize and one often needs to be able to reduce the dimension of the dataset to view it. This comes very handy when one has to ""view"" the results of clustering on a higher dimensional dataset.  Numerical Accuracy: Eigenvalues are generally handy in order to understand condition numbers of matrices and hence be able to figure out if results of Linear regression or other methods that require to solve Ax=b would be numerically accurate. Positive Definiteness of a matrix might also be able to guarantee bounds on numerical accuracies. Recommendations: Some methods like collaborative filtering use matrix factorization (SVD) tuned smartly to solve recommendation problems.Regularization: Regularization is commonly used to reduce over-fitting in machine learning problems. Most of these regularization techniques like Lasso, Tikhonov etc. have both optimization and Linear Algebra at their heart. ";;;
2380;2;2014-10-31T04:44:23.100;;The importance of a concept in mathematics depends on the circumstances of its application. Sometimes, its importance relies on the fact that it allows you to carry on with what you are doing. For example, you usually need column independence (independent variables between predictors) because multiple regression will behave badly with highly correlated variables. Even worst, when some of your columns (or rows) are dependent, your matrix is not invertible. Why? Because matrix inversion A^-1 involves the determinant 1/|A|, which is 0 when columns or rows are linearly dependent.Eigenvalues is a common occurrence in calculations related to maximization/minimization in machine learning. Let's say you are interested in principal component analysis. A very important idea there is dimensional reduction (you have a dataset with many variables and want to reduce the number of variables without losing too much explanatory power.) One solution is to project your data onto a lower dimensional space (e.g. taking your data with 50 variables and reducing them to 5 variables.) Turns out a good projection to use is one that includes as much variation as possible and maximization of this variation results in the eigenvalue equation S u  = λ u. In other cases, you explicitly include the eigenvalue equation of some quantity of interest because in doing so, you're changing the coordinate system in which you represent the variables. Take the case of a (multivariate) Gaussian distribution in which the argument of the exponent is given by Δ = (x-μ)^T Σ (x-μ). If you consider the eigenvalue equation of Σ, the exponent can be written as Δ = y_1^2 / λ_1 + y_2^2 / λ_2 (in two dimensions) This is the equation of an ellipse only if λ_1 and λ_2 are positive. Therefore, you obtain the following graphical interpretation (Bishop, PRML, p.81):Positive semi-definite matrices are used as a matter of convenience. They are well-behaved and well-understood. For instance, their eigenvalues are non-negative, and if you remember the previous paragraph, the argument Δ required positive eigenvalues. By now, you can see why some concepts are very popular: You need them for your calculations or they need each other.I can recommend a couple of books:Linear Algebra: A Modern Introduction by David Poole Understanding Complex Datasets: Data Mining with Matrix Decompositions  by David Skillicorn. The second recommendation is more specialized and requires a decent understanding of the basics, but it is of great help to understand matrix decompositions.;;;
2381;1;2014-10-31T08:50:23.160;How to find a model for a short discrete time-series?;I'm examining the activity of customers over the years which have about one event per year. This results is many short time-series for which I found the distributions (hit/miss over 4 years sorted by probability in the data):0000 : 0.318340001 : 0.175820010 : 0.136050100 : 0.135541000 : 0.128860011 : 0.017171100 : 0.016500110 : 0.015780101 : 0.012201010 : 0.011171001 : 0.008830111 : 0.005711110 : 0.005651111 : 0.004961101 : 0.003841011 : 0.00351Apparently a purely uncorrelated binomial model wouldn't do, but one can observe that if both, the number of 1's and 11's coincide, then the probabilities are approximately equal (apart from a small recency effect of 0001).Can you see a way to approach such data to deduce a probabilistic model? Basically where I have only a few probability parameters which roughly explain this distribution?;[education, open-source];59;
2382;1;2014-10-31T12:51:13.363;Data set: Time series forecasting;I need data set for energy forecasting for my project. Currently I am using east/west wind energy dataset from http://www.nrel.gov/. I need one more data set for solar energy. Please let me know where I can find one for free.;[education, open-source];78;
2383;2;2014-10-31T13:05:49.263;;Gather app reviews for single app and try to find out the most important features that you can mine for the app developertry predicting the next word in a sentence based on previous words.Eg. Swiftkey does this.Get questions data from stack overflow and try to predict tags for every post on stack overflow.;;;
2384;1;2014-10-31T17:55:25.723;Text-Classification-Problem, what is the right approach?;"I'm planing to write a classification program that is able to classify unknown text in around 10 different categories and if none of them fits it would be nice to know that. It is also possible that more then one category is right.My predefined categories are:c1 = ""politics""c2 = ""biology""c3 = ""food""...I'm thinking about the right approach in how to represent my training-data or what kind of classification is the right one. The first challenge is about finding the right features. If I only have text (250 words each) what method would you recommend to find the right features? My first approach is to remove all stop-words and use the POS-Tagger (Stanford NLP POS-Tagger) to find nouns, adjective etc. I count them an use all frequently appeared words as features.e.g. politics, I've around 2.000 text-entities. With the mentioned POS-Tagger I found:law:           841capitalism:    412president:     397democracy:     1007executive:     112...Would it be right to use only that as features? The trainings-set would then look like:Training set for politics:feature law         numericfeature capitalism  numericfeature president   numericfeature democracy   numericfeature executive   numericclass politics,all_otherssample data:politics,5,7,1,9,3politics,14,4,6,7,9politics,9,9,9,4,2,1politics,5,8,0,7,6...all_others,0,2,4,1,0all_others,0,0,1,1,1all_others,7,4,0,0,0...Would this be a right approach for binary-classification? Or how would I define my sets? Or is multi-class classification the right approach? Then it would look like:Training set for politics:feature law         numericfeature capitalism  numericfeature president   numericfeature democracy   numericfeature executive   numericfeature genetics    numericfeature muscle      numericfeature blood       numericfeature burger      numericfeature salad       numericfeature cooking     numeric class politics,biology,foodsample data:politics,5,7,1,9,3,0,0,2,1,0,1politics,14,4,6,7,9,0,0,0,0,0,1politics,9,9,9,4,2,1,1,1,1,0,3politics,5,8,0,7,6,2,2,0,1,0,1...biology,0,2,4,1,0,4,19,5,0,2,2biology,0,0,1,1,1,12,9,9,2,1,1biology,7,4,0,0,0,10,10,3,0,0,7...What would you say?";[education, open-source];203;1
2385;2;2014-10-31T22:40:24.513;;Kaggle has a bunch of good practice datasets and basic tutorials.;;;
2386;2;2014-11-01T10:07:09.843;;The following great article by Sebastian Raschka on Bayesian approach to text classification should be very helpful for your task. I also highly recommend his excellent blog on data science topics, as an additional general reference: http://sebastianraschka.com/articles.html.You may also check this educational report on text classification: http://www.datasciencecentral.com/profiles/blogs/my-data-science-apprenticeship-project. It might provide you with some additional ideas.;;;
2387;2;2014-11-01T10:16:55.557;;Check my answer on the related question here: http://datascience.stackexchange.com/a/843/2452. However, given your situation, you might need to work on something simpler. For this, it might be a good idea to review recommended literature in the open source data science master's curriculum: http://datasciencemasters.org. Many referenced there sources are free and easily available online. I'm pretty sure that many of these sources contain examples of simple projects that you could (re)implement, extend or otherwise use for your task.;;;
2391;1;2014-11-02T07:20:32.603;Anomaly detection in multiple parameters;I am a newbie to data science with a typical problem. I have a data set with metric1, metric2 and metric3. All these metrics are interdependent on each other. I want to detect anomalies in metric3. Currently, I am using Nupic from numenta.org for my analysis and it doesn't seem to be effective. Is there any ML library which can detect anomalies in multiple parameters?;[education, open-source];122;
2392;1;2014-11-02T11:36:28.143;scikit-learn OMP mem error;"I tried to use OMP alogorithm avialable in scikit-learn. My net datasize which includes both target signal and dictionay ~ 1G. However when I ran the code, it exited with mem-error.The machine has 16G RAM, so I dont think this should have happened. I tried with some logging where the error came and  found that the data got loaded completely into numpy arrays. And it was tha algorithm itself that caused the error. Can someone help me with thisor sugggest more memory efficient algorithm for feature selection, or is subsampling the data my only option. Are there some deterministic good subsampling techniquesEDIT:Relevant code piece        n=8;    y=mydata[:,0];    X=mydata[:,[1,2,3,4,5,6,7,8]];    #print y;    #print X;    print ""here"";    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5,copy_X = False, normalize=True);    omp.fit(X,y);    coef = omp.coef_;    print omp.coef_;    idx_r, = coef.nonzero();    for id in idx_r:            print coef[id], vars[id],""\n"";The error I get:File ""/usr/local/lib/python2.7/dist-packages/sklearn/base.py"", line 324, in score    return r2_score(y, self.predict(X), sample_weight=sample_weight)  File ""/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.py"", line 2332, in r2_score    numerator = (weight * (y_true - y_pred) ** 2).sum(dtype=np.float64)MemoryError";[education, open-source];34;
2393;2;2014-11-03T03:22:30.393;;I think perhaps the first thing to decide that will help clarify some of your other questions is whether you want to perform binary classification or multi-class classification. If you're interested in classifying each instance in your dataset into more than one class, then this brings up a set of new concerns regarding setting up your data set, the experiments you want to run, and how you plan to evaluate your classifier(s). My hunch is that you could formulate your task as a binary one where you train and test one classifier for each class you want to predict, and simply set up the data matrix so that there are two classes to predict - (1) the one you're interested in classifying and (2) everything else.In that case, instead of your training set looking like this (where each row is a document and columns 1-3 contain features for that document, and the class column is the class to be predicted): 1           2           3           classfeature1    feature2    feature3    politicsfeature1    feature2    feature3    lawfeature1    feature2    feature3    presidentfeature1    feature2    feature3    politicsit would look like the following in the case where you're interested in detecting the politics class against everything else:1           2           3           classfeature1    feature2    feature3    politicsfeature1    feature2    feature3    non-politicsfeature1    feature2    feature3    non-politicsfeature1    feature2    feature3    politicsYou would need to do this process for each class you're interested in predicting, and then train and test one classifier per class and evaluate each classifier according to your chosen metrics (usually accuracy, precision, or recall or some variation thereof).As far as choosing features, this requires quite a bit of thinking. Features can be highly dependent on the type of text you're trying to classify, so be sure to explore your dataset and get a sense for how people are writing in each domain. Qualitative investigation isn't enough to decide once and for all what are good features, but it is a good way to get ideas. Also, look into TF-IDF weighting of terms instead of just using their frequency within each instance of your dataset. This will help you pick up on (a) terms that are prevalent within a document (and possibly a target class) and (b) terms that distinguish a given document from other documents. I hope this helps a little.;;;
2394;1;2014-11-03T12:41:51.737;Best format for recording time stamp and GPS;In most data acquisition settings it is useful to tag your data with time and location. If I write the data to csv file, what are the best formats that I can use for this two variables if I want to create a heatmap on Google Maps? ;[education, open-source];47;1
2395;1;2014-11-03T16:51:47.467;Modelling Unevenly Spaced Time Series;I have a continuous variable, sampled over a period of a year at irregular intervals. Some days have more than one observation per hour, while other periods have nothing for days. This makes it particularly difficult to detect patterns in the time series, because some months (for instance October) are highly sampled, while others are not.My question is what would be the best approach to model this time series?I believe most time series analysis techniques (like ARMA) need a fixed frequency. I could aggregate the data, in order to have a constant sample or choose a sub-set of the data that is very detailed. With both options I would be missing some information from the original dataset, that could unveil distinct patterns.Instead of decomposing the series in cycles, I could feed the modelwith the entire dataset and expect it to pick up the patterns. Forinstance, I transformed the hour, weekday and month in categoricalvariables and tried a multiple regression with good results (R2=0.71)I have the idea that machine learning techniques such as ANN can also pick these patterns from uneven time series, but I was wondering if anybody has tried that, and could provide me some advice about the best way of representing time patterns in a Neural network.;[education, open-source];89;1
2397;2;2014-11-03T22:10:13.263;;"As Spacedman put it, ""best"" is pretty subjective. However, as we have found, a good format for time is Unix time (aka POSIX time, aka Epoch time). Most databases support it and it is still pretty human readable.For location, we like decimal degrees as it is easy to read and stored and is compatible with Google Maps API. It's also easy to convert to other formats if needed.";;;
2398;1;2014-11-04T00:58:00.137;Python Machine Learning Experts;I'd like to apply some of the more complex supervised machine learning techniques in python - deep learning, generalized addative models, proper implementation of regularization, other cool stuff I dont even know about, etc.Any recommendations how I could find expert ML folks that would like to collaborate on projects?;[education, open-source];154;
2399;2;2014-11-04T04:23:37.107;;Check out LightSide for a GUI introduction to text mining in general, and more specifically for generating features quickly. It was developed (and I believe is still being developed) by researchers at CMU and it's how I got hooked on text mining. There is quite a bit of functionality right out of the box, and you can quickly import CSV data into the application, extract features, and start running experiments. It also makes use of suites of algorithms from several other prominent, well-regarded open source toolkits like Weka and LibLinear so you know you're using something credible under the hood. That being said, both of these last mentioned toolkits are definitely worth checking out for added functionality, even if they have a bit of a steeper learning curve. Hope that helps.;;;
2401;2;2014-11-04T13:41:52.240;;You could try some competitions from kaggle.Data Science courses from Coursera, edX, etc also provide forums for discussion.Linkedin or freelance sites could be other possibilities.;;;
2403;1;2014-11-05T09:24:58.003;Data science without knowledge of a specific topic, is it worth pursuing as a career?;I had a conversation with someone recently and mentioned my interest in data analysis and who I intended to learn the necessary skills and tools. They suggested to me that while it is great to learn the tools and build the skills there is little point in doing so unless i have specialized  knowledge in a specific field. They basically summed it to that I'd just be like a builder with a pile of tools who could build a few wooden boxes and may be build better things (cabins, cupboards etc), but without knowledge in a specific field I'd never be a builder people would come to for a specific product.Has anyone found this or have any input on what to make of this ? It would seem if it was true one would have to learn the data science aspects of things and then learn  a new field just to  become specialized.;[education, open-source];188;1
2405;2;2014-11-05T15:16:36.867;;Try out AirXcell : AirXcell calculation software.See documentation Use AirXCell as an r Console;;;
2406;2;2014-11-05T17:03:40.513;;"Drew Conway published the Data Science Venn Diagram, with which I heartily agree:On the one hand, you should really read his post. On the other hand, I can offer my own experience: my subject matter expertise (which I like better as a term than ""Substantive Expertise"", because you should really also have ""Substantive Expertise"" in math/stats and hacking) is in the retail business, my math/stats are forecasting and inferential statistics, and my hacking skills lie in R.From this vantage point, I can talk to and understand retailers, and someone who doesn't have at least a passing knowledge of this field will have to face a steep learning curve in a project with retailers. As a side gig, I do statistics in psychology, and it's exactly the same there. And even with quite some knowledge of the hacking/math/stats part of the diagram, I would have a hard time getting up to speed in, say, credit scoring or some other new subject field.Once you have a certain amount of math/stats and hacking skills, it is much better to acquire a grounding in one or more subjects than in adding yet another programming language to your hacking skills, or yet another machine learning algorithm to your math/stats portfolio. After all, once you have a solid math/stats/hacking grounding, you could if need be learn such new tools from the web or from textbooks in a relative short time period. But the subject matter expertise, on the other hand, you will likely not be able to learn from scratch if you start from zero. And clients will rather work with some data scientist A who understands their specific field than with another data scientist B who first needs to learn the basics - even if B is better in math/stats/hacking.Of course, all this will also mean that you will never become an expert in either of the three fields. But that's fine, because you are a data scientist, not a programmer or a statistician or a subject matter expert. There will always be people in the three separate circles who you can learn from. Which is part of what I like about data science.";;;
2407;2;2014-11-05T17:07:02.563;;Another library that I use that has not been mentioned yet is gensim.  Its Dictionary module allows you to convert a list of words into a list of (id,count) pairs.  It also has an allow_update variable that updates the dictionary in the case that new words are encountered at runtime.It also has built-in support for TF-IDF, LSI, and LDA, among other models.;;;
2410;2;2014-11-05T17:22:00.213;;"ARIMA, Exponential Smoothing and others indeed require evenly spaced sample points. As you write, you could bucketize your data (say into days), but as you also write, you would lose information. In addition, you may end up with missing values, so you would need to impute, since ARIMA is not very good at handling missing values.One alternative, as you again write, is to feed time dummies into a regression framework. I personally do not really like categorical dummies, because this implies a sharp cutoff between neighboring categories. This is usually not very natural. So I would rather look at periodic splines with different periodicities. This approach has the advantage of dealing with your uneven sampling and also with missing values.Be very careful about interpreting $R^2$. In-sample fit is notoriously misleading as a measure of out-of-sample forecast accuracy (see here). I would argue that this disconnect between in-sample fit and out-of-sample forecast accuracy also means that there is no connection between in-sample fit and how well a model ""understood"" the data, even if your interest lies not in forecasting, but only in modeling per se. My philosophy is that if you can't forecast a time series well, you haven't understood it in any meaningful sense.Finally, don't overdo the modeling. Just from eyeballing your data, it is obvious that something happened in June, on one day in August and in September/October. I suggest you first find out what this something was and include this in your model, e.g., as explanatory variables (which you can include in ARIMAX if you want to). What happened there is obviously not seasonality.";;;
2411;2;2014-11-05T17:50:39.573;;Sure, you can. Companies are clamoring for data scientists. Be careful though that they all interpret the term differently. Depending on the company you might find yourself asked to do anything from statistics to writing production code. Either one is a full-time job in itself and you have to be prepared for both, so asking for deep specialized knowledge on top of that's not reasonable, in my opinion, and the companies I've talked to stressed the other two areas (esp. the programming). However, I found that it helps to be familiar with the types of problems that you might face. Depending on the sector, that could be anomaly detection, recommendation/personalization, prediction, record linkage, etc. These are things you can learn as examples at the same times as maths and programming.;;;
2412;1;2014-11-05T18:57:55.037;Learning resources for Data Science for applications in road traffic data?;My BackgroundI am a graduate student in Civil Engineering. For the analyses of road traffic data (vehicle trajectories as time series) I work with big data sets mostly about a million data points or more.I started using R language when MS Excel could not open the big data files. Using basic statistics knowledge and R code I developed few algorithms to identify certain patterns in the data which worked for many applications. But I still lack serious programming skills in R.Now, I am familiar with basic inferential statistics and R packages (plyr, dplyr, ggplot2, etc). Recently I came to know that Machine Learning algorithms also help in defining patterns in the data through supervised/ unsupervised learning and their application might improve the accuracy of prediction of certain 'behaviors' of drivers using the traffic data.QuestionHaving the basic knowledge of Statistics and R, I want to learn about the data science/ machine learning as a beginner. I know that some concepts in Stats. and ML overlap and that might bridge the gap in my learning of ML. Keeping my background in mind, what resources (books/ online courses) would you recommend me to start learning data science and apply it in my field?;[education, open-source];76;1
2413;2;2014-11-05T21:22:09.193;;One way to use both metric1 and metric2 in order to find anomalies in metric3 is to consider residual analysis. In your case, this would require, creating a predictive model with metric1 and metric2 as the predictors and metric3 as the response variable. Then, calculate the residuals for metric3 as its predicted value subtracted from its true value. Now, you can report the all members of the lowest decile [or any other percentile] as one kind of an anomaly and all the members of the highest decile [or any other percentile]   as another kind of an anomaly. ;;;
2414;2;2014-11-05T21:55:43.587;;"The best way to learn data science is through problem solving. I suggest you to head over to Kaggle and work through the for-knowledge problems. To get a good start on Machine Learning problems, acclimate yourself with the tree package in R. This will help you understand how decision trees work, and building upon that, how random forests, gradient boosting machines and other sophisticated tree based algorithms work. Then, there are SVMs and deep learning models. To get an understanding of unsupervised learning problems, learn k-means and employ it for clustering. Other general concepts/ ideas to understand are: cross-validation overfitting, regularization bias-variance trade offdimensionality reduction/ variable selectiongeneralization errorensemble learning For books, the most common recommendation to anyone who is familiar with statistics and wants to get into Machine Learning is ""The Elements of Statistical Learning"" by Hastie, Tibshirani, and Friedman.  ";;;
2416;1;2014-11-06T07:38:33.490;Similarity measure for ordered binary vectors;"I would like to ask your opinion on how to choose a similarity measure. I have a set of vectors of length N, each element of which can contain either 0 or 1. The vectors are actually ordered sequences, so the position of each element is important. Suppose I have three vectors of length 10, x_1 x2, x3: x1 has three 1 at positions 6,7,8 (indexes start from 1. Both x2 and x3 have an additional 1, but x2 has it in position 9 while x3 has it in position 1. I am looking for a metric according to which x1 is more similar to x2 than to x3, in that the additional 1 is closer to the ""bulk"" of ones. I guess this is a relatively common problem, but I am confused on the best way to approach it.Many thanks in advance!";[education, open-source];74;
2417;2;2014-11-06T08:52:22.617;;"If speed isn't a great concern you could use a KDE with a high bandwidth to pick up the similarity between neighboring elements, then an appropriate metric like the K-L divergence. Similarity and divergence are complementary, of course, so as a final step you would have to relate them; e.g., sim(A, B) = exp[- KLD(A, B)]";;;
2418;2;2014-11-06T13:56:02.497;;The R Programming Wikibook is a nice collaborative handbook for R.;;;
2419;2;2014-11-06T15:19:35.017;;One thing you could do is fuzzify your vectors: replace each 1 by (for example) 0.4 in its position, 0.2 in the neighbouring positions, and 0.1 in the second position over. Then add up what's in each position. With these fuzzified vectors, you can apply a similarity metric either based on a distance or one like cosines similarity. Your example would produce: (showing only first decimal)0000011100 -> 0001378731  0000011110 -> 00013788731000011100 -> 4211378731cos(x1, x2) = 0.9613,  cos(x1,x3) = 0.9469;;;
2420;2;2014-11-06T17:56:06.257;;"Keep in mind that ""big data"" is an increasingly trendy thing for a company to say they're involved in.  Higher ups might read an article about it in HBR, and say to themselves, ""I've got to get me some of that"" (not that they're necessarily wrong). What this means for you is that the advanced analytics isn't as necessary for that company as just getting something up and running might be. Luckily for you, most of the components said companies might need are free.  Moreover, I believe both Hortonworks and Cloudera have free ""sandbox"" virtual machines, which you can run on your PC, to play around with and get your bearings. Advanced analytics on big data platforms are valuable, to be sure, but many companies need to learn to crawl before they can run. ";;;
2421;2;2014-11-06T21:36:51.897;;This is a really strange question in my opinion. Why you're going to move in a new direction if you are not sure that you love this new direction or at least find it very interesting? If you do love Big Data, why do you care about the PhD intelligent creatures that are already in the field? The same amount of PhD creatures are in every area of IT. Please have a quick read at this very nice article http://www.forbes.com/sites/louisefron/2013/09/13/why-you-cant-find-a-job-you-love/ and then ask yourself if you love Big Data enough and you are ready to add your grain of sand to the mountain of knowledge;;;
2422;1;2014-11-07T04:13:44.583;Clarification about Octave data size limit;I'm just starting to work on a relatively large dataset after ML course in Coursera.Trying to work on https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD.Got an accuracy of 5.2 in training and test set with linear regression using gradient descent in octave.I tried adding all possible quadratic features (515345 instances and 4275 features), but the code just won't stop executing in my HP Pavilion g6 2320tx, with 4GB RAM in Ubuntu 14.04.Is this beyond the data size capacity of Octave ?;[education, open-source];213;
2423;2;2014-11-07T05:53:25.600;;You have about 4GB of RAM on your machine and Octave is an in memory application. If you want to work with 515345 instances and 4275 features, assuming that you are using double precision (i.e. 8 bytes), you would need a memory of 515345*4275*8/1000000/1024 bytes ~ 17.6 GB. Even if you were using 4 bytes for each data point, you would require at least 9 GB for the computation to go through. This issue might not be the Octave memory restriction in this case. See here for further details on Octave's memory usage. ;;;
2424;2;2014-11-07T08:09:40.260;;I would recommend those materials:PythonPython for Data Analysis - book which nicely covers Pandas workflow with IPython.RCoursera Data Science Specialisation - free nine one month time consuming courses which introduces R step by step from beginning to machine learning.Data Camp courses - at least 4 free courses covering topics from data analysis.;;;
2425;2;2014-11-07T09:04:17.603;;Another possibility is the Earth Mover's Distance. I applied it to Computer Vision, but I think that it may be adapted to your specific problem. ;;;
2426;1;2014-11-07T21:36:00.080;Validity of data;"I have a data set that is pivoted in to the following format:[key] [id] [0] [1] [5] [10] [15] [60] [120] [180],.. [365]So key could be[Products] [1000] [15,000] [4000]... etcWhere products is the category of item being reviewed and key is the identifier for the product; the only fields (0, 1,... 180,.. [365]) are individual daily samples identify how many of ""x"" product were logged as either sold, in-stock etc.What I need to do is perform some kind of analysis on an entire slew of products and their inventory levels. i.e. each import of data I need to make sure the incoming data is accurate or predictably accurate and that some human did not typo a stock level. The problem is, using a simple average or rolling average can introduce significant variance and smoothing out the average renders my analysis less reliable. Ideally this analysis would trigger an alarm that someone would have to investigate.Is there a better and more accurate way of performing this analysis?Thanks!";[education, open-source];68;
2427;1;2014-11-08T12:22:17.920;The meaning of multi-class classification rules;The meaning of multi-class classification rulesExample: I have two classification rules (Refund is a predictor and Cheat is a binary response):(Refund, No) → (Cheat, No) Support = 0.4, Confidence = 0.57(Refund, No) → (Cheat, Yes) Support = 0.3, Confidence = 0.43=> multi-class classification rules:(Refund, No) → (Cheat, No) v (Cheat, Yes)When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?;[education, open-source];79;
2428;2;2014-11-08T18:33:37.063;;(Cheat, No) will be selected (cheat will be classified as No) for the rule (Refund, No). In a binary response variable like cheat, all the information can be inferred from just the first rule: (Refund, No) → (Cheat, No) Support = 0.4, Confidence = 0.57 The other rule is redundant. However, in the case of a multi-class response variable, we would like to have all the rules written out so we exactly know the likelihood of the rule implying each of the different classes in the response variable. To keep things consistent, this is also done for the case when the response variable is binary. ;;;
2429;2;2014-11-08T20:29:37.377;;"The first thing you should do is to identify how large an error your analysis can handle. That will make your job much easier because you won't have to find everything.A standard way of identifying ""suspicious"" data is is Benford's Law, which predicts the distribution of the first digit of each number. It can also be generalized for for other digits. http://en.wikipedia.org/wiki/Benford's_lawAs for finding outliers, I'd probably use boxplots, particularly because you can achieve high data density with them, reducing the time to manually skim them.One thing that might be useful is to compare the ratio of one variable to another- in my company we use this method all the time.";;;
2431;2;2014-11-09T10:36:51.290;;"In my experience to have a PhD doesn't mean necessarily be good in the enviroment of data science company, I work as data scientist and I'm just an engineer but I've known some universitary teachers who works in collaboration with my company and sometimes I've said them that Their point of view was not right because despite of their ideas and reasonings were right they are not applicables to the company activities, so we had to modify some data models to make them usefull for the company and the results lost their value so we had to seek new models. What I mean is that Data Science is a multidisciplinar area so many different people working together is needed so I think that your skills could be very useful in a data scientist team, you only have to find where you fit ;)";;;
2432;1;2014-11-09T14:11:18.350;What technologies are fastest at performing joins on large datasets?;"By ""large"", I mean in the range of 100m to 10b rows.I'm currently using both Hadoop MapReduce and Amazon RedShift. MapReduce has been a little disappointing here. Redshift works very well if the data is distributed well for the given query.Are there other technologies that I should be looking at here? If so, what are the trade offs?";[education, open-source];144;1
2433;1;2014-11-10T00:10:26.947;FUZZY ARTMAP for continuous data;I was going through an IEEE Research paper which has used Fuzzy ARTMAP for predicting the price of electricity given some highly correlated data. As per my basic understanding about Fuzzy ARTMAP it is a classification algorithm, so how will it be able to predict continuous data?  The text from research paper is: In the architecture of the FA network, the preprocessing stages take  the input vector and contribute to produce complement coding, which  avoids category proliferation, i.e., the creation of a relatively  large number of categories to represent the training data. A sequence  of input vectors (price and demand) and their respective target  vectors are introduced to the FA network in order to classify the  input pattern correctly. The classiﬁed input patterns are then grouped  into labels using membership functions.I was using MATLAB to implement the same, so is there a library in MATLAB to approach towards the solution. ;[education, open-source];35;
2434;2;2014-11-10T02:15:27.147;;More importantly than the technology is the type of join you are using. For instance if the join keys are sorted, you can use sort merge joins and use join orders to get a better performance. That being said, you can use in memory solutions for fastest joins if the size of your intermediate results will not blow up your cluster memory. Look at Spark SQL or Mem-SQL for instance.;;;
2435;2;2014-11-10T09:40:26.753;;If you are working on collaborative filtering you should pose the problem as a low-rank matrix approximation, wherein both the users are items are co-embedded into the same low-dimensionality space. Similarity search will be much simpler then. I recommend using LSH, as you suggested. Another fruitful avenue for dimensionality reduction not yet mentioned is random projection.;;;
2436;1;2014-11-10T09:45:13.063;Large categorical dataset for regression;I need to collect several large datasets (thousands of samples, dozens of features) for regression with only categorical inputs. I already look for such datasets in the UCI repository, but I did not find any suitable one.Does anybody know of any such dataset, or of any additional dataset repository on the Internet?;[education, open-source];227;
2437;1;2014-11-10T11:13:07.987;"How does the supposed ""Unified Architecture for NLP"" from Collobert and Weston 2008 really works?";"n this paper (here)  they suppose a ""unified architecture for NLP"" with deep neural networks with multitask learningMy problem is to understand the layer architecture in figure 1, see below:Is someone able to give me a concrete, reproducible example how this architecture processing 3 sentences through their layers?What are the outputs after each layer? Why they choose which layer?Thans in advance!";[education, open-source];63;1
2438;2;2014-11-10T11:16:07.537;;You can easily have an RStudio server installed in Digital Ocean using this package:https://github.com/sckott/analogsea;;;
2439;1;2014-11-10T16:20:22.647;Classification of DNA Sequences;I have a database of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other.I want to formulate a supervised classifier.My present approach is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?;[education, open-source];84;1
2441;2;2014-11-10T18:46:42.993;;"You should probably start with a very basic approach: bag-of-words representation (vector as long as your vocabulary, 1 if the word is found in the text, 0 if it's not), and a simple classifier like naive bayes. This works surprisingly well to find topics (a little less for sentiment classification).For preprocessing you would probably want to do stop-word removal and stemming (in order to reduce the vocabulary) rather than POS tagging. The problem with the basic approach is that you would have a n-class classifier, and no ""this fits multiple categories"" or ""this fits 0 categories"" answers. If you want to include that aspect, then the best is to design n 2-class classifiers, one for each of your classes, where each classifier decides whether the text fits the class or not.But I would try out-of-the box naive bayes first, just to see how it works. You can use Weka, it's free, open-source, and can be integrated with java. You can also do the preprocessing (stemming) with the Python NLTK. ";;;
2442;2;2014-11-10T19:27:03.397;;"All you need are data sets with enough records and enough features for your purposes. You can simply convert any continuous variables into categorical ones by grouping. Some sources for large sets can be found by a search for ""large free data sets"". If you are dead set on lots of categorical data, try insurance data (given that I'm an actuary, I should have thought of that earlier). Those tend to be laden with categorical variables, as I well know from first person experience.";;;
2443;2;2014-11-11T08:34:22.210;;May be it will be a little offtopic, but I'd like to highly recommend you to go through this MOOC https://www.coursera.org/course/statistics. This is a very good and clear introduction to statistics. It give you a base principles about core field in data science. I hope it will be a good start point for beginning friendship between you and statistics.;;;
2445;1;2014-11-11T15:03:51.680;Analyzing mobile usage. What kind of approach should I apply?;I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.[date] [CDR/Position] [#calls] [#sms] [#internetConnections]My purpose is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I'd like to find the reason they are similar/dissimilar.What can I apply?;[education, open-source];80;
2446;1;2014-11-11T18:34:40.213;Opening biosemi bdf data using MNE and biosig using python;"I have a biosemi bdf of EEG data which contains 32 channel. I've opened it using biosig, everything works great, a first list is channel and inside each list there are eeg data.But if I open it using MNE it the first list is eeg data, and the second list (inside the list of eeg data) are two list of eeg data.this is how I open the data using MNEraw_file=read_raw_edf(""E:\eeg DATA\\256\s02_reduced.bdf"",preload=True,verbose=True)am I missing something here?";[education, open-source];59;
2447;1;2014-11-11T19:22:00.073;How can I create a custom tag in JPMML?;"I'm trying to create a logistic regression model in jpmml, then write the PMML to a file. The problem I'm having, is that I can't find any way to create a custom tag, such as ""shortForm"" and ""longForm"" in the following example:<MapValues outputColumn=""longForm"">  <FieldColumnPair field=""gender"" column=""shortForm""/>  <InlineTable>    <row><shortForm>m</shortForm><longForm>male</longForm>    </row>    <row><shortForm>f</shortForm><longForm>female</longForm>    </row>  </InlineTable></MapValues>Here's what I have so far:MapValues mv = new MapValues(""output"")  .withFieldColumnPairs(        new FieldColumnPair( new FieldName(""gender""), ""shortForm"" )  ).withInlineTable(        new InlineTable().withRows(                new Row().with???( new ??? ))))In short, I am asking for an API call I can use to instantiate the ""shortForm"" element in the example, and attach it to the ""row"" object. I've been all through the API, examples, and Google/SO, and can't find a thing. Thanks for your help!";[education, open-source];60;1
2448;1;2014-11-12T01:31:01.377;Method to create master product database to validate entries, and enrich data set;"We have a ruby-on-rails platform (w/ postgreSQL db) for people to upload various products to trade. Of course, many of these products listed are the same, while they are described differently by the consumer (either through spelling, case etc.) ""lots of duplicates""For the purposes of analytics and a better UX, we're aiming to create an evolving ""master product list"", or ""whitelist"", if you will, that will have users select from an existing list of products they are uploading, OR request to add a new one. We also plan to enrich each product entry with additional information from the web, that would be tied to the ""master product"".Here are some methods we're proposing to solve this problem:A) Take all the ""items"" listed in the website (~90,000), de-dupe as much as possible by running select ""distinct"" queries (while maintaining a key-map back to original data by generating an array of item keys from each distinct listing in a group-by.)THENA1) Running this data through mechanical turk, and asking each turk user to list data in a uniform format.ORA2) Running each product entry through the Amazon products API and asking the user to identify a match.orA3) A better method?";[education, open-source];20;
2449;2;2014-11-12T03:12:11.550;;There are two straight forward (vanilla) ways without going for any fancy featurization: Clustering:Run a clustering algorithm. Something like k-means should work well with this kind of a dataset. While doing this, I would not feed the day_of_week information into the clustering algorithm. I would suggest running k-means (after normalizing each of the columns). Choose a small number of clusters that is easy to investigate (or you could use the number of clusters that maximizes the BIC). Investigate the clusters to understand membership by day_of_week in each of these clusters. Multi-class Classification:Treat the day_of_week as the response that you would like to predict. Build a decision tree of a fixed depth to predict the day_of_week given the columns. By examining this tree, you can easily tell, which decisions led to a set of leaves being labeled Sunday vs the set of decisions that led to a set of leaves being labeled Monday. These decisions will also help you understand the similarities between different days. ;;;
2450;2;2014-11-12T06:14:15.090;;Try the 1998 KDD Cup dataset. Its a regression problem with categorical and integer predictors. For your task, you could either treat integer predictors as categorical or ignore them completely. ;;;
2451;1;2014-11-12T09:21:19.490;Modeling when the response variable has too many 0's and few continuous values?;For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions. I have two questions regarding such a dataset: What algorithms can we use to ensure that the model not only predicts the fraudulent transactions accurately, but also predicts the value of fraud associated with these.  Assuming that we can quantify the cost involved in each false positive (tagging a non-fraudulent transaction as fraudulent) and cost incurred due to a false negative (tagging a fraudulent transaction as non-fraudulent), how can we optimize the model to maximize savings (or minimize losses)?;[education, open-source];98;
2452;2;2014-11-12T11:08:44.973;;"You can/should use a generic Java Architecture for XML Binding (JAXB) approach.Simply put, call Row#withContent(Object...) with instances of org.w3c.dom.Element that represent the desired XML content.For example:Document document = documentBuilder.newDocument();Element shortForm = document.createElement(""shortForm"");shortForm.setTextContent(""m"");Element longForm = document.createElement(""longForm"");longForm.setTextContent(""male"");row = row.withContent(shortForm, longForm);";;;
2453;2;2014-11-12T14:51:42.213;;If you prefer quick hands-on/interactive tutorials, below are my suggestions - Python - codeacademy, Google Python ClassR - CodeSchool's 'Try R' and DataCamp (suggested above);;;
2454;1;2014-11-12T17:27:57.850;Processing data stored in Redshift;We're currently using Redshift as our data warehouse, which we're very happy with. However, we now have a requirement to do machine learning against the data in our warehouse. Given the volume of data involved, ideally I'd want to run the computation in the same location as the data rather than shifting the data around, but this doesn't seem possible with Redshift. I've looked at MADlib, but this is not an option as Redshift does not support UDFs (which MADlib requires). I'm currently looking at shifting the data over to EMR and processing it with the Apache Spark machine learning library (or maybe H20, or Mahout, or whatever). So my questions are: is there a better way?if not, how should I make the data accessible to Spark? The options I've identified so far include: use Sqoop to load it into HDFS, use DBInputFormat, do a Redshift export to S3 and have Spark grab it from there. What are the pros/cons for these different approaches (and any others) when using Spark?Note that this is off-line batch learning, but we'd like to be able to do this as quickly as possible so that we can iterate experiments quickly.;[education, open-source];187;1
2455;2;2014-11-12T18:22:07.253;;"NLP is very vast and varied. Here are a few basic tools in NLP:Sentence splitting: Identifying sentence boundaries in textTokenization: Splitting a sentence into individual wordsLemmatization: Converting a word to its root form. E.g. says, said,saying will all map to root form - say Stemmer: It is similar to alemmatizer, but it stems a word rather than get to the root form.e.g. laughed, laughing will stem to laugh. However, said, sayingwill map to sa - which is not particularly enlightening in terms ofwhat ""sa"" meansPOS tagger: Tags a word with the Part of Speech - what is a noun, verb, preposition etc.Parser: Links words with POS tags to other words with POS tags. E.g. John ate an apple. Here John and apple are nouns linked by the verb - eat. John is the subject of the verb, and apple is the object of the verb.If you are looking for the state of the art for these tools, check out StanfordCoreNLP, which has most of these tools and a trained model to identify the above from a document. There is also an online demo to check out stanfordCoreNLP before downloading and using it with your application. NLP has several subfields. Here are a few of them: Machine Translation: Automatic Translation from one language to anotherInformation Retrieval: Something like a search engine, that retrieves relevant information from a large set of documents based on a search queryInformation Extraction: Extract concepts and keywords - such as names of people, locations, times, synonyms etc.Deep Learning has lately become a new field of NLP, where a system tries to understand a document like a human understands it. ";;;
2456;2;2014-11-12T18:41:33.053;;"You have a great idea going, and it might work for your specific project. However there are a few considerations you should take into account: In your first sentence, Obama in incorrectly classified as an organization, instead of a person. This is because the training model used my NLTK probably does not have enough data to recognize Obama as a PERSON. So, one way would be to update this model by training a new model with a lot of labeled training data. Generating labeled training data is one of the most expensive tasks in NLP - because of all the man hours it takes to tag sentences with the correct Part of Speech as well as semantic role. In sentence 2, there are 2 concepts - ""Former Vice President"", and ""Dick Cheney"". You can use co-reference to identify the relation between the 2 NNPs. Both the NNP are refering to the same entity, and the same entity could be referenced as - ""former vice president"" as well as ""Dick Cheney"". Co-reference is often used to identify the Named entity that pronouns refer to. e.g. ""Dick Cheney is the former vice president of USA. He is a Republican"". Here the pronoun ""he"" refers to ""Dick Cheney"", and it should be identified by a co-reference identification tool. ";;;
2457;2;2014-11-12T19:00:05.013;;TF IDF will give you the degree of measure of how relevant a document is to your query. owever, to evaluate your IR system you need to use metrics such as - Precision, Recall and F score. Precision: Out of all the documents that your system retrieves, which ones are relevant? This measures how much noise there is in the output of your IR system. Recall: Out of all the documents that are relevant, which ones did your system retrieve? This measures how much coverage does your IR system have? It is possible to get 100% recall all the time by basically retrieving ALL documents from a collection for any query. However, the precision in this case will be very low. It is possible to get a very high precision by hand modeling an IR system ti produce very accurate results. However, it would produce a very bad recall as there will not be coverage over all the documents. So we need to measure F score- which is the harmonic mean between Precision and RecallCheck out Chapter 8 of the Stanford IR book. If you are looking for datasets only here are a few that are relevant: http://zitnik.si/mediawiki/index.php?title=Datasetshttp://www.daviddlewis.com/resources/testcollections/http://boston.lti.cs.cmu.edu/callan/Data/;;;
2458;2;2014-11-12T19:31:27.170;;There are really so many good resources now.  If you want to stay away from textbooks, both O'Reilly Media and Packt Publishing offer much lighter but effective reading on a lot of great topics.  These books are much more applied in practice.  As far as learning the languages go, Coursera, Udacity, Code Acadmey, and Code School have great tutorials.  I would recommend taking a look at the following:Coursera AI and Stats Courseshttps://www.coursera.org/courses?orderby=upcoming&lngs=en&cats=stats,cs-aiUdacity Data Science courseshttps://www.udacity.com/courses#!/data-science;;;
2459;2;2014-11-12T19:35:01.550;;Also, Green Tea Press offers free books on related topics such as an intro to Python and using python with Probability and Stats.  http://www.greenteapress.com/index.html;;;
2462;2;2014-11-13T07:17:18.147;;One way would be to create 20 features (each feature representing a codon). In this way, you would have a dataset with 3190 instances and 20 categorical features. There is no need to treat the sequence as a Markov chain. Once the dataset has been featurized as suggested above, any supervised classifier can work well. I would suggest using a gradient boosting machine as it might be better suited to handle categorical features. ;;;
2463;1;2014-11-13T08:58:25.847;Geospatial Social Network Analysis Visualization;I have a data set that keep tracks of who referred someone to a program and the geo coords of both parties, what will be the best way to visualize this kind of data set. This visualization should also be able to use the geo coordinates to place this entities in the map to form clusters or on a real map.Am interested in Algorithm and/or Library to do this preferable Java, Python, Scala or NodeJS lib.  The record can be a big as thousand or hundreds of thousands. Thanks.;[education, open-source];101;
2464;1;2014-11-13T09:19:22.797;Clustering of documents using the topics derived from Latent Dirichlet Allocation;"I want to use Latent Dirichlet Allocation for a project and I am using Python with the gensim library. After finding the topics I would like to cluster the documents using an algorithm such as k-means(Ideally I would like to use a good one for overlapping clusters so any recommendation is welcomed). I managed to get the topics but they are in the form of:0.041*Minister + 0.041*Key + 0.041*moments + 0.041*controversial + 0.041*PrimeIn order to apply a clustering algorithm, and correct me if I'm wrong, I believe I should find a way to represent each word as a number using either tfidf or word2vec.Do you have any ideas of how I could ""strip"" the textual information from e.g. a list, in order to do so and then place them back in order to make the appropriate multiplication?For instance the way I see it if the word Minister has a tfidf weight of 0.042 and so on for any other word within the same topic I should be to compute something like:0.041*0.42 + ... + 0.041*tfidf(Prime) and get a result that will be later on used in order to cluster the results.Thank you for your time.";[education, open-source];279;1
2466;1;2014-11-14T08:53:21.510;Dimension reduction for logical arrays;"I have measurements of 4 devices at two different points of time. A measurement basically consists of an array of ones and zeros corresponding to a bit value at the corresponding location:whos measurement1_dev1_time1Name                         Size               Bytes  Class      Attributesmeasurement1_dev1_time1      4096x8             32768  logicalI assume that for a specific device the changes between time 1 and 2 of the measurements are unique. However, since I am dealing with 32768 bits at different locations, it is quite hard to visualize if there is some kind of dependency. As every bit at location xcan be regarded as one dimension of an observation I thought to use PCA to reduce the number of dimensions.Thus, for every of the 5 devices:I randomly sample n measurements at point t1and t2 seperatlyI prepare an array as input for pca() with m*n columns (m< 32768; its a subset of all the observed bits, as the original data might be too big for pca) and 4 rows (one row for each device).On this array A I calculate the pca: ``[coeff score latent] = pca(zscore(A))```Then I try to visualize it using biplot: biplot(coeff(:,1:2), 'score', score(:,1:2))However, this gives me really strange results. Maybe PCA is not the right approach for this problem? I also modified the input data to do the PCA not on the logical bit array itself. Instead, I created a vector, which holds the indices where there is a '1' in the original measurement array. Also this produces strange results.As I am completely new to PCA I want to ask you if you either see a flaw in the process or if PCA is just not the right approach for my goal and I better look for other dimension reduction approaches or clustering algorithms.";[education, open-source];26;
2468;2;2014-11-14T15:15:20.617;;"Storing user profilesIf you just want to store all user profiles... just save them into normal RDBMS. Assuming one user profiles takes 10Kb of storage, you need only ~9.5Gb for every million of users, which is pretty little and gives you all advantages of mature relational databases. It makes sense to use HBase only when you have really many users (say, > 1B) or when data is very sparse (most columns are empty). But don't expect it to be as convenient as good old SQL databases. In advertising, and especially in real-time bidding, very fast retrieval of user profiles is needed. Aerospike becomes more and more popular for this task.Analysing data slicesCommon use of business logs is to analyse specific slices of data, e.g. number of users from France that visited sites from ""game"" category on November 1-14, 2014. Standard way to manage such data efficiently is to organize them into data cubes. You won't get individual records (e.g. users), but you'll get aggregated statistics really fast. Such cubes may have many different dimensions, but in 99% of cases they have date field that they are partitioned by. It makes great sense, because almost every query includes time period to get data from. As for software, Vertica is great for such aggregations. Cheaper* solution from Hadoop world is Impala, which is also great. (* - if you count only license price)Machine learningIt really depends on concrete tasks and ML toolkit in use. For real-time bidding you would want blazing fast access to user profile vectors and would probably prefer Aerospike. For online learning Spark Streaming may be used as a data source, and no storage used at all. For offline machine learning there's excellent MLlib from the same Spark project, which works with a variety of sources. ";;;
2469;1;2014-11-14T15:20:44.463;survey data analysis (discrete data);"I did small survey and get such data:|-------------| Yes | No | Dont_Know |  |-------------|     |    |           |  | Employee    | 60  | 5  | 5         |  | Workers     | 17  | 0  | 1         |  | Businessmen | 71  | 5  | 10        |  | Jobless     | 4   | 30 | 0         |  R codedt <- data.frame(workers = c(""Employee"",                             ""Workers"",                              ""Businessmen"",                              ""Jobless""),                  yes = c(60,17,71,4),                  no = c(5,0,5,30),                  dont_know = c(5,1,10,0)                )What kind of test I must do, if I want to show, that the Jobless people are often choosing No answer?   Is the difference between Jobless and Businessmen answers significant? And what is about other groups?  What another information I can get from such data or what kind questions I can ask from such data?";[education, open-source];70;
2470;1;2014-11-14T16:39:29.663;Non-linear transformations input dataset for support vector machines;I have two classes (A,B) that I want to classify using a SVM. Say that I have a class C and a function f. Can I do this:A' =  f(A,C) = |A-C|B' =  f(B,C) = |B-C|and then perform the classification on A' and B' instead? In the context of my problem A and B are classes where elements are vectors. The f function measures the Mahalanobis distance of each vector with respect to the distribution imposed by C.;[education, open-source];43;
2471;2;2014-11-14T17:11:13.060;;I would definitely use a graph (Though, this clearly depends on the final application, maybe you could add more information) For the nodes, you should consider as nodes not only tanks but also points were pipelines change name or bifurcate. For instance, following your example:    e1     e2        e3          e4        e5          e7               +---+----------.---------+----------.----------+-----------          |          |         |                     |          |      [R tank 1]  [S tank 1] [S tank 2]              |e6     [S tank 3]                                               |                                                            [S tank 4]            Now adding the nodes:      e1 n2  e2     n4   e3   n6   e4    n8  e5     n9   e7           n1 +---+----------.---------+----------.----------+----------+         |          |         |                     |              |        n3         n5        n7                     |e6           n11                                                   |                                                              n10                Lastly, you need some kind of mapping. Some of the nodes will map to tanks:[R tank 1]     n3                                                   [S tank 1]     n5                                                   [S tank 2]     n7                                                   [S tank 4]     n10                                                  [S tank 3]     n11                                                  And the pipelines will be represented by paths in the graphPipeline 1     e1 e2                                                Pipeline 2     e3 e4                                                Pipeline 3     e5 e7                                                Pipeline 4     e6                                                   ;;;
2472;2;2014-11-14T17:32:26.050;;You are just predicting if Play = Yes or Play = No.The confusion matrix would look like this:             Predicted          +------+------+          |  Yes |  No  |    +-------------------+A   |     |      |      |c   | Yes |  TP  |  FP  |t   |     |      |      |u   +-------------------+a   |     |      |      |l   | No  |  FN  |  TN  |    |     |      |      |    +-----+------+------+TP: True positivesFP: False positives FN: False negatives TN: True negativesThe accuracy can then be calculated as (TP + TN)/(TP + FP + TN + FN). ;;;
2473;1;2014-11-14T19:03:35.603;Choosing the right data mining method to find the effect of each parameter over the target;I am dealing with a lot of categorical data right now and I would like to use an appropriate data mining method in any tool [preferably R] to find the effect of each parameter [categorical parameters] over my target variable. To give a brief notion about the data that am dealing with, my target variable denotes the product type [say, disposables and non-disposables] and I have parameters like root cause,symptom,customer name, product name etc. As my target can be considered as a binary value, I tried to find the combination of values leading to the desired categories using Apriori but, I have more than 2 categories in that attribute and I want to use all of them and find the effect of the mentioned parameters over each category. I really wanted to try SVM and use hyperplanes to separate the content and get n-dimensional view. But, I do not have enough knowledge to validate the technique, functions am using to do the analysis. Currently I have like 9000 records and each of them represents a complaint from the user. There are lot of columns available in the dataset which is what I am trying to use to determine the target variable [ myForumla <- Target~. ] I tried with just 4 categorical columns too. Not getting a proper result.Can just the categorical variables be used to develop a SVM model and get visualization with n hyper planes? Is there any appropriate data mining technique available for dealing with just the categorical data?;[education, open-source];149;
2474;1;2014-11-14T22:48:10.173;Parsing data from a string;"I think this is something that experienced programmers do all the time. But, given my limited programming experience, please bear with me.I have an excel file which has particular cell entries that read [[{""from"": ""4"", ""response"": true, ""value"": 20}, {""from"": ""8"", ""response"": true, ""value"": 20}, {""from"": ""9"", ""response"": true, ""value"": 20}, {""from"": ""3"", ""response"": true, ""value"": 20}], [{""from"": ""14"", ""response"": false, ""value"": 20}, {""from"": ""15"", ""response"": true, ""value"": 20}, {""from"": ""17"", ""response"": false, ""value"": 20}, {""from"": ""13"", ""response"": true, ""value"": 20}]]Now, for each such entry I want to take the information in each of the curly brackets and make a row of data out of it. Each such row would have 3 columns. For example, the row formed from the first entry within curly brackets should have the entries ""4"" ""true"" and ""20"" respectively. The part I posted should give me 6 such rows, and for n such repetitions I should end up with a matrix of 6n rows, and 4 columns ( an identifier, plus the 3 columns mentioned).What would be most efficient way to do this? By ""doing this"" I mean learning the trick, and then implementing it. I have access to quite a few software packages(Excel, Stata, Matlab, R) in my laboratory, so that should not be an issue.";[education, open-source];65;
2475;2;2014-11-14T23:55:51.953;;This lies in the area of . Online algorithms are specially suited for very large scale tasks where it is impractical to run an algorithm over all the data at the same time. So we run the algorithm piecemeal, and observe the changing trends in teh data. Other examples it can be used for:1) topic detection - observing the topics change with time2) Clustering - observing clusters change with time;;;
2476;2;2014-11-15T01:26:16.583;;"Here are some things to try.Plot a bar graph. The bar graph will clearly show jobless people are often choosing NO. Try an 1-way ANOVA test. If the p < delta (i.e. delta=0.05), try a post-hoc test (i.e. Tukey's HSD) to do a pairwise comparison.Like I said earlier, try a multiple comparison test (1-way ANOVA) first, if there is a statistically significant difference, you can try a pairwise comparison test (post-hoc test).Maybe try a clustering algorithm? Be careful, because the marginal sums (by rows or columns) are not equal. Maybe create a similarity matrix by profession? To me, it seems that Employees and Businessmen are in one group (very similar), while Workers and Jobless are each in their own group. If you turn those frequencies into proportions, then you might just have 2 groups; one for employees + workers + businessmen, and one for jobless.Use contingency table analysis to see if the responses (yes/no/don't know) are associated with profession. ";;;
2477;2;2014-11-15T01:30:00.870;;How aboutOrdinary Least Square (OLS) regression? Since you have a class imbalance, you might want to combine that with boosting algorithms.If you have a function to quantify the cost involved with FP's and FN's, use any optimization technique you can find. My favorite is genetic algorithms. You may also try linear programming. ;;;
2478;2;2014-11-15T03:18:13.223;;"Data volume is not the only criterion for using Hadoop. Big Data is often characterized by the 3 V's: volume, velocity, and variety. More V's than these 3 have been invented since. I suppose the V's were a catchy way to characterize what is Big Data.But as hinted, computational intensity is a perfect reason for using Hadoop (if your algorithm is computationally expensive). And, as hinted, the problem you describe is perfect for Hadoop, especially since it is embarrassingly parallel in nature. Is Hadoop a good choice for you? I would argue, yes. Why? Because Hadoop is open source (compared with proprietary systems which may be expensive and black boxes), your problem lends itself well to the MapReduce paradigm (embarassingly parallel, shared-nothing), Hadoop is easily scalable with commodity hardware (as opposed to specialized hardware, and you should get linear speed-up in your performance with just throwing hardware at the problem, and you can just spin a cluster as needed on cloud service providers),Hadoop allows multiple client languages (Java is only one of many supported languages),there might be a library available already to do your cross-product operation, andyou're shipping compute code, not data, around the network (which you should benefit from, and as opposed to other distributed platforms where you are shipping data to compute nodes which is the bottleneck).Please note, Hadoop is not a distributed file system (as mentioned, and corrected already). Hadoop is distributed storage and processing platform. The distributed storage component of Hadoop is called the Hadoop Distributed File System (HDFS), and the distributed processing component is called MapReduce. Hadoop has now evolved slightly. They keep the HDFS part for distributed storage. But they have a new component called YARN (Yet Another Resource Negotiator), which serves to appropriate resources (CPU, RAM) for any compute task (including MapReduce). On the ""overhead"" part, there is noticeable overhead with starting/stopping a Java Virtual Machine (JVM) per tasks (map tasks, reduce tasks). You can specify for your MapReduce Jobs to reuse JVMs to mitigate this issue. If ""overhead"" is really an issue, look into Apache Spark, which is part of the Hadoop ecosystem, and they are orders of magnitude faster than MapReduce, especially for iterative algorithms. I have used Hadoop to compute pairwise comparisons (e.g. correlation matrix, similarity matrix) that are O(N^2) (n choose 2) in worst case running time complexity. Imagine computing the correlations between 16,000 variables (16,000 choose 2); Hadoop can easily process and store the results if you have the commodity resources to support the cluster. I did this using the preeminent cloud service provider (I won't name it, but you can surely guess who it is), and it cost me < $100 and under 18 hours.  ";;;
2479;2;2014-11-15T09:34:47.653;;You can try Bayesian belief networks (BBNs). BBNs can easily handle categorical variables and give you the picture of the multivariable interactions. Furthermore, you may use sensitivity analysis to observe how each variable influences your class variable. Once you learn the structure of the BBN, you can identify the Markov blanket of the class variable. The variables in the Markov blanket of the class variable is a subset of all the variables, and you may use optimization techniques to see which combination of values in this Markov blanket maximizes your class prediction.;;;
2480;1;2014-11-15T15:49:07.213;Visualization using D3;I am new to D3 programming (any programming, for that matter). I have protein-protein interaction data in JSON format and csv format. I would like to use that data for network visualization.Data attributes: Protein Name, Protein Group, Protein type, Protein Source Node, Protein Target NodeCan anyone suggest good network visualizations for such data. How does it work with hive plots?;[education, open-source];102;
2482;2;2014-11-16T01:05:02.603;;Why dont you have a look at the following example?http://bl.ocks.org/mbostock/2066421You can also find a fiddle here: http://jsfiddle.net/boatrokr/rk2s5/Pay attention to the part where the links are defined.;;;
2483;2;2014-11-16T01:13:21.363;;"If you have R is quite simpleCopy the lines into a file, let's say: ""mydata.json""Be sure you have installed the rjson package install.packages(""rjson"")Import your data library(""rjson"") json_data <- fromJSON(file=""mydata.json"")";;;
2486;1;2014-11-16T21:03:09.037;Visualizing Support Vector Machines (SVM) with Multiple Explanatory Variables;I was wondering if anyone was aware of any methods for visualizing an SVM model where there are more than three continuous explanatory variables. In my particular situation, my response variable is binomial, with 6 continuous explanatory variables (predictors), one categorical explanatory variable (predictor). I have already reduced the number of predictors and I am primarily using R for my analysis.(I am unaware if such a task is possible/ worth pursuing.)Thanks for your time.;[education, open-source];166;1
2487;1;2014-11-17T11:54:39.933;Ontology database;"this is my first ever stack exchange question.I'm trying to build a tool right now and one of the features of the tool is the ability to break down a product or service into it's associated attributes/properties/classes/keywords/entities. (Choose which word best suits, as I have no idea).For example if we had a Camera as the product. I would like to be able to generate a breakdown of everything that is associated to a camera. Such as;Digital, Film, Optical, LCD, Glass, CCD, CMOS, RGB, Lens, Shutter, Negative, Polaroid, Darkroom, Flash, Resolution, Stabilisation, Batteries, Zoom, Angle, Telephoto, Macro, Filters, Memory, CF, SDThe list could go on for quite some time, those were jsut a few off the top of my head.How on earth could I go about retrieving such attributes automatically? Is there a database out there that has such info? Are there any special tricks anyone has up their sleeve to be able to accumulate datasets such as the example above?Very interested in your answers.Thanks :)";[education, open-source];43;
2488;2;2014-11-18T01:26:51.900;;"It seems to me as if a good starting point would be to read up on the semantic web, perhaps starting with DBpedia and maybe LinkedData. You could go from there and build up your own database.Example of a SPARQL query starting with the DBpedia page of 'Camera':select ?label where {  ?prod dbpedia-owl:product :Camera .  ?prod dcterms:subject ?categories .  ?entity dcterms:subject ?categories.  ?entity rdf:type yago:PhysicalEntity100001930 .  ?entity rdfs:label ?label .  filter langMatches( lang(?label), 'en').}Generating a lot of words somehow related to 'Camera'....""Shutter button""@en""Rangefinder camera""@en""Still camera""@en""Lomo LC-A""@en""Flexaret""@en""Land Camera""@en""Robot (camera)""@en""Speed Graphic""@en""Ansco Panda""@en""Image trigger""@en""Still video camera""@en""Hidden camera""@en""Mainichi Shimbun""@en""Ōhiradai Station""@en""Depth-of-field adapter""@en""Banquet camera""@en""Digital versus film photography""@en""Fernseh""@en""Remote camera""@en""Professional video camera""@en....The above result is just an excerpt.";;;
2489;1;2014-11-18T09:07:00.867;Reduction of multiple answers to single variable;The questionnaire for the data is hereThe first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?The clean data is available here.NB: The Column CompuPlat has missing values.part of datasetCMFam CMHobb  CMNone  CMOther CMPol   CMProf  CMRel0   0   1   0   0   0   00   0   0   0   0   0   01   1   0   0   0   1   00   0   0   1   0   0   00   0   0   0   1   1   01   0   0   0   0   1   1Community Membership_FamilyCommunity Membership_HobbiesCommunity Membership_NoneCommunity Membership_OtherCommunity Membership_PoliticalCommunity Membership_ProfessionalCommunity Membership_ReligiousCommunity Membership_SupportI want to club all of them in a variable CM;[education, open-source];115;1
2490;2;2014-11-18T09:48:19.333;;Your dataset can be viewed as a directed graph. The party's location (latitude and longitude) can be denoted as a node and the directed edge can denote who referred whom. Once the dataset can be viewed as this, the problem boils down to joining co-ordinates with lines. ;;;
2491;1;2014-11-18T09:56:06.157;Change aliases of filter items in Tableau;Data sample contains a single feature: random integer number from 1 to 4.Is it possble to change 1,2,3,4 representation on the filter card to some custom names, say: Type1,Type2,Type3,Type4? (not changing data set);[education, open-source];167;
2492;2;2014-11-18T10:19:59.510;;"The variable represents the answer to the first question. One straightforward way is to allow for all possible categories in this variable. For example, if there are 5 options in this answer, you will have to treat it as a categorical variable with 2^5 = 32 categories. However, the number of categories increase exponentially with the number of options (check boxes) provided for the answer. In that case, it might be better to restrict the number of categories to, for example, 5. This can be done by leaving the top 4 choices/ options (by count) as they are and treating every other choice as ""other"".";;;
2493;1;2014-11-18T11:04:28.837;Analyze paragraphs using Neuroph;Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.Any ideas or starting point on how to do this using Neuroph or maybe in other framework/approaches?Edit for more info as suggested.I have very little experience on neural networks though I have some introduction with it in college. However I am very much familar with javaThe data is around 3 megabytes only and consists of rules and relationships for a single domain. This means that the data is complex but relatively limited though still free-form English language.;[education, open-source];65;
2494;1;2014-11-18T12:41:54.973;What is the best practice to classify category of named entity in sentence;I have 1-4 gram text data from wikipedia for 14 categories, which I am using for NE classification.I feed named entity from sentence to lucene indexer which searches named entity from these 14 categories. Issue I am facing is, for single entity I get multiple classes as a result with same score.like while search titanic, indexer gives this resultScore    - 11.23Title    - titanicCategory - BookScore    - 11.23Title    - titanicCategory - MovieScore    - 11.23Title    - titanicCategory - Productnow problem is which class to be considered?I already tried with classifiers (NB,ME in nltk,scikit learn), but as it consider each entity from dataset as feature, it works as indexer only.Why lucene?;[education, open-source];102;
2495;1;2014-11-18T14:03:26.760;How to run R programs on multicore using doParallel package?;I am running SVM algorithm in R.It is taking long time to run the algorithm.I have system with 32GB RAM.How can I use that whole RAM memory to speed my process.;[education, open-source];190;
2496;2;2014-11-18T19:52:54.050;;Does it matter that the model is created in the form of SVM?If no, I have seen a clever 6-D visualization. Its varieties are becoming popular in medical presentations. 3 dimensions are shown as usual, in orthographic projection.Dimension 4 is color (0..255)Dimension 5 is thickness of the symbol Dimension 6 requires animation. It is a frequency of vibration of a dot on the screen. In static, printed versions, one can replace frequency of vibration by blur around the point, for a comparable visual perception.If yes, and you specifically need to draw separating hyperplanes, and make them look like lines\planes, the previous trick will not produce good results. Multiple 3-D images are better.;;;
2499;1;2014-11-19T01:07:10.283;API for historical housing prices;I'm looking for an (ideally free) API that would have time series avg/median housing prices by zip code or city/state. Quandl almost fits the bill, but it returns inconsistent results across different zip codes and the data is not as up to date as I'd like (it's mid November, and the last month is August).I also looked at Zillow, but storing their data is against TOS, and at 1,000 calls daily--it would take forever to pull in the necessary data.Any suggestions (even if they aren't free) would be much appreciated!;[education, open-source];82;
2500;1;2014-11-19T09:26:58.700;Recommendation engine with mahout;I have a list user data: user name, age, sex, address, location etc., and a set of product data: Product name, Cost, description etc.Now I would like to build a recommendation engine that will be able to:1 Figure out similar productseg :name  :   category   :  cost    :   ingredientsx     :     x1   :        15  :       xx1, xx2, xx3y     :    y1   :        14   :     yy1, yy2, yy3z     :    x1  :          12   :     xx1, xy1 here x and z are similar.2 Recommend relevant products from the product list to a userHow can I implement this using mahout?;[education, open-source];282;
2501;1;2014-11-19T12:38:05.597;Data scheduling for recommender;I do at the moment some data experiments with the Graphlab toolkit. I have at the first next SFrame, with the three columns:Users Items RatingThe pair in the same row from every Users and Items values build the unique key and the Rating is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)Take the logarithm by every rating valueAfterward I create a recommender model and evaluate the basic metrics for it.In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.PSMy dataset is comming from some music site, the users rated some tracks. I have approximately 100 000 users and 300 000 tracks. Total number of ratings is over 3 millions (actually the matrix is sparse). This is the most simple data set, which I analyze now. In the future I can (and will) use some additional information about the users and tracks (f.e. duration, year, genre, band etc). At the moment I just interest to collect some methods for rating normalisation without to use additional information (users & items features). My problem is, the data set doesn't have any Rating at the first. I create someself the column Rating, based on the number of events for unique User-Item pair (I have this information). You can of course understand that some users can hear some tracks many times, and another users only one time. Consequently the dispersion is very high and I want to reduce it (normalise the ratings value).;[education, open-source];98;
2502;1;2014-11-19T19:25:02.770;Good-Turing Smoothing Intuition;"I'm working through the Coursera NLP course by Jurafsky & Manning, and the lecture on Good-Turing smoothing struck me odd.The example given was: You are fishing (a scenario from Josh Goodman), and caught:  10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish  ...  How likely is it that the next species is new (i.e. catfish or bass)  Let's use our estimate of things-we-saw-once to estimate the new things.  3/18 (because N_1=3)I get the intuition of using the count of uniquely seen items to estimate the number of unseen item types (N = 3), but the next steps seem counterintuitive.Why is the denominator left unchanged instead of incremented by the estimate of unseen item types? I.e., I would expect the probabilities to become: Carp : 10 / 21  Perch : 3 / 21  Whitefish : 2 / 21  Trout : 1 / 21  Salmon : 1 / 21  Eel : 1 / 21  Something new : 3 / 21It seems like the Good-Turing count penalizes seen items too much (trout, salmon, & eel are each taken down to 1/27); coupled with the need to adjust the formula for gaps in the counts (e.g., Perch & Carp would be zeroed out otherwise), it just feels like a bad hack.";[education, open-source];107;1
2503;1;2014-11-20T05:38:27.303;Creating Data model for mahout recommendation engine;I am trying to build an item-item similarity matching recommendation engine with mahout. The data set is as in the following format ( attributes are in text not in numerals format )name : category : cost : ingredientsx : xx1 : 15 : xxx1, xxx2, xxx3y : yy1 : 14 : yyy1, yyy2, yyy3z : xx1 : 12 : xxx1, xxy1So in-order to use this data set for mahout to train, what is the right way to convert this in to numeric (as CSV Boolean data set) format accepted by mahout.;[education, open-source];79;1
2504;1;2014-11-20T06:49:00.357;Deep Learning vs gradient boosting: When to use what?;I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50. I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? ;[education, open-source];817;1
2505;2;2014-11-20T09:33:20.320;;Have you looked into tourr package in R. This package does hyperplane reduction. In addition it has an optimizer that tries to find the best reduction.There is a very nice video in https://www.youtube.com/watch?v=iSXNfZESR5I That shows what  R is capable even beyound tourr package. Also I refer you to http://stackoverflow.com/questions/8017427/plotting-data-from-an-svm-fit-hyperplane;;;
2506;2;2014-11-20T13:07:45.903;;Just go through Neo4j (graph data base and will be useful for social network analysis) also.. may be helpful ;;;
2507;1;2014-11-20T14:26:10.463;Log file analysis: extracting information part from value part;"I'm trying to build a data set on several log files of one of our products.The different log files have their own layout and own content; I successfully grouped them together, only one step remaining...Indeed, the log ""messages"" are the best information. I don't have the comprehensive list of all those messages, and it's a bad idea to hard code based on those because that list can change every day.What I would like to do is to separate the indentification text from the value text (for example: ""Loaded file XXX"" becomes (identification: ""Loaded file"", value: ""XXX"")). Unfortunately, this example is simple, and in real world there are different layouts and sometimes multiple values.I was thinking about using string kernels, but it is intended for clustering ... and cluseting is not applicable here (I don't know the number of different types of messages and eventhough, it would be too much).Do you have any idea?Thanks for your help.P.S: For those who programs, this can be easier to understand. Let's say that the code contains as logs printf(""blabla %s"", ""xxx"") -> I would like to have ""blabla"" and ""xxx"" seperatated";[education, open-source];303;
2508;1;2014-11-20T15:02:17.937;How to run R scripts without closing X11;I would like to run an R script using a single command (e.g. bat file or shortcut).This R script asks the user to choose a file and then plots information about that file. All is done via dialog boxes.I don't want the user to go inside R - because they don't know it at all.So, I was using r cmd and other similar stuffs, but as soon as the plots are displayed, R exits and closes the plots.What can I do?Thanks for your help.;[education, open-source];62;
2509;2;2014-11-20T16:28:48.343;;How about considering each string as a process trace and applying alpha-algorithm?That would give you a graph and nodes with a big number out-edges will most likely point to values.You can mark these nodes and for every new string parse/traverse the graph until you reach those areas.;;;
2510;1;2014-11-20T17:54:11.990;Graduate Degree Choices for Data Science;I'm currently finishing up a B.S. in mathematics and would like to attend graduate school (a master's degree for starters, with the possibility of a subsequent Ph.D.) with an eye toward entering the field of data science. I'm also particularly interested in machine learning.What are the graduate degree choices that would get me to where I want to go?Is there a consensus as to whether a graduate degree in applied mathematics, statistics, or computer science would put me in a better position to enter the field of data science?Thank you all for the help, this is a big choice for me and any input is very much appreciated. Typically I ask my questions on Mathematics Stack Exchange, but I thought asking here would give me a broader and better rounded perspective.;[education, open-source];271;2
2511;2;2014-11-20T21:11:12.187;;"I would add a comment but I do not have enough reputation points. I might suggest Using ""R revolution open"". It is a Build of R that includes a lot of native support for multi-core processing. I have not used it much as my computer is very old, but it is defiantly worth looking at. Plus it is free. ";;;
2512;2;2014-11-21T10:24:18.623;;UCL - CSML. It covers computer science, machine learning and statistics.Firstly, reputation of the university.Secondly, you are from Mathematics background, hence I assume you don't have sufficient programming knowledge.Thirdly, Statistics and Machine Learning dominates this field. Employers would prefer these 2 before Mathematics.In short, this course provides everything that you are lacking. HOWEVER, they don't teach programming languages like Java, C++,... but Matlab, R, and Mathematica. Hence, it would be essential if you pick up the former from somewhere.;;;
2513;1;2014-11-21T10:34:31.673;Please enlighten me with Platt's SMO algorithm (for SVM);From A_Roadmap_to_SVM_SMO.pdf, pg 12.Assume I am using linear kernel, how will I be able to get both the first and second inner product?My guess, inner product of datapoint with datapoint j labelled class A for the first inner product of the equation and inner product of datapoint j with datapoints labelled class B for second inner product?;[education, open-source];59;1
2514;2;2014-11-21T10:44:06.783;;There are no unseen item types in the given data, by definition. 3 is the count of items seen once, and they are already included in the denominator 18. If the next item were previously unseen, it would become seen once when it appears. Since 3-of-18 examples were seen-once items, this is an estimate of the probability that the next item will be seen-once too on its first appearance.It is certainly a heuristic. There is no way to know whether there are 0 or 1000 other types out there.;;;
2515;2;2014-11-21T11:50:47.717;;"First of all, word ""sample"" is normally used to describe subset of population, so I will refer to the same thing as ""example"". Your SGD implementation is slow because of this line: for each training example i:Here you explicitly use exactly one example for each update of model parameters. By definition, vectorization is a technique for converting operations on one element into operations on a vector of such elements. Thus, no, you cannot process examples one by one and still use vectorization. You can, however, approximate true SGD by using mini-batches. Mini-batch is a small subset of original dataset (say, 100 examples). You calculate error and parameter updates based on mini-batches, but you still iterate over many of them without global optimization, making the process stochastic. So, to make your implementation much faster it's enough to change previous line to: batches = split dataset into mini-batchesfor batch in batches: and calculate error from batch, not from a single example. Though pretty obvious, I should also mention vectorization on per-example level. That is, instead of something like this: theta = np.array([...])  # parameter vectorx = np.array([...])      # exampley = 0                    # predicted responsefor i in range(len(example)):    y += x[i] * theta[i]error = (true_y - y) ** 2  # true_y - true value of responseyou should definitely do something like this: error = (true_y - sum(np.dot(x, theta))) ** 2which, again, easy to generalize for mini-batches:true_y = np.array([...])     # vector of response valuesX = np.array([[...], [...]]) # mini-batcherrors = true_y - sum(np.dot(X, theta), 1)error = sum(e ** 2 for e in errors)";;;
2516;1;2014-11-21T15:23:37.360;Relation mining of multivariant categorical timeseries without excluding the temporal nature;To all:I have been wracking my brain at this for a while and thought maybe someone here would know of a package or algorithm to handle the following:I have nominal multivariant timeseries that look like the following:          Time Var1 Var2 Var3 Var4 Var5 ... VarN             0     A     A   B    C    A   ... H             1     A     A   B    D    D   ... H             2     B     A   C    D    D   ... H             ..And so on from times 0 to 1,000,000. What I would like to do is search the time series for rules of the type:Given Var3 is in state B in the previous step and Var5 is in state D in the previous step, than Var1 will be in state B. What I want to do is have the rules that include the time interval explicitly. A simpler case of interest would simply be to reduce the time series to                Time    Var1 Var2 Var3 Var4 Var5 ... VarN                0        0    0    0     0   0   ... 0                1        0    0    0     1   1   ... 0                2        1    0    1     0   0   ... 0Where the the variable is 1 if its state is different from the previous step and zero otherwise. Then I just want to have rules that say something like:If Var4 and Var5 changed in the previous step than Var1 will change in the current step. Which would be easy for a lag of one, as I could just make the data into something like:   Var1 Var2 Var3 Var4 Var5 ... VarN Var1_t-1 Var2_t-1 Var3_t-1 ...and then do sequence mining, but if I want to have rules that aren't just a single lag but could be lags from 1 to 500 than my data set begins to be a little difficult to work with. Any help would be greatly appreciated. Edit to respond to comment:Each column could be in one of 7 different states. As far as a target, it is non-specific, any rules between the columns would be of interest. However, predicting columns 30-40 and 62-75 would be particularly interesting.;[education, open-source];58;
2517;2;2014-11-21T18:55:07.860;;Dimension reduction (like PCA) is an excellent way to visualize the results of classification on a high-dimensional feature space.The simplest approach is to project the features to some low-d (usually 2-d) space and plot them. Then either project the decision boundary onto the space and plot it as well, or simply color/label the points according to their predicted class. You can even use, say, shape to represent ground-truth class, and color to represent predicted class.This is true for any categorical classifier, but here's an SVM-specific example: http://www.ece.umn.edu/users/cherkass/predictive_learning/Resources/Visualization%20and%20Interpretation%20of%20SVM%20Classifiers.pdfIn particular, see figures 1a and 2a.;;;
2518;2;2014-11-21T21:33:06.777;;Try using the item-based similarity algorithm available under Apache Mahout. It is easy to implement and you will have a good sense how the recommendation system for your data set will work. You could provide ingredients and category as the major inputs to get the similar products.As a neophyte to this field, I would say that this approach is an easy way for all the neophytes to get a good heads up of what kind of a result one can expect from building a recommendation system of their own.   ;;;
2519;2;2014-11-21T21:39:29.617;;This looks like a similar kind of a problem.Solutions: (taken from above source)Just sleep via  Sys.sleep(10) which would wait ten seconds.Wait for user input via readLines(stdin()) or something like that [untested]Use the tcltk package which comes with R and is available on all platforms to pop up a window the user has to click to make the click disappear. That solution has been posted a few times over the years on r-help.2nd option is better to use for the user.P.S. Since I did not come up with the answer myself, I tried to put it in comment but my reputation is too low for that.;;;
2520;2;2014-11-22T08:10:40.480;;"Every field has their own variation of ""data science,"" so I would suggest choosing a subject that interest you and going from there.  I can't offer what the go to subject is for your particular interest.  A graduate degree that would ""get you where you want to go"" is quite a personal understanding, so I can' answer that.  But what I will say is, from my own personal experience, when I graduated with my undergrad degree in economics, I was really interested in data science, and economics allowed me to use data science in a field I'm really interested in.  So I applied to Ph.D programs to further my knowledge and am using data science extensively in many different forms of analysis.  My suggestion is to apply to graduate degrees that have interesting subject matter to you and will allow you to use data science as understanding.  You would fit well in an economics degree because of your background :)";;;
2521;1;2014-11-22T11:54:54.780;DBPedia as Table not having all the properties;"I browsed a sample for available data at http://dbpedia.org/page/Sachin_Tendulkar. I wanted these properties as columns, so I downloaded the CSV files from http://wiki.dbpedia.org/DBpediaAsTables.Now, when I browse the data for the same entity ""Sachin_Tendulkar"", I find that many of the properties are not available. e.g. the property ""dbpprop:bestBowling"" is not present.How can I get all the properties that I can browse through the direct resource page.";[education, open-source];87;
2522;2;2014-11-22T16:08:49.147;;Why not do an MSc in ooh... Data Science?I wrote a quick review of UK Data Science Masters' offerings recently. That should help you get an idea what is offered. Mostly they are mashups of stats and computing, but there are specialisms (health, finance for example) that might interest you.Note that list was compiled for courses that have already started, so some of those courses might not be available for starting next October, or have different syllabus contents.;;;
2523;1;2014-11-23T01:05:06.473;Relationship between KS, AUROC, and Gini;Common model validation statistics like the Kolmogorov–Smirnov test (KS), AUROC, and Gini coefficient are all functionally related. However, my question has to do with proving how these are all related. I am curious if anyone can help me prove these relationships. I haven't been able to find anything online, but I am just genuinely interested how the proofs work. For example, I know Gini=2AUROC-1, but my best proof involves pointing at a graph. I am interested in formal proofs. Any help would be greatly appreciated!;[education, open-source];189;1
2524;2;2014-11-23T02:26:42.470;;"The official answer:Date: Sun, 23 Nov 2014 03:08:19 +0100From: Petar Ristoski <petar.ristoski@informatik.uni-mannheim.de>To: 'Barry Carter' <carter.barry@gmail.com>Subject: RE: CSV tables don't have all properties?Hi Carter,The question was already answered on the DBpedia mailing list, but I willtry to clarify it again. On the DBpedia as Tables web page says that ""Foreach class in the DBpedia ontology (such as Person, Radio Station, IceHockey Player, or Band) we provide a single CSV/JSON file which contains allinstances of this class. Each instance is described by its URI, an Englishlabel and a short abstract, the MAPPING-BASED INFOBOX data describing theinstance (extracted from the English edition of Wikipedia), geo-coordinates,and external links."" As you can see we only provide the mapping-basedinfobox properties (dbpedia-owl namespace), while the properties from thedbpprop (raw infobox properties) namespace are completely ignored.Therefore, dbpprop:bestBowling is missing from the file.Also, there is a section ""Generating your own Custom Tables"" [1], where weexplain how to generate your own tables that will contain the properties youneed.Regards,Petar[1] http://wiki.dbpedia.org/DBpediaAsTables#h347-4 ";;;
2525;1;2014-11-23T02:27:10.670;Can some one explain how PCA is relevant in extracting parameters of Gaussian Mixture Models;I am having some difficulty in seeing connection between PCA on second order moment matrix in estimating parameters of Gaussian Mixture Models. Can anyone connect the above??;[education, open-source];55;1
2526;2;2014-11-23T02:27:31.947;;"The question was already answered on the DBpedia-discussion mailing list, by Daniel: Hi Abhay, the DBpediaAsTables dataset only contains the properties in the  dbpedia-owl namespace (mapping-based infobox data) and not those from  the dbpprop (raw infobox properties) namespace (regarding the  differences see [1]). However, as you are only interested in the data about specific  entities, take a look at the CSV link at the bottom of the entity's  description page, e.g., for your example this link is [2]. Cheers,   Daniel [1] wiki.dbpedia.org/Datasets#h434-10 [2] dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE+%3Chttp://dbpedia.org/resource/Sachin_Tendulkar%3E&format=text%2FcsvOn the DBpediaAsTables web page, you can find out which datasets were used to generate the tables: instance_types_en, labels, short_abstracts_en, mappingbased_properties_en, geo_coordinates_en. Also, I want to clarify that DBpediaAsTables contains all instances from DBpedia 2014, and with ""we provide some of the core DBpedia data"" we want to say that not all datasets are included in the tables (but only the 5 I stated before) If you want to generate your own tables that will contain custom properties, please refer to the section Generate your own Custom Tables.Cheers,Petar";;;
2527;1;2014-11-23T14:58:34.127;Using Clustering in text processing;"Hi this is my first question in the Data Science stack. I want to create an algorithm for text classification. Suppose i have a large set of text and articles. Lets say around 5000 plain texts. I first use a simple function to determine the frequency of all the four and above character words. I then use this as the feature of each training sample. Now i want my algorithm to be able to cluster the training sets to according to their features, which here is the frequency of each word in the article. (Note that in this example, each article would have its own unique feature since each article has a different feature, for example an article has 10 ""water and 23 ""pure"" and another has 8 ""politics"" and 14 ""leverage""). Can you suggest the best possible clustering algorithm for this example?";[education, open-source];161;
2528;2;2014-11-23T17:49:04.487;;If you want to proceed on your existing path I suggest normalizing each term's frequency by its popularity in the entire corpus, so rare and hence predictive words are promoted. Then use random projections to reduce the dimensionality of these very long vectors down to size so your clustering algorithm will work better (you don't want to cluster in high dimensional spaces).But there are other ways of topic modeling. Read this tutorial to learn more.;;;
2529;2;2014-11-23T23:00:30.007;;"Store the edges (relations) in your server:(TeamID, playerID)When you want to find common elements just filter all edges where:TeamID=""TeamA"" or TeamID=""TeamB""(You could use indexes to speed is up, etc)Then group by playerID and check how many items are in each group. The groups with two items belong to both teams and are shared.";;;
2530;2;2014-11-24T09:20:47.543;;I recommend you to take a look to Oryx (https://github.com/cloudera/oryx). Oryx is based on Apache Mahout (actually one of the creators of Mahout Sean Owen built it) and provides recommendation using collaborative filtering. Oryx is a very practical tool for implementing recommendation. I have used it in several projects: recommending products in retail stores (small businesses), building an e-commerce recommender and user similarity from mobile app interaction. You just have to represent data in the form:UserId ItemId ValueWhere value is a measure (subjective) of the importance or influence of the interaction between that user and the item. User and item can be anything actually, and the same procedure can be used for tagging. For example, for recommending songs, finding similar songs and bands, and finding similar users according to their music tastes you can represent data asUserId SongId NumberOfPlaysWhere NumberOfPlays is the amount of times a song has been played by user (in an online music service for example). This exampl was given in Myrrix the predecessor of Oryx. They also show how to recommend tags to questions in StackOverflow.The github site is not that well documented but it will be enough to get it running (and working :));;;
2534;1;2014-11-25T03:08:34.843;Confused about description of YearPrediction Dataset;"https://archive.ics.uci.edu/ml/datasets/YearPredictionMSDAccording to the description given in the above link, the Attribute information specifies ""average and covariance over all 'segments', each segment being described by a 12-dimensional timbre vector"". So the covariance matrix should have 12*12 = 144 elements. But why is the number of timbre covariance features only 78 ?";[education, open-source];31;
2535;2;2014-11-25T03:38:33.600;;You are right, the covariance matrix should have n^2 elements. However, since cov_{i,j} = cov_{j,i}, there is no need to have a repeated feature cov_{j,i} if cov_{i,j} is already accounted for. Hence there will be only n*(n+1)/2 = 12*13/2 = 78 unique covariances and thus only 78 unique covariance based features (n of those will be variances). ;;;
2536;2;2014-11-25T09:17:45.020;;"This problem is one of estimating the lag. Once that is estimated, you could create additional features representing the lagged values and move forward with ""sequence mining"" as you have already suggested in the question itself. For each variable, Var_i, you will have to estimate its lag l_i. This lag can be calculated by estimating the order of a Markov chain with seven symbols (you could use either BIC or AIC to estimate this order; both would require calculating likelihood of candidate orders and pick the order that maximizes either of these criteria). Once you are done calculating the order of the Markov chain for each of the variables, then you could represent your dataset such that each row will have the current value of Var_i and its preceding values, all the way back to its estimated lag l_i. While this methodology is laborious, it pays rich dividends as its automated and parsimonious way of representing the necessary information. ";;;
2537;1;2014-11-25T09:40:20.040;Why isn't dimension sampling used with gradient boosting machines (GBM)?;GBMs, like random forests, build each tree on a different sample of the dataset and hence, going by the spirit of ensemble models, produce higher accuracies. However, I have not seen GBM being used with dimension sampling at every split of the tree like is common practice with random forests. Are there some tests that show that dimensional sampling with GBM would decrease its accuracy because of which this is avoided, either in literature form or in practical experience? ;[education, open-source];22;
2538;1;2014-11-25T10:30:17.567;Error analysis for better accuracy;I have historic error of time series.  I want to analyze error series to improve forecast series. Are there any methods to do this?;[education, open-source];80;
2540;2;2014-11-26T02:07:06.460;;I have just finished my Ph.D. and have used some NLP in it. My university didn't offer any NLP courses. So I ended up teaching myself NLP. I used the following book:http://www.nltk.org/book_1ed/Which serves as a great introduction to NLP using NLTK (Natural Language Tool Kit). It gives a good introduction into programming with Python. So handy if you've never programmed in Python before too.I would highly suggest using nltk from nltk.org (sorry can't post more than two links)The book I used is now out of date as NLTK is now on version 3.0, the book mentioned previously is for NLTK 2.x. But you're in luck. The Authors are working on a new version of the book for NLTK 3.X You can view the unfinished book over athttp://www.nltk.org/book/I would highly suggest using NLTK and if you're new to natural language processing. I would highly suggest you try and get yourself a copy of the following book: Foundations of Statistical Natural Language Processing by Manning and ShutzeEven though it doesn't contain any code, it servers a great introduction to natural language processing.;;;
2541;1;2014-11-26T04:40:17.840;Evaluating Recommendation engines;What is the standard way for evaluating and comparing different algorithms while developing recommendation system? Whether we need to have a predetermined annotated ranked dataset and then compare with precision/recall/F measure of  different algorithms ? Is this the best way for evaluation ? Or is there any other way to compare results of various recommendation algorithms ?;[education, open-source];74;
2543;1;2014-11-26T08:47:49.650;Can Machine Learning be applied in software developement;I'm from programming background. I'm now learning Analytics. I'm learning concepts from basic statistics to model building like linear regression, logistic regression, time-series analysis, etc.,As my previous experience is completely on programming, I would like to do some analysis on the data which programmer has.Say, Lets have the details below(I'm using SVN repository)personname, code check-in date, file checked-in, number of times checkedin, branch, check-in date and time, build version, Number of defects, defect date, file that has defect, build version, defect fix date, defect fix hours, (please feel free to add/remove how many ever variables needed)I Just need a trigger/ starting point on what can be done with these data. can I bring any insights with this data.or can you provide any links that has information about similar type of work done.;[education, open-source];274;2
2544;2;2014-11-26T09:37:43.970;;"Definately - Yes.Good question. Was thinking about it myself.(1) Collect the data.The first problem you have: gather enough data. All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.(2) Ask good questions.The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.Frequency of possible bugs? Tracking inaccurate ""committers""? Risky branches? Want to see some groups of users/bugs/commits according to some metrics?(3) Select the model.As soon as you have the questions formulated, you should follow the general approach in data science - extract needed features in your data, select appropriate model, train you model and test it, apply it. This is too broad process to discuss it this thread, so please use this site to get right answers.";;;
2545;2;2014-11-26T09:57:00.047;;"As you are also looking for examples, then github is a good place to check out.I took a random repository and went to ""Graphs"" on the right hand side, which opens up contribution frequency graph. There's several tabs next to it that display other aspects of a repository and commit history graphically - commits, code frequency, punch card, etc.";;;
2546;2;2014-11-26T11:18:13.227;;If you're simply trying to separate textual and numeric information then there is a solution based on regular expressions or even just string splitting.You could even do something like finding the first numeric character and split the text in half right before that.With regular expressions you can match all numeric characters that follow eachother. The pattern would be ([0-9]+) with a global flag. It would match all the groups of numbers and you can do whatever you with with them afterwards.Regex Tester is good for playing around with that stuff.;;;
2547;1;2014-11-26T20:01:23.187;Statistical comparison of 2 small data sets for 2X increase in the population mean;I am trying to determine whether or not we are 90% confident that the mean of a proposed population is at least 2 times that of the mean of the incumbant population based on samples from each population which is all the data I have right now. Here are the data.incumbantvalues = (7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)proposedvalues =  (17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)I have no idea if either population is or will be normal.The ratio of the sample means does exceed 2.0 but how does that translate to confidence that the proposed population mean will be at least twice that of the mean of the incumbant population with 90% confidence ?Can re-sampling (bootstrapping with replacement) help answer this question ?;[education, open-source];56;
2549;2;2014-11-26T21:20:52.973;;"Without a doubt you can. The key is to have a set of hypotheses (i.e. assumptions \ scenarios that you want to evaluate) and wrangle the data together to prove \ disprove what you thought is true. Here are a few things to watch-out for:Be ready for Disappointments: Often times, once you have invested time and energy in building these models, analysts tend to get biased towards publishing results (publication bias). Treat this as an exploration that with a lot of dead-ends and the goal should be to find the ones that are not.Know your Data: You cannot will your data into doing things magically without truly understanding it. Ensure that you know the different attributes (predictors and dependents) very well. Knowing your data well will allow you to cleanse it and think about appropriate models. All models don't work equally well on all data - data that has a lot of categorical variables might require creative solutions like Dimension reduction before it can be modeled.Know the ""Operational"" Processes: Knowing how things operate within your firm will help you refine the set of hypothesis that you want to test. For e.g. in your scenario above, knowing how developers work with your change management software and what types of administrative setups have been done will help you figure out why the data is coming in the way it is. Some developers might only be focused on certain modules that are more mature than others, might work only on certain shifts and that might limit how many lines of code are checked in, how many bugs are found etc.Having said that here are some scenarios you might want to test:Developer Effectiveness : How different developers working on same modules overtime has resulted into increase or decrease in bugs. Does more line of code results in more bugs? Maybe this might be an indicator that the programs need to be split further into smaller componentsFolks might be more productive during certain times of day than others - does time of day affect bug introductions?Module Maturity: Which Modules have the most number of issues? Are they worked upon by more developers or less?Do defects keep aging for a long time before they are fixed?Of course, these questions will change depending on what you are working on. Hope this helps.";;;
2550;1;2014-11-27T03:21:11.110;How to ensemble classifier incorporating all features in python?;I am doing a text classification task(5000 essays evenly distributed by 10 labels). I explored LinearSVC and got an accuracy of 80%. Now I guess whether accuracy could be raised by using ensemble classifier with SVM as base estimator?However, I do not know how to employ an ensemble classifier incorporating all the features? Please note that I do not want to combine the different features directly in a single vector.Therefore, My first question: in order to improve the current accuracy, is it possible to use ensemble classifier with svm as base estimator?My second question How to employ an ensemble classifier incorporating all features?;[education, open-source];77;1
2551;2;2014-11-27T03:27:46.193;;If these data are available in the actual excel spreadsheet cells (ie, before you export them to the JSON format provided in your question), you can use the following to get them into R:highlight the region of interest within excelcopy it to the clipboard (eg. Ctrl-C)At an R prompt type:d <- read.delim('clipboard')The data will now be available as a data.frame in R. d  from response value1    4     TRUE    202    8     TRUE    203    9     TRUE    204    3     TRUE    205   14    FALSE    206   15     TRUE    207   17    FALSE    208   13     TRUE    20  ;;;
2552;2;2014-11-27T03:37:08.010;;The standard way to evaluate a recommendation engine is by using the RMSE (root mean square error) of the predicted values and the ground truth.It is almost a SOP that, after finishing developing a recommendation engine, we will evaluate this engine by comparing its RMSE with other famous, common recommendation algorithms like SVD, tranditional CF, even RBM, etc.Some terms mentioned above do not seem to be related with recommendation, but you can easily find on the internet how these techniques can be used in this topic.;;;
2553;2;2014-11-27T04:23:31.500;;Yes, in principle, resampling can help answer this question. incumbent <- c(7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)proposed  <- c(17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)set.seed(42)M  <- 2000rs <- double(M)for (i in 1:M) {    rs[i] <- mean(sample(proposed, replace=T)) - 2 * mean(sample(incumbent, replace=T))}To make the assessment, you should choose one (not both) of the following:  A. The (two-tailed) 90% confidence interval for the difference in the (weighted) means using Hall's method is:ci.hall <- 2 * (mean(proposed)-2*mean(incumbent)) - rev(quantile(rs,prob=c(0.05, 0.95)))names(ci.hall) <- rev(names(ci.hall))ci.hall   5%   95% -0.29  2.95 This is appropriate if you have any concern about missing the possibility that mean(proposed) might actually be less than 2 * mean(incumbent).B. The proportion of resample means >= 0 provides the (one-tailed) estimate that mean(proposed) is at least twice mean(incumbent):sum(rs>=0)/M[1] 0.8915The problem is that the samples are really rather small and resampling estimatescan be unstable for small n. The same issue applies if you want to assess normality and go with parametric comparisons. If you can get to, say, n >= 30, the approach described here should be fine.;;;
2555;2;2014-11-27T07:21:56.277;;Check these :Repository of Test Domains for Information Extraction : http://www.isi.edu/info-agents/RISE/repository.htmlDBpedia : http://wiki.dbpedia.org/Downloads32;;;
2557;2;2014-11-27T21:58:15.113;;Here is what I programmed within a loop.randomly take 10 values (with replacement) from the incumbant sample, determine its meanrandomly take 10 values (with replacement) from the proposed sample, determine its meanform the ratio of the above two means and append it to a master listrepeat steps 1 thru 3 many times (I chose 1 million)% Confidence=(number of ratios that equal or exceed 2.0/1000000)*100Results:Exactly 897450 ratios were found to be greater than or equal to 2.0, producing a confidence of 89.745%.Conclusion:We are less than 90% confident that the proposed population will have a mean at least twice that of the incumbant population.;;;
2558;1;2014-11-28T03:02:44.323;Smoothing Proportions :: Massive User Database;What are some possible techniques for smoothing proportions across very large categories, in order to take into account the sample size? The application of interest here is to use the proportions as input into a predictive model, but I am wary of using the raw proportions in cases where there is little evidence and I don't want to overfit.Here is an example, where the ID denotes a customer and impressions and clicks are the number of ads shown and clicks the customer has made, respectively.;[education, open-source];34;1
2559;2;2014-11-28T06:02:59.210;;"Data analysis is always driven by the request. It could be: ""I want to find out this, so I need to collect those data first. Then I would use this model to analyze"". If you just want to practice, by reviewing your data set, there is one:Task: Which issue affects the ""number of check in "" most? Data set: what you haveModel: Correlation (e.g. Spearman, which is nonparametric measure of statistical dependence between two variables)";;;
2561;2;2014-11-28T06:17:34.480;;Cannot say it is the best one, but Latent Semantic Analysis could be one option. Basically it is based on co-occurrence, you need to weight it first.http://en.wikipedia.org/wiki/Latent_semantic_analysishttp://lsa.colorado.edu/papers/dp1.LSAintro.pdfThe problem is that LSA does not have firm statistic support.Have fun;;;
2562;2;2014-11-28T08:55:25.813;;"I don't know if you ever read SenseCluster by Ted Pedersen : http://senseclusters.sourceforge.net/. Very good paper for sense clustering. Also, when you analyze words, think that ""computer"", ""computers"", ""computering"", ... represent one concept, so only one feature. Very important for a correct analysis.To speak about the clustering algorithm, you could use a hierarchical clustering. At each step of the algo, you merge the 2 most similar texts according to their features (using a measure of dissimilarity, euclidean distance for example). With that measure of dissimilarity, you are able to find the best number of clusters and so, the best clustering for your texts and articles.Good luck :)";;;
2563;1;2014-11-28T09:19:32.867;Visualization of three-dimensional report;"I have a visualization problem.Creating a comparison report of PR event efficiency. Say, show or exhibition.There are two dimensions of comparison:compare vs the same event performance in the past years compare vs another type of analogical/competitive events There is also a number of comparison aspects:AudienceMedia CoverageSocial BuzzROI.... etcEach aspect is a set of some final KPI-s (just numbers, which can be compared vs another ""dimensions""), plus maybe some descriptive text and pictures (which couldn't be a metric but should be attached to the report).So finaly it looks like a three-dimensional coube:YearsAnother EventsAspectsIf I put it in plain Word or PPT it will look like a document with dozen of slides/papers and linear structure.Any ideas how to compile an elegant user-friendly report?";[education, open-source];58;
2564;1;2014-11-28T11:39:09.537;Using the Datumbox Machine Learning Framework for website classification - guidelines?;A short while ago, I came across this ML framework that has implemented several different algorithms ready for use. The site also provides a handy API that you can access with an API key.I have need of the framework to solve a website classification problem where I basically need to categorize several thousand websites based on their HTML content. As I don't want to be bound to their existing API, I wanted to use the framework to implement my own.However, besides some introductory-level data mining courses and associated reading, I know very little as to what exactly I would need to use. Specifically, I'm at a loss as to what exactly I need to do to train the classifier and then model the data.The framework already includes some classification algorithms like NaiveBayes, which I know is well suited to the task of text classification, but I'm not exactly sure how to apply it to the problem.Can anyone give me a rough guidelines as to what exactly I would need to do to accomplish this task?;[education, open-source];87;
2565;2;2014-11-28T12:48:14.443;;"AUC and accuracy are fairly different things. AUC applies to binary classifiers that have some notion of a decision threshold internally. For example logistic regression returns positive/negative depending on whether the logistic function is greater/smaller than a threshold, usually 0.5 by default. When you choose your threshold, you have a classifier. You have to choose one.For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set.AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else. More importantly, AUC is not a function of threshold. It is an evaluation of the classifier as threshold varies over all possible values. It is in a sense a broader metric, testing the quality of the internal value that the classifier generates and then compares to a threshold. It is not testing the quality of a particular choice of threshold.AUC has a different interpretation, and that is that it's also the probability that a randomly chosen positive example is ranked above a randomly chosen negative example, according to the classifier's internal value for the examples.AUC is computable even if you have an algorithm that only produces a ranking on examples. AUC is not computable if you truly only have a black-box classifier, and not one with an internal threshold. These would usually dictate which of the two is even available to a problem at hand.AUC is, I think, a more comprehensive measure, although applicable in fewer situations. It's not strictly better than accuracy; it's different. It depends in part on whether you care more about true positives, false negatives, etc.F-measure is more like accuracy in the sense that it's a function of a classifier and its threshold setting. But it measures precision vs recall (true positive rate), which is not the same as either above.";;;
2566;2;2014-11-28T13:16:02.520;;NARMAX Methodology and Residual analysis both address this issue. Search for the following articles:(Error = Residual = Noise)Chaotic Time Series Prediction with residual Analysis Method Using Hybrid Elman–NARX Neural Networks, Muhammad Ardalani-Farsa (2010)Orthogonal Least Squares Methods and their Application to Non-Linear System Identification, S. Chen, S. A. Billings, W. Luo (1989)Any article working on NARMAX, NARMA and Residual Analysis. Remember in NARX and NAR there is no error estimation and analysis.Notice in general you can follow this steps:Estimate a time series and calculate Error or Residuals using any .Consider errors or residuals as a new time series. Try to estimate Error-Time-Series. Now you can add this estimations to your initial model.You can do this residual analysis as many times as you need. In practice 2 or 3 times suffices. Remember in practice, residual time series are noisy and SNR in this time series is so small. So you should use some Noise-Robust methods for residual analysis.;;;
2567;1;2014-11-29T16:05:08.810;What regression to use to calculate the result of election in a multiparty system?;I want to make a prediction for the result of the parliamentary elections. My output will be the % each party receives. There is more than 2 parties so logistic regression is not a viable option. I could make a separate regression for each party but in that case the results would be in some manner independent from each other. It would not ensure that the sum of the results would be 100%.What regression (or other method) should I use? Is it possible to use this method in R or Python via a specific library?;[education, open-source];141;
2568;1;2014-11-30T00:42:28.237;Sentiment analysis using python;"I have some text files containing moview reviews I need to find out whether the review is good or bad. I tried the following code but its not working:import nltkwith open(""c:/users/user/desktop/datascience/moviesr/movies-1-32.txt"", 'r') as m11:    mov_rev = m11.read()mov_review1=nltk.word_tokenize(mov_rev)bon=""crap aweful horrible terrible bad bland trite sucks unpleasant boring dull moronic dreadful disgusting distasteful flawed ordinary slow senseless unoriginal weak wacky uninteresting unpretentious ""bag_of_negative_words=nltk.word_tokenize(bon)bop=""Absorbing Big-Budget Brilliant Brutal Charismatic Charming Clever Comical Dazzling Dramatic Enjoyable Entertaining Excellent Exciting  Expensive Fascinating Fast-Moving First-Rate Funny Highly-Charged Hilarious Imaginative Insightful Inspirational Intriguing Juvenile Lasting Legendary Pleasant Powerful Ripping Riveting Romantic Sad  Satirical Sensitive  Sentimental Surprising Suspenseful Tender Thought Provoking Tragic Uplifting Uproarious""bop.lower()bag_of_positive_words=nltk.word_tokenize(bop)vec=[]for i in bag_of_negative_words:    if i in mov_review1:        vec.append(1)    else:        for w in bag_of_positive_words:            if w in moview_review1:                vec.append(5)so i am trying to check whether the review contains a positive word or a negative word. If it contains negative word then a value 1 will be assigned to the vector vec else a value of 5 will be assigned.but the output i am getting is an empty vector.please help..Also please suggest others way of solving this problem.thanks in advance";[education, open-source];279;
2569;2;2014-11-30T20:01:23.930;;Robert is right, multinomial logistic regression is the best tool to use. Although you would need to have a integer value representing the party as the dependent variable, for example:1= Conservative majority, 2= Labour majority, 3= Liberal majority....(and so on)You can perform this in R using the nnet package. Here is a good place to quickly run through how to use it: http://www.ats.ucla.edu/stat/r/dae/mlogit.htmHope this helps.;;;
2570;2;2014-11-30T20:14:15.560;;On what do you want to base your prediction? I've tried to predict multiparty election results for my thesis based on previous years and then using results for some polling stations from this year predict the results in all other polling stations. For this the linear model with which I compared estimated the number of votes each party would obtain by regressing over the votes from previous years. If you have the estimated number of votes for all parties you can calculate the percentage from that. See http://amstat.tandfonline.com.proxy.ubn.ru.nl/doi/abs/10.1198/016214504000001835#.VHt5tDGG-OM for the relevant paper, which extends the linear model.;;;
2571;2;2014-12-01T00:23:30.580;;The United States Census Bureau has many free housing datasets (some of which are updated more than once every 10 years). There is an API for American Community Survey 1 Year Data that includes housing data. There are raw data sets at American Fact Finder.;;;
2572;2;2014-12-01T00:26:39.927;;There is real estate data for sale at DataQuick or Real Quest.;;;
2573;2;2014-12-01T09:58:00.280;;tryvec =[]for word in bag_of_negative_words:    if word in mov_review1:        vec.append(1)for word in bag_of_positive_words:    if word in moview_review1:         vec.append(5);;;
2574;2;2014-12-01T12:23:03.083;;I suggest using machine learning libraries with already functional linear regressionSpark MLlib or hivemall.;;;
2575;1;2014-12-01T14:52:31.827;Hive: How to calculate the Kendall coefficient of correlation of a pair of a numeric columns in the group?;In this wiki page there is a function corr() that calculates the Pearson coefficient of correlation, but my question is that: is there any function in Hive that enables to calculate the Kendall coefficient of correlation of a pair of a numeric columns in the group?;[education, open-source];147;
2576;1;2014-12-01T20:10:29.967;API for Company Data Enrichment Suggestions;I'm looking for API suggestions for enriching data on companies. Currently I use the Crunchbase API to look up a company's name or domain and I am trying to gather the domain/name (if I don't already have both), contact email (this one is a long shot), and the location of their headquarters. This works incredibly well if Crunchbase has the company in their API, but I'd say this only happens about 25% of the time.I'd love to get some suggestions on some free APIs that I could use along with Crunchbase. I'd also love to see if anyone has had positive or negative experiences with paid APIs! ;[education, open-source];69;1
2577;2;2014-12-01T21:59:47.540;;"This is not a regression but a multi-class classification problem. The output is typically the probabilities of all classes for any given test instance (test row). So in your case, the output for any given test row from the trained model will be of the form:prob_1, prob_2, prob_3,..., prob_kwhere prob_i denotes the probability of the i-th class (in your case i-th party), assuming  there are k classes in the response variable. Note that the sum of these k probabilities is going to be 1. The class prediction in this case is going to be the class that has the maximum probability. There are many classifiers in R that do multi-class classification. You could use logistic regression with multi-class support through the nnet package in R and invoking the multinom command. As an alternative, you could also use the gbm package in R and invoke the gbm command. To create a multi-class classifier, just use distribution=""multinomial"" while using thegbm` function. ";;;
2578;1;2014-12-02T10:58:30.950;Graphlab vs Mahout;I have some question regarding to the choice of the better implementation. I would know the differences and advantages of Mahout Apache (Java implementation) versus Graphlab (Python implementation) in the area of the data sciences. Specially in the area of recommenders and classifiers. Can anybody here get some (qualified) feedback about both possibilities?;[education, open-source];144;
2579;1;2014-12-02T11:12:06.103;Mahout Similarity algorithm comparison;Which of the following is best (or widely used) for calculating item-item similarity measure in mahout and why ?Pearson CorrelationSpearman CorrelationEuclidean DistanceTanimoto CoefficientLogLikelihood SimilarityIs there any thumb-rule to chose from these set of algorithm also how to differentiate each of them ?;[education, open-source];477;2
2580;2;2014-12-02T14:35:33.447;;The advantage of mahout is that it is scalable, apache license , good community and documentation support. Also Fast-prototyping and evaluationto evaluate a different configuration of the same algorithm we just need to update a parameter and run again.;;;
2581;1;2014-12-02T16:09:35.333;Attributes extraction from unstructured product descriptions;I am trying to match new product description with the existing ones. Product description looks like this: ￼Panasonic DMC-FX07EB digital camera silver. These are steps to be performed:Tokenize description and recognize attributes: Panasonic => Brand, DMC-FX07EB => Model, etc.Get few candidates with similar featuresGet the best candidate.I am having problem with the first step (1). In order to get 'Panasonic => Brand', DMC-FX07EB => Model, silver => color, I need to have index where each token of the product description correspond to certain attribute name (Brand, model, color, etc.) in the existing database. The problem is that in my database product descriptions are presented as one atomic attribute e.g. 'description' (no separated product attributes).Basically I don't have training data, so I am trying to build index of all product attributes so I can build training data. So far I have attributes from bestbuy.com and semantics3.com APIs, but both sources lack most of attributes or contain irrelevant ones. Any suggestions for better APIs to get product attributes? Better approach to do this? P.S. For every product there is a matched product description in the Database, which is as well in a form of one atomic attribute. I have checked this question on SO, it helped me and it seems we have same approach but I am still trying to get training data. ;[education, open-source];135;
2582;1;2014-12-02T16:19:19.043;Consequence of Feature Scaling;I am currently using SVM and scaling my training features to the range of [0,1].I first fit/transform my training set and then apply the same transformation to my testing set. For example:    ### Configure transformation and apply to training set    min_max_scaler = MinMaxScaler(feature_range=(0, 1))    X_train = min_max_scaler.fit_transform(X_train)    ### Perform transformation on testing set    X_test = min_max_scaler.transform(X_test)Let assume that a given feature in the training set has a range of [0,100], and that same feature in the testing set has a range of [-10,120]. In the training set that feature will be scaled appropriately to [0,1], while in the testing set that feature will be scaled to a range outside of that first specified, something like [-0.1,1.2].I was wondering what the consequences of the testing set features being out of range of those being used to train the model? Is this a problem?;[education, open-source];114;
2583;2;2014-12-02T20:07:14.887;;"Try to search from the database's of officials ""bad words"" that google publish in this link Goolgles official list of bad words...Also here is the link for the good words Not official list of good words...For the code i would do it like this...textArray = file('dir_to_your_text','r').read().split()#Bad words should be listed like this for the split function to work# ""*** ****** **** ****"" the stars are for the cenzuration :PbadArray = file('dir_to_your_bad_word_file).read().split()goodArray = file('dir_to_your_good_word_file).read().split()#Than you use maching algoritem from difflib on good and bad word for ewery word in array of wordsimport difflibgoodMachingCouter = 0;badMacihngCouter = 0;for iGood in range(0, len(goodArray)):    for iWord in range(0, len(textArray)):        goodMachingCounter += difflib.SequenceMatcher(None, goodArray[iGood], textArray[iWord]).ratio()for iBad in range(0, len(badArray)):    for iWord in range(0, len(textArray)):        badMachingCounter += difflib.SequenceMatcher(None, badArray[ibad], textArray[iWgoodord]).ratio()goodMachingCouter *= 100/(len(goodArray)*len(textArray))badMacihngCouter *= 100/(len(badArray)*len(textArray))print('Show the good measurment of the text in %: '+goodMachingCouter)print('Show the bad measurment of the text in %: '+badMacihngCouter)print('Show the hootnes of the text: ' + len(textArray)*goodMachingCounter)The code will be slow but accurate :) I didn't run and test it pls do it for me and post the correct code :) because i wana test it too :)";;;
2584;2;2014-12-02T22:23:33.700;;"Left you a quick response on SO. The gist is that you can collect a lot of information from electronics shops and manufacturers' web sites, and lots you can annotate manually. If your goal is to only get training data, that's all you need:My answer form the cross-post:""Having developed a commercial analyzer of this kind, I can tell you that there is no easy solution for this problem. But there are multiple shortcuts, especially if your domain is limited to cameras/electronics.Firstly, you should look at more sites. Many have product brand annotated in the page (proper html annotations, bold font, all caps in the beginning of the name). Some sites have entire pages with brand selectors for search purposes. This way you can create a pretty good starter dictionary of brand names. Same with product line names and even with models. Alphanumeric models can be extracted in bulk by regular expressions and filtered pretty quickly.There are plenty of other tricks, but I'll try to be brief. Just a piece of advice here: there is always a trade-off between manual work and algorithms. Always keep in mind that both approaches can be mixed and both have return-on-invested-time curves, which people tend to forget. If your goal is not to create an automatic algorithm to extract product brands and models, this problem should have limited time budget in your plan. You can realistically create a dictionary of 1000 brands in a day, and for decent performance on known data source of electronic goods (we are not talking Amazon here or are we?) a dictionary of 4000 brands may be all you need for your work. So do the math before you invest weeks into the latest neural network named entity recognizer.""";;;
2585;2;2014-12-03T05:00:30.500;;This was meant as a comment but it is too long.The fact that your test set has a different range might be a sign that the training set is not a good representation of the test set. However, if the difference is really small as in your example, it is likely that it won't affect your predictions. Unfortunately, I don't think I have a good reason to think it won't affect a SVM in any circumstance.Notice that the rationale for using MinMaxScalar is (according to the documentation):  The motivation to use this scaling include robustness to very small  standard deviations of features and preserving zero entries in sparse  data.Therefore, it is important for you to make sure that your data fits that case.If you are really concerned about having a difference range, you should use a regular standardization (such as preprocessing.scale) instead.;;;
2586;1;2014-12-03T07:46:30.967;Can data analytics be a basis for artificial intelligence?;I'm very passionate about how computers can be made able to think intelligently and independently (in our favour, of course!). I'm currently studying Bachelors science of information technology at UTS (University of Technology:Sydney). I have two months before I start my second year, and have not yet been able to decide on which major should I select that can lead myself towards dedicated study of Artificial Intelligence (which I love with my life).I have the following majors available:Internetworking and Applications Data Analytics(there are other two as well, but business oriented).Here is the link to my subjects. I believe that being able to play with data is a sign of intelligence (I may be wrong too!). Will one of these subjects form me a good foundation for my further study in A.I.? Or should I jump into Engineering? Or Pure Science?;[education, open-source];90;
2587;1;2014-12-03T11:05:59.490;N - fold cross validation in mahout;Is there a method/class available in Apache Mahout to perform n-fold cross validation?If yes how it can be done?;[education, open-source];149;
2588;2;2014-12-03T13:21:34.153;;The difference between these methods is the assumptions they make about the task.  Multi-class classification assumes that each document has exactly one label.  So a document can either be about sports or weather, not both.Multi-label classification allows a document to have any combination of labels, including none.  So a document can be about only sports, only weather, sports AND weather, or neither.You could train a multi-label classifier with data where each document has exactly one label, but there is no guarantee that the predictions made at test time will have only one label.  Also you are forcing the classifier to do more work (and potentially make more errors) by considering more possible labelings than it needs to.Therefore, if the multi-class assumption makes sense for your problem, you are better off with a multi-class classifier.The method that you describe for training individual binary classifiers corresponds to multi-label classification.  The binary classifiers that you use could each be trained from one-class data or two-class data.  However, this is only one of the many ways to do multi-label classification (see the wikipedia page above for more).Unfortunately, the problem that you describe does not cleanly fit into either multi-class or multi-label classification, since you want each document to have at most one label.;;;
2589;2;2014-12-03T13:55:16.150;;I believe the claim that you are referring to is that the maximum-likelihood estimate of the component means in a GMM must lie in the span of the eigenvectors of the second moment matrix.  This follows from two steps:Each component mean in the maximum-likelihood estimate is a linear combination of the data points.  (You can show this by setting the gradient of the log-likelihood function to zero.)Any linear combination of the data points must lie in the span of the eigenvectors of the second moment matrix.  (You can show this by first showing that any individual data point must lie in the span, and therefore any linear combination must also be in the span.);;;
2590;1;2014-12-03T14:18:34.003;Cross-sell models and additional holders;I would like to pose a question about how to treat additional holders in the propensity-to-buy models of banking products.Up to now I was only taking into considerations the clients as first holders. For example, if a client ‘1’ appears as the first holder of a saving account ‘A’ with a balance at the end of the month of 100€ and as an additional holder of a saving account ‘B’ with a balance at the end of the month of 50€, the saving balance at the end of the month for the client is considered to be just 100€.Moreover, if a client only appears as an additional holder (and he/she is not a first account holder of ANY product), he/she is dismissed by the model.However I have been told to include additional holders in the models (additional holders have the same rights of the first holders).One possibility is to recalculate all the variables summing up the position as first and additional holder (in the previous example, the balance at the end of the month of client ‘1’ would be 150€). Together with this, I would create some variable that represents the maximum degree of intervention of the client in the account (ex. 'first holder', 'second holder').Another possibility would be to “double” all the variables, considering the client as first and additional holder (in the example, we would create two variables:  the balance at the end of the month as FH =100€, :  the balance at the end of the month as AH =50€).Did any of you encounter a similar problem?It would be very helpful to understand how you solved it.Thanks;[education, open-source];23;
2591;1;2014-12-03T14:23:38.830;Decision trees, categorizacion and oversampling;I want to create a model to predict the propensity to buy a certain product. As my proportion of 1's is very low, I decided to apply oversampling (to get a 10% of 1's and a 90% of 0's). Now, I want to discretize some of the variables. To do so I run a tree for each variable against the target. My question is...shall I define the prior probabilities when I do this (run the trees), or it doesn't matter and I can use the over-sampled dataset just like that?Thanks.;[education, open-source];43;
2593;1;2014-12-03T15:56:50.687;How to connect data-mining with machine learner process;I want to write a data-mining service in Google Go which collects data through scraping and APIs.However as Go lacks good ML support I would like to do the ML stuff in Python.Having a web background I would connect both services with something like RPC but as I believe that this is a common problem in data science I think that there is some better solution.For example most (web) protocols lack at:buffering between processesclustering over multiple instancesSo what (type of libraries) do data scientists use to connect different languages/processes?Bodo;[education, open-source];75;1
2594;2;2014-12-03T18:22:29.333;;"For those not familiar, item-item recommenders calculate similarities between items, as opposed to user-user (or user-based) recommenders, which calculate similarities between users. Although some algorithms can be used for both, this question is in regard to item-item algorithms (thanks for being specific in your question).Accuracy or effectiveness of recommenders is evaluated based on comparing recommendations to a previously collected data set (training set). For example, I have shopping cart data from the last six months; I'll use the first 5 months as training data, then run my various algorithms, and compare the quality against what really happened during the 6th month.The reason Mahout ships with so many algorithms is because different algorithms are more or less effective in each data set you may work with. So, ideally, you do some testing as I described with many algorithms and compare the accuracy, then choose the winner.Interestingly, you can also take other factors into account, such as the need to minimize the data set (for performance reasons), and run your tests only with a certain portion of the training data available. In such a case, one algorithm may work better with the smaller data set, but another may work with the complete set. Then, you get to weigh performance VS accuracy VS challenge of implementation (such as deploying on a Hadoop cluster).Therefore, different algorithms are suited for different project. However, there are some general rules:All algorithms always do better with unreduced data sets (more data is better).More complex algorithms aren't necessarily better.I suggest starting with a simple algorithm and ensuring you have high quality data. If you have additional time, you can implement more complex algorithms and create a comparison which is unique to your data set. Most of my info comes from This study. You'll find lots of detail about implementation there.";;;
2595;2;2014-12-03T18:36:00.880;;Within each class, you'll have distributions of values for the features. That in itself is not a reason for concern. From a slightly theoretical point of view, you can ask yourself why you should scale your features and why you should scale them in exactly the chosen way.One reason may be that your particular training algorithm is known to converge faster (better) with values around 0 - 1 than with features which cover other orders of magnitude. In that case, you're probably fine. My guess is that your SVM is fine: you want to avoid too large numbers because of the inner product, but a max of 1.2 vs. a max of 1.0 won't make much of a difference.(OTOH, if you e.g. knew your algorithm to not accept negative values you'd obviously be in trouble. )The practical question is whether your model performs well for cases that are slightly out of the range covered by training. This I believe can best and possibly only be answered by testing with such cases / inspecting test results for performance drop for cases outside the training domain. It is a valid concern and looking into this would be part of the validation of your model. Observing differences of the size you describe is IMHO a reason to have a pretty close look at model stability. ;;;
2596;1;2014-12-03T21:47:34.907;Method for solving problem with variable number of predictors;I've been toying with this idea for a while. I think there is probably some method in the text mining literature, but I haven't come across anything just right...What is/are some methods for tackling a problem where the number of variables it its self a variable.  This is not a missing data problem, but one where the nature of the problem fundamentally changes. Consider the following example:Suppose I want to predict who will win a race, a simple multinomial classification problem. I have lots of past data on races, plenty to train on.  Lets further suppose I have observed each contestant run multiple races. The problem however is that the number or racers is variable. Sometimes there are only 2 racers, sometimes there are as many as 100 racers.One solution might be to train a separate model for each number or racers, resulting in 99 models in this case, using any method I choose.  E.g. I could have 100 random forests. Another solution might be to include an additional variable called 'number_of_contestants' and have input field for 100 racers and simply leave them blank when no racer is present.  Intuitively, it seems that this method would have difficulties predicting the outcome of a 100 contestant race if the number of racers follows a Poisson distribution (which I didn't originally specify in the problem, but I am saying it here).    Thoughts?;[education, open-source];75;
2597;2;2014-12-04T03:42:48.357;;I don't see the problem. All you need is a learner to map a bit string as long as the total number of contestants, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would have a learning-to-rank problem.If the contestant landscape can change it would help to find a vector space embedding for them so you can use the previous embeddings as an initial guess and rank anyone, even hypothetical, given their vector representation. As the number of users increases the embedding should stabilize and retraining should become less costly. The question is how to find the embedding, of course. If you have a lot of training data, you could probably find a randomized one along with the ranking function. If you don't, you would have to generate the embedding by some algorithm and estimate only the ranking function. I have not faced your problem before so I can't direct you to a particular paper, but the recent NLP literature should give you some inspiration, e.g. this. I still think it is feasible.;;;
2598;1;2014-12-04T05:18:03.720;Item based and user based recommendation differnce in Mahout;I would like to know how exactly mahout user based and item based recommendation differ from each other.It defines thatUser-based: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.Item-based: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed off line.But though there are two kind of recommendation available, what I understand is that both these will take some data model ( say 1,2 or 1,2,.5 as item1,item2,value or user1,user2,value where value is not mandatory) and will perform all calculation as the similarity measure and recommender build-in function we chose and we can run both user/item based recommendation on the same data ( is this a correct assumption ?? ).So I would like to know how exactly and in which all aspects these two type of algorithm differ. ;[education, open-source];1045;
2599;2;2014-12-04T06:08:25.000;;You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.In the user-based approach the algorithm produces a rating for an item i by a user u by combining the ratings of other users u' that are similar to u. Similar here means that the two user's ratings have a high Pearson correlation or cosine similarity or something similar.In the item-based approach we produce a rating for i by u by looking at the set of items i' that are similar to i (in the same sense as above except now we'd be looking at the ratings that items have received from users) that u has rated and then combines the ratings by u of i' into a predicted rating  by u for i.The item-based approach was invented at Amazon (http://dl.acm.org/citation.cfm?id=642471) to address their scale challenges with user-based filtering. The number of things they sell is much less and much less dynamic than the number of users so the item-item similarities can be computed offline and accessed when needed.;;;
2600;1;2014-12-04T07:47:32.740;How does one feed graph optimization problems into Python's anneal function in SciPy?;I am interested in graph problems like 2-color, max-clique, stable sets, etc but the documentation for scipy.optimize.anneal seems to be for ordinary functions. How would one apply this library towards graph formulations?;[education, open-source];48;
2601;1;2014-12-04T08:54:32.863;"How to use ""sann"" function in R to solve graph problems?";"I came across a package in R which has a function called sann for simulated annealing. sann uses parameters fn and gr to optimize and to select new points, respectively. For something like the max-clique or max-stable set problems, fn would be a summing function, but it's less clear how one would formulate gr to fix these graph computations. In these cases, how would gr ""select""?";[education, open-source];157;
2602;2;2014-12-04T15:12:57.327;;Item Based Algorithmfor every item i that u has no preference for yet  for every item j that u has a preference for    compute a similarity s between i and j    add u's preference for j, weighted by s, to a running average return the top items, ranked by weighted averageUser Based Algorithmfor every item i that u has no preference for yet for every other user v that has a preference for i   compute a similarity s between u and v   add v's preference for i, weighted by s, to a running average return the top items, ranked by weighted averageItem vs User based:1) Recommenders scale with the number of items or users they must deal with, so there are scenarios in which each type can perform better than the other2) Similarity estimates between items are more likely to converge over time than similarities between users3) We can compute and cache similarities that converge, which can give item based recommenders a performance advantage4) Item based recommenders begin with a list of a user's preferred items and therefore do not need a nearest item neighborhood as user based recommenders do;;;
2604;2;2014-12-04T18:36:57.947;;"Custom Google SearchYou can use the Custom Google Search for datasets:Google Custom Search: DatasetsIt includes 230 sources and meta-sources of datasets, including all mentioned in this question. Please, feel free to exclude .gov and any other websites from results by adding "" -.gov"" or "" -site.com"" to the search line. Other Google Search Operators work.Don't hesitate to contact me if you have ideas what websites to add.IOGDSThe following service categorizes more than 1,000,000 public datasets:IOGDS: International Open Government Dataset Search";;;
2605;2;2014-12-04T20:25:54.433;;Yes, see the following links:http://www.quora.com/In-what-way-can-I-use-the-CrossFoldLearner-class-in-Mahout-v0-8-0-9-to-perform-K-fold-cross-validationhttps://mahout.apache.org/users/classification/logistic-regression.html;;;
2606;2;2014-12-04T22:13:30.747;;Classes related to Artificial Intelligence are typically taught in Computer Science departments. Looking at the IT Project Subjects offered by your university, I suspect Data Analytics would indeed be more relevant to AI than Internetworking and Applications.Looking at the courses offered by your department, the following likely involve aspects of AI:Image Processing and Pattern RecognitionIntelligent AgentsBuilding Intelligent AgentsFor self-directed study in AI, I recommend starting with Russell & Norvig's essential textbook Artificial Intelligence: A Modern Approach.As to what it will take to create a human-like strong AI, I recommend this collection of essays: The Philosophy of Artificial Intelligence... even though the material is getting a bit out-of-date by now.Good luck!;;;
2607;2;2014-12-04T23:26:15.803;;Would it be possible to use Approximate Bayesian computation (ABC)? If you assume a distribution for the number of competitors (e.g. Poisson), select a subset of competitors each iteration and simulate your data using multinomial distributions with probabilities based on competitors' features, after discarding parameters that don't match your training data, you should be able to obtain parameters for each competitor (that is, posterior distributions) and generate more races. This might not work if the number of competitors is so important that it affects the coefficients of features for each competitor.;;;
2608;1;2014-12-05T00:26:12.983;Simple Excel Question: VLookup Error;My data looks like this: Why is this error showing up?;[education, open-source];51;
2609;1;2014-12-05T05:50:29.903;Good books for Hadoop, Spark, and Spark Streaming;Can anyone suggest any good books to learn hadoop and map reduce basics?Also something for Spark, and Spark Streaming?Thanks;[education, open-source];446;3
2610;2;2014-12-05T11:05:15.257;;for hadoop try hadoop in action  or Hadoop: The Definitive Guide;;;
2611;2;2014-12-05T11:22:01.423;;Here's a good open source book for spark;;;
2612;1;2014-12-05T14:03:58.777;Name Anonymization Software;"Although I have seen a few good questions asked about data anonymization, I was wondering if there were answers to this more specific variant.I am seeking a tool (or to design one) that will anonymize human names from a specific country: particularly first names in unstructured text. Many of the tools that I have seen have considered the wider dimensions of data anonymization; with an equal focus on dates of birth, addresses, etc.An imperative aspect is that it needs to have near absolute recall. The major pitfalls, as far as I can see, are diminutive variants (""Tommy"" instead of ""Thomas"", ""Ben"" instead of ""Benjamin"", etc.) and typos. These two factors prevent a simple regex based on a database of names (based on censuses, etc.)";[education, open-source];96;1
2613;2;2014-12-05T15:28:39.640;;Ruby together with Nokogiri allows to access HTML and XML documents via XPath and CSS selectors. Here is a tutorial.;;;
2614;2;2014-12-05T15:29:04.743;;I don't think you really need some special software, but rather to employee existing tools, such as encryption algorithms.Why not just encrypt the names with any key-based algorithm and store the key securely?If you didn't need to be able to recover the names, but just to identify variation to the level of differences in diminutives, then you could simply use hashing rather than encryption.I'm not sure what environment you want to carry this out it, but any language such as R or SQL/NoSQL database could easily carry this out programmatically.;;;
2615;2;2014-12-05T15:31:06.383;;"This is a common result of imprecise matching, such as with whitespace problems. For example, if you imagine that the grey shading used in codeblocks on StackExchange represents the whitespaces in your Excel cells, then   AK is technically the same as AK  You can trim and/or use wildcard chars to allow matching to skip spaces. For example:=VLOOKUP(CONCATENATE(""*"",TRIM(A2),""*""), $E$2:$F$5, 2,FALSE)";;;
2616;2;2014-12-05T15:38:09.867;;"There's such an overwhelming amount of literature that with programming, databases, and Big Data I like to stick to the O'reilly series as my go-to source. O'reilly books are extremely popular in the industry and I've been very satisfied.A current version of Hadoop: The Definitive Guide, MapReduce Design Patterns, and Learning Spark might suit your needs by providing high quality, immediately useful information and avoiding information overload -- all are published by O'reilly.Spark Streaming is covered in Chapter 13 of ""Learning Spark"".";;;
2617;2;2014-12-05T15:43:40.720;;In terms of open source NLG components, I'm most familiar with Mumble and FUF/SURGE. They've got both similarities and differences, so it's hard to say which is better...Mumble:written in LispEPL licensebased on tree-adjoining grammarfocuses on linguistic message planningFUF/SURGE:written in LispGPL licensebased on functional unification grammarfocuses on syntactic realizationSince it sounds like you're interested in abstractive summarization (which is much harder than traditional extractive summarization), I'd recommend the following academic papers:Text Generation for Abstractive SummarizationFramework for Abstractive Summarization using Text-to-Text GenerationTowards a Framework for Abstractive Summarization of Multimodal Documents -- full disclosure: I'm the author of this oneAlso, consider checking out this textbook to get started: Building Natural Language Generation Systems;;;
2618;1;2014-12-05T15:51:54.690;Ethically and Cost-effectively Scaling Data Scrapes;Few things in life give me pleasure like scraping structured and unstructured data from the Internet and making use of it in my models. For instance, the Data Science Toolkit (or RDSTK for R programmers) allows me to pull lots of good location-based data using IP's or addresses and the tm.webmining.plugin for R's tm package makes scraping financial and news data straightfoward. When going beyond such (semi-) structured data I tend to use XPath.However, I'm constantly getting throttled by limits on the number of queries you're allowed to make. I think Google limits me to about 50,000 requests per 24 hours, which is a problem for Big Data.From a technical perspective getting around these limits is easy -- just switch IP addresses and purge other identifiers from your environment. However, this presents both ethical and financial concerns (I think?).Is there a solution that I'm overlooking?;[education, open-source];184;5
2619;2;2014-12-05T15:57:07.737;;"I'm not sure I fully understand your question, but it seems to me that you're trying to determine the category of the string/entity ""titanic"" out of context. Your data tells you that ""titanic"" could be a book, a movie, or a product, and you want to figure out which one is correct -- is that what you're trying to do?If so, the problem is that you've dropped the context in which the string/entity ""titanic"" appears in your original text. For example...In the sentence ""I couldn't stop reading Titanic,"" the word ""titanic"" refers to a book.In the sentence ""Titanic was one of the highest-grossing films of all time,"" the word ""titanic"" refers to a movie.In the sentence ""The Titanic was the world's largest ocean liner,"" the word ""titanic"" refers to a product.Without that context, there's no way to know which is the correct category. I'd suggest looking into how named entity recognition tools like Stanford NER work -- that will help you better understand how to do something like this. You'll see that the input to an NER tool generally needs to be a sentence, in order to take advantage of the context to properly categorize the extracted entities.";;;
2620;2;2014-12-05T16:14:01.097;;From an older version of the OpenNLP README: Training the Tools There are training tools for all components expect the coref component. Please consult the help message of the tool and the javadoc to figure out how to train the tools. The tutorials in our wiki might also be helpful. The following modules currently support training via the WordFreak opennlp.plugin v1.4 (http://wordfreak.sourceforge.net/plugins.html).  coreference: org.annotation.opennlp.OpenNlpCoreferenceAnnotator (use opennlp 1.4.3 for training, models are compatible)  Note: In order to train a model you need all the training data. There is not currently a mechanism to update the models distributed with the project with additional data.As you can see, OpenNLP does not provide training tools for the coreference component. However, it seems at one point it was possible to train new models for OpenNLP's coref component using the third-party WordFreak plugin... however, it hasn't been updated in over a decade, so your mileage may vary.;;;
2621;2;2014-12-05T16:35:42.910;;"First, some clarification on terminology.A package in R is a collection of R functions, data, and compiled code in a well-defined format. SANN (sann) is not a package. Depending on which package you're using, sann is either a function or, more often, a method used within an optimization function.Packages containing sann include optim, trustOptim, consPlan, and constrOptim.In the package optim, the sann method is implemented as:> func <- function(x){+   out <- (x[1]-2)^2 + (x[2]-1)^2+   return <- out+   }> > optim(par=c(0,0), fn=func, gr = NULL,+       method = c(""SANN""),+       lower = -Inf, upper = Inf,+       control = list(), hessian = T)As you said, for the ""SANN"" (sann) method gr is used to generate a new candidate point. If it is NULL a default Gaussian Markov kernel is used.Now in your use case -- the case of a graph -- what you probably want to do is to use par and value to pass values to fn and gr. This is a nice feature of this implementation of SANN in optim which is covered a little more than half way  through this documentation page.par is the best set of parameters found and value is the value of fn corresponding to par.";;;
2622;2;2014-12-05T16:37:54.327;;The Data Science Toolkit is a powerful library (or collection of libraries, technically) which are available in a number of languages. For instance, I use the implementation called RDSTK in R.In the case of your preferred language, Google Go, there's a list of web-related libraries here which looks very useful.;;;
2623;1;2014-12-05T16:54:13.967;How to Interpret Multinomial Specification in R's `mnlogit` package;The mnlogit package in R allows for the fast estimation of multinomial logit models.The specification of forumlas is a bit different from most other regression models/packages in R, however. Using the Fish dataset as a reproducible example, > require(mnlogit)Loading required package: mnlogitPackage: mnlogitVersion: 1.1.1Multinomial Logit Choice Models.Scientific Computing Group, Sentrana Inc, 2013.> data(Fish, package ='mnlogit')> head(Fish)           mode   income     alt   price  catch chid1.beach   FALSE 7083.332   beach 157.930 0.0678    11.boat    FALSE 7083.332    boat 157.930 0.2601    11.charter  TRUE 7083.332 charter 182.930 0.5391    11.pier    FALSE 7083.332    pier 157.930 0.0503    12.beach   FALSE 1250.000   beach  15.114 0.1049    22.boat    FALSE 1250.000    boat  10.534 0.1574    2I'm trying to understand the difference between the model specification of fm <- formula(mode ~ 0 + price | income | catch)andfm <- formula(mode ~ 0 + price | income + catch)while the documentation covers the detail of such changes  in the general coeffcient area of the forumla (i.e. where price is), I don't see an explanation of how operators like + affect the alternative-specific area of the formula/code, relative to |.;[education, open-source];76;1
2624;2;2014-12-05T17:13:14.073;;"For many APIs (most I've seen) ratelimiting is a function of your API Key or OAuth credentials. (Google, Twitter, NOAA, Yahoo, Facebook, etc.) The good news is you won't need to spoof your IP, you just need to swap out credentials as they hit there rate limit. A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.https://github.com/rawkintrevo/angemilnerhttps://pypi.python.org/pypi/angemilner/0.2.0It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses. Load keys:from angemilner import APIKeyLibrarianl= APIKeyLibrarian()l.new_api_key(""your_assigned_key1"", 'noaa', 1000, .2)l.new_api_key(""your_assigned_key2"", 'noaa', 1000, .2)Then when you run your scraper for instance the NOAA api:url= 'http://www.ncdc.noaa.gov/cdo-web/api/v2/stations' payload= {  'limit': 1000,        'datasetid':  'GHCND',         'startdate': '1999-01-01' }r = requests.get(url, params=payload, headers= {'token': 'your_assigned_key'})becomes:url= 'http://www.ncdc.noaa.gov/cdo-web/api/v2/stations'payload= {  'limit': 1000,            'datasetid':  'GHCND',            'startdate': '1999-01-01' }r = requests.get(url, params=payload, headers= {'token': l.check_out_api_key('noaa')['key']})so if you have 5 keys, l.check_out_api_key returns the key that has the least uses and waits until enough time has elapsed for it to be used again. Finally to see how often your keys have been used / remaining useage available:pprint(l.summary())I didn't write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported. Thats how you can technically get around rate limiting.  Ethically ...UPDATE The example uses Google Places API here";;;
2625;2;2014-12-05T18:09:27.670;;If your only motivation for using Google Go is webscraping, and you want to do you ML in python, I would recommend the following stack:Python requests for scraping dataMongoDB for caching data (MongoDB's page oriented format makes it a natural home for storing JSON objects commonly returned by APIs)pymongo for interfacing python and mongodbscikit-learn for doing your machine learningThis all happens in python and you can extend it multiple processors with multiprocessing or to multiple nodes with django;;;
2626;2;2014-12-05T20:13:17.793;;If you are willing to pay for a vendor solution, Teradata is designed to solve the problem of large scale joins with low latency.;;;
2627;1;2014-12-06T00:41:24.933;3D map using leaflet;I'm trying to create 3D bars on this map. Can anyone please advise if this is possible, and how?http://leafletjs.com/examples/choropleth.htmlMy data: UFO sightings in the USA (location wise).Count of these sightings per location will be the height of the 3D bar.Base map is a choropleth with US population density values.I don't mind integrating Javascript or d3.js into the code to create the 3D bars.;[education, open-source];30;
2628;1;2014-12-06T01:10:30.817;SAS Nested Likelihood Ratio Test for a Logistic Model;"Using SAS Studio (online, student version)...Need to do a ""nested likelihood ratio test"" for a logistic regression. Entirety of instructions are: ""Perform a nested likelihood ratio test comparing your full model (all predictors included)to a reduced model of interest.""The two models I have are:Proc Logistic Data=Project_C;Model Dem (event='1') = VEP TIF Income NonCit Unemployed Swing;Run;andProc Logistic Data=Project_C;Model Dem (Event='1') = VEP TIF Income / clodds=Wald clparm=Wald expb rsquare;Run;I honestly have no idea where to even start. Any suggestions would be appreciated. Thanks!";[education, open-source];47;
2629;1;2014-12-06T06:53:14.617;"What is ""data science""?";"In recent years, the term ""data"" seems to have become a term widely used without specific definition. Everyone seems to use the phrase. Even people as technology-impaired as my grandparents use the term and seem to understand words like ""data breach."" But I don't understand what makes ""data science"" a new discipline. Data has been the foundation of science for centuries. Without data, there would be no Mendel, no Schrödinger, etc. You can't have science without interpreting  and analyzing data. But clearly it means something. Everyone is talking about it.  So what exactly do people mean by data when they use terms like ""big data"" and why has this become a discipline in itself? Also, if it is an emerging discipline, where can I find more serious/in-depth information so I can better educate myself? Thanks!";[education, open-source];75;
2630;2;2014-12-06T13:02:19.373;;"I get asked this question all the time, so earlier this year I wrote an article (What is Data Science?) based on a presentation I've given a few times. Here's the gist...First, a few definitions of data science offered by others:Josh Wills from Cloudera says a data scientist is someone ""who is better at statistics than any software engineer and better at software engineering than any statistician.""A frequently-heard joke is that a ""Data Scientist"" is a Data Analyst who lives in California.According to Big Data Borat, Data Science is statistics on a Mac.In Drew Conway's famous Data Science Venn Diagram, it's the intersection of Hacking Skills, Math & Statistics Knowledge, and Substantive Expertise.Here's another good definition I found on the ITProPortal blog: ""A data scientist is someone who understands the domains of programming, machine learning, data mining, statistics, and hacking""Here's how we define Data Science at Altamira (my current employer):The bottom four rows are the table stakes -- the cost of admission just to play the game. These are foundational skills that all aspiring data scientists must obtain. Every data scientist must be a competent programmer. He or she must also have a solid grasp of math, statistics, and analytic methodology. Data science and ""big data"" go hand-in-hand, so all data scientists need to be familiar with frameworks for distributed computing. Finally, data scientists must have a basic understanding of the domains in which they operate, as well as excellent communications skills and the ability to tell a good story with data.With these basics covered, the next step is to develop deep expertise in one or more of the vertical areas. ""Data Science"" is really an umbrella term for a collection of interrelated techniques and approaches taken from a variety of disciplines, including mathematics, statistics, computer science, and software engineering. The goal of these diverse methods is to extract actionable intelligence from data of all kinds, enabling clients to make better data-driven decisions. No one person can ever possibly master all aspects of data science; doing so would require multiple lifetimes of training and experience. The best data scientists are therefore ""T-shaped"" individuals -- that is, they possess a breadth of knowledge across all areas of data science, along with deep expertise in at least one. Accordingly, the best data science teams bring together a set of individuals with complementary skillsets spanning the entire spectrum.";;;
2631;1;2014-12-06T15:04:03.823;What would be a good way to use clustering for outlier detection?;For simplicity let's assume the feature space is the XY plane.;[education, open-source];97;
2632;1;2014-12-06T17:56:53.157;Timing sequence in MapReduce;I'm running a test on MapReduce algorithm in different environments, like Hadoop and MongoDB, and using different types of data. What are the different methods or techniques to find out the execution time of a query.If I'm inserting a huge amount of data, consider it to be 2-3GB, what are the methods to find out the time for the process to be completed.;[education, open-source];24;
2633;1;2014-12-07T04:40:53.677;Can hadoop with Spark be configured with 1GB RAM;I'm trying to set up a cluster (1 namenode, 1 datanode) on AWS.I'm using free one year trial period of AWS, but the challenge is, instance is created with 1GB of RAM.As I'm a student, I cannot afford much. Can anyone please suggest me some solution?Also, it would be great if you could provide any links for setting up multi cluster hadoop with spark on AWS.Note: I cannot try in GCE as my trial period is exhausted. ;[education, open-source];173;
2634;1;2014-12-07T10:43:01.697;Linear combination of weak estimators over fuzzy classifiers?;"Having:a set of soft fuzzy classifiers (classification onto overlapping sets) $C_i(x) \to [0,1]$;a corresponding set of weak estimators $R_i(z)$ of the form $R_i(z) = \mathit{EX}(y\mid z)$.The estimators $R_i$ are just some kind of regression, Kalman or particle filters. The classifiers $C_i$ are fixed and static. How to make a strong estimator out of a weighted combination of the form:$$L(x, z) = \sum_{i}C_i(x)R_i(z)Q_i$$In other words how to choose the weights $Q_i$? Is there some kind of online approach to this problem? Here is brief description of a practical application. When an event $E$ is registered, multiple measurements are made. Based on these measurements, the classifiers $C_i$ make a soft assignment of the event to multiple overlapping categories. What we get is fit ratios for the soft clusters.Now there is some chance that event $E$ may trigger a subsequent event $D$, depending on another variable $z$ -- independent from the event $E$. We know that all the soft cluster ""memberships"" may influence the probability of event $D$ being triggered.We want to estimate the probability that $E$ triggers $D$, given the $C_i$ fitness ratios and value of $z$.";[education, open-source];40;
2635;2;2014-12-07T11:29:14.057;;I am not 100% if a message queue library will be the right tool for this job but so far it looks to me so.With a messaging library like:nsqzeromqmqtt (?)You can connect different processes operating on different environment through a TCP based protocol. As these systems run distributed it is possible to connect multiple nodes.For nsq we even have a library in Python and Go!;;;
2636;2;2014-12-07T12:16:10.207;;Perhaps you could cluster the items, then those items with the furthest distance from the midpoint of any cluster would be candidates for outliers.;;;
2637;2;2014-12-07T13:37:40.373;;So if 4GB of RAM isn't sufficient, 1GB isn't going to be. That is really too little to run an HDFS namenode, a datanode, YARN, Spark driver alone, let alone leaving room for your workers.Much more reasonable is to simply run Spark locally on that instance without Hadoop at all.But I would question whether Spark is the right choice if you are definitely limited to such a small machine.;;;
2638;1;2014-12-07T15:05:31.213;Questions about Q-Learning using Neural Networks;"I have implemented Q-Learning as described here. In order to approximate $Q(S,A)$, I use a neural network structure like the following:Activation sigmoid;Inputs, number of inputs +1 for Action neurons (All Inputs Scaled 0-1);Outputs, single output. Q-Value;N number of M Hidden Layers;Exploration method random 0 < $rand()$ < propExplore;At each learning iteration using the following formula,I calculate a Q-Target value then calculate an error using,$$\mathit{error} = \text{QTarget} - \text{LastQValueReturnedFromNN}$$and back propagate the error through the neural network.Questions:Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1).From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won't learn even after 40k 50k iterations.";[education, open-source];52;
2639;2;2014-12-07T15:39:00.947;;You might find a solution for this by checking out Viola & Jones face detection algorithm (and object detection in general) http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf. Particularly the AdaBoost algorithm for building a strong classifier from weak classifiers. https://www.cs.princeton.edu/~schapire/papers/explaining-adaboost.pdfIn this algorithm this is used particularly for feature selection, where it is wanted to select just the features that grouped together classify better, discarding the other as noisy features.The approach to obtain this strong classifier is having a set of examples X, a set of weights W and a set of expected results (classifications) Y. For a two-class example (e.g. face o no face image) the first weak classifier is selected and then the classification error is found between this classifier and the expected output. For the samples that were misclassified their weights are incremented, so the next weak classifier to find is more biased to neglect this wrong classification.The algorithm (AdaBoost) converges when the sign of the sum of classifications for each sample outputs the correct classification.For example:Y = +1 -1 +1 +1WC1=+1 +1 +1 +1WC2=-1 -1 -1 -1WC3=+1 -1 +1 +1So the strong classifier is WC1+WC2+WC3:SC=+1 -1 +1 +1 == YHope this solves your question.;;;
2640;2;2014-12-07T15:46:40.670;;A very robust clustering algorithm against outliers is PFCM from Bezdek http://www.comp.ita.br/~forster/CC-222/material/fuzzyclust/fuzzy01492404.pdf.In this paper Bezdek proposes Possibilistic-Fuzzy-C-Means which is an improvement of the different variations of fuzzy posibilistic clustering. This algorithm is particularly good at detecting outliers and avoiding them to influence the clusterization. So using PFCM you could find which points are identified as outliers and at the same time have a very robust fuzzy clustering of your data.;;;
2641;2;2014-12-07T16:17:20.613;;Following link contains a list of positive and negative polarised emotions on the scale of [-5, 5]. Just try to count up the scores based on the word matches and you can get overall movie review score. http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010;;;
2642;1;2014-12-07T16:24:43.997;Machine Learning & Partial Differential Equations;Are there any algorithms which were developed using partial differential equations for tackling some of the machine learning problems? Most works I see online are in the field of computer vision and a few bizarre ones in topic modelling. But just curious if someone has used or seen it being used for some decision making process or classification problems?;[education, open-source];434;
2643;1;2014-12-07T16:31:57.913;How to set up multi cluster spark without hadoop on Google Compute engine;I'm new to apache spark. Is it possible to configure multi cluster spark without hadoop?If so, can you please provide the steps. I would like to create clusters on Google Compute Engine  (1-master, 1-worker);[education, open-source];234;
2644;1;2014-12-08T08:15:51.563;Critical To Quality (CQT) diagram for a corporation, problem solving, six sigma, lean manufacturing question;"What would you put on a CTQ or Voice of Customer diagram for the following hypothetical scenario? Sinclair Corporation   THE COMPANY   Sinclair Corporation is based in Traverse City, MI and  acquired Premium Photo Products seven months ago.  Premium makes several types of film. The “Premium  1000” series is made with a high quality, supersensitive  emulsion. It is nationally marketed in roll form for  amateurs and sheet and film-pack forms for  professionals.   Roll film is similar to most film commonly used by the  general public.   Sheet film is handled in total darkness, negatives being  withdrawn from between interleaf papers by the  photographer and inserted into a light-tight holder  which is then placed in the camera.   Film-pack is a self-contained unit ready to install in the  camera. The photographer merely pulls a paper tab to  shift the exposed negative out of position and a new  one into position.   The procedure for using and developing all of this film  has not changed since introduction. “Premium 1000” is  produced in small lots at the Denver plant. After  packaging, the film is shipped to dealers across the  country for display and sale; a remarkably stable film  with a shelf life of eighteen months, “Premium 1000”  has an excellent reputation in the trade.   THE SITUATION   During the last 48 hours Sales, Manufacturing and  Research have been working to resolve this sudden and  serious problem with the “Premium 1000” film.   Customer and dealer complaints about fogged sheet  film are mounting. This crisis now requires immediate  action before “Premium 1000” has to be removed from  the market entirely.   During the past ten days, dealers in several sales  districts have had customer complaints of fogged sheet  film. In some cases, dealers have accumulated  complaints for several days before they reported them.   Complaints, mounting every day, have since spread to  all sales districts in the country. Right now there are  complaints from 682 of our 7000 dealers. Although  Sales is replacing each box of fogged film with two new  boxes, this has not placated anyone. Two commercial  studios, for example, threaten total cancellation of  further use of our products because our film, which  they used to photograph unique events, was fogged and  the pictures consequently ruined.   Sales has been very concerned by several  manufacturing problems which have resulted in failures  to meet delivery requirements. For example: a serious  fire two months ago in the “Premium 1000” film  emulsion department resulted in the discharge of  several employees for carelessness and the intensive  training of the new emulsion mixers; delivery quantity  difficulties on interleaf paper for over a month after the  acquisition; delay in arrival of the new bulk raw  ingredient storage containers. Though the interleaf  delivery difficulties cleared up by acquiring a new  suppler, all of the new bulk storage containers have not  arrived and so the old ones are still being used.   Sales of sheet film, as well as film-pack, have quickly  dropped and some professionals have begun using  competitive brands. So far, replacing the fogged film  and Premium’s good reputation has kept most dealers  from defecting.   Sales found that all “Premium 1000” film is shipped,  distributed, stocked and sold by dealers the same way.  They also found that all complaints so far are coming  from small outlets where turnover is very slow, but they  expect to hear complaints from the larger outlets soon.   None of the film was outdated and the fogging was  seen as soon as it was developed rather than later.  Sales asks that we stop production for a couple of days  until Manufacturing thoroughly checks out their  operation (i.e., effects of the fire, effectiveness of  emulsion mixer training, and possible contamination by  the new storage containers). The expense involved in  doing this would be about \$30,000.   Manufacturing believes the new zip-top sheet film  package is responsible. Because of substantial  customer complaints that our package was hard to  open, Sales rushed the introduction of the easier to  open zip-top package five weeks ago. There was no Page 2 of 2  time for thorough testing. The problem started since  then. Manufacturing recommends going back to the  former package. It was used for three years without  difficulty. This will gain time to further investigate the  zip-top package. They urge an immediate hold on  expansion of the zip-top package to other “Premium  1000” film. This proposal would require a commitment  of only \$20,000, but could invite an added source of  known customer dissatisfaction at this critical time.  Research has conducted tests on sample returns. Their  findings are below.   THE LAB REPORT   Memo from the Research Director (delivered 4 days  after the first reports started coming in)  We have studied the samples of returned fogged film  provided by Sales and have attempted to reproduce the  fogging exactly as it appears on the returns.   We analyzed fifty returned boxes of film and found two  things they all had in common: all negatives in a box are  fogged; fogging is even across the entire surface of each  negative.   We then took thirty-six samples of good sheet film from  the plant warehouse and carefully subjected these to a  variety of tests. The only way we were able to  reproduce exactly the unique fogging effect was to  expose the sheet film to a source of radiation (such as  an x-ray machine).   Recognizing that this type of film is supersensitive and is  more susceptible to contamination, and having clearly  demonstrated that radiation produces the exact  characteristics of the returned fogged film, we believe  that a source of radioactivity is the root of the problem.   We know that traces of radioactive fallout have  contaminated various products in the past. With the  current high incidence of Strontium 90 in th";[education, open-source];48;
2645;1;2014-12-08T10:47:52.340;Assign new point to a class using spectral clustering;Say I used spectral clustering to cluster a data-set $D$ of points $X_0 - X_n$ into a number $C$ of clusters. How can I efficiently assign a new single point $X_{n+1}$ to his convenient cluster?Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set $X_0 - X_{n+1}$), or is there an optimized way to extend to the point $X_{n+1}$?;[education, open-source];79;
2646;1;2014-12-08T14:51:27.613;Extract most informative parts of text from documents;Are there any articles or discussions about extracting part of text that holds the most of information about current document.For example, I have a large corpus of documents from the same domain. There are parts of text that hold the key information what single document talks about. I want to extract some of those parts and use them as kind of a summary of the text. Is there any useful documentation about how to achieve something like this. It would be really helpful if someone could point me into the right direction what I should search for or read to get some insight in work that might have already been done in this field of Natural language processing.;[education, open-source];217;1
2648;2;2014-12-08T15:48:25.647;;"What you're describing is often achieved using a simple combination of TF-IDF and extractive summarization.In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its ""importance."" Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary.Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: A Smattering of NLP in Python.";;;
2649;2;2014-12-08T20:55:55.677;;I think you do not, since I was able to find several papers that proposed algorithms for the same:Incremental spectral clustering by efficiently updating the eigen-systemIncremental kernel spectral clustering for online learning of non-stationary dataA Fast Incremental Spectral Clustering for Large Data SetsSuch algorithms are called sequential, incremental, streaming, or online. Armed with this knowledge, you can find more papers on your own.;;;
2650;2;2014-12-08T22:32:05.817;;Neil is correct. There are partial derivatives evwrywhere in gradient computation for machine learning models training.For instance you can look at the gradient descent method used in the backpropagation method for a neural network. The course from AndrewNg on coursera describes it very well.;;;
2651;1;2014-12-08T22:37:32.777;Deep learning basics;I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?;[education, open-source];852;17
2652;1;2014-12-09T00:58:03.057;java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream;"Im traying to integrate Hadoop and R, I was install the pachages rJava and Rhipe in R, I do this steps to start Hadoop and R:-starting Hadoop services.,-loading rJava and Rhipe packages by library function.-Calling rhinit() to initialize Rhipe.the problem here is when I call rhinit() funtion, it show this error:  Initializing Rhipe v0.73    Error in .jnew(""org/godhuli/rhipe/PersonalServer"") :      java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream' please some helps to fixe this problem.";[education, open-source];564;
2653;2;2014-12-09T01:19:32.290;;The subject is new so most of the wisdom is scattered in papers, but here are two recent books:Deep Learning, Yoshua Bengio, Ian J. Goodfellow, Aaron Courville.Deep Learning: Methods and Applications, Li Deng and Dong Yu.And some practical material: http://deeplearning.net/tutorial/ACL 2012 + NAACL 2013 Tutorial: Deep Learning for NLP (without Magic);;;
2654;1;2014-12-09T02:28:51.430;Web Framework Built for Recommendations;I'm wondering if there is a web framework well suited for placing recommendations on content.In most cases, a data scientist goes through after the fact and builds (or uses) a completely different tool to create recommendations. This involves analyzing traffic logs, a history of shopping cart data, ratings, and so forth. It usually comes from multiples sources (the web server, the application's database, Google Analytics, etc) and then has to be cleaned up and processed, THEN delivered back to the application in way it understands.Is there a web framework on the market which handles collecting this data up front, as to minimize the retrospective data wrangling?;[education, open-source];60;
2655;2;2014-12-09T08:26:57.997;;Neural Networks and Deep Learning by Michael Nielsen. The book is still in progress, but it looks quite interesting and promising. And it's free! Here's the link: http://neuralnetworksanddeeplearning.com/ There are only 5 chapters so far, and the most of them talk about usual neural networks, but it's still worth having a look. Update: the book has been finished!;;;
2656;2;2014-12-09T08:30:28.000;;Found it myself.Go to context menu right clicking to the dimension field.Go to Aliases... and change the labels.;;;
2658;1;2014-12-09T20:49:37.093;Using NLP to automate the categorization of user description;I have a huge file of customer complaints about the products my company owns and I would like to do a data analysis on those descriptions and tag a category to each of them. For example: I need to figure out the number of complaints on Software and Hardware side of my product from the customer complaints. Currently, I am using excel to do the data analysis which do seek a significant amount of manual work to get a tag name to the complaints.Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?;[education, open-source];113;2
2659;1;2014-12-09T21:13:10.797;kNN - what happens if more than K observation have the same distance to the centroid of the cluster;EDIT It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I'm still new to this topic and confuse the terms quite often. So here is the changed question.I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation.Imagine that we pick kNN for some dataset. I want to remain as general as possible, thus $k$ will not be specified here. Further we select, at some point, an observation where the number of neighbors that fulfill the requirement to be in the neighbourhood are actually more than the specified $k$.What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighborhood (number of neighbors). Which observations will be left out and why? Also is this a problem that occurs often, or is it something of an anomaly?;[education, open-source];100;1
2660;2;2014-12-09T21:19:23.747;;"One way to handle this is to use 'supervised classification'. In this model, you manually classify a subset of the data and use it to train your algorithm. Then, you feed the remaining data into your software to classify it.This is accomplished with NLTK for Python (nltk.org).If you are simply looking for strings like ""hardware"" and ""software"", this is a simple use case, and you will likely get decent results using a 'feature extractor', which informs your classifier which phrases in the document are relevant. While it's possible to implement an automated method for finding the keywords, it sounds like you have a list in mind already, so you can skip that step and just use the tags you are aware of. (If your results aren't satisfactory the first time, this is something you might try later on).That's an overview for getting started. If you are unhappy with the initial results, you can refine your classifier by introducing more complex methods, such as sentence segmentation, identification of dialogue act types, and decision trees. The sky is the limit (or more likely, your time is the limit)!More info at:http://www.nltk.org/book/ch06.html";;;
2661;2;2014-12-09T21:33:31.200;;K-means does not make an assumption regarding how many observations should be assigned to each cluster. K is simply the number of clusters one chooses to generate. During each iteration, each observation is assigned to the cluster having the nearest mean. There is no assumption that all clusters should have a comparable number of observations assigned (i.e., for N observations, there is no expectation that each cluster should have ~ N/K observations assigned).It is quite possible that the numbers of observations in the various clusters are highly imbalanced. This can be due to the distribution of the data, the number of clusters chosen (K), or even how the cluster means are initialized.;;;
2662;2;2014-12-09T22:32:22.660;;"You are mixing up kNN classification and k-means.There is nothing wrong with having more than k observations near a center in k-means. In fact, this it the usual case; you shouldn't choose k too large. If you have 1 million points, a k of 100 may be okay. K-means does not guarantee clusters of a particular size. Worst case, clusters in k-means can have only one element (outliers) or even disappear.What you probably meant to write, but got mixed up, is what to do if a point is at the same distance to two centers.From a statistical point of view, it doesn't matter. Both have the same squared error.From an implementation point of view, choose any deterministic rule, so that your algorithm converges and doesn't go into an infinite loop of reassignment.Update: with respect to kNN classification:There are many ways to resolve this, that will surprisingly often work just as good as the other, without a clear advantage of one over the other:randomly choose a winner from the tied objectstake all into account with equal weightingif you have m objects at the same distance where you expected only r, then put a weight of r/k on each of them.E.g. k=5.distance   label   weight    0        A       1    1        B       1    1        A       1    2        A      2/3    2        B      2/3    2        B      2/3yields A=2.66, B=2.33The reason that randomly choosing works just as good as the others is that usually, the majority decision in kNN will not be changed by contributions with a weight of less than 1; in particular when k is larger than say 10.";;;
2663;2;2014-12-09T22:33:38.640;;Gaussian mixture modeling can - if your data is nicely gaussian-like - be used for outlier detection. Points with a low density in every cluster are likely to be outliers.Works well in idealistic scenarios.;;;
2664;2;2014-12-10T01:59:48.700;;I haven't seen anything like that and very much doubt that such frameworks exist, at least, as complete frameworks. The reason for this is IMHO the fact that data transformation and cleaning is very domain- and project-specific. Having said that, there are multiple tools that can help with these activities in terms of partial automation and integration with and between existing statistical and Web frameworks.For example, for Python, the use of data manipulation library pandas as well as machine learning library scikit-learn can be easily integrated with Web frameworks (especially Python-based, but not necessarily), as these libraries are also Python-based. These and other Python data science tools that might be of interest can be found here: http://pydata.org/downloads. Specifically, for cleaning and pre-processing tasks, which you asked about, pandas seem to be the first tool to explore. Again, for Python, the following discussion on StackOverflow on methods and approaches might be helpful: http://stackoverflow.com/q/14262433/2872891.Consider an example of another platform. The use of pandas for data transformation and cleaning is rather low-level. The platform that I like very much and currently use as the platform of choice for data science tasks is R. Rich ecosystem of R packages especially shines in the area of data transformation and cleaning. This is because, in addition to very flexible low-level methods of performing these tasks, there are some R packages, which take a higher-level approach to the problem, which may potentially improve developer's productivity and decrease the amount of defects. In particular, I'm talking about two packages, which I find very promising: editrules and deducorrect. You can find more detailed information about these and other R packages for data transformation and cleaning in my another answer here on Data Science StackExchange (paper that I reference in the last link there could be especially useful, as it presents an approach to data transformation and cleaning that is generic enough, so that could be used as a framework for this on any decent platform): http://datascience.stackexchange.com/a/722/2452.UPDATE: On the topic of recommender systems and their integration with data wrangling tools and Web frameworks, you may find my other answer here on DS SE useful: http://datascience.stackexchange.com/a/836/2452.;;;
2667;1;2014-12-10T04:33:31.350;Python interface to Titan Database;How can I connect to Titan database from Python ? What I understand is that Titan (Graph database) provides an interface (Blueprint) to Cassandra (Column Store) and bulb is a python interface to graph DB.Now how can I start programming in python to connect with titan DB?Is there any good documentation/tutorial available ?;[education, open-source];362;
2668;1;2014-12-10T06:37:17.193;Data Science Tools Using Scala;I know that Spark is fully integrated with Scala. It's use case is specifically for large data sets. Which other tools have good Scala support? Is Scala best suited for larger data sets? Or is it also suited for smaller data sets? ;[education, open-source];232;2
2669;2;2014-12-10T07:12:33.640;;It is important to see all the rules if one of the states of target column is more important than others. For example, if you are predicting fraudulent transactions, you might want to flag something as fraud even if is has 5% probability.;;;
2670;1;2014-12-10T10:15:00.940;Visualizing deep neural network training;I'm trying to find an equivalent of Hinton Diagrams for multilayer networks to plot the weights during training. The trained network is somewhat similar to a Deep SRN, i.e. it has a high number of multiple weight matrices which would make the simultaneous plot of several Hinton Diagrams visually confusing. Does anyone know of a good way to visualize the weight update process for recurrent networks with multiple layers? I haven't found much papers on the topic. I was thinking to display time-related information on the weights per layer instead if I can't come up with something. E.g. the weight-delta over time for each layer (omitting the use of every single connection). PCA is another possibility, though I'd like to not produce much additional computations, since the visualization is done online during training.;[education, open-source];540;1
2671;2;2014-12-10T13:54:43.770;;[Since you've updated your question to refer to a different algorithm (changed k-means to kNN), I'm adding this as a separate answer specifically for kNN.]It appears you may still be confusing kNN with k-means. kNN does not have multiple stages of refinement, nor does it require computing centroids. It is a lazy learner that classifies a new observation by examining the k training observations that are closest to the new observation and picks whichever class is most prevalent among those k training observations. Note that the distances are not relative to a cluster centroid. You don't have to worry about there being more than k training observations in the neighborhood of the new observation because the neighborhood isn't based on a threshold distance - it is defined simply by the k points nearest to the new observation (i.e., it changes for each new observation being evaluated).A possible pathological case is when multiple training observations lie at exactly the same distance from the new observation, which would require you to evaluate more than k neighbors. But you would still simply pick the class that is most prevalent among that group of observations.;;;
2673;1;2014-12-10T14:18:36.933;Training Neural Networks with unknown length of input;"I'm currently going into the world of machine learning and Neural Networks, thanks to synaptic (js) that interests me a lot.So I read a lot, wikipedia links and synaptic's NN 101, but there's a lot of basics questions that I don't understand (but I'd like to) in the use of machine learning (NN) and the point of these technologies.Let's say, I wan't my network to (kind of) learn (something like) gravity, so to train it I set in input 10 objects with a mass, and a position x, y (and z) and I set output the new x, y (and z) of each objects.I guess I should give it several configurations and everything but here is the question; can it, then, be able to compute the interactions between 10000, 100000 objects?At this stage in my learning, what I don't clearly get is what is the point of teaching/training neurons to compute XOR like it's shown in synaptic's documentation:var trainingSet = [    {        input: [0,0],        output: [0]    },    {        input: [0,1],        output: [1]    },    {        input: [1,0],        output: [1]    },    {        input: [1,1],        output: [0]    },];var trainer = new Trainer(myNetwork);trainer.train(trainingSet);Were we just give it all the possible inputs and outputs to a XOR.Well, as I'm all new to the technologies I think my questions are full of non-sense and everything but thanks for reading and help you might bring :)";[education, open-source];94;
2675;2;2014-12-10T15:37:41.853;;Sheldon is correct, this sounds like a fairly typical use case for supervised classification. If all of your customer complaints are either software or hardware (i.e., zero individual complaints cover both categories, and zero are outside these two classes), then all you need is a binary classifier, which makes things simpler than they otherwise could be.If you're looking for a Java-based NLP toolkit that supports something like this, you should check out the Stanford Classifier: http://nlp.stanford.edu/software/classifier.shtml -- it's licensed as open source software under the GPL.Their wiki page should help you get started using the classifier: http://www-nlp.stanford.edu/wiki/Software/Classifier -- keep in mind that you'll need to manually annotate a large sample of your data as a training set, as Sheldon mentioned.;;;
2676;2;2014-12-10T15:41:03.987;;GravityBasically you'd like to find a function that maps an input (an object with a mass and a location) to an output (new location). It's not necessary to have one set of input neurons for each different object. It's sufficient to encode the input variables generically for all objects.Like a function: $$f( x, y, z, mass ) \to ( x_n, y_n, z_n )$$XORTraining the XOR function is special as the early neural networks, the perceptrons, were unable to learn the XOR function (because it is not linear separable). Multi-Layer Perceptrons however are able to learn the XOR function. This is just to demonstrate that the NN implementation of Synaptic is capable of learning problems that are not linear separable;;;
2677;1;2014-12-10T16:08:03.537;Document classification: tf-idf prior to or after feature filtering?;I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.I found out that tf-idf could be very useful for this. However, I was unsure as to when exactly to use it.Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:Retrieve site content, parse for plain text Normalize and stem content Tokenize into unigrams (maybe bigrams too)Retrieve a count of each unigram for the given document, filtering low length and low occurrence wordsTrain a classifier such as NaiveBayes on the resulting setMy question is the following: Where would tf-idf fit in here? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing? Any insight would be greatly appreciated.Edit:Upon closer inspection, I think I may have run into a misunderstanding at to how TF-IDF operates. At the above step 4 that I describe, would I have to feed the entirety of my data into TF-IDF at once? If, for example, my data is as follows: [({tokenized_content_site1}, category_string_site1),  ({tokenized_content_site2}, category_string_site2), ... ({tokenized_content_siten}, category_string_siten)}]Here, the outermost structure is a list, containing tuples, containing a dictionary (or hashmap) and a string.Would I have to feed the entirety of that data into the TF-IDF calculator at once to achieve the desired effect? Specifically, I have been looking at the scikit-learn TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.;[education, open-source];179;1
2678;2;2014-12-10T17:36:20.550;;As you've described it, Step 4 is where you want to use TF-IDF. Essentially, TD-IDF will count each term in each document, and assign a score given the relative frequency across the collection of documents.There's one big step missing from your process, however: annotating a training set. Before you train your classifier, you'll need to manually annotate a sample of your data with the labels you want to be able to apply automatically using the classifier.To make all of this easier, you might want to consider using the Stanford Classifier. It will perform the feature extraction and build the classifier model (supporting several different machine learning algorithms), but you'll still need to annotate the training data by hand.;;;
2679;2;2014-12-10T19:50:17.797;;The closes thing I know is ConvNetJS: ConvNetJS is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in your browser. Open a tab and you're training. No software requirements, no compilers, no installations, no GPUs, no sweat.Demos on this site plot weighs and how do they change with time (bear in mind, its many parameters, as practical networks do have a lot of neurons). Moreover, if you are not satisfied with their plotting, there is access to networks parameters and you can plot as you wish (since it is JavaScript).;;;
2682;1;2014-12-10T22:47:49.880;Use Cases of Neo4J and Spark GraphX;I have used Neo4J to implement a content recommendation engine. I like Cypher, and find graph databases to be intuitive.Looking at scaling to a larger data set, I am not confident No4J + Cypher will be performant. Spark has the GraphX project, which I have not used in the past.Has anybody switched from Neo4J to Spark GraphX? Do the use cases overlap, aside from scalability? Or, does GraphX address a completely different problem set than Neo4J?;[education, open-source];925;
3684;1;2014-12-11T06:33:37.140;Error in Spark installation on GCE;Can anyone please help me in resolving below issue,I'm trying to install spark on centos7, steps that i followed are,1) gctuil for ssh2) installed java 1.73) installed scala 2.10.14) installed spark 1.1.15) executing - sbt/sbt assembly/results in below error [warn] Multiple resolvers having different access mechanism configured with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate project resolvers (resolvers) or rename publishing resolver (publishTo).[info] Set current project to spark-parent (in build file:[error] Expected key[error] assembly/[error]          ^;[education, open-source];83;
3685;2;2014-12-11T07:08:00.540;;"Based on my cursory understanding of the topics, associated with your question, I think that Gephi (https://gephi.github.io; the original gephi.org link redirects there) should be able to handle neural network dynamic visualization. It seems that, in order to achieve your goal, you need to stream your graph(s) with corresponding weights (https://forum.gephi.org/viewtopic.php?t=1875). For streaming, you most likely will need this plug-in: https://marketplace.gephi.org/plugin/graph-streaming.UPDATE: You may also find useful SoNIA software: http://web.stanford.edu/group/sonia.";;;
3687;1;2014-12-11T13:37:29.430;R-Cassandra connection with RJDBC;"I am trying to connect with cassandra in R using RJDBC. When I execute casscon <- dbConnect(cassdrv, ""jdbc:cassandra://ipaddrs:9160/demodb"")I am gettingError in .jcall(drv@jdrv, ""Ljava/sql/Connection;"", ""connect"", as.character(url)[1],: java.lang.StringIndexOutOfBoundsException: String index out of range: -1I can't figure out the problem. I need a solution for this.";[education, open-source];86;
3688;1;2014-12-11T19:23:55.907;Multiple labels in supervised learning algorithm;"I have a corpus of text with a corresponding topics. For example ""A rapper Tupac was shot in LA"" and it was labelled as [""celebrity"", ""murder""]. So basically each vector of features can have many labels (not the same amount. The first feature vector can have 3 labels, second 1, third 5).If I would have just one label corresponded to each text, I would try a Naive Bayes classifier, but I do not really know how should I proceed if I can have many labels.Is there any way to transform Naive Bayes into multi label classification problem (if there is a better approach - please let me know)?P.S. few things about the data I have.approximately 10.000 elements in the datasettext is approximately 2-3 sentencesmaximum 7 labels per text";[education, open-source];126;
3689;2;2014-12-11T20:27:27.847;;From listening to presentations by Martin Odersky, the creator of Scala, it is especially well suited for building highly scalable systems by leveraging functional programming constructs in conjuction with object orientation and flelxible syntax. It is also useful for development of small systems and rapid prototyping because it takes less lines of code than some other languages and it has an interactive mode for fast feedback. One notable Scala framework is Akka which uses the actor model of concurrent computation. Many of Odersky's presentations are on YouTube and there is a list of tools implemented with Scala on wiki.scala-lang.org.An implicit point is that tools and frameworks written in Scala inherently have Scala integration and usually a Scala API. Then other APIs may be added to support other languages beginning with Java since Scala is already integrated and in fact critically depends on Java.  If a tool or framework is not written in Scala, it is unlikely that it offers any support for Scala.  That is why in answer to your question I have pointed towards tools and frameworks written in Scala and Spark is one example.  However, Scala currently has a minor share of the market but its adoption rate is growing and the high growth rate of Spark will enhance that. The reason I use Scala is because Spark's API for Scala is richer than the Java and Python APIs. The main reasons I prefer Scala generally is because it is much more expressive than Java because it allows and facilitates the use of functions as objects and values while retaining object oriented modularity, which enables development of complex and correct programs with far less code than Java which I had preferred because of widespread use, clarity and excellent documentation.;;;
3690;1;2014-12-11T23:56:03.023;Exporting R model to OpenCV's Machine Learning Library;I'm wonder if it's possible to export a model trained in R, to OpenCV's Machine Learning (ML) library format? The latter appears to save/read models in XML/YAML, whereas the former might be exportable via PMML. Specifically, I'm working with Random Forests, which are classifiers available both in R and OpenCV's ML library.Any advice on how I can get the two to share models would be greatly appreciated.;[education, open-source];384;
3691;2;2014-12-12T00:13:10.680;;"For starters, Naive Bayes is probably not appropriate here.  It assumes independence among the inputs (hence the ""Naive"") and words in a sentence are very dependent.But, assuming you really want to run with NB as an algorithm to start your experimentation, there are two options I'd consider:Ungraceful: Lots of NB classifiersThis would be an alternative approach.  Make a corupus of all the words observed as your vector of inputs.  Make a corpus off all the tags that are observed as your vector of outputs.  An NB classifier with multiple outputs is the equivalent of having multiple NB classifiers with one output each (so do whichever is easier to implement in whatever software framework you're using).  Treat each element as a training sample where a given input (a word) is a 1 if that word is present and a 0 if that word isn't.  Use the same binary scheme for the output.This brute forces the application of the NB Classifier to your data, and leaves you to find meaning by still haivng to mine the huge set of classifiers you'll be left with.More Graceful: Process your dataThis is the approach I'd recommend if you want to run with one multiple-class NB Classifier.  Your goal here is to figure out how to map each set of tags to a single class.  I'm sure there is some sort of clustering scheme or network analysis (perhaps [""celebrity""] linked to [""murder""] could become a the segment [""debauchery""]) that will sensibly map your tags to one single cluster.  If you treat tags as nodes and two given tags together as links, then you'll want to look into community detection algorithms (which is where I'd start).  But, if you just want something working, then some sort of hack on the tags that converts a list of tags to only the tag that's most commonly seen in your dataset would be enough.This method front-loads the work of cleaning your data and would make the NB Classifier's output easier to understand.";;;
3692;2;2014-12-12T01:30:26.723;;"Apply your clustering algorithmCalculate distance from all data points to its assigned clusterLabel the data points furthest from a center as an outlierRandomly generating 100 data points from three gaussians, clustering them with k-means, and marking the 10 'furthest from a center' data points gave the following graph:see this notebook for the full exampleThe burden of solving what ""distance"" means will already have to be solved for you to run a clustering algorithm.  It will still be up to you to pick off what distance means an outlier.  In this example, I just picked the N most distant data point, though you'll probably want to pick any number of data points over a certain number of standard deviations from a center.";;;
3693;1;2014-12-12T03:00:57.507;What are the best practices to anonymize user names in data?;I'm working on a project which asks fellow students to share their original text data for further analysis using data mining techniques, and, I think it would be appropriate to anonymize student names with their submissions.  Setting aside the better solutions of a url where students submit their work and a backend script inserts the anonymized ID, What sort of solutions could I direct students to implement on their own to anonymized their own names?I'm still a noob in this area. I don't know what are the norms. I was thinking the solution could be a hashing algorithm. That sounds like a better solution than making up a fake name as two people could pick the same fake name.possible people could pick the same fake name. What are some of the concerns I should be aware of? ;[education, open-source];226;1
3694;2;2014-12-12T09:02:16.350;;A standard practice in psychology (where you want to code participants in order to link different measurements together) is to have participants choose their mother's maiden name initials and birthdate, e.g., in the format XX-YYMMDD.This if course can still run into conflicts. Then again, I don't think there is any surefire conflict-free anonymization algorithm your students could do without knowing all the other students. Mothers' names and birthdates could be identical, own birthdates could be identical, shoe sizes could be, favorite superhero characters... The only thing I could think of would be (US) Social Security numbers, but you really don't want to use them.Bottom line: anonymize on the backend. Or, as @Emre suggests, think about whether you really need an identifier at all. Maybe the DB-generated index is enough?;;;
3696;2;2014-12-12T22:39:58.470;;Re: size of dataThe short answerScala works for both small and large data, but its creation and development is motivated by needing something scalable.  Scala is an acronym for “Scalable Language”.The long answerScala is a functional programming language that runs on the jvm.  The 'functional' part of this is a fundamental difference in the language that makes you think differently about programming.  If you like that way of thinking, it lets you quickly work with small data.  Whether you like it or not, functional languages are fundamentally easier to massively scale.  The jvm piece is also important because the jvm is basically everywhere and, thus, Scala code can run basically everywhere.  (Note there are plenty of other languages written on the jvm and plenty of other functional programming languages, and languages beyond Scala do appear in both lists.)This talk give a good overview of the motivation behind Scala.Re: other tools that have good Scala support:As you mentioned, Spark (distributable batch processing better at iteratative algorithms than its counterpart) is a big one.  With Spark comes its libraries Mllib for machine learning and GraphX for graphs.  As mentioned by  Erik Allik and Tris Nefzger, Akka and Factorie exist.  There is also Play.Generally,  I can't tell if there is a specific use case you're digging for (if so, make that a part of your question), or just want a survey of big data tools and happen to know Scala a bit and want to start there.;;;
3697;2;2014-12-12T22:48:41.970;;"Data becomes ""big"" when a single commodity computer can no longer handle the amount of data you have.  It denotes the point at which you need to start thinking about building supercomputers or using clusters to process your data.";;;
3698;2;2014-12-13T05:28:04.013;;If your Data points are dense and noise points are away from the dense region, you can try DBSCAN algorithm. http://en.wikipedia.org/wiki/DBSCANTweak its parameters until u get a best fit. ;;;
3699;1;2014-12-13T17:42:54.927;How to optimize the parameters of a classifier on scikit-learn?;For an imbalanced set of data is it better to choose an L1 or L2 regularization?Is there any special preprocessing to improve the model score (log_loss in particular)?Note: I'm a beginner in Machine Learning and I'm learning through examples and competitions on Kaggle.  ;[education, open-source];154;
3700;1;2014-12-13T19:27:56.213;Choice of weights for the Laplacian Eigenmaps algorithm;In his thesis (section 2.3.3) Belkin uses the heat equation to derive an approximation for $\mathcal{L}f$:$$\mathcal{L}f(x_i)\approx \frac{1}{t}\Big(f(x_i)-\alpha \sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}f(x_j)\Big)$$where $$\alpha=\Big(\sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}\Big)^{-1}$$.However, I'm not sure how these considerations lead to this choice of weights for the weight matrix (which will be used to construct the Laplacian):$$W_{ij} =\begin{cases} e^{-\frac{||x_i-x_j||^2}{4t}} & if\ ||x_i-x_j||<\epsilon \\ 0 & otherwise \end{cases}$$A very vague idea of mine was that the factors $\alpha$ and $\frac{1}{t}$ don't change for a given $x_i$ so if one choses the weights like above the resulting discrete Laplacian would (let aside those two constants) converge to the continuous version.Any ideas or tips what I'd have to read up to in order to get a better understanding?;[education, open-source];40;
3701;2;2014-12-13T20:37:07.843;;"I suspected you were using the names as identifiers. You shouldn't; they're not unique and they raise this privacy issue. Use instead their student numbers, which you can verify from their IDs, stored in hashed form. Use the student's last name as a salt, for good measure (form the string to be hashed by concatenating the ID number and the last name).";;;
3702;1;2014-12-13T23:08:53.930;K nearest neighbour;Is the k-nearest neighbour algorithm a discriminative or a generative classifier? My first thought on this was that it was generative, because it actually used Bayes's theorem to compute the posterior. Searching further for this, it seems like it is a discriminative model, but I couldn't find the explanation.So is KNN discriminative first of all? And if it is, is that because it doesn't model the the priors or the likelihood?;[education, open-source];112;2
3703;2;2014-12-14T03:42:25.213;;Instead of exporting your models, consider creating an R-based interoperable environment for your modeling needs. Such environment would consists of R environment proper as well as integration layers for your third-party libraries. In particular, for the OpenCV project, consider either using r-opencv open source project (https://code.google.com/p/r-opencv), or integration via OpenCV C++ APIs and R Rcpp package (http://dirk.eddelbuettel.com/code/rcpp.html). Finally, if you want to add PMML support to the mix and create a deployable-to-cloud solution, take a look at the following excellent blog post with relevant examples: http://things-about-r.tumblr.com/post/37861967022/predictive-modeling-using-r-and-the.;;;
3705;1;2014-12-16T01:19:12.477;Sci-kit Pipeline and GridsearchCV returns indexError: too many indices for array;I'm trying to get to grips with sci-kit learn for some simple machine learning projects but I'm coming unstuck with Pipelines and wonder what I've done wrong...I'm trying to work through a tutorial on KaggleHere's my code:import pandas as pdtrain = pd.read_csv(local path to training data) train_labels =pd.read_csv(local path to labels)from sklearn.decomposition import PCAfrom sklearn.svm import LinearSVCfrom sklearn.grid_search import GridSearchCVpca = PCA()clf = LinearSVC()n_components = arange(1, 39)loss = ['l1','l2']penalty = ['l1','l2']C = arange(0, 1, .1)whiten = [True, False]from sklearn.pipeline import Pipeline#set up pipelinepipe = Pipeline(steps=[('pca', pca), ('clf', clf)])#set up GridsearchCVestimator = GridSearchCV(pipe, dict(pca__n_components = n_components, pca__whiten = whiten, clf__loss = loss, clf__penalty = penalty, clf__C = C)) > estimatorReturns:GridSearchCV(cv=None,       estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',     random_state=None, tol=0.0001, verbose=0))]),       fit_params={}, iid=True, loss_func=None, n_jobs=1,       param_grid={'clf__penalty': ['l1', 'l2'], 'clf__loss': ['l1', 'l2'], 'clf__C': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9]), 'pca__n_components': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,       35, 36, 37, 38]), 'pca__whiten': [True, False]},       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,       verbose=0)But when I try to train data:estimator.fit(train, train_labels)The error is:    428         for test_fold_idx, per_label_splits in enumerate(zip(*per_label_cvs)):    429             for label, (_, test_split) in zip(unique_labels, per_label_splits):--> 430                 label_test_folds = test_folds[y == label]    431                 # the test split can be too big because we used    432                 # KFold(max(c, self.n_folds), self.n_folds) instead ofIndexError: too many indices for arrayCan anyone point me in the right direction?;[education, open-source];182;
3706;2;2014-12-16T02:44:28.950;;It turns out that the Pandas dataframe is the wrong shape.estimator.fit(train.values, train_labels[0].values)works, although I also had to drop the penalty term.;;;
3707;2;2014-12-16T06:12:35.423;;There's a lot of online tutorial. Especially in youtube, but if you will want accurate website you can see from here http://ttic.uchicago.edu/~shai/icml08tutorial/ or http://cs229.stanford.edu/materials.html. you can visit them now.;;;
3708;2;2014-12-16T08:51:30.987;;You can see if you can mix Spark streaming (https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html) and Spark ML Library (https://spark.apache.org/docs/1.1.0/mllib-guide.html).Spark Streaming permits to process live data streams and Spark ML Library is a Machine Learning Library for Spark. So maybe you can do something good! But this is a very interesting subject, I am working on it. It can be good to create a Google Community on it (https://plus.google.com/communities)?;;;
3709;1;2014-12-16T10:13:07.037;Data Science use cases for Public Transport domain;What are the common applications of Data Science techniques used by public transport operators?So far, what i can think of is: Analyzing passengers' information (travel time, itinerary, frequency of use, etc.) to better adapt the transport offer to passengers' needs.Finding patterns in road traffic data for optimizing transport schedule.Predicting a probability of a given line's tardiness/accidents, based on the data of each vehicle's technical state  (say, engine temperature, tire pressure, etc.)Analyzing geographical/demographic data of a city/country for predicting each station's attendance, deciding where to build a new station/line.Offering a better information to passengers in real time?My vision of the domain is far from being clear, need some guide advice. Thanks.;[education, open-source];39;
3711;1;2014-12-16T13:07:55.063;How does the naive Bayes classifier handle missing data in training?;"Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.When classifying instances, the attribute with the missing value is simply not included in the probability calculation (http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf)In training, ""the instance [with the missing data] is not included in frequency count for attribute value-class combination."" (http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html)Does that mean that particular training record simply isn't included in the training phase? Or does it mean something else?";[education, open-source];528;1
3713;2;2014-12-17T02:58:06.057;;See a similar answer here. To clarify, k nearest neighbor is a discriminative classifier. The difference between a generative and a discriminative classifier is that the former models the joint probability where as the latter models the conditional probability (the posterior) starting from the prior. In the case of nearest neighbors, the conditional probability of a class given a data point is modeled. To do this, one starts with the prior probability on the classes.  ;;;
3715;2;2014-12-17T09:07:11.320;;This seems to be a standard regression problem in which there are two goals:Obtain a predictive model that can be used for prediction.Which variables seem to be the most important ones to be used. For both the above problems use an ensemble model. Consider both a random forest and a gradient boosted machine. Both these models will use the independent variables and predict the Hospital time. Additionally, through variable importances, you can obtain which variables are the most important ones and have the most impact in predicting the output. ;;;
3716;2;2014-12-17T09:32:53.703;;In the way that you've defined or set up the problem, i.e.sales = alpha*quality + beta*position + epsilonWe can easily quantify beta given that your model is correct. You just need to run it through linear regression and it will give you the coefficient for beta*.If you would like to model click through rates, you would have to train a classifier. So you would have to fit a logistic model that models: clicks ~ alpha*quality + beta*position + epsilon*I believe you would have to restrict the training set to contain results where all impressions were obtained on the first page otherwise your model will not hold (I would guess that beta is going to be strongly dependent on the page). ;;;
3717;2;2014-12-17T18:15:42.133;;In general, you have a choice when handling missing values hen training a naive Bayes classifier. You can either choose to eitherOmit records with any missing values,Omit only the missing attributes.I'll use the example linked to above to demonstrate these two approaches.  Suppose we add one more training record to that example. Outlook  Temperature  Humidity   Windy   Play-------  -----------  --------   -----   ----rainy    cool        normal    TRUE    norainy    mild        high      TRUE    nosunny    hot         high      FALSE   nosunny    hot         high      TRUE    nosunny    mild        high      FALSE   noovercast cool        normal    TRUE    yesovercast hot         high      FALSE   yesovercast hot         normal    FALSE   yesovercast mild        high      TRUE    yesrainy    cool        normal    FALSE   yesrainy    mild        high      FALSE   yesrainy    mild        normal    FALSE   yessunny    cool        normal    FALSE   yessunny    mild        normal    TRUE    yesNA       hot         normal    FALSE   yesIf we decide to omit the last record due to the missing outlook value, we would have the exact same trained model as discussed in the link. We could also choose to use all of the information available from this record.  We could choose to simply omit the attribute outlook from this record. This would yield the following updated table.            Outlook            Temperature           Humidity   ====================   =================   =================            Yes    No            Yes   No            Yes    No Sunny       2     3     Hot     3     2    High      3     4Overcast    4     0     Mild    4     2    Normal    7     1 Rainy       3     2     Cool    3     1          -----------         ---------            ---------- Sunny     2/9   3/5     Hot   3/10   2/5    High    3/10   4/5 Overcast  4/9   0/5     Mild  4/10   2/5    Normal  7/10   1/5Rainy     3/9   2/5     Cool  3/10   1/5            Windy        Play=================    ========      Yes     No     Yes   NoFalse 7      2       10     5True  3      3      ----------   ----------False  7/10    2/5   10/15  5/15True   3/10    3/5Notice there are 15 observations for each attribute except Outlook, which has only 14. This is since that value was unavailable for the last record. All further development would continue as discussed in the linked article.  For example in the R package e1071 naiveBayes implementation has the option na.action which can be set to na.omit or na.pass.;;;
3718;1;2014-12-17T21:17:13.340;Interactive Graphing while logging data;I'm looking to graph and interactively explore live/continuously measured data. There are quite a few options out there, with plot.ly being the most user-friendly. Plot.ly has a fantastic and easy to use UI (easily scalable, pannable, easily zoomable/fit to screen), but cannot handle the large sets of data I'm collecting. Does anyone know of any alternatives?I have MATLAB, but don't have enough licenses to simultaneously run this and do development at the same time. I know that LabVIEW would be a great option, but it is currently cost-prohibitive.Thanks in advance!;[education, open-source];196;2
3719;1;2014-12-18T04:36:06.107;Neo4j vs OrientDB vs Titan;I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database. But it seams Neo4j doesn't scale well. The alternative I found out are Titan  and oriebtDB. I have gone through this comparison on these three Databases, But I would like to get more details on  these databases. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support of these databases. Also is there any other good database options ?;[education, open-source];2724;
3720;2;2014-12-18T04:41:33.493;;Visualizing large datasets is a long standing problem. One of the issues is to understand how we can show over a million points on a screen that has only about ~ million pixels. Having said that, here are a few tools that can handle big data: Tableau: you could use their free desktop tool. Tabplot: built on top of ggplot2 in R to handle larger datasets. See this review for 5 other products that can help you do your job. ;;;
3721;1;2014-12-18T04:48:49.820;Machine Learning for hedging/ portfolio optimization?;With increasingly sophisticated methods that work on large scale datasets, financial applications are obvious. I am aware of machine learning being employed on financial services to detect fraud and flag fraudulent activities but I have a lesser understanding of how it helps to predict the price of the stock the next day and how many stocks of a particular company to buy. Do the hedge funds still employ portfolio optimization techniques that are right out of the mathematical finance literature or have they started to use machine learning to hedge their bets? More importantly, what are the features that are used by these hedge funds and what is a representative problem set up? ;[education, open-source];239;2
3722;2;2014-12-18T08:10:00.630;;"That is a rather broad question, and there is tons of literature about quantitative analysis and stock market prediction using machine learning.The most classical example of predicting the stock market is employing neural networks; you can use whatever feature you think might be relevant for your prediction, for example the unemployment rate, the oil price, the gold price, the interest rates, and the timeseries itself, i. e. the volatility, the change in the last 2,3,7,..., days etc. - a more classical approach is the input-output-analysis in econometrics, or the autoregression analysis, but all of it can be modeled using neural networks or any other function approximator / regression in a very natural way.But, as said, there are tons of other possibilities to model the market, to name a few: Ant Colony Optimization (ACO), Classical regression analysis, genetic algorithms, decision trees, reinforcement learning etc. you name it, almost EVERYTHING has probably been applied to the stock market prediction problem.There are different fond manager types on the markets. There are still the Quants which are doing a quantitative analysis using classical financial maths and maths borrowed from the physics to describe the market movements. There are still the most conservative ones which do a long-term, fundamental analysis of the corporation, that is, looking in how the corporation earns money and where it spends money. Or the tactical analysts who just look for immediate signals to buy / sell a stock in the short term. And those quantitative guys who employ machine learning amongst other methods.";;;
3723;2;2014-12-18T13:30:54.210;;For this answer, I have assumed that you prefer open source solutions to big data visualization. This assumption is based on budgetary details from your question. However, there is one exclusion to this - below I will add a reference to one commercial product, which I believe might be beneficial in your case (provided that you could afford that). I also assume that browser-based solutions are acceptable (I would even prefer them, unless you have specific contradictory requirements).Naturally, the first candidate as a solution to your problem I would consider D3.js JavaScript library: http://d3js.org. However, despite flexibility and other benefits, I think that this solution is too low-level.Therefore, I would recommend you to take a look at the following open source projects for big data visualization, which are powerful and flexible enough, but operate at a higher level of abstraction (some of them are based on D3.js foundation and sometimes are referred to as D3.js visualization stack).Bokeh - Python-based interactive visualization library, which supports big data and streaming data: http://bokeh.pydata.orgFlot - JavaScript-based interactive visualization library, focused on jQuery: http://www.flotcharts.orgNodeBox - unique rapid data visualization system (not browser-based, but multi-language and multi-platform), based on generative design and visual functional programming: https://www.nodebox.netProcessing - complete software development system with its own programming language, libraries, plug-ins, etc., oriented to visual content: https://www.processing.org (allows executing Processing programs in a browser via http://processingjs.org)Crossfilter - JavaScript-based interactive visualization library for big data by Square (very fast visualization of large multivariate data sets): http://square.github.io/crossfilterbigvis - an R package for big data exploratory analysis (not a visualization library per se, but could be useful to process large data sets /aggregating, smoothing/ prior to visualization, using various R graphics options): https://github.com/hadley/bigvisprefuse - Java-based interactive visualization library: http://prefuse.orgLumify - big data integration, analysis and visualization platform (interesting feature: supports Semantic Web): http://lumify.ioSeparately, I'd like to mention two open source big data analysis and visualization projects, focused on graph/network data (with some support for streaming data of that type): Cytoscape and Gephi. If you are interested in some other, more specific (maps support, etc.) or commercial (basic free tiers), projects and products, please see this awesome compilation, which I thoroughly curated to come up with the main list above and analyzed: http://blog.profitbricks.com/39-data-visualization-tools-for-big-data.Finally, as I promised in the beginning, Zoomdata - a commercial product, which I thought you might want to take a look at: http://www.zoomdata.com. The reason I made an exclusion for it from my open source software compilation is due to its built-in support for big data platforms. In particular, Zoomdata provides data connectors for Cloudera Impala, Amazon Redshift, MongoDB, Spark and Hadoop, plus search engines, major database engines and streaming data.Disclaimer: I have no affiliation with Zoomdata whatsoever - I was just impressed by their range of connectivity options (which might cost you dearly, but that's another aspect of this topic's analysis).;;;
3724;2;2014-12-18T18:07:18.170;;ScalaNLP is a suite of machine learning and numerical computing libraries with support for common natural language processing tasks. http://www.scalanlp.org/;;;
3725;2;2014-12-19T01:20:39.907;;Recall the definition he makes for the graph Laplacian earlier, $L = D -W $. Now consider the map in the RHS parentheses which I'll call $L^*$,$$ L^*f(x_i) := f(x_i) - \alpha \sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}f(x_j).$$The suggested weight matrix definition is natural because it lets us write$$ L^* := I - D^{-1}W. $$ Here's a reference to a related paper with some easy to read exposition. Hope this helps!;;;
3726;2;2014-12-19T01:42:36.780;;"This is the fundamental challenge to all data modeling.  We don't just want to memorize the the link between a given input and a given output (otherwise you wouldn't be modeling data, you'd be memorizing 1:1 connections with a dict / hash / relational database table / etc).  We want to capture the underlying pattern in the data from only looking at the training data.Let's expand a little on your gravity example.  You have your 10 training samples showing the start and ending position of an object dropped.  For consistency, let's say the object was dropped the moment the object's location was initially recorded and the ending location was recorded at some precise time interval later (but before the object hit the ground).  Let's also say the model (neural network in this case) managed to precisely learn the expected change in location since it just comes down to subtraction in one axis.  You can show it another 10, 100, 1000 examples that all leverage the connection found and your model will keep performing well.Why not keep going to 10k, 100k, or even more samples?  Theoretically, if you managed to isolate the connection and run the experiment the same way each time, your model will always work.  But realistically, something is going to eventually change in the system.  You hire a new lab assistant who tends to press the 'record location' button well after having dropped the object (giving the object more initial velocity, which you won't notice having only recorded location).  Maybe you lost your initial ball and had to use something else which is lighter and catches the wind more (so it goes slower).  .... the longer you run the experiment, the more small changes will creep into your system.  Eventually these changes will alter the connection enough to make your initial model wrong.When modeling data, we want to capture the underlying patterns and acknowledge that the model only matters as long as those underlying patterns stay relevant.  It's not really about the number of samples.  It's about the connections / the model itself.  The number of samples just happens to be one of the better proxies we have - the more samples you use, the more confident you have some underlying pattern.  'Statistical validity' is one stab at solving this, though it's validity is still up for question in the era of big data.  There is plenty of work done trying to solve for how to gain confidence in good generalization in neural networks specifically, but it's still very much an open question.For a different example, if you're looking at user behavior, you'll see differences between day and night; weekdays and weekends; summer and winter; year of a person's life; culture a person grew up in... even if you prove you found a pattern in your initial sample, the system will eventually change and it's up to luck whether the connection(s) you found are a part of the system that changed or a part of the system that didn't.";;;
3728;1;2014-12-19T11:42:04.547;What software is being used in this image recognition system?;I was wondering if anyone knew which piece of software is being used in this video? It is an image recognition system that makes the training process very simple.http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098The example is with car images, though the video should start at the right spot.;[education, open-source];112;
3729;2;2014-12-19T16:02:06.330;;"I'm pretty sure that the software you're referring to is a some kind of internal research project software, developed by Enlitic (http://www.enlitic.com), where Jeremy Howard works as a founder and CEO. By ""internal research project software"" I mean either a proof-of-concept software, or a prototype software.";;;
3730;1;2014-12-19T16:02:48.693;R random forest on Amazon ec2 Error: cannot allocate vector of size 5.4 Gb;"I'm training random forest models in R using randomForest() with 1000 trees and data.frames with about 20 predictors and 600K rows. On my laptop everything works fine but when I move to amazon ec2 to run the same thing i get:Error: cannot allocate vector of size 5.4 GbExecution haltedI'm using the c3.4xlarge instance type so it's pretty beefy... does anyone know a workaround for this to get it to run on this instance? I would love to know the memory nuances that causes this problem only on the ec2 instance and not on my laptop (OS X 10.9.5 Processor  2.7 GHz Intel Core i7; Memory  16 GB 1600 MHz DDR3)Thanks!";[education, open-source];492;
3731;2;2014-12-19T16:33:44.423;;Here's some advice (use at your own risk!):make sure that your R environment on EC2 is identical to the one on your laptopmake sure you're using 64-bit images for your virtual machine instancetry to create/enable swap space: http://stackoverflow.com/a/22247782/2872891see this discussion: http://stackoverflow.com/q/5171593/2872891see this discussion: http://stackoverflow.com/q/12322959/2872891see this discussion: http://stackoverflow.com/q/1358003/2872891If the above-mentioned simpler measures don't help OR you want to achieve more scalability and/or performance, including an ability to parallelize the process on a single machine or across multiple machines, consider using bigrf R package: http://cran.r-project.org/web/packages/bigrf. Also see this discussion: http://stackoverflow.com/q/1358003/2872891.;;;
3732;2;2014-12-19T23:24:55.017;;Additional to other ideas: reduce your data until you figure out what you can run on the Amazon instance. If it can't do 100k rows then something is very wrong, if it fails at 590k rows then its marginal.The c3.4xlarge instance has 30Gb of RAM, so yes it should be enough.;;;
3733;1;2014-12-20T03:37:21.820;Which Optimization method to use?;I have a non-function (not in closed form) that takes in a few parameters (about 20) and returns a real value. A few of these parameters are discrete while others are continuous. Some of these parameters can only be chosen from a finite space of values. Since I don't have the function in closed form, I cannot use any gradient based methods. However, the discrete nature and the boxed constraints on a few of those parameters restrict even the number of derivative free optimization techniques at my disposal. I am wondering what are the options in terms of optimization methods that I can use. ;[education, open-source];84;1
3734;1;2014-12-20T07:03:57.837;Approaches to high dimension pattern matching problem;My apologies in advance as I am new to this. I have searched the internet and tried various processes and nothing seems to work or address this situation. I have a dataset of 30,000 transactions and 500,000 items. Average item size for a transaction is 50. The dataset is sparse, so the support number must be set quite low. Furthermore, the rules become more valuable the larger the number of items in the rule. I have tried running this in arules and the tests fail after exceeding 64 gb of RAM (the limit of the machine). I have tried reducing items and transactions to smaller subsets, but still hit this memory limit.Ultimately, I am looking for ways to cluster large groups of similar accounts by selection of items and generate confidence and lift of various next items selected from those clusters.My question: are there alternative, more efficient ways to do this, or other approaches to consider?Thank you.;[education, open-source];107;
3735;2;2014-12-20T08:06:13.077;;"According to the following discussion on StackOverflow, a situation like that you've described can occur, when one of the variables in a data set is of unexpected type (for example, a factor instead of a character): http://stackoverflow.com/q/7246412/2872891.Also, consider using package bigmemory, recommended in the accepted answer, or similar packages for big data analysis. For the latter, please see section ""Large memory and out-of-memory data"" in CRAN Task View High-Performance and Parallel Computing with R.Finally, an additional note. There is an ecosystem of R packages, built around the arules package, which includes supporting packages for algorithms (arulesNBMiner), applications (arulesSequences, arulesClassify) and visualization (arulesViz). You are likely aware of that, but I have decided to include this good-to-know fact just in case and for the sake of completeness.";;;
3736;2;2014-12-21T00:40:12.337;;Bayesian optimization is a principled way of sequentially finding the extremum of black-box functions. What's more, there a numerous software packages that make it easy, such as BayesOpt and MOE. Another flexible Bayesian framework that you can use for optimization is Gaussian processes: Global Optimisation with Gaussian Processes;;;
3737;2;2014-12-21T11:43:35.100;;"This is a tricky question, because it's easy and difficult at the same time. Easy, because there is a lot of resources that potentially can help you make a decision on the topic. Difficult, because the situation is very different for a particular person (not to mention that their interest might change at any time), which makes extremely difficult for other people to give you a good advice and for you to make the right decision.As for data science career options, you can certainly consider a degree path (MS or MS + PhD), but you need to be aware of other options. For a comprehensive resource, dedicated to data science and related degree programs (both traditional and online), please visit this page: http://www.kdnuggets.com/education/index.html. A comprehensive review of all these offerings is IMHO an enormous task and is far beyond an answer here on Stack Exchange or, even, a lengthy blog post.However, nowadays one is not limited to traditional educational options and I think it's important to be aware of other educational options. One of the other options include certifications (linked at the above-mentioned page, but, in my opinion, the only certification worth considering is the Certified Analytics Professional as a solid and vendor-neutral certification from a reputable INFORMS). Another option is recently booming data science intensive educational offerings, from short-term (and often too commercial, to put it lightly) bootcamps to more solid offerings, including free, but competitive, ones, such as Insight Data Science Fellows Program, where one needs to be a PhD to apply, or its sister program Insight Data Engineering Fellows Program, which doesn't have such requirement. Finally, there is yet another option: self-study. It partially intersects with the certificate option, if one uses massive open online courses (MOOC) (a review of which deserves a separate comprehensive post), but there are open curricula that might suit one better, such as the Open Source Data Science Masters curriculum, linked in my earlier relevant answer.P.S. While your question focuses on data science, I think that it may be wise to at least consider another career path, given your math background. I'm talking about operations research field, which is not that far away from data science (and even somewhat intersects with it). While similar, data science is IMHO more statistics-focused, whereas operations research is more math-focused, at least that's how I see it. Despite all the popularity and ""gold rush"" of data science, operations research career is a solid one, just not as hot. Of course, if you're excited about things like artificial intelligence, machine learning and, especially, deep learning, data science career is the way to go. Whatever you will choose, the good thing is that with your math background it will be easy to change focus, should you decide to. Hope this helps. Good luck!";;;
3738;1;2014-12-22T03:30:45.673;How to deal with time series which change in seasonality or other patterns?;"BackgroundI'm working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.One of the things I've been working on is clustering of these time series. My work is academic for the moment, and while I'm doing other analysis of the data as well, I have a specific goal to carry out some clustering.I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I've found several papers related to this.QuestionWill the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.In case the above is unclear, consider these examples:Example 1A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.Example 2A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.Possible DirectionsI've wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I'd need to be able to detect such changes. Also, I just don't know if this is a suitable way or working with the data.I've also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I'd then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I think this would work, but it's potentially quite onerous and I'd hate to go down this path if there's a better way that I'm missing.Further NotesThese are things that have come up in comments, or things I've thought of due to comments, which might be relevant. I'm putting them here so people don't have to read through everything to get relevant information.I'm working in Python, but have rpy for those places where R is more suitable. I'm not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I'm happy to figure out implementation details myself.I have a lot of working ""rough draft"" code - I've done some DTW runs, I've done a couple of different types of clustering, etc. I think I largely understand the direction I'm taking, and what I'm really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: http://www.cs.ucr.edu/~eamonn/selected_publications.htmP.S. - I can't post in Meta or even a comment yet, so sorry for the off-topic bit, but: I love this site and hope it picks up steam! I was so happy when I found it, I immediately found topics which weren't covered at all or as helpfully at SO and CV. I feel a lot more comfortable asking this question here because it's not a pure statistics question or a programming question. I would feel like I was asking something potentially off-topic at either of those sites, and I feel like I'm more likely to get an answer which really covers my question and which I can understand by asking here.";[education, open-source];359;8
3739;2;2014-12-22T05:14:16.130;;If you want to just mine for seasonal patterns, then look into autocorrelation.  If you're looking for a model that can learn seasonal patterns and make forecasts from it, then Holt-Winters is a good start, and ARIMA would be a good thing to follow up with.  Here[pdf] is the tutorial that got me off the ground.;;;
3740;1;2014-12-22T09:31:31.747;Improving Naive Bayes accuracy for text classification;"I am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased).My problem is that I have an over-represented category which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one category, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points).My first question:What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?My second question:After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as ""Entertainment"", it might receive ""News"" now). Is this indicative of an error in my implementation, or expected?";[education, open-source];141;1
3741;1;2014-12-22T13:04:38.577;Stochastic gradient descent in matrix factorization, sensitive to label's scale?;I'm trying to figure out a strange phenomenon, when I use matrix factorization (the Netflix Prize solution) for a rating matrix: $R = P^T * Q + B_u + B_i$with ratings ranging from 1 to 10.Then I evaluate the model by each label's absolute mean average error in test set, the first column is origin_score, the second(we don't transform the data, then train and its  prediction error), the third(we transform the data all by dividing 2, train, and when I use this model to make prediction, firstly reconstruct the matrix and then just multiply 2 and make it back to the same scale)As you see, in grade 3-4 (most samples are label from 3-4), it's more precise while in high score range(like 9 and 10, just 2% of the whole traiing set), it's worse.+----------------------+--------------------+--------------------+| rounded_origin_score | abs_mean_avg_error | abs_mean_avg_error  | +----------------------+--------------------+---------------------+| 1.0                  | 2.185225396100167  |  2.559125413626183  || 2.0                  | 1.4072212825108161 |  1.5290497332538155 || 3.0                  | 0.7606073396581479 |  0.6285151230269825 || 4.0                  | 0.7823491986435621 |  0.6419077576969795 || 5.0                  | 1.2734369551159568 |  1.256590210555053  || 6.0                  | 1.9546560495715863 |  2.0461809588933835 || 7.0                  | 2.707229888048017  |  2.8866856489147494 || 8.0                  | 3.5084244741417137 |  3.7212155956153796 || 9.0                  | 4.357185793060213  |  4.590550124054919  || 10.0                 | 5.180752400467891  |  5.468600926567884  |+----------------------+--------------------+---------------------+I've re-train the model several times, and got same result, so I think it's not effect by randomness.;[education, open-source];67;
3742;1;2014-12-22T14:06:45.610;Data transposition code in R;I've been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.Example:ID    date        timeframe  fruit_amt   veg_amt <br/> 4352  05/23/2013  before     0.25        0.75 <br/> 5002  05/24/2014  after      0.06        0.25 <br/> 4352  04/16/2014  after      0           0 <br/> 4352  05/23/2013  after      0.06        0.25 <br/> 5002  05/24/2014  before     0.75        0.25 <br/>Desired:ID    B_fr05/23/2013   B_veg05/23/2013  A_fr05/23/2013  A_veg05/23/2013   B_fr05/24/2014   B_veg05/24/2014   (etc)  <br/>4352  0.25             0.75             0.06            0.25              .                .  <br/>5002  .                .                .               .                 0.75             0.25 <br/>;[education, open-source];65;
3743;1;2014-12-22T15:52:12.133;What does 'contextual' mean in 'contextual bandits'?;I recently read a lot about the n-armed bandit problem and its solution with various algorithms, for example for webscale content optimization. Some discussions were referring to 'contextual bandits', I couldn't find a clear definition what the word 'contextual' should mean here. Does anyone know what is meant by that, in contrast to 'usual' bandits?;[education, open-source];71;1
3745;1;2014-12-22T18:33:35.363;Bayesian Decision Tree;I was looking to learn about Bayesian theory in decision tree and how it avoids overfitting but couldn't find any tutorials for someone just starting. Do you know any resources to learn about it?;[education, open-source];66;
3746;2;2014-12-22T18:44:18.983;;A contextual bandit algorithm not only adapts to the user-click feedback as the algorithm progresses, it also utilizes pre-existing information about the user's (and similar users) browsing patterns to select which content to display.So, rather than starting with no prediction (cold start) with what the user will click (traditional bandit and also traditional A/B testing), it takes other data into account (warm start) to help predict which content to display during the bandit test.See: http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf;;;
3747;2;2014-12-22T19:12:53.943;;Regarding your first question...Do you anticipate the majority category to be similarly over-represented in real-world data as it is in your training data? If so, perhaps you could perform two-step classification:Train a binary classifier (on all your training data) to predict membership (yes/no) in the majority class.Train a multi-class classifier (on the rest of the training data) to predict membership in the remaining minority classes.;;;
3748;2;2014-12-22T19:14:26.313;;There's also Richard Socher's recent PhD dissertation on intersection of NLP and deep learning: Recursive Deep Learning for Natural Language Processing and Computer Vision;;;
3749;2;2014-12-22T23:55:11.037;;Try 'arrange(Data.frame.name, ID)' function from package 'dplyr';;;
3750;1;2014-12-23T02:33:54.430;How to eliminate wrong organization names?;"I have a list of several organization names (1,000,000+), but some of then actually refer to the same organizations , for example 309th hospital and 309 hospital PLA and 309 hospital Chinese. I am now using TDA (a software which is used to cluster organization names and combine different names(standard and non-standard names) of the same organization) to do organization names cleaning. To make it simple, below is the output from TDA:For organization ""309th hospital"", it find one standard name and several non-standard names. But this software is not omnipotent，it also makes some mistakes, for example, 88th hospital and 150th hospital are also in it. So, my question is: ""Is there a solution to eliminate/reduce these errors instead of doing it manually?"".";[education, open-source];18;1
3751;1;2014-12-23T07:15:34.500;Route to picking up Deep learning;I would like to pick up on the topic of deep learning. Should I begin from the topic of AI before working my way into Deep learning?;[education, open-source];127;2
3752;2;2014-12-23T17:10:10.777;;Regarding the being new to decision trees and wanting to get off the ground, I wrote a tutorial on decision trees that will help.Regarding methods to avoid overfitting:  The game for any model is to limit its complexity to what is reasonable given the data you have.  Complexity in decision trees is manifested as adding new decision boundaries, so any limit in complexity is a limit in the decision boundaries it can draw.  Two common ways to do this is to place constraints on when a new decision can be created (a minimum of data in a leaf, significant increase in information, etc) or more simply to limit the max depth of the tree.;;;
3753;2;2014-12-23T18:11:24.117;;"You can use the reshape2 package for this task.First, transform the data to the long format with melt:library(reshape2)dat_m <- melt(dat, measure.vars = c(""fruit_amt"", ""veg_amt""))where dat is the name of your data frame.Second, cast to the wide format:dcast(dat_m, ID ~ timeframe + variable + date)The result:    ID after_fruit_amt_04/16/2014 after_fruit_amt_05/23/2013 after_fruit_amt_05/24/2014 after_veg_amt_04/16/20141 4352                          0                       0.06                         NA                        02 5002                         NA                         NA                       0.06                       NA  after_veg_amt_05/23/2013 after_veg_amt_05/24/2014 before_fruit_amt_05/23/2013 before_fruit_amt_05/24/20141                     0.25                       NA                        0.25                          NA2                       NA                     0.25                          NA                        0.75  before_veg_amt_05/23/2013 before_veg_amt_05/24/20141                      0.75                        NA2                        NA                      0.25> ";;;
3754;1;2014-12-23T19:46:57.267;Hadoop/Pig Aggregate Data;"I am working on a project with two data sets. A time vs. speed data set (let's call it traffic), and a time vs. weather data set (called weather). I am looking to find a correlation between these two sets using Pig. However the traffic data set has the time field, D/M/Y hr:min:sec, and the weather data set has the time field, D/M/Y. Due to this I would like to average the speed per day and put it into a single D/M/Y value inside the traffic file.I then plan to use:data = JOIN speed BY day, JOIN weather BY day with 'merge'I will then find the correlation using: (I am borrowing this code from elsewhere)set = LOAD 'data.txt' AS (speed:double, weather:double)rel = GROUP set ALLcor = FOREACH rel GENERATE COR(set.speed, set.weather)dump cor;This is my first experience with Pig (I've never even used SQL), so I would like to know a few things:1. How can I merge the rows of my traffic file (ie. average D/M/Y hr:min:sec into D/M/Y)?2. Is there a better way to find a correlation between the fields of different datasets?3. Are the JOIN BY and the COR() functions used appropriately in my above code?  ";[education, open-source];77;
3757;2;2014-12-24T13:46:47.367;;Apart from the fancier methods you could try the Bayes formulaP(I | p1 ... pn) =   P(p1 ... pn | I) P(I) / sum_i (P(p1 ... pn | i) P(i))P(I | p1 ... pn) is the probability that a user belongs to age group I if he liked p1, .., pnP(i) is the probability that a user belongs to age group iP(p1 .. pn | i) is the probability that a user liked p1, .., pn if he belongs to age group i.You already have the estimates for P(i) from your data: this is just the proportion of users in age group I. To estimate P(p1 ... pn |i), for each age group i estimate the probability (frequency) p_ij to like a page j. To have p_ij non-zero for all j, you can mix in the frequency for the whole population with a small weight.Then log P(p1...pn| i) = sum(log p_ij, i = p1, .., pn), the sum over all pages that a new user likes. This formula would be approximately true assuming that a user likes the pages in his age group independently. Theoretically, you should also add log (1-p_ij) for all i that he hasn't liked, but in practice you should find that the sum of log (1-p_ij) will be irrelevantly small, so you won't need too much memory.If you or someone else has tried this, please comment about the result.;;;
3758;2;2014-12-24T20:02:42.213;;The larger your target scores, the larger latent variables should be (well, it's not only a magnitude that matters, but also a variance, but it still applies to your case). There's no problem with larger coefficients of latent vectors unless you use regularization (and, likely, you do). In case of regularization your optimal solution will tend towards smaller values, and'd sometimes prefer to sacrifice some accuracy for lower regularization penalty.Gradient Descent doesn't suffer from problem of large coefficients (unless you run into some sort of numerical issues): if the learning rate is tuned properly (there are lots of stuff on it, google), it should arrive to equivalent parameters. Otherwise nobody guarantees you convergence :-)The common rule of thumb when doing regression (and your instance of matrix factorization is a kind of regression) is to standardize your data: make it having zero mean and unit variance.;;;
3759;1;2014-12-25T05:24:55.590;How are the findings learnt from the data set are generalized compared to Statistics?;"I am new to data science/ machine learning world. I know that in Statistics we assume that a certain event/ process has some particular distribution and the samples of that random process are part of some sampling distribution. The findings from the data could then be generalized by using confidence intervals and significance levels.How do we generalize our findings once we ""learn"" the patterns in the data set? What is the alternative to confidence levels here?";[education, open-source];41;1
3760;1;2014-12-25T10:13:16.273;Neural Network Hidden Neuron Selection Strategy;"I'm trying to determine what is the best number of hidden neurons for my MATLAB neural network. I was thinking to adopt the following strategy:Loop for some values of hidden neurons, e.g. 1 to 40;For each NN with a fixed number of hidden neurons, perform a certain number of   training (e.g. 40, limiting the number of epoch for time reasons: I was thinking to doing this because the network seems to be hard to train, the MSE after some epochs is very high)Store the MSE obtained with all the nets with different number of hidden neuronsPerform the previous procedure more than 1 time, e.g. 4, to take into account the initial random weight, and take the average of the MSEsSelect and perform the ""real"" training on a NN with a number of hidden neurons such that the MSE previously calculated is minimizedThe MSE that I'm referring is the validation MSE: my samples splitting in trainining, testing and validation to avoid overfitting is 70%, 15% and 15% respectively)Other informations related to my problem are:fitting problem9 input neurons2 output neurons1630 samples  This strategy could be work? Is there any better criterion to adopt? Thank youEdit: Test done, so the result suggest me to adopt 12 neurons? (low validation MSE and  number of neurons lower than 2*numberOfInputNeurons? but also 18 could be good...";[education, open-source];94;1
3761;2;2014-12-25T15:40:43.283;;"A rule of thumb approach is:start with a number of hidden neurons equal (or little higher) that the number of features. In your case it would be 9. My suggestion is to start with 9*2 = 18 to cover a wider range of possibilities.Be sure your test and validation sets are selected ""fairly"": a random selection and varying the seed some number of times to test different configurations would be ok.In general, a number of neurons equal to the number of features will tend to make each hidden neuron try to learn that special thing that each feature is adding, so will could say it is ""learning each feature"" separately. Although this sounds good it might tend to overfitting. Since your number of inputs and your dataset size is small its ok to start with a hidden layer size of the double (18) and start lowering down. When the training error and test error stabilize in a difference lower than a threshold then you could have found a better generalizing model.Neural networks are very good at finding local optima by exploring deeply a solution from a starting point. However, the starting point it is also very important. If you are not getting a good generalization you might try to find good initial starting points with methods of Hybrid Neural Networks. A common one, for example, is using genetic algorithms to find an initial combination of weights and then start the neural from that point. Given that your search space would be better covered (in case your problem actually needs that).As for every problem in machine learning is very important to clean your data before introducing it to the NN. Try to be very detailed in order to avoid the NN to learn things you already know. For example if you know how two features are correlated improve the input data by making this correlation explicit so less workload is given to the NN (that might actually get you in trouble).";;;
3762;2;2014-12-25T18:10:13.447;;Top level:The rule is to chose the most simple network that can perform satisfactorily.  See this publication and it's pdf.  The Methodology:So do your proposed test (training many networks at each number of hidden nodes) and plot the results.  At the minimum number of nodes, you'll see the worst performance.  As you increase the number of nodes, you'll see an increase in performance (reduction of error).  At some point N, you'll see the performance seems to hit an upper limit and increasing nodes beyond this will stop giving significant performance gains.  Further increases may start to hurt performance a little as training gets more difficult).  That point N is the number of nodes you want.How it worked for me:The first time I used this methodology, it created a beautiful almost-sigmoid-looking function with a very clear number of nodes that were needed to achieve good results.  I hope this works for you as well as it worked for me.;;;
3763;1;2014-12-26T06:53:29.670;"How to apply AdaBoost to more ""complex"" (non-binary) classifications/data fitting?";"In this article, Chris McKinlay says he used AdaBoost to choose the proper ""importances"" of questions he answered on okcupid.If you haven't read and don't want to read the article, or are unfamiliar with okcupid and the question system, here's the data and problem he had:The goal is to ""match"" as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions. These questions may have between 2 and 4 answers each, and for the sake of simplicity, let's pretend that the formula for a match% $\ M $ between you and another user is given by$\ M = Q_a/Q_c $Where $\ Q_c $ is the number of questions you and the other user have in common, and$\ Q_a $ is the number of questions you both answered with the same value.The real formula is slightly more complex, but the approach would be the same regarding ""picking"" a correct answer (he actually used boosting to find the ideal ""importance"" to place on a given question, rather than the right answer).In any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of $\ M $ over all users.Now I've watched the MIT course on AI up to and including the lecture on boosting, but I don't understand how you would apply it to a problem like this. Honestly I don't even know where to begin with choosing rules for the weak learners. I don't have any ""rules"" about what values to choose for each question (if the user is under 5'5, choose A, etc) - I'm just trying to fit the data I have.Is this not the way boosting is supposed to be used? Is there likely some other optimization left out of how he figured this out?";[education, open-source];95;1
3764;2;2014-12-26T10:01:46.500;;After reading your question, I became curious about the topic of time series clustering and dynamic time warping (DTW). So, I have performed a limited search and came up with basic understanding (for me) and the following set of IMHO relevant references (for you). I hope that you'll find this useful, but keep in mind that I have intentionally skipped research papers, as I was more interested in practical aspects of the topic.Resources:UCR Time Series Classification/Clustering: main page, software page and corresponding paperTime Series Classification and Clustering with Python: a blog postCapital Bikeshare: Time Series Clustering: another blog postTime Series Classification and Clustering: ipython notebookDynamic Time Warping using rpy and Python: another blog postMining Time-series with Trillions of Points: Dynamic Time Warping at Scale: another blog postTime Series Analysis and Mining in R (to add R to the mix): yet another blog postAnd, finally, two tools implementing/supporting DTW, to top it off: R package and Python module;;;
3766;1;2014-12-26T16:25:27.373;predict with Multinomial Logistic Regression;"If I execute the following code I have no problem:require(foreign)require(nnet)require(ggplot2)require(reshape2)ml <- read.dta(""http://www.ats.ucla.edu/stat/data/hsbdemo.dta"")ml$prog2 <- relevel(ml$prog, ref = ""academic"")test <- multinom(prog2 ~ ses + write, data = ml)predict(test, newdata = dses, ""probs"")but if I try:require(caret)ml <- read.dta(""http://www.ats.ucla.edu/stat/data/hsbdemo.dta"")ml$prog2 <- relevel(ml$prog, ref = ""academic"")test <- train(prog2 ~ ses + write,method=""multinom"" ,data = ml)predict(test$finalModel, newdata = dses, ""probs"")it returns Error in eval(expr, envir, enclos) : object 'sesmiddle' not found, why?";[education, open-source];135;1
3767;2;2014-12-26T17:26:02.033;;"That's not the error I get and I'm thinking that you left out some code. I get:predict(test$finalModel, newdata = dses, ""probs"")Error in as.data.frame(newdata) : object 'dses' not foundI don't know why I see this so much, but you should avoid using the finalModel object for prediction. train is doing some things that the model from multinom sin't aware of (such as expanding dummy variables). Try using predict(test, newdata = dses, ""probs"")and never use predict(test$finalModel) again...Max";;;
3770;1;2014-12-28T11:29:14.240;How to merge monthly, daily and weekly data?;"Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data. What I have done so far is to break each serie into daily data, for exemple: from:2013-03-03 - 2013-03-09 37to:2013-03-03 37 2013-03-04 372013-03-05 372013-03-06 372013-03-07 372013-03-08 372013-03-09 37But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)The other approach would be to ""merge"" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks. In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don't know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example). So three questions: For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?I feel like breaking weekly/monthly data into daily data like i've done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question: For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?Last but not least: when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ? I think this is a compromise between the number of data and the complexity of the model but I can't see any strong argument to choose between those options. Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.";[education, open-source];831;3
3771;1;2014-12-28T14:42:52.403;Time series: variations as a feature;I am trying to predict clients comportement from market rates. The value of the products depends on the actual rate but this is not enough. The comportement of the client also depends on their awareness wich depends on the evolution of rates. I've added this in model using past 6 month rates as features in polynomial regression. In fact media coverage of rate mostly depends on rate variations and I wanted to add that in my model.  The idea would be to add a derivative/variation of rate as a feature. But I anticipated something wrong, example with only two month , my variation will be of the form $x_n - x_{n-1}$ that is a simple linear combination of actual and past rates. So for a 1d polynomial regression i will have:$$ x_{n+1} = a * x_{n} +  b * x_{n-1} + c * (x_{n} - x_{n-1})$$ instead of: $$ x_{n+1} = a_0 * x_{n} +  b_0 * x_{n-1}$$wich is strictly equivalent with $ a + c = a_0 $ and $b-c= b_0$. Higher polynomial degree results in a more or less equivalent result. I am thinking about a way to include derivative information but it seems not possible. So I am wondering if all the information is included in my curve. Is this a general idea ? all information is somewhat directly contained in data and modifications of features will result in higher order objective function ?;[education, open-source];92;
3772;1;2014-12-28T14:58:45.320;Time series prediction;I am trying to predict a time serie from another one. My approach is based on a moving windows. I predict the output value of the serie from the following features: the previous value and the 6 past values of the source serie. Is it usefull to add the previous value of the time serie ? I feel like I don't use all the information contained in the curve to predict futures values. But I don't see how it would be possible to use all previous data to predict a value (first, the number of features would be growing trough time...).What are the caveats of a 6 month time-window approach ?Is there any paper about differents method of feature selection for time-series ? ;[education, open-source];190;1
3773;2;2014-12-28T20:10:11.300;;"I'm fairly new to this myself, but have spent a lot of time recently learning about time series and hope that I can help fellow learners. If I had the reputation to comment I'd ask you a few things first, but I can't. I'll happily do further work and edit this response if you respond or make edits to your question. With those caveats out of the way:Is it useful to use the previous value as a feature?One of the first things I would say is that the correct aspects to be looking at in your data very much depends on the nature of the data, as well as what you're trying to do with it:It sounds like you have monthly values, but it's not clear how far into the future you're wanting to predict, or how much historic data you have access to.We also don't know what these two series represent, or why one time series is being used to predict the other - and without that, I don't think anyone will be able to tell you whether the previous value of the series to be predicted is valuable information or not.Any caveats to using a 6 month time window?One obvious caveat to only using the last 6 months is that if there's any seasonality over the year-long period then you're going to miss it.If you're not sure: if you have multiple years of information, try plotting the series you want to predict over multiple years. You may well be able to see whether the series generally increases or decreases at certain times of year. If you can share this plot here, it might help people answer your questions in more depth.As far as caveats about this time-window approach, I'm not too clear from your post what algorithm you're using to predict values. More information on that would be helpful; it's possible that rather than questioning what features to select, you should be questioning what methodology to use for forecasting.Helpful further reading?Once you've provided more information I'll be happy to tackle your last question on suitable reading if I'm able to. For now, I will say that there's a lot of information available, but a lot of it is academic in nature. Quite frequently papers aren't easy to digest, seem to contradict one another, or are only relevant to specific situations. This is a rapidly growing and changing field, so it's sometimes difficult to find a clear best practice or consensus opinion.That said, it might be worth looking at some of the free, online courses available to see if any would help you understand the area you're interested in:Coursera ""data science"" related courses";;;
3774;2;2014-12-29T06:53:32.200;;Let me give you a few simple approaches in time series analysis.The first approach consists in using previous values of your time series $Y_{t}$ as in $Y_{t} = \phi_{1}Y_{t-1} + ... + \phi_{n}Y_{t-n}$. In case you don't know, these models are called autoregressive (AR) models. This answers your first question. Of course it is useful to include the previous value of your time series. There is a whole set of models based on that idea.The second approach is taking a window and extracting some features to describe the time series at each point in time. Then you use a conventional machine learning technique to predict future values as typically done. This is more common in a classification or regression setting but future values can be thought of as classifying future values. This technique has the advantage of dramatically reducing the number of features, although you usually lose characteristics associated with time. This addresses your second concern.Another model that could be helpful in your case is the vector autoregressive model (VAR) (using Wikipedia's notation):$$\left( \begin{array}{ccc}y_{1,t} \\y_{2,t} \end{array}\right) = \left( \begin{array}{ccc}c_{1} \\ c_{2}\end{array}\right) + \left( \begin{array}{ccc}A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2}\end{array}\right)\left( \begin{array}{ccc}y_{1,t-1} \\y_{2,t-1} \end{array}\right) + \left( \begin{array}{ccc}e_{1,t} \\e_{2,t} \end{array}\right)$$Here you can see that $y_{1,t}$ has a contribution from its previous value $t_{1,t-1}$ but also includes the value of the other series $y_{2,t-1}$ in a linear combination. As usual, the purpose is to find the elements of $A_{i,j}$ that minimize some measure of error between observed values and estimated values.A general suggestion: The first thing you need to do is to test the autocorrelation of your first series in order to confirm that an autoregressive approach is suitable and then test the cross correlation between both series to support the idea that using the second series to improve your predictions is appropriate.;;;
3775;2;2014-12-29T08:38:44.673;;"This does not seem a Data Science problem. However there are very nice tools to do exactly that, checkout: logstash, flume and fluentd. Actually if you want to be able to filter in fast and ""smart"" way checkout Kibana from the guys of ElastichSearch (http://www.elasticsearch.org/overview/kibana). Those tools are enough to solve your problem in a very efficient way.";;;
3776;2;2014-12-29T09:03:08.853;;Bias-Variance decomposition is one way, and V-C dimension/bound is another...Both of these are metrics you can use to get a feeling of how confident you should be that your training results will generalize to out-of-sample.V-C dimension focuses on the results of this learning algorithm outcome.Bias-Variance focuses on the expected outcome of the algorithm itself.Pick your poison - I sincerely hope that this was helpful.;;;
3777;2;2014-12-29T09:21:26.780;;Essentially machine learning uses non parametric methods...The assumption is that you have enough data and (computation)  time You Identify best model by cross validation (rather than eg  assessing significance of coefficients) ,  and estimate prediction error by using test set.  Confidence intervals can also be generated by bootstrapping. ;;;
3779;1;2014-12-29T17:27:59.763;Which Graph Properties are Useful for Predictive Analytics?;Let's assume I'm building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it's a history of which pages each user has viewed. I've used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I'd like to explore alternatives to see which method gives the most relevant recommendations.In addition to my simple traversal, I'm curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I've looked at SNAP, it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on. There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying?;[education, open-source];91;
3780;1;2014-12-30T06:43:24.340;How to transform one graph to a spectrum?;"Recently, I studied a paper called ""What Does Your Chair Know About Your Stress Level?It can be download at the link below. http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdfat page 210, Fig.5 (picture 5) mentioned that Spectra of the norm of the CoP vector during the experiment (stages 3–7) for the same subject used in Fig. 4.How did they do to transform Fig.4 to Fig.5? And what do x-axis and y-axis mean in Fig.5?Fig4Fig5";[education, open-source];35;
3781;1;2014-12-30T10:33:08.240;Scikit Learn: KMeans Clustering 3D data over a time period (dimentionality reduction?);I have a dataset of xyz coordinates with a date component in a pandas dataframeex:date1: $[x_1,y_1,z_1]$,date2: $[x_2,y_2,z_2]$,date3: $[x_3,y_3,z_3]$,..I would like to classify a sample of object positions over the period of a week(using indexes to re-map the classification label back to the date), like this:Week 1: $[x_1,y_1,z_1], [x_2, y_2, z_2], [x_3,y_3,z_3], [x_4,y_4,z_4], [x_5,y_5,z_5], [x_6,y_6,z_6], [x_7,y_7,z_7]$,Week 2: $[x_8,y_8,z_8],[x_9,y_9,z_9],[x_{10},y_{10},z_{10}],[x_{11},y_{11},z_{11}],[x_{12},y_{12},z_{12}],[x_{13},y_{13},z_{13}],[x_{14},y_{14},z_{14}]$,When I try to run KMeans it returnsk_means = KMeans(n_clusters=cclasses)k_means.fit(process_set.hpc)date_classes = k_means.labels_ValueError: Found array with dim 3. Expected <= 2Questions:Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?Are there any other methods I could use?Am I doing everything completely backwards and should consider a different approach, any thoughts?Thanks!;[education, open-source];415;
3783;1;2014-12-30T12:55:06.247;"How to model this ""un predicatability"" problem?";"Imagine modeling the ""input(plaintext) - output(ciphertext)"" pairs of an encryption algorithm as a data science problem. Very informally the strength of an encryption scheme is measured by the randomness (unpredictable) of the output. This is counter intuitive to classic regression problem which is used for prediction. Say Informally, the strength of the encryption scheme is determined by the number of such input-output pairs needed beyond which it becomes predictable ( the more the stronger). How do we model this as data science problem ? Given all the pairs of two different encryption schemes can we determine which is stronger just by using the input-output pairs of both the schemes? Is there any other way apart from regression ? to solve this problem ?";[education, open-source];116;2
3784;1;2014-12-30T14:23:58.323;Does a NB wrapper consider feature subset size?;while comparing two different algorithms to feature selection I stumbled upon the follwing question: For a given dataset with a discrete class variable we want to train a naive bayes classifier. We decide to conduct feature selection during preprocessing using naive bayes in a wrapper approach. Does this method of feature selection consider the size of the used feature subsets? When considering how NB classifies a given instance, the size of the feature subset being used for classification only influences the number of parts that the product of the conditional dependencies has but that does not make a difference, or does it?It'd be great if someone could offer a solid explanation since for me it's more of a gut feeling at the moment.;[education, open-source];38;
3785;2;2014-12-30T19:45:57.283;;The correct term for what you're describing here is 'class imbalance' or 'class imbalance problem' . It'd be great if you could include that in the title to have a more meaningful title.Concerning your first question:Have you plotted a confusion matrix of the resulting classifications? Is the reason why the accuracy is not satisfying really that instances are wrongly classified as the most common class?Given your context of application it seems that you could use a certain degree of oversampling. To what extent this can be applied should depend on the frequency of each underrepresented class.If there is a reasonably high variation in the value distributions on the underrepresented class instances then one could argue that this should be considered when applying the oversampling.Also, the approach Charlie suggested in his answer could be considered, given that the instances of the underrepresented classes would form a dataset that is suitable for classification.Edit: I haven't used naive bayes for text classification yet, so I'm not too sure how your attributes look like exactly. Do you just use the frequency of the terms that scored best with tfidf? More general, do you have discrete or continously valued attributes? If the latter is the case you should consider using some kind of discretisation.Regarding your second question:Are you splitting the dataset in any way? Normally, given that the classifier has been trained on the same data, the outcome for the same instance should also be the same.;;;
3786;1;2014-12-30T21:45:13.760;How to create separate nodes for each column in a spreadsheet using Neo4J?;"I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the captions in the properties section.Sample Create statement:LOAD CSV WITH HEADERS FROM ""File Location"" AS row CREATE (:NodeName {DATE_OCCURED: row.Date});Query to visualize the relationship between nodes:MATCH (a)-[:`REL NAME`]->(b) RETURN a,b LIMIT 25;This gives me the values the node ""a"" and random numbers for all the node ""b"". I don't know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don't get the actual values of the related nodes.  ";[education, open-source];48;
3787;2;2014-12-31T00:09:35.943;;"Short VersionThe difference is in the trade-off between complexity of model and how hard it will be to train.  The more states your variable can take, the much more data you'll need to train it.  It sounds like you're reducing the feature space through some front-end processing.ExampleContext:Say you're using NBC to model if you go outside given the high temperature that day.  Let's say temperature is given to you as an int in Celsius which just happened to range between 0 and 40 degrees.  Over one year of running the experiment - recording the temperature and if you went outside - you'll have 365 data points.  Internal Representation:The internal structure the NBC uses will be a matrix with 82 values (41 values for your one input variable * 2 because you have two states in your output class).  That means each bin will have an average of 365/82 = 8.9ish samples.  In practice, you'll probably see a few bins with lots more samples than this and a many bins with 0 samples.  A PitfallSay you saw 8 cases at a temperature of 5 C (all of which you stayed inside) and nothing at a temp of 3 or 4 C.  If, after the model is built, you 'ask it' what class a temp of 4 C should be in.  It won't know.  Intuitively, we'd say ""stay inside"", but the model won't.  One way to fix this is to bin the classes 0,1,2,... into larger groups of temperatures (i.e., class 0 for temp 0-3, class 1 for temp 4-7, etc).  The extreme case of this would be two temperature states ""high"" and ""low"".  The actual cut-off should depend more on the data observed, but one such scheme is converting temperatures within 0-20 to ""low"" and 21-40 is ""high"".  DiscussionThis front end processing seems to be what you're talking about as the ""wrapper"" around the NBC.  The result of our ""high"" and ""low"" binning scheme will result in a much smaller model (2 input states and 2 output classes gives you a 2*2 = 4 number of classes.  This will be way easier to process and will take way less data to confidently train at the expense of complexity.  One example of a drawback of such a scheme is:  Say the actual pattern is that you love spring and fall weather and only go outside when it's not too hot or not too cold.  So your outside adventures are evenly split across the upper part of the ""low"" class and the lower part of the ""high"" class giving you a really useless model.  If this were the case, a 3-class bin scheme of ""high"", ""medium"", and ""low"" would be best.";;;
3788;1;2014-12-31T02:50:32.863;Creating a Graph from Address Data;What's the best / easiest way to create a graph from address data?  For example, if I have 100 houses from all across a city is there any easy way to determine the shortest distance between two houses and all that good stuff?  Would this require changing the data into coordinates and using GIS software or can I get away with using Python or R?;[education, open-source];64;1
3791;2;2014-12-31T16:18:21.237;;If you have latitude/longitude coordinate data, there should be no problem accomplishing this using great circle calculations which could indeed be accomplished in Python, R, or essentially any other language.Here is an article on several methods and calculations for this:Calculate distance, bearing and more between Latitude/Longitude pointsThe main issue with street address data alone is of course the lack of contextual information regarding the physical layout of the streets. Given a sufficiently complete scale map of the relevant area, you could also calculate a distance. That said, the haversine formula discussed in the article above would have a greater accuracy unless the scale map mentioned was also plotted as the surface of a sphere.;;;
3792;1;2014-12-31T17:57:48.243;Object Recognition for classification, is it being used in industry?;I'm wondering if e-commerce companies where products are offered by users, such as EBay, are using Object Recognition to ensure that an uploaded image corresponds to an specific type of object (clothing, shoes, glasses, etc) either to classify automatically or more importantly to filter undesired images (such as non related or even illegal types).If so, which algorithms and/or open platforms could be use for doing so? From what I've looked it seems that HOG+Exemplar SVM might be one of the most accurate methods developed so far (http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf), even having couple of public repo's with Matlab implementations (https://github.com/quantombone/exemplarsvm), but I'm still wondering if this is being used in industry.;[education, open-source];77;2
3793;2;2014-12-31T21:47:37.490;;Here's a simple solution using the R package ggmap. start <- '95 clark st, new haven, ct'end <- 'maison mathis, new haven, ct'legs <- route(start,end, alternatives = TRUE)Will find routes from start to end.       m    km     miles seconds   minutes       hours  startLon startLat    endLon   endLat leg route1    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     A2   718 0.718 0.4461652      95 1.5833333 0.026388889 -72.91678 41.31541 -72.92100 41.30979   2     A3   436 0.436 0.2709304      64 1.0666667 0.017777778 -72.92100 41.30979 -72.92555 41.31171   3     A4   431 0.431 0.2678234      68 1.1333333 0.018888889 -72.92555 41.31171 -72.92792 41.30829   4     A5    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     B6  1276 1.276 0.7929064     179 2.9833333 0.049722222 -72.91678 41.31541 -72.92430 41.30543   2     B7   421 0.421 0.2616094      62 1.0333333 0.017222222 -72.92430 41.30543 -72.92869 41.30729   3     B8   129 0.129 0.0801606      28 0.4666667 0.007777778 -72.92869 41.30729 -72.92792 41.30829   4     B9    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     C10  421 0.421 0.2616094      58 0.9666667 0.016111111 -72.91678 41.31541 -72.91924 41.31211   2     C11  522 0.522 0.3243708     101 1.6833333 0.028055556 -72.91924 41.31211 -72.92502 41.31382   3     C12  240 0.240 0.1491360      33 0.5500000 0.009166667 -72.92502 41.31382 -72.92555 41.31171   4     C13  431 0.431 0.2678234      68 1.1333333 0.018888889 -72.92555 41.31171 -72.92792 41.30829   5     CAnd then we can find the length of time estimated to walk there with something like this. tapply(legs$minutes, legs$route, max)Hope this helps!;;;
3795;1;2015-01-02T05:36:54.783;Are Correlithm Objects used for anything in the industry?;"This refers to a system described in a book by Nick Lawrence titled ""Correlithm Object Technology"". The author coined the term ""correlithm"" as a combination of ""correlation"" and ""algorithm"".The Correlithm Objects (""COs"") described in the book are, in the author's view, ""primary data tokens"" in biological neural systems and are central to ""all high-level data representation, storage, and manipulation"". In addition to this the author describes the COs as ""important mathematical objects of the statistics of bounded, high-dimensional spaces"".Considering these descriptions it seems like these COs could be useful for AI. What I was curious about is whether they are actually used anywhere in the industry, and if so, in what kind of situations?";[education, open-source];26;
3796;2;2015-01-02T10:58:38.187;;As far as I am aware, object recognition is not extensively used in industry yet. Google's image search, for instance, is based on exploiting the text on the web pages rather than the images themselves. I have seen several start-up companies that market prototypes based on object recognition, but these products are either prototypes or not widely used.Regarding state-of-the-art techniques, you should take a look to recent literature on deep learning and deep neural networks applied to object recognition, along with more specific techniques like convolutional networks. ;;;
3797;1;2015-01-02T20:03:59.950;Lambda Architecture - How to implement the Merge Layer / Query Layer;I am reading up about lambda architecture.It makes sense. we have queue based data ingestion. we have an in-memory store for data which is very new and we have HDFS for old data.So we have our entire data set. in our system. very good.but the architecture diagram shows that the merge layer is able to query both the batch layer and the speed layer in one shot.How to do that?Your batch layer is probably a map reduce job or a HIVE query. The speed layer query is probably a scala program which is execution on the spark.Now how will you merge these?Is there any guidance.;[education, open-source];528;1
3798;2;2015-01-03T01:07:15.080;;"Try exploring the rich field of ""Anomaly Detection in Time Series"". Control charts and CUSUMs (or cumulative sum control charts) might help you.Simple Bullet Graphs might be all you need. Based on historical data and domain knowledge, define normal variance. Then make it clear to stakeholders when the current value is outside of predefined ranges.Stephen Few is an expert in business dashboards. Any of his books will help you.If you are open to R, try Shiny to create simple interactive web applications (It is very straightforward). There is also an open source package for anomaly detection, both local and global.  Create quick prototypes and get feedback!";;;
3799;2;2015-01-03T01:17:47.957;;I have shared a number of resources on time series classification and clustering in one of my recent answers here on Data Science StackExchange: http://datascience.stackexchange.com/a/3764/2452. Hopefully, you will find them relevant to this question and useful.;;;
3800;2;2015-01-03T03:15:08.960;;This won't be a very satisfying answer, but here's my take... For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ? For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?Same answer for both: you can't do this for unknown properties, and for known properties it will depend on how the values were computed.As you alluded to: (an average of fiancial rates would be a non-sense for example)There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you'll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc. when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won't necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion.;;;
3801;2;2015-01-03T07:58:10.663;;" when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?For your timeseries analysis you should do both: get to the highest granularity possible with the daily dataset, and also repeat the analysis with the monthly dataset. With the monthly dataset you have 120 data points, which is sufficient to get a timeseries model even with seasonality in your data. For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?To obtain say weekly or monthly data from daily data, you can use smoothing functions. For financial data, you can use moving average or exponential smoothing, but if those do not work for your data, then you can use the spline smoothing function ""smooth.spline"" in R: https://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.htmlThe model returned will have less noise than the original daily dataset, and you can get values for the desired time points. Finally, these data points can be used in your timeseries analysis.  For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?To obtain daily data when you have monthly or weekly data, you can use interpolation. First, you should find an equation to describe the data. In order to do this you should plot the data (e.g. price over time). When factors are known to you, this equation should be influenced by those factors. When factors are unknown, you can use a best fit equation. The simplest would be a linear function or piecewise linear function, but for financial data this won't work well. In that case, you should consider piecewise cubic spline interpolation. This link goes into more detail on possible interpolation functions: http://people.math.gatech.edu/~meyer/MA6635/chap2.pdf. In R, there is a method for doing interpolation of timeseries data. Here you would create a vector with say weekly values and NAs in the gaps for the daily values, and then use the ""interpNA"" function to get the interpolated values for the NAs. However, this function uses the ""approx"" function to get the interpolated values, which applies either a linear or constant interpolation. To perform cubic spline interpolation in R, you should use the ""splinefun"" function instead.Something to be aware of is that timeseries models typically do some sort of averaging to forecast future values whether you are looking at exponential smoothing or Auto-Regressive Integrated Moving Average (ARIMA) methods amongst others. So a timeseries model to forecast daily values may not be the best choice, but the weekly or monthly models may be better. ";;;
3802;2;2015-01-03T08:48:51.070;;"I'm not an expert in this area, but I believe that your question is concerned with time series aggregation and disaggregation. If that is the case, here are some hopefully relevant resources, which might be helpful in solving your problem (first five items are main, but representative, and last two are supplementary):Temporal Aggregation and Economic Time SeriesTemporal Disaggregation of Time Series (IMHO, an excellent overview paper)CRAN Task View: Time Series Analysis (R-focused)Introduction to R's Time Series FacilitiesWorking with Financial Time Series Data in RNotes on chapters contents for the book ""Time Series Analysis and Forecasting""Discussion on Cross Validated on daily to monthly data conversion (Python-focused)";;;
3804;1;2015-01-03T22:38:39.097;Format for storing textual data;For an upcoming project, I'm mining textual posts from an online forum, using Scrapy. What is the best way to store this text data? I'm thinking of simply exporting it into a JSON file, but is there a better format? Or does it not matter?;[education, open-source];120;
3805;2;2015-01-03T23:08:17.697;;for python, the standard tool is pandas. It was specifically designed to deal with financial data timeseries.pandas timeseries;;;
3806;2;2015-01-04T05:22:48.670;;The optimal solution very much depends on multiple factors, including your (current and future) business and IT processes, stakeholders' needs and preferences, and, in general, business and IT architectures. So, IMHO it's difficult to answer this wide and not well-defined question. Having said that, I hope that you will find the following earlier answers of mine here on Data Science StackExchange relevant and useful.On data analysis workflow (introductory): http://datascience.stackexchange.com/a/1006/2452On data analysis workflow (more detailed): http://datascience.stackexchange.com/a/759/2452On dashboard visualization: http://datascience.stackexchange.com/a/907/2452On big data visualization: http://datascience.stackexchange.com/a/3723/2452;;;
3807;1;2015-01-04T10:59:42.973;How i can generate the probabilistic graph for my dataset?;"Im doing my academic project. im having the base paper for reference  the paper is IEEE paper ""effective and efficient clustering methods for correlated probabilistic graph"". i wish to do this in R tool. in this paper two algorithm are implemented. i like to implement the peedr algorithm in the paper. how can i give the input for that algorithm.? suggest the packgages in R tool the paper can be found herehttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474";[education, open-source];88;1
3808;1;2015-01-04T11:22:28.817;Compare Neural Network generalization results;I'm trying to develop my neural network with both early stopping and bayesian regularization (matlab implementation, lm algorithm is used for both).Since in bayesian regularization I have not the validation set, how can I compare the generalization capability of the networks obtained with the two methodologies? Thanks;[education, open-source];47;
3810;2;2015-01-04T19:29:48.930;;"Søren Højsgaard has many quality resources for graphical models in R. He has a tutorial ""Graphical Models with R"" and a list of CRAN packages.Additionally, mclust is one of the best clustering packages in R.";;;
3811;1;2015-01-04T21:52:38.977;normalize identification values properly;I'm building a neural network to analyze a business' sales. I'm normalizing all input values to the range {0,1}.I'm struggling with the day of the week column. Business days are identified by a number ranging {1-5} (1=Monday). Normalizing these values to the range {0,1} is straightforward, but results in a major bias in the final output.The reason is that the full range of normalized values for the business day column is explored with every week worth of data, whereas other price-related column explore their full range of normalized values infrequently.The business day column ends up being the largest contributor to the final output.How can I normalize it to make its contribution more in tune with the rest of the inputs?;[education, open-source];31;
3812;2;2015-01-04T22:29:14.980;;Don't forget about MLlib (the hot kid on the block), cloudera-ml, cloudera oryx, cloudera oryx2, and a myriad of other efforts to make old algorithms like k-means solve nonexistant problems on wannabe big data... There is no one size fits all.Thus, there is no answer to your question, in particular no concise answer. ;;;
3813;2;2015-01-04T23:57:20.150;;"In general, use a storage method that allows you to quickly query it. If your collection is huge, you might need something Lucene-based, like ElasticSearch. If you are a SQL crack and your favorite DB supports it, a full-text index might do the trick. For small sizes like the 5000 documents, even Linux' LocateDb+grep or OSX' spotlight could be enough.The important point is to be able to quickly verify assumptions about the content of your data - how many documents contain X and Y, does any document contain W but not V, etc.This will be useful at both the whole set level as well as for analyzing your topic clusters. Finally, a few GNU tools or SQL mastery can also help you profile your document sets more efficiently (n-gram counts/ranks, collocations, concordances, etc)Edit: that means, for the above reasons and given your collection size, good old plain text (in a file system or a database) might be more efficient than any ""fancy"" format.";;;
3814;1;2015-01-05T18:34:55.920;"Communicating clearly about ""samples""";It is often pointed out that sample is an overloaded term in statistics and the sciences being supported by statistics.  In my field (geological sciences) as in most other sciences, the process of collecting meaningful data is critical and discussions about the traps and pitfalls in that process talk about sampling.  Not far down the road from that, particularly when lab results are back, conversations involving statisticians, data scientists, geomathematicians, GIS analysts and even normal geologists are likely to attempt to include multiple meanings of sample in the same sentence!Q: Have any data scientists (or statisticians) found practical ways to communicate these different meanings?  One way is to always add soil, rock, statistical and so on before sample.  But I was curious if there are any other approaches to effective communication that are in use.;[education, open-source];66;
3815;2;2015-01-05T22:16:06.093;;"""Sample"" as a noun usually refers to a single data point.  ""Sample"" as a verb is the act of extracting a subset of data points from some larger body (reality or a larger dataset).  The only way to be less ambiguous is use more specific words than ""data"" or ""sample"".Example:Say you collect 1MM data points from four different sensors in the field giving you four sets of 250k data points.  Say this data is too big for some demo of a model you're testing or an analysis you're running, so you pick 100k data points evenly split across the four sensors (giving four sets of 25k data points).  In this example, you're sampling twice.  First, to gather your 1MM data points sampling from reality.  Second, you sample again to decrease the size of your data set to something more manageable.  'Data' or 'sample' could refer to reality, the 1MM dataset, the 100k dataset, or any of the sensor-specific subsets.  To make it less ambiguous, establish a unique name as soon as possible for each possible definition you'll be working with.  ('reality' for the set of all possibly observed samples.  'the complete dataset', something derived from the source of the dataset, or even X for the full 1MM dataset.  'our trial dataset', or even Y for the small 100k dataset.What you actually do comes down to context and what's appropriate for your intended audience, but the general answer is to use more specific words.";;;
3816;2;2015-01-05T22:33:02.010;;As Neil said in the comments, split out a test set (data you don't use in training either model) and see how each trained model performs on the test set.;;;
3817;2;2015-01-05T22:49:27.293;; For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?Aggregation.  For example, you have the number of time people searched for 'widgets' every day.  Add up the daily totals for a month to get monthly totals.  I would need to see more specifics about the actual data collected at each granularity to give you a more complete version. For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?You can't.  In physics, a comparable idea is the Nyquist frequency.  The general idea is that you can't add more information than what you already have present in your data without bringing in more data.  Given only the day someone ran a query, how can you tell what time of day that query was ran?  You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system.  There are things you can do to make informed guesses at the daily state of monthly variables (as gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily. When given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?That totally depends on what you're trying to answer.  The smaller granularity will be more sensitive to noise and other anomalies.  The lager granularity will be able to answer questions more confidently, but loose some of it's usefulness.  For example, if you're trying to see when people start looking up venues to weekend plans to know when to launch marketing campaigns for a new night club, you'll want to be looking at daily data, if not smaller.  If you're looking at the general trending of night clubs to figure out who you want to invest in, then monthly would probably be better.;;;
3818;2;2015-01-05T23:09:25.800;;It is possible that the other variables you're feeding into the NN are simply bad at predicting sales.  Sell prediction is a notoriously hard problem.Specifically the addressing of mapping a multi-state categorical variable to the NN's {0,1} input range:  Another idea is to change that one, 5-state variable into five boolean variables.  Rather than {0,0.25,0.5,0.75,1.0} on your one variable, make each of the five boolean variables represent a single day and make [1,0,0,0,0] equal Monday, [0,1,0,0,0] equal Tuesday, etc.  I've personally had more success both with training good networks and introspecting the network itself when spreading out states of classes like that.Other hacks you can try: * Take out the the 'day' column all together and see if any of the other variables get used. * Plot the distribution of spend as a function of day.  Even if nothing else comes of this current model, it sounds like you've found one interesting insight already. * Consider also trying different models.;;;
3819;2;2015-01-06T01:00:54.457;;Being able to aggregate data based on date segments is a piece of cake using Tableau software. You would simply plug your data into the tool, and then you can drag and drop both the metric and date dimensions onto a report body space. Tableau will instantaneously whip up aggregate visualizations and/or detail data tables, on the fly. You can group/sum by year, quarter, month, week, day, hour, etc. (standard, built in, out of the box functionality offered by the tool)Also, if you wanted to incorporate additional data later on (which I assume is what you meant by 'unknown properties'), you can import another data set and easily append it onto the first one, as long as it also offers dates.I would recommend checking out the free version, which I believe is called Tableau Public. ;;;
3820;2;2015-01-06T01:26:41.587;;You don't need a tool and I don't recommend you use one.Convert the html to well-formed XML (XHTML) - I recommend the tagsoup.Once you've done that the data is just another XML feed and you can write an XSLT transformation (or XQuery) to access and pull out the data you want in the format you want. That might mean learning  XSLT/XQuery if you don't already know it but you will be learning skills that (unlike scraping tools) have multiple rather than just than one useful application.;;;
3821;1;2015-01-06T04:34:09.967;Finding unpredictability or uncertainty in a time series;"I am interested in finding a statistic that tracks the unpredictability of a time series. For simplicity sake, assume that each value in the time series is either 1 or 0. So for example, the following two time series are entirely predictableTS1: 1 1 1 1 1 1 1 1TS2: 0 1 0 1 0 1 0 1 0 1 0 1However, the following time series is not that predictable:TS3: 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1I am looking for a statistic that given a time series, would return a number between 0 and 1 with 0 indicating that the series is completely predictable and 1 indicating the series in completely unpredictable.I looked at some entropy measures like Kolmogorov Complexity and Shannon entropy, but neither seem to fit my requirement. In Kolmogorov complexity, the statistic value changes depending on the length of the time series (as in ""1 0 1 0 1"" and ""1 0 1 0"" have different complexities, so its not possible to compare predictability of two time series with differing number of observations). In Shannon entropy, the order of observations didn't seem to matter. Any pointers on what would be a good statistic for my requirement?";[education, open-source];115;2
3823;2;2015-01-06T07:40:12.223;;"Since you have looked at Kolmogorov-Smirnov and Shannon entropy measures, I would like to suggest some other hopefully relevant options. First of all, you could take a look at the so-called approximate entropy $ApEn$. Other potential statistics include block entropy, T-complexity (T-entropy) as well as Tsallis entropy: http://members.noa.gr/anastasi/papers/B29.pdfIn addition to the above-mentioned potential measures, I would like to suggest to have a look at available statistics in Bayesian inference-based model of stochastic volatility in time series, implemented in R package stochvol: http://cran.r-project.org/web/packages/stochvol (see detailed vignette). Such statistics of uncertainty include overall level of volatility $\mu$, persistence $\phi$ and volatility of volatility $\sigma$: http://simpsonm.public.iastate.edu/BlogPosts/btcvol/KastnerFruwhirthSchnatterASISstochvol.pdf. A comprehensive example of using stochastic volatility model approach and stochvol package can be found in the excellent blog post ""Exactly how volatile is bitcoin?"" by Matt Simpson.";;;
3825;1;2015-01-06T14:08:35.533;Mimic a Mahout like system;I have a data set, in excel format, with account names, reported symptoms, a determined root cause and a date in month year format for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system?Here is an example:Account : X Symptoms : AB, AD, AB, ABAccount : Y  Symptoms : AE, AE, AB, AB, EAFor the sake of this example, let's assume that all the dates are this month. O/P: Account : X Symptom: AEHere both of them have reported AB 2 or more times. I could fix such number as a threshold to look for probable symptoms. ;[education, open-source];48;
3826;1;2015-01-06T20:43:35.480;Correlation threshold for Neural Network features selection;I'm trying to do a correlation analysis between inputs and outputs inspecting the data in order to understand which input variables to include. What could be a threshold in the correlation value to consider a variable eligible to be an input for my Neural Network?;[education, open-source];36;
3827;2;2015-01-07T01:26:17.560;;Given non-linearity of neural networks, I believe correlation analysis isn't a good way to estimate importance of variables. For example, imagine that you have 2 input variables - x1 and x2 - and following conditions hold: cor(x2, y) = 1 if x1 = 1cor(x2, y) = 0 otherwisex1 = 1 in 10% of casesThat is, x2 is a very good predictor for y, but only given that x1 = 1, which is the case only in 10% of data. Taking into account correlations of x1 and x2 separately won't expose this dependency, and you will most likely drop out both variables. There are other ways to perform feature selection, however. Simplest one is to train your model with all possible sets of variables and check the best subset. This is pretty inefficient with many variables, though, so many ways to improve it exist. For a good introduction in best subset selection see chapter 6.1 of Introduction to Statistical Learning. ;;;
4826;2;2015-01-07T07:39:04.027;;Suppose that a forum post has, on average, 2000 characters, which is more or less the equivalent of a page of text, than the total memory needed is 10MB if text is ASCII. Even if the text is Unicode encoded in an Asian language it will take 40MB. This is far too little for modern computers, so a simple text format is the best since it can be parsed in the fastest way, and loaded into RAM all at once.;;;
4827;1;2015-01-07T16:50:18.803;"Can we use package ""dplyr"" on R base 3.0.2?";"Is there any way to use package ""dplyr"" on RStudio having R base 3.0.2 ?  I am not interested  in ""plyr"" package.ThanksNavin";[education, open-source];84;
4828;2;2015-01-07T22:03:37.357;;dplyr 0.3 requires R 3.1+. If you're stuck on R 3.0.x, you have to use dplyr 0.2.x. ;;;
4829;2;2015-01-08T00:25:05.277;;We faced this problem and analysed the issue.Cloudera, Hue or Hive didn't have any error reported but the users via Beeswax were unable to run queries.It was good to analyze below:Duplicate PID for Hive (check for all hive servers. If you do have a tool to check the duplicate PID then use that or use the kinit to login to cloudera admin node and analyze it.)We killed the duplicate PIDANDrestarted hive server. It fixed the issue.;;;
4830;2;2015-01-08T07:55:31.067;;I think one has to be very careful when storing textual data. If they are user comments then, for security concerns it's better if it is encoded in some format before storage.A protobuf object can then be defined to resolve the encoding. Depending on the query pattern, and accepted latency in retrieval of the data, DB should be decided. Just a recommendation, if the idea is to store comments over a period of time for each user, consider HBase or Cassandra. They are optimized for time range queries.Recommend read: http://info.mapr.com/rs/mapr/images/Time_Series_Databases.pdf;;;
4831;1;2015-01-08T10:09:32.010;Similarity measure based on multiple classes from a hierarchical taxonomy?;Could anyone recommend a good similarity measure for objects which have multiple classes, where each class is part of a hierarchy?For example, let's say the classes look like:1 Produce  1.1 Eggs    1.1.1 Duck eggs    1.1.2 Chicken eggs  1.2 Milk    1.2.1 Cow milk    1.2.2 Goat milk2 Baked goods  2.1 Cakes    2.1.1 Cheesecake    2.1.2 ChocolateAn object might be tagged with items from the above at any level, e.g.:Omelette: eggs, milk (1.1, 1.2)Duck egg omelette: duck eggs, milk (1.1.1, 1.2)Goat milk chocolate cheesecake: goat milk, cheesecake, chocolate (1.2.2, 2.1.1, 2.1.2)Beef: produce (1)If the classes weren't part of a hierarchy, I'd probably I'd look at cosine similarity (or equivalent) between classes assigned to an object, but I'd like to use the fact that different classes with the same parents also have some similarity value (e.g. in the example above, beef has some small similarity to omelette, since they both have items from the class '1 produce').If it helps, the hierarchy has ~200k classes, with a maximum depth of 5.;[education, open-source];239;2
4833;2;2015-01-08T12:08:43.113;;While I don't have enough expertise to advise you on selection of the best similarity measure, I've seen a number of them in various papers. The following collection of research papers hopefully will be useful to you in determining the optimal measure for your research. Please note that I intentionally included papers, using both frequentist and Bayesian approaches to hierarchical classification, including class information, for the sake of more comprehensive coverage.Frequentist approach:Semantic similarity based on corpus statistics and lexical taxonomyCan’t see the forest for the leaves: Similarity and distance measures for hierarchical taxonomies with a patent classification example (also see additional results and data)Learning hierarchical similarity metricsA new similarity measure for taxonomy based on edge countingHierarchical classification of real life documentsHierarchical document classification using automatically generated hierarchySplit-Order distance for clustering and classification hierarchiesA hierarchical k-NN classifier for textual dataBayesian approach:Improving classification when a class hierarchy is available using a hierarchy-based priorBayesian aggregation for hierarchical genre classificationHierarchical classification for multiple, distributed web databases;;;
4834;1;2015-01-08T13:44:20.160;Correcting Datasets with artificially low starting values;I am working on a project where we would like to take the ratio of two measurements A/B and subject these ratios to a ranking algorithm. The ratio is normalized prior to ranking (though the ranking/normalization are not that import to my question).In most cases measurement A (the starting measurement) is a count with values greater than 1000.  We expect an increase for measurement B for positive effects and a decrease in measurement B for negative effects.  Here is the issue, some of our starting counts are nearly zero which we believe is an artifact of experimental preparation.  This of course leads to some really high ratios/scaling issues for these data points.  What is the best way to adjust these values in order to better understand the real role in our experiment?  One suggestion we received was to add 1000 to all counts (from measurement A and B) to scale the values and remove the bias of such a low starting count,  is this a viable option?  Thank you in advance for your assistance, let me know if I am not being clear enough.;[education, open-source];24;
4835;2;2015-01-08T14:22:33.347;;Yes, the general idea is to add a baseline small count to every category. The technical term for this is Laplace smoothing. Really it's not so much of a hack, as encoding the idea that you think there is some (uniform?) prior distribution of the events occurring.;;;
4836;1;2015-01-08T15:47:34.103;Tutorials on topic models and LDA;I would like to know if you people have some good tutorials (fast and straightforward) about topic models and LDA, teaching intuitively how to set some parameters, what they mean and if possible, with some real examples.;[education, open-source];718;2
4837;1;2015-01-08T16:13:36.613;Learning cost function for linear regression;(Me: Never learned calculus or advanced math and I started Stanford openclasses for machine learning. I know basic matrix calculations.)One chapter of my course is about cost function. I have been trying to find any example calculation of it with numbers. Googling only finds the same formula everytime, and also on Octave. But I want to do the same thing first with pen+paper and without it, I cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.I require a cost function calculation example for following sample dataset:#Rooms = Rent1 = 40002 = 100003 = 220004 = 30000;[education, open-source];78;1
4838;2;2015-01-08T20:58:44.547;;I highly recommend this tutorial: Getting Started with Topic Modeling and MALLETHere are some additional links to help you get started...Good introductory materials (including links to research papers): http://www.cs.princeton.edu/~blei/topicmodeling.htmlSoftware:MALLET (Java): http://mallet.cs.umass.edu/topics.phptopic modeling developer's guide: http://mallet.cs.umass.edu/topics-devel.phpgensim (Python): http://radimrehurek.com/gensim/topicmodels (R): http://cran.r-project.org/web/packages/topicmodels/index.htmlStanford Topic Modeling Toolbox (designed for use by social scientists): http://www-nlp.stanford.edu/software/tmt/tmt-0.4/Mr.LDA (scalable topic modeling using MapReduce): http://lintool.github.io/Mr.LDA/If you're working with massive amounts of input text, you might want to consider using Mr.LDA to build your topics models -- its MapReduce-based approach might be more efficient when working with lots of data.Even more here on the Biased Estimates blog: Topic Models Reading List;;;
4839;2;2015-01-08T21:07:57.643;;Let me assume you intend to use Python libraries to analyze the data, since you are using Scrapy to gather the data.If this is true, then a factor to consider for storage would be compatibility with other Python libraries.  Of course, plain text is compatible with anything.  But e.g. Pandas has a host of IO tools that simplifies reading from certain formats.  If you intend to use scikit-learn for modeling, then Pandas can still read the data in for you, if you then cast it from a DataFrame to a Numpy array as an intermediate step.These tools allow you to read CSV and JSON, but also HDF5 ... particularly, I would draw your attention to the experimental support for msgpack, which seems to be a binary version of JSON.  Binary means here that the stored files will be smaller and therefore faster to read and write.  A somewhat similar alternative is BSON, which has a Python implementation  — no Pandas or Numpy involved.Considering these formats only makes sense if you intend to give at least some formatting to the stored text, e.g. storing the post title separately from the post content, or storing all posts in a thread in order, or storing the timestamp ... If you considered JSON at all, then I suppose this is what you intended.  If you just intend to store the plain post contents, then use plain text. ;;;
4840;1;2015-01-08T22:10:29.767;Where to start for transient sound classification?;"To learn machine learning, I'd like to develop an app similar to TableDrum (link here). Although I've been coding for a few years, I've never done machine learning before. I think what I'm supposed to do is:Choose a bunch of ""features"" that describe my soundsProvide training data to create some algorithmGenerate features in real time for recorded soundsAre there machine learning libraries out there (ideally that would work on mobile platforms) that can get me started? Thanks!";[education, open-source];19;
4842;2;2015-01-09T00:18:05.037;;You have a few problems here.  The first is cleaning your data.  That's a whole separate issue form anonymization and belongs in another question if you're still having problems with it.The second is your anonymization.  After you have some sort of identifier you're satisfied with (sounds like you're using people's real names), try hashing their names to generate a new id.  This id is useful because you'll always be able to take the original name and figure out what id it is but won't be able to derive the real names from just the hashed id (providing your hashing algorithm is good).Further reading:http://security.stackexchange.com/a/61878http://stackoverflow.com/a/21563966/4435034;;;
4843;2;2015-01-09T00:25:16.067;;Looks like lots of marketing around a specific case of what everyone was already doing to sound bigger than it is.  So, are people implementing mathematical frameworks in software to extract and use the structure within data?  Absolutely.  Are people using the word 'Correlithm' when doing it?  Not as far as I've seen.;;;
4844;1;2015-01-09T03:46:04.837;Hardware requirements for Linux server to run R & RStudio;I want to build a home server/workstation to run my R projects. Based on what I have gathered, it should probably be Linux based. I want to buy the hardware now, but I am confused with the many available options for processors/ram/motherboards. I want to be able to use parallel processing, at least 64GB? of memory and enough storage space (~10TB?). Software wise, Ubuntu?, R, RStudio, PostgreSQL, some NOSQL database, probably Hadoop. I do a lot of text/geospatial/network analytics that are resource intensive. Budget ~$3000US.My Questions:What could an ideal configuration look like? (Hardware + Software)What type of processor?Notes:No, I don't want to use a cloud solution.I know it is a vague question, but any thoughts will help, please?If it is off-topic or too vague, I will gladly delete.Cheers B;[education, open-source];683;1
4845;1;2015-01-09T05:42:54.587;Approaches to Bag-Of-Words Information Retrieval;I'm interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.  I'm specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.I've spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it's hard to get a bird's eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).All these vector space models tend to use cosine-similarity as the similarity function, however I have seen some literature discussing similarity functions which are more exotic.Is this basically where we currently are in attacking this particular type of problem? ;[education, open-source];129;2
4846;2;2015-01-09T08:22:40.920;;There is no ideal configuration, for R or in general - product selection is always a difficult task and many factors are at play. I think that the solution is rather simple - get the best computer that your budget allows.Having said that, since you want to focus on R development and one of R's pressing issues is its critical dependence on the amount of available physical memory (RAM), I would suggest favoring more RAM to other parameters. The second most important parameter, in my opinion, would be number of cores (or processors - see details below), due to your potential multiprocessing focus. Finally, the two next most important criteria I'd pay attention to would be compatibility with Linux and system/manufacturer's quality.As far as the storage goes, I suggest considering solid state drives (SSD), if you'd rather prefer to have a bit more more speed than more space (however, if your work will involve intensive disk operations, you might want to investigate the issue of SSD reliability or consult with people, knowledgeable in this matter). However, I think that for R-focused work, disk operations are much less critical than memory ones, as I've mentioned above.When choosing a specific Linux distribution, I suggest using a well-supported one, such as Debian or, even better, Ubuntu (if you care more about support, choose their LTS version). I'd rather not buy parts and assemble custom box, but some people would definitely prefer that route - for that you really need to know hardware well, but potential compatibility could still be an issue. The next paragraph provides some examples for both commercial-off-the-shelf (COTS) and custom solutions.Should you be interested in the custom system route, this discussion might be worth reading, as it contains some interesting pricing numbers (just to get an idea of potential savings) and also sheds some light on multiprocessor vs. multi-core alternatives (obviously, the context is different, but nevertheless could be useful). As I said, I would go the COTS route, mainly due to reliability and compatibility issues. In terms of single-processor multi-core systems, your budget is more than enough. However, when we go to multiprocessor workstations (I'm not even talking about servers), even two-processor configurations can go over your budget easily. Some, not far away, such as HP Z820 Workstation. It starts from 2439 USD, but in minimal configuration. When you upgrade it to match your desired specs (if it's even possible), I'm sure that we'll be talking about 5K USD price range (extrapolating from the series' higher-level models). What I like about HP Z820, though, is the fact that this system is Ubuntu certified. Considering system compatibility and assuming your desire to run Ubuntu, the best way to approach your problem is to go through Ubuntu-certified hardware lists and shortlist systems that you like. Just for the sake of completeness, take a look at this interesting multiprocessor system, which in compatible configuration might cost less than from HP or other major vendors. However, it's multimedia-oriented as well as it's reliability and compatibility are unknown, not to mention that it's way over your specified budget.In terms of R and R-focused software, I highly recommend you to use RStudio Server instead of RStudio, as that will provide you with an opportunity to be able to work from any Internet-enabled location (provided you computer will be running, obviously). Another advice that I have is to keep an eye on alternative R distributions. I'm not talking about commercial expensive ones, but about emerging open source projects, such as pqR: http://www.pqr-project.org. Will update as needed. I hope this is helpful.;;;
4847;2;2015-01-09T10:02:08.437;;"There is a worked out example in the Wikipedia page for ""simple linear regression""Just for the sake of it, let me plug in your example into the formulas:The fitted model should be a straight line with parameters $\alpha$ (value at $x = 0$) and $\beta$ (the slope):$$f(x) = \alpha + \beta x$$The values for these parameters that minimize the distance between line and data points are called $\hat{\alpha}$ and $\hat{\beta}$.  They can be computed out of the data point values by using these formulae, derived here:$$\begin{align}\hat{\beta} & = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 } , \\\\\hat{\alpha} & = \bar{y} - \hat{\beta}\bar{x}\end{align}$$where an expression with an overline $\overline{xy}$ means the sample average of that expression: $\overline{xy} = \tfrac{1}{n} \sum_{i=1}^n{x_iy_i}$. Here are the values I find for the datapoints you have listed in your question:$$\begin{align}\overline{xy} &= \frac{1}{4} \sum{<(1 \times 4000), (2 \times 10000), (3 \times 22000), (4 \times 30000)>} \\              &= \frac{1}{4}(4000 + 20000 + 66000 + 120000) = 52500, \\\overline{x} &= \frac{1}{4} \sum{<1, 2, 3, 4>} = 2.5 ,\\\overline{y} &= \frac{1}{4} \sum{<4000, 10000, 22000, 30000>} = 16500 , \\\overline{x^2} &= \frac{1}{4} \sum{<1^2, 2^2, 3^2, 4^2>} = 7.5 , \\\overline{x}^2 &= 2.5^2 = 6.25\end{align}$$and the fitted line should be:$$\begin{align}\\hat{\beta} &= \frac{52500 - 2.5 \times 16500}{7.5 - 6.25} = \frac{41250}{1.25} = 33000 , \\\hat{\alpha} &= 16500 - 33000 \times 2.5 = - 66000 , \\\Rightarrow f(x) &= - 66000 + 33000 x \end{align}$$Therefore, the model would predict, for a house with 10 rooms, a rent of:$$ f(x) = -66000 + 33000 \times 10 = 264000 $$";;;
4848;2;2015-01-09T11:25:11.623;;What you are asking about is, in my view, the main problem of implementing a lambda architecture.  Here are some suggestions on how to solve it.The combination of Spark and Spark Streaming largely supersedes the original lambda architecture (which usually involved Hadoop and Storm).  Read here an example of how to use a SparkContext and a separate StreamingContext to produce different RDDs, one for batch processed results and another for real-time results.Once you have replicated that in your system, you still have to think about how to query both kind of RDDs.  The trivial case would be to just union both of them:scala> rdd1.union(rdd2).collectOr maybe you can create a new DStream, similar to stateStream in the linked example, where some keys are kept for real-time results, and others for batch results. ;;;
4850;2;2015-01-09T18:26:08.933;;"I think this point is the core of your question:  How should I define an ""exception""? How can I know if my definition is a good one? I would go about it as follows: Go back to the managers who receive the current reporting and ask them to give you examples of reported exceptions that were actually exceptions, i.e. they were acted upon and the action yielded some benefit.Ask them also if there were any features in that report that were not plainly viewable, but led them to think it was an exception, e.g. the month-on-month difference was not reported, but it could be computed from the week-on-week difference.Treat the examples from the first as labels for training; you want to learn ""what is an exception"" and you need to have an expert answer that question for you.Treat the suggested features as new features for your classification.If you cannot get the experts to answer the questions you make, then try to infer them from their interaction with your reports: how many of those reports were downloaded, how many times are they mentioned, how are they ever used in decision-making ...  Try to separate the important ones from the uninteresting ones, and there you have your labels.";;;
4851;2;2015-01-09T19:05:38.040;;I don't have enough reputation to ask questions in comment for clarification before answering it, so I'm going to do both here. Here are the things that would help answering this question for now:Can you post part of the process_set.hpc? What's its format? Is it a numpy array? Is it a Pandas dataframe?What's the value of cclasses?And now the answer:First of all, k-means algorithm is able to find clusters in any n-dimensional data. If n is too big, it is better to use PCA but for n=3 that wouldn't necessarily add any value. The second thing that looks suspicious to me is that in the documentation for kmeans in scikit-learn, there is no compute_labels option, as seen here. However, that option exists for MiniBatchKMeans as seen here.Also, if you make your data in the form of a pandas dataframe (if it is not already so), things would be much easier to track and you won't have to reattach the timing information to your data afterwards.I may be able to give you a more thorough answer if I know a bit more about the format of the data.Good luck!;;;
4852;1;2015-01-10T00:15:45.990;Storing Big Matrix in DataBase;I have a mysql database with the following format:id      string1        foo1...2        foo2.....       ...There are >100k entries in this db.What I want to do is for each string, compare it to each other string and store some metric of the comparison.  Doing this will essentially yield a 2D matrix of size NxN where N is the number of row in the db.My initial thought was creating another db where each index corresponds to the string of the index in the first db and each column is the value from comparing the two strings. For example, id 1 column 2 in the second db would be the value outputted from comparing id1 and id2 in the first db.The format of the second db:id    col1    col2    col3    ....1       1      0.4     0.5    ........    ...     ...      ...This way of creating the second db would result in 100k rows x 100k columns, which is the issue at hand.  What is the best way to handle large data sets like this?  Is storing the data in a text file more efficient (say each text file corresponds to one row in the second db.);[education, open-source];269;
4853;2;2015-01-10T04:33:10.407;;It is usually more practical to compute the distances on the fly rather than storing $N^2$ values. If possible, you will want to use a heuristic to select only the items could be sufficiently similar so you don't waste time computing distance to irrelevant items. PostgreSQL can create an index on character trigrams in strings that enables efficient querying for other strings that are sufficiently similar (measured by the Jaccard similarity coefficient).Storing an $N$ column wide table is not possible because the maximum number of columns allowed in MySQL is 4096. Even if that were not the case, using an RDBMS in such a way is considered bad practice and such a table would be difficult to work with.If for some reason you must store a dense distance matrix, a format such as HDF5 would be more efficient than an RDBMS.;;;
4855;1;2015-01-10T22:53:14.353;Shifting dataPoints up by a constant (Is there an issue with too many 0's for features?);I am currently collecting second by second data regarding buyer vs seller initiated trades for different financial instruments (securities mostly). If there are more buyer initiated trades in a given second, then that second's data point would contain a positive value in the pertinent feature. If there are more seller initiated trades, then there would be a negative value. And if either there is an equal amount of buy vs seller initiated trades OR if there are simply not any trades in a given second, there will be a 0 for the feature in that data point. Along with this feature, there are several other features that are based on what occurred in the preceding seconds (eg if the value discussed above was 12 for the data point immediately preceding the current point, then the second feature for the current data point would be 12 - please let me know if this is not clear) After much troubleshooting, I have concluded that if there are too many data points with too many 0's for features, the classifier simply wont work. When I print out the probabilities of evaluation data points falling into different classes, I simply get0:NaN,1:NaNfor all model evaluation points I try to classify. (I am using logistic regression from apache-mahout. In total have 183 features, but over 40million data points. There are three categories to which the data point can be classified)I have found that if I set the default value to 1, then I no longer encounter this error, e.g. if there are no trades, the value will be a 1, if there is one seller initiated trade, the value will be 0.So with all this in mind, I have two related questions:1) Has anyone else encountered this issue? e.g. if you have a vector with x features, and for a majority of the data points, a majority of the features contain 0's, is this know to give issues?2) Is shifting all values up by a constant (such as 1) a valid fix to this issue? I assume that if this constant is applied to all values, then it shouldn't skew the data, but I figure it won't hurt to check with the experts.Also, I'm new to this, so if you believe that my question could use more info please let me know, and if you could give me ideas of what information to include, it would be greatly appreciated.thanks in advance;[education, open-source];21;
4856;2;2015-01-11T01:57:49.490;;The derivative is a linear transform, and you're using a linear model. As you've demonstrated, nothing is gained by adding linear combination of other features (like the derivative) as a new feature in a linear model.You may want a derivative coefficient to help interpret the model. You could use the normal model $x_{n+1} = a * x_n + b * n_{n-1}$ and then report the value $a-b$ to the client. A more general approach is to examine the frequency response of the model (the derivative is a high-pass filter, your model is a FIR filter).Alternately you can rotate the basis of the input such that the derivative is a feature and the dimensions are still independent as follows:You can think of the independent variables $x_n$ and $x_{n-1}$ as forming a standard basis for the two dimensions of your model's input, that is $\langle 1, 0\rangle$ and $\langle 0, 1\rangle$. Neither $a\langle 1, 0 \rangle$ or $b \langle 0, 1 \rangle$ are orthogonal to  c$\langle1, -1\rangle$.Therefore, if you want to use $c(x_n - x_{n-1})$ as a feature, I would also use the orthogonal feature $d(x_n + x_{n-1})$. That is,$$ x_{n+1} = c(x_n - x_{n-1}) + d(x_n + x_{n-1}) $$.You could think of this model as the derivative plus the integral. The same logic applies to the choice of scaling function for the Haar wavelet.;;;
4859;1;2015-01-11T11:43:38.253;Aspect based sentiment analysis using machine learning approach;"I am very new in machine learning. I have annotated data with category, aspect, opinion word and sentiment. for example, for the bellow text""The apple was really tasty""I have category->food, aspect-> apple, opinion word ->tasty and sentiment->positive. I have training data like this format.How can I train a SVM classifier using this format of training set?How to extract features like n-gram, POS and sentiment word to train the classifier?Could you please suggest any beginning step for this aspect based sentiment analysis using machine learning algorithms?";[education, open-source];302;1
4860;2;2015-01-11T13:45:04.360;;"I would recommend you to start from reading the draft of the introductory book ""Sentiment analysis and opinion mining"" by Bing Liu. The draft in a PDF document format is available for free here.More details about the new upcoming book of this author, as well as comprehensive information on the topic of aspect-based sentiment analysis, with references and links to data sets, are available at this page: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html.Another interesting resource is a survey book ""Opinion mining and sentiment analysis by Bo Pang and Lillian Lee. The book is available in print and as a downloadable PDF e-book in a published version or an author-formatted version, which are almost identical in terms of contents.";;;
4861;2;2015-01-11T17:38:19.187;;While reading blogs and papers is helpful to identify the latest and greatest, having a solid foundation helps a lot, too. But I assume you already have gone over Manning's great (and free in e-book form) book on IR, right?http://nlp.stanford.edu/IR-book/It contains information on creating your own thesaurus from your document collection to solve synonymy problems, LSA for polysemy, etc..As for similarity measures, you will see there that Okapi BM25 (Robertson et al.) is considered superior to cosine similarity (but more expensive to implement and run).Regarding the current state of the art, there was a small emergence of Bayesian Network-based classifiers in the early nineties (starting with Turtle & Croft), but that went quiet for a while.However, right now, using BNs for IR is again finding some revival, particularly in  biomedical IR.In that respect, I think most ongoing work is directed towards using Bayesian models incl. topic models and deep learning for word-sense disambiguation (WSD) and semantic similarity.Here is a pointer to a recent paper with good references on the topic.http://arxiv.org/abs/1412.6629;;;
4862;2;2015-01-12T05:47:37.127;;If you can provide more details about the processing you're doing to the data, I think the responses would be a little more helpful.Here are a few things to consider:What's the shape of your data going into k-means? Are you aggregating up to the week level? If so, then your data won't look like the example you posted, since that seems like daily data.Try using .shape on your output data set. Does the # of rows match the data set you put into it? (This ties in to question #1. You'll be unable to join back directly if your original data is daily and the k-means data is weekly.I'm not sure if this is what you're attempting, but just declaring data_classes as the labels isn't going to add them back in to your original data. Like oxtay suggested, using pandas is a good choice because it will allow you to join.;;;
4863;2;2015-01-12T07:58:39.780;;I would like to recommend to check the following open data repositories and meta-repositories (they are not focused on categorical data, but I'm sure that many data sets, listed there, contain such data):http://www.kdnuggets.com/datasetshttp://www.data.govhttp://www.datasciencecentral.com/profiles/blogs/big-data-sets-available-for-freeAlso check built-in data sets in the open source software Parallel Sets, which is focused on the categorical data visualization: https://eagereyes.org/parallel-sets.;;;
4864;1;2015-01-12T09:27:15.250;What is the underlying pattern, or algorithm(s) used in these numbers?;I have 11 lottery tickets (used) and I have discovered that in each ticket, the 3rd digit's value is +1 of the value of the 6th digit. I have 11 tickets, each ticket is composed of 16 digits. Would someone (anyone) help me find the logic used here, in creating the other digits and their interrelation? I am a complete noob in data analysis, and any help would be greatly appreciated. For those who would like to know, I incidentally discovered the certain pattern, and made me realize these numbers are not totally random and if I could find the underlying pattern I'd be able to predict. The excel file is here Excel file;[education, open-source];152;1
4865;2;2015-01-12T10:31:34.573;;This article shows some details on scalability, special advantage for Titan.https://groups.google.com/forum/#!topic/orient-database/CpPh42ukfH4;;;
4866;2;2015-01-12T10:37:27.507;;11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it's not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.Even if there was a correlation or correlation between all the instances of lottery numbers, correlation doesn't imply causation and these patterns, if found any, couldn't really give any information about the key generation system itself.If you are really serious about it. you could go to this page and enter your data as a sequence it would show you the pattern, if there is any:The On-Line Encyclopedia of Integer Sequences;;;
4867;2;2015-01-12T10:41:57.830;;"From your question I am not sure how you are using logistic regression. The ""vanilla"" version of logistic regression gives a binary classifier, but your target has three values: a positive value, a negative value, and zero. The Apache site, on the page for logistic regression refers to the thesis of Paul Komarek, in which ""logistic regression"" refers to the binary classifier. So your problem might be that there your target variable has 3 values.Assuming that that is the case, you have many options, for example: - a) use ""multi-nomial logistic"",  - b) use ordinary logistic regression 3 times to fit 3 models, one per level, to distinguish it from the other two combined  ";;;
4868;2;2015-01-12T16:39:40.640;;The similarity function at the core of the method will define all the values for your distances $d_1,d_2, \ldots, d_n$. The initial query should have some words as a reference point to compare to the words in the document. Not knowing whether the query is a sentence or arbitrary list, you are restricted to a method that does some kind of histogram comparison of the frequency of the words matching in the documents. You can perform naive summations of keyword mappings counts, look at keyword likelihoods in the normalized distributions, or give a distribution of weighting based on the strongest matches. More exotic functions will be based on your prior belief of how the words should be compared. Working within a Bayesian Framework you can see your prior assumptions explicitly. Cosine similarity or any other vector based measure will be slightly arbitrary without knowing the desired nature of comparison between query and document.There is not much more you can do without looking at some type of features, or attempt to cross compare the documents together, or use the initial query's structure. In short, my answer is to use normalized frequency similarities of the document to the queries and produce a ranking, and with more specific goals in mind to apply measures like cosine similarity on test datasets to search for the best measure. ;;;
4870;2;2015-01-13T17:00:04.647;;"Aleksandar Blekh has given some really nice links about the big picture of how to do sentiment analysis; I'll try to provide some links to software and talk about the nitty-gritty of how to make it work. I'll point you to the example using scikit-learn (http://scikit-learn.org/stable/ ), a machine learning library in Python. You would first want to take your dataset and load it into scikit-learn in a sparse format. This link (http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html ) gives examples of how to load text in a bag-of-words representation, and the same module (scikit-learn.feature_extraction.text) can also count n-grams. It then describes how to run Naive Bayes and SVM on that dataset. You can take that example and start playing with it. ";;;
4872;2;2015-01-14T01:08:04.997;;"Using derivatives as features is almost the same as using past values, as both reconstruct phase or state space for dynamic system behind the time series. but they differ in some points, like noise amplification and how they carry information.(see Ref: State space reconstruction in the presence of noise; Martin Casdagli; Physica D - 1991 - section 2)Notice all information is embedded in time series, but using derivatives is going to reinterpret this information, which may be useful or useless.In your case, if you use all parameters and terms, i believe there is no use in it. but in case of using some algorithms like orthogonal forward regression (OFR) it may be beneficial. (see Ref: Orthogonal least squares methods and their application to non-linearsystem identification; S. CHEN, S. A. BILLINGS; INT. J. CONTROL, 1989)";;;
4873;1;2015-01-14T09:08:00.483;Querying DBpedia from Python;How can I get information about an entity from DBpedia using Python?Eg: I need to get all DBpedia information about USA  (http://dbpedia.org/page/United_States) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.I tried :PREFIX db: <http://dbpedia.org/resource/>SELECT ?p ?oWHERE { db:United_States ?p ?o }But here all DBpedia information is not displaying.How can I do this and which all are the possible plugins/api available for python to connect with DBpedia ?Also what will be the SPARQL query for generating the above problem result?;[education, open-source];367;
4874;2;2015-01-14T12:46:41.357;;"You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint.  Here is an option for the library and here is the URL to point it to: http://dbpedia.org/sparqlYou need to issue a DESCRIBE query on the United_States resource page:PREFIX dbres: <http://dbpedia.org/resource/>DESCRIBE dbres:United_StatesPlease note this is a huge download of resulting triplets.Here is how you would issue the query:from SPARQLWrapper import SPARQLWrapper, JSONdef get_country_description():    sparql = SPARQLWrapper(""http://dbpedia.org/sparql"")    sparql.setReturnFormat(JSON)    sparql.setQuery(query)  # the previous query as a literal string    return sparql.query().convert()";;;
4875;1;2015-01-14T15:42:48.013;Determine highly correlated segments;Given a dataset that has a binary (0/1) dependent variable and a large collection of continuous and categorical independent variables, is there a process and ideally a R package that can find combinations/subsets/segments of the IVs that are highly correlated with the DV?Simple example: DV: college education (0/1), and IVs: age (20 to 120), income (0 to 1 million), race (white, black, hispanic etc), gender (0/1), state, etc.Then finding correlations combining IVs and subsets of IVs (e.g. women between 30 and 50, with incomes over 100k are highly positively correlated with the DV), and then being able to compare the combinations (e.g. to find out women between 30 and 40, with incomes over 100k have a higher correlation than women between 40 and 50, with incomes over 100k);[education, open-source];80;
4876;1;2015-01-14T15:52:10.633;Appropriate way to store data in R;"I have data, which looks like this:These data are only for one subject. I will have a lot more.These data will be analyzed in R.Now I'm storing them like this:subject <- rep(1, times = 24)measurement <- factor(x = rep(x = 1:3, each = 8),                       labels = c(""Distance"", ""Frequency"", ""Energy""))speed <- factor(x = rep(x = 1:2, each = 4, times = 3),                 labels = c(""speed1"", ""speed2""))condition <- factor(x = rep(x = 1:2, each = 2, times = 6),                     labels = c(""Control"", ""Experm""))Try <- factor(x = rep(x = 1:2, times = 12),               labels = c(""Try1"", ""Try2""))result <- c(1:8,             11:18,             21:28)dt <- data.frame(subject, measurement, speed, condition, Try, result)What is the appropriate way to store these data in R (in a data frame)? ";[education, open-source];135;
4877;2;2015-01-14T17:06:58.750;;"I am no expert in that particular case, but doing a bit of research, it seems that the measure you want to construct is called ""Point-biserial correlation coefficient"", i.e. the inferred correlation between a continuous variable $X$ and a categorical variable $Y$, e.g. $Y∈\{−1,0,1\}$.  See a related question on Cross Validated SE.And yes, there is an R package for that :)";;;
4878;2;2015-01-14T17:56:58.563;;Without more information all I can say is that:the say you're storing it is fine in general you can further transform/store your data depending on your use caseTo expand on #2, if I want to study Distance vs Energy across all subjects, then I would format my data like this:> library(reshape2)> dt2 <- dt[dt$measurement %in% c('Distance','Energy'),]> dt_cast <- dcast(dt2, subject+Try~measurement+speed+condition, value.var='result')The transformed data (dt_cast) would then look like:  subject  Try Distance_speed1_Control Distance_speed1_Experm Distance_speed2_Control1       1 Try1                       1                      3                       52       1 Try2                       2                      4                       6  Distance_speed2_Experm Energy_speed1_Control Energy_speed1_Experm Energy_speed2_Control1                      7                    21                   23                    252                      8                    22                   24                    26  Energy_speed2_Experm1                   272                   28Allowing me to, for example, look at the relationship between the Distance_speed1_Control vs Energy_speed1_Control columns.Basically subset/aggregate your data and then use the dcast to get the rows and columns the computer needs.;;;
4879;1;2015-01-15T00:48:48.383;Training a function that maps n-dim to n-dim;As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.What would be the proper name for that? Which are examples of algorithms for the job?EDIT:More specifically, I want to train audio source separation. The input is a mixed sound (spectrogram) and the output is the sound with some energy removed in certain frequencies. The function needs to recognize some pattern in the input and decide what to remove.;[education, open-source];38;
4880;1;2015-01-15T01:57:38.387;Generate tags for live chat transcripts;I'm wondering if there's a way to automatically generate a list of tags for live chat transcripts without domain knowledge.  I've tried applying NLP chunking to the chat transcripts and keep only the noun phrases as tag candidates.  However, this approach would generate too many useless noun phrases.  I could use some rules to prune out some of them, but it would be hard to generalize the rules.;[education, open-source];62;
4881;1;2015-01-15T11:35:08.843;Probability distribution in input-output pairs;This question might sound silly. But I have been wondering why do we assume that there isa hidden probability distribution between input-output pairs in machine learning setup ?For example, if we want to learn a function $f: \mathcal{X} \rightarrow \mathcal{Y}$, we generally tend to assume a probability distribution $\rho(x,y)$ on $Z=\mathcal{X} \times \mathcal{Y} $ and try to minimize the error $$\mathcal{E}(f) = \int (f(x)-y)^2 \ d\rho(x,y)$$Is the probability distribtution $\rho$ inherent to the very nature of $Z$ or depends on $f$ ?Can anyone please provide a good intuitive explanation for this ?;[education, open-source];80;1
4882;1;2015-01-15T14:15:51.923;Tools for generative models;There are quite a number of generative models here, e.g.:Gaussian mixture model and other types of mixture modelHidden Markov modelProbabilistic context-free grammarNaive BayesAveraged one-dependence estimatorsLatent Dirichlet allocationRestricted Boltzmann machineWhat I want to do is, given a set of strings learnt by each of the generative model, I want to make use of the learnt model to output a set of strings in descending order of their probabilities. Is there any readily available tools (preferably in Python, other are also ok) that support for generative model that allows me to achieve this?;[education, open-source];36;
4883;1;2015-01-15T15:13:54.713;What kind of data is not appropriate using CF to do recommendation?;"I am currently working on a recommendation system for daily news. At first, I evaluated all the recommender algorithms and their corresponding settings (e.g., similarities, factorizers, ...etc) implemented in Mahout. Since we want to recommend daily news for users, we use the reading behavior of each user collected two days ago as training set, data of the next day as the testing set. The evaluated RMSE is good, the best recommender is SVD+SGD, so we implemented the recommender on our system for several days of trial run.However, the result, the actually recommended news, seems to be not very attractive for real users (""not attractive"" here means, the users feel like ""why you recommend this to me?""). So we decided another approach: use the tags and categories and their relationship to do the main job of recommendation, the result from CF is for just supporting.This makes me wonder if CF if not appropriate for some kind of content. Because I also worked on movie and music recommendation, CF is a good tool. But for news, it seems not the case.Can anyone explain why this happening, and also give some guideline about how to choose appropriate recommendation methods? Thanks:)";[education, open-source];36;1
4884;1;2015-01-15T18:32:36.883;What are the current killing machine learning methods?;I was wondering whether we could list machine learning winning methods to apply in many fields of interest: NLP, image, vision, medical, deep package inspection, etc. I mean, if someone will get started a new ML project, what are the ML methods that cannot be forgotten?;[education, open-source];86;
4885;2;2015-01-15T20:07:51.903;;If you're working in R, Carson Sievert's tutorial on using LDA to model topics in movie reviews is an excellent starting point:  http://cpsievert.github.io/LDAvis/reviews/reviews.htmlThis tutorial makes use of LDAvis, an interactive visualization of topic and word distributions that can really aid intuition.Also, although not short, David M. Blei's lectures on topic models are a great resource for understanding the meaning behind the parameters: http://videolectures.net/mlss09uk_blei_tm/;;;
4886;2;2015-01-15T23:27:20.597;;"I am just 5 months late but with CRFSuite you can actually use those float features as numbers, not as strings. For this, you just need to invent an unique label for each dimension then add a "":"" followed by the value.For example, a word ""jungle"" is represented in 5 dimensions:0.1 0.4 0.8 0.2 0.9Then CRFSuite would take that word + feature as:LABEL f1:0.1 f2:0.4 f3:0.8 f4:0.2 f5:0.9where of course you replace ``LABEL'' by an actual string and you separate all spaces with tabs (that's the format for CRFSuite).Not sure though for other packages.";;;
4887;2;2015-01-16T01:00:45.890;;"The question is very general. However, there are some studies being conducted to test which algorithms perform relatively well in a broad range of problems (I'll add link to papers later), concerning regression and classification.Lately Random Decision Forests, Support Vector Machines and certain variations of Neural Networks are being said to achieve the best results for very broad variety of problems.This does not mean that these are ""the best algorithms"" for any problem, that does not exist, and actually is not very realistic to pursue. Also it must be observed that both RDF and SVM are rather-easy methods to initially grasp and obtain good results, so they are becoming really popular. NN have been used intensively since couple of decades (after they revived), so they appear often in implementations.If you are interested in learning further you should look for an specific area and deal with a problem that can be solved nicely by machine learning to understand the main idea (and why is impossible to find  the method).You will find common the task to try to predict the expected behavior of something given some known or observable characteristics (to learn the function that models the problem given input data), the issues related to dealing with data in high-dimensional spaces, the need for good quality data, the notable improvements that can give data pre-processing, and many others.";;;
4888;2;2015-01-16T01:10:21.240;;"Why to use deep networks?Let's first try to solve very simple classification task. Say, you moderate a web forum which is sometimes flooded with spam messages. These messages are easily identifiable - most often they contain specific words like ""buy"", ""porn"", etc. and a URL to outer resources. You want to create filter that will alert you about such suspecious messages. It turns to be pretty easy - you get list of features (e.g. list of suspicious words and presence of a URL) and train simple logistic regression (a.k.a. perceptron), i.e. model like: g(w0 + w1*x1 + w2*x2 + ... + wnxn)where x1..xn are your features (either presence of specific word or a URL), w0..wn - learned coefficients and g() is a logistic function to make result be between 0 and 1. It's very simple classifier, but for this simple task it may give very good results, creating linear decision boundary. Assuming you used only 2 features, this boundary may look something like this: Here 2 axes represent features (e.g. number of occurrences of specific word in a message, normalized around zero), red points stay for spam and blue points - for normal messages, while black line shows separation line. But soon you notice that some good messages contain a lot of occurrences of word ""buy"", but no URLs, or extended discussion of porn detection, not actually refferring to porn movies. Linear decision boundary simply cannot handle such situations. Instead you need something like this: This new non-linear decision boundary is much more flexible, i.e. it can fit the data much closer. There are many ways to achieve this non-linearity - you can use polynomial features (e.g. x1^2) or their combination (e.g. x1*x2) or project them out to a higher dimension like in kernel methods. But in neural networks it's common to solve it by combining perceptrons or, in other words, by building multilayer perceptron. Non-linearity here comes from logistic function between layers. The more layers, the more sophisticated patterns may be covered by MLP. Single layer (perceptron) can handle simple spam detection, network with 2-3 layers can catch tricky combinations of features, and networks of 5-9 layers, used by large research labs and companies like Google, may model the whole language or detect cats on images. This is essential reason to have deep architectures - they can model more sophisticated patterns. Why deep networks are hard to train?With only one feature and linear decision boundary it's in fact enough to have only 2 training examples - one positive and one negative. With several features and/or non-linear decision boundary you need several orders more examples to cover all possible cases (e.g. you need not only find examples with word1, word2 and word3, but also with all possible their combinations). And in real life you need to deal with hundreds and thousands of features (e.g. words in a language or pixels in an image) and at least several layers to have enough non-linearity. Size of a data set, needed to fully train such networks, easily exceeds 10^30 examples, making it totally impossible to get enough data. In other words, with many features and many layers our decision function becomes too flexible to be able to learn it precisely.There are, however, ways to learn it approximately. For example, if we were working in probabilistic settings, then instead of learning frequencies of all combinations of all features we could assume that they are independent and learn only individual frequencies, reducing full and unconstrained Bayes classifier to a Naive Bayes and thus requiring much, much less data to learn. In neural networks there were several attempts to (meaningfully) reduce complexity (flexibility) of decision function. For example, convolutional networks, extensively used in image classification, assume only local connections between nearby pixels and thus try only learn combinations of pixels inside small ""windows"" (say, 16x16 pixels = 256 input neurons) as opposed to full images (say, 100x100 pixels = 10000 input neurons). Other approaches include feature engineering, i.e. searching for specific, human-discovered descriptors of input data. Manually discovered features are very promising actually. In natural language processing, for example, it's sometimes helpful to use special dictionaries (like those containing spam-specific words) or catch negation (e.g. ""not good""). And in computer vision things like SURF descriptors or Haar-like features are almost irreplaceable. But the problem with manual feature engineering is that it takes literally years to come up with good descriptors. Moreover, these features are often specific Unsupervised pretrainingBut it turns out that we can obtain good features automatically right from the data using such algorithms as autoencoders and restricted Boltzmann machines. I described them in detail in my other answer, but in short they allow to find repeated patterns in the input data and transform it into higher-level features. For example, given only row pixel values as an input, these algorithms may identify and pass higher whole edges, then from these edges construct figures and so on, until you get really high-level descriptors like variations in faces. After such (unsupervised) pretraining network is usually converted into MLP and used for normal supervised training. Note, that pretraining is done layer-wise. This significantly reduces solution space for learning algorithm (and thus number of training examples needed) as it only needs to learn parameters inside each layer without taking into account other layers. And beyond...Unsupervised pretraining have been here for some time now, but recently other algorithms were found to improve learning both - together with pretraining and without it. One notable example of such algorithms is dropout - simple technique, that randomly ""drops out"" some neurons during training, creatig some distortion and preventing networks of following data too closely. This is still a hot research topic, so I leave this to a reader. ";;;
4889;2;2015-01-16T04:44:23.923;;If you have existing properly tagged chat transcripts, you can try treating it as a supervised learning problem. If you're starting from a blank slate, that won't work. ;;;
4890;2;2015-01-16T04:54:40.550;;"The key is establishing a proper validation metric.I notice you talk about how you tried different recommendation algorithms, but at the end of the day you evaluated them all with RMSE. But there's no particular reason to believe that minimizing RMSE generates a ""subjectively better"" recommendation experience for the user - it just happens to be convenient, and happens to work well in some industries, but there is no real reason why it must.RMSE is measuring how well your recommender algorithm is predicting user behavior. But that's not the same as measuring recommendation quality. Maybe users value something else - familiarity, or serendipity, or some other quality of the item being recommended. Users don't really care about being predicted. Given your results, if you want to understand your users further, I'd focus my efforts in coming up with a mathematical metric that more closely matches the target you care about - user satisfaction - rather than RMSE. Once you know what metric you're trying to optimize, the algorithm to optimize it is much easier to select! ";;;
4891;2;2015-01-16T05:30:35.787;;Machine learning is a good example of a problem type where Spark-based solutions are light-years ahead of mapreduce-based solutions, despite the young age of spark-on-yarn. ;;;
4892;2;2015-01-16T05:44:41.907;;You can try RAKE(Rapid Automatic Keyword Extraction) and there is a python implementation here: https://github.com/aneesha/RAKE. RAKE is an document-oriented keyword extraction algorithm and also language-independent(theoretically, since RAKE use a generated stop word list to partition candidate keywords, and considering different languages, we need to find a better way to generated stop word list.). However, about English documents, RAKE can extract keywords(or tags) in a acceptable precision and recall. RAKE is also efficient, because to use it we don't have to training a whole corpus, RAKE can generate a keyword list by calculating the word's degree and frequency then comes up a score for every candidate keyword then pick the top N words.Hope this answer helps you or lead you a direction for your next step investigation.;;;
4894;2;2015-01-17T04:29:24.393;;"AdaBoost is a supervised learning method; it starts with a table of 'correct' answers and generates a predictive model for a target, which is known. It is then possible to inspect this model to figure out how it works, what it judged was more important. With that in mind, here is my guess for what he did:First, he created a training dataset. The dataset was created, according to the article, like this:  Now he’d do the same for love. First he’d need data. While his  dissertation work continued to run on the side, he set up 12 fake  OkCupid accounts and wrote a Python script to manage them. The script  would search his target demographic (heterosexual and bisexual women  between the ages of 25 and 45), visit their pages, and scrape their  profiles for every scrap of available information: ethnicity, height,  smoker or nonsmoker, astrological sign—“all that crap,” he says. To find the survey answers, he had to do a bit of extra sleuthing.  OkCupid lets users see the responses of others, but only to questions  they’ve answered themselves. McKinlay set up his bots to simply answer  each question randomly—he wasn’t using the dummy profiles to attract  any of the women, so the answers didn’t mat­ter—then scooped the  women’s answers into a database. ""So he used python scripts to collect lots of information! At the end of this, he had a table of data, where each row had three pieces of information:- A bot's answer to all the questions, and their weights- A woman's answer to all the questions- Their match percentage. On this, he could use AdaBoost to create a predictive model, predicting the match percentage from the available information. The weak learners were probably decision stumps, greedily choosing one variable at a time to split on, that's the standard that people refer to when talking about AdaBoost. Once the predictive model, was in place, it could be used for optimization - determining which weights to put on the questions when holding all other variables constant, maximizing the average match percentage to women in his target audience. Of course, this is just a guess. The article doesn't have much detail. But it's a potential way to use AdaBoost for that purpose. ";;;
4895;2;2015-01-17T04:42:47.530;;Your results are reasonable. Your data brings several ideas to mind: 1) It is quite reasonable that as you change the available features, this will change the relative performance of machine learning methods. This happens quite a lot. Which machine learning method performs best often depends on the features, so as you change the features the best method changes.2) It is reasonable that in some cases, disparate models will reach the exact same results. This is most likely in the case where the number of data points is low enough or the data is separable enough that both models reach the exact same conclusions for all test points. ;;;
4896;2;2015-01-17T05:20:32.763;;"A simple way would be to consider Laplace Smoothing (http://en.wikipedia.org/wiki/Additive_smoothing ) or something like it. Basically, instead of calculating your response rate as (Clicks)/(Impressions) you calculate (Clicks + X)/(Impressions + Y), with X and Y chosen, for example, so that X/Y is the global average of clicks/impressions. When Clicks and Impressions are both high, this smoothed response rate is basically equal to the true response rate (signal dominates the prior). When Clicks and Impressions are both low, the this smoothed response rate will be close to the global average response rate - a good guess when you have little data and don't want to put much weight on it!The absolute scale of X and Y will determine how many data points you consider ""enough data"". It's been argued that the right thing to do is set X to 1, and Y appropriately given that. ";;;
4897;2;2015-01-17T05:30:01.813;;"You may want to consider gradient boosted trees rather than random forests. They're also an ensemble tree-based method, but since this method doesn't sample dimensions, it won't run in to the problem of not having a useful predictor available to split on at any particular time. Different implementations of GBDT have different ways of handling missing values, which will make a big difference in your case; I believe R does ternary splits which is likely to work fine. ";;;
4898;2;2015-01-17T16:53:08.033;;Late answer, but here is an eclectic list of 100+ Interesting Data SetsThe blog post is fun and easy to read through (I have no affiliation). It's worth to scan through, and to scrape a few from the top:Last words of every Texas inmate executed since 1984 10,000 annotated images of cats2.2 million chess matches;;;
4899;2;2015-01-18T10:22:35.777;;I may still misunderstand what you mean, but the general simple formulation is to minimize sum of loss over all training examples. Converting to your formulation, that 'assumes' the joint distribution of the input and output is just the empirical distribution found in the data. That's the best assumption you can make without additional information. If you had reason to assume something else, you would. ;;;
4900;2;2015-01-18T19:14:41.420;;If I've understood your question correctly, one easy solution would be to concatenate the bits together and make a 7-bit binary sequence then convert it to integer. So, for your sample dataset quoted in the question you would get:CM160988667;;;
4901;1;2015-01-19T09:37:56.340;Does high error rate in regression imply the data set is unpredictable?;"I have a data set of video watching records in a 3G network. In this data set, 2 different kind of features are included: user-side information, e.g., age, gender, data plan and etc; Video watching records of these users, each of  which associated with a download ratio and some detailed network condition metrics, say, download speed, RTT, and something similar.Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytesNow, Given this data set, I want to predict the download ratio of each video.Since it is a regression problem, so I use gradient boosting regression tree as model and run 10-fold cross validation.However, I have tried different model parameter configurations and even different models (linear regression, decision regress tree), the best root-mean-square error I can get is 0.3790, which is quite high, because if I don't use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890. There is not obvious difference.For this problem, I have some questions:Does this high error rate imply that the label in data set is unpredictable? Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?";[education, open-source];66;1
4902;2;2015-01-19T10:07:11.270;;"It is always helpful to just Google the exact error that you are seeing, excluding specifics like the actual memory of the vector.  For me, the first hit was an interesting documentation called ""R: Memory limits of R"", where, under ""Unix"", one can read: The address-space limit is system-specific: 32-bit OSes imposes a limit of no more than 4Gb: it is often 3Gb. Running 32-bit executables on a 64-bit OS will have similar limits: 64-bit executables will have an essentially infinite system-specific limit (e.g., 128Tb for Linux on x86_64 cpus). See the OS/shell's help on commands such as limit or ulimit for how to impose limitations on the resources available to a single process. For example a bash user could use ulimit -t 600 -v 4000000 whereas a csh user might use limit cputime 10m limit vmemoryuse 4096m to limit a process to 10 minutes of CPU time and (around) 4Gb of virtual memory. (There are other options to set the RAM in use, but they are not generally honoured.)So, you should checkWhat type of OS are you running on your EC2 instanceWhat type of R build are you running on that OS, and make sure you run a 64bit versionIf both are already 64bit, then use ulimit to set memory to e.g. 8Gb: ulimit -v 8000000 ";;;
4903;1;2015-01-19T14:26:57.117;What is the difference between feature generation and feature extraction?;Can anybody tell me what is the purpose of feature generation? and why feature space enrichment is needed before classifying an image? Is it a necessary step?Is there any method to enrich feature space?;[education, open-source];341;3
4904;2;2015-01-19T14:55:25.580;;"Feature Generation -- This is the process of taking raw, unstructured data and defining features (i.e. variables) for potential use in your statistical analysis. For instance, in the case of text mining you may begin with a raw log of thousands of text messages (e.g. SMS, email, social network messages, etc) and generate features by removing low-value words (i.e. stopwords), using certain size blocks of words (i.e. n-grams) or applying other rules.Feature Extraction -- After generating features, it is often necessary to test transformations of the original features and select a subset of this pool of potential original and derived features for use in your model (i.e. feature extraction and selection). Testing derived values is a common step because the data may contain important information which has a non-linear pattern or relationship with your outcome, thus the importance of the data element may only be apparent in its transformed state (e.g. higher order derivatives). Using too many features can result in multiply colinearity or otherwise confound statistical models, whereas extracting the minimum number of features to suit the purpose of your analysis follows the principal of parsimony.Enhancing your feature space in this way is often a necessary step in classification of images or other data objects because the raw feature space is typically filled with an overwhelming amount of unstructured and irrelevant data that comprises what's often referred to as ""noise"" in the paradigm of a ""signal"" and ""noise"" (which is to say that some data has predictive value and other data does not). By enhancing the feature space you can better identify the important data which has predictive or other value in your analysis (i.e. the ""signal"") while removing confounding information (i.e. ""noise"").";;;
4905;2;2015-01-19T18:03:17.467;;Some notes:In the case of supervised learning you assume there is a function $f:\mathcal{X} \rightarrow \mathcal{Y}$, which means there is a connection somehow between inputs and outputs and you want to use it in order to predict. But how this function might look? Without considering the model itself, usually what happens is that there is some noise. And this noise can be in $Y$ and also can be in $X$. This noise gives use the probabilistic setup of the problem, because without it we would have only to solve some equations. Now is important to understand that the noise defines the distribution. So a random variable can be imagined as a function having something fixed and well defined and something not fixed, but taking values according with a distribution. If the variable part does not exist, that you would not have a random variable, right, it would be a simple formula. But its not. Now the $P(X)$ incorporates what happens in $X$ alone, and $P(Y)$ what is in $Y$ alone. When you predict the decision theory says that you are interested in saying which is the most probable value $y_i$ given some input value $x_i$. So you are interested in finding $P(Y|X)$.A joint probability is not always completely described by marginal probabilities. In fact, is completely described only when marginal probabilities are independent. Which means for r.v. $X, Y$ knowing $P(X)$ and $P(Y)$ does not put you in position to know the joint density $P(X, Y)$ (thought for independence you have  $P(X,Y)=P(X)P(Y)$). From here you can go directly and try to estimate $P(Y|X)$. In fact if your only interest in only in prediction, this might be a fair bet. A lot of supervised learning algorithms tries to estimate directly this probability. They are called discriminant classifiers. The reason is because they are used to classify something to  the class with maximal conditional probability, you discriminate (choose) the maximum value. Now arriving at your question. Notice the obvious $P(X,Y) = P(Y|X)P(X)$ than you see that by trying learning the joint probability, you also learn the conditional (what you need for prediction). This kind of approach is called generative, because knowing the joint probability you can not only predict, but you can generate new data for your problem. More than that, knowing the join probability can give you more insights related with how what are you model works. You can find such kind of information which is not contained only in marginal distributions. Some final notes:Technically you do not minimize the error function, but it's expectation. The error function remains as it is.$\mathcal{Z}$ is only a domain it's impossible to describe a probability only by it's domain. It's late, I hope I was not completely incoherent.;;;
4912;2;2015-01-19T21:18:54.347;;"It's a little hasty to make too many conclusions about your data based on what you presented here. At the end of the day, all the information you have right now is that ""GBT did not work well for this prediction problem and this metric"", summed up by a single RMSE comparison. This isn't very much information - it could be that this is a bad dataset for GBT and some other model would work, it could be that the label can't be predicted from these features with any model, or there could be some error in model setup/validation. I'd recommend checking the following hypotheses: 1) Maybe, with your dataset size and the features you have, GBT isn't a very high-performance model. Try something completely different - maybe just a simple linear regression! Or a random forest. Or GBDT with very different parameter settings. Or something else. This will help you diagnose whether it's an issue with choice of models or with something else; if a few very different approaches give you roughly similar results, you'll know that it's not the model choice that is causing these results, and if one of those models behaves differently, then that gives you additional information to help diagnose the issue. 2) Maybe there's some issue with model setup and validation? I would recommend doing some exploration to get some intuition as to whether the RMSE you're getting is reasonable or whether you should expect better. Your post contained very little detail about what the data actually represents, what you know about the features and labels, etc. Perhaps you know those things but didn't include them here, but if not, you should go back and try to get additional understanding of the data before continuing. Look at some random data points, plot the columns against the target, look at the histograms of your features and labels, that sort of thing. There's no substitute for looking at the data. 3) Maybe there just aren't enough data points to justify complex models. When you have low numbers of data points (< 100), a simpler parametric model built with domain expertise and knowledge of what the features are may very well outperform a nonparametric model.  ";;;
4913;1;2015-01-20T12:50:36.823;Spatial Co-location pattern in Bag of Phrases generation for image;What are the methods that are present to perform spatial co-location pattern mining in case of generating Bag of visual phrases in image classification? On what basis the threshold and window size for finding the frequently co-located pairs are determined is there any pre defined way to do that?Thanks in advance;[education, open-source];20;
4914;1;2015-01-20T15:27:38.160;When to use what - Machine Learning;"Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you: is there any comprehensive framework matching tasks with approaches or methods related to different types of machine learning related problems?How do I learn a simple Gaussian? Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.How do I learn a mixture of Gaussians (MoG)? Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)How do I learn any density? Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l  ́ 2 error; Kernel density estimation (KDE), optimal kernel, KDE theoryHow do I predict a continuous variable (regression)? Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.How do I predict a discrete variable (classification)? Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theoryWhich loss function should I use? Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentismWhich model should I use? AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived boundsHow can I learn fancier (combined) models? Ensemble learning theory; boosting; bagging; stackingHow can I learn fancier (nonlinear) models? Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regressionHow can I learn fancier (compositional) models? Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammarsHow do I reduce or relate features? Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learningHow do I create new features? principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learningHow do I reduce or relate the data? Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational dataHow do I treat time series? ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time seriesHow do I treat non-ideal data? covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustnessHow do I optimize the parameters? Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EMHow do I optimize linear functions? computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reductionHow do I optimize with constraints? Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVMHow do I evaluate deeply-nested sums? Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagationHow do I evaluate large sums and searches? Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVDHow do I treat even larger problems? Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learningHow do I apply all this in the real world? Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are";[education, open-source];451;17
4915;2;2015-01-21T03:06:39.243;;"That's a good list covering a lot.  I've used some of these methods since before anything was called machine learning, and I think you will see some of the methods you list coming in and out of use over time.  If a method has been out of favour for too long, it might be time for a revisit.  Some methods can obfuscate behind different names resulting from different fields of study.  One of the main areas I have used these methods is in mineral potential modelling, which is geospatial and to support that you could add some additional categories relating to spatial and oriented data methods.Taking your broad question to specific fields will probably be where you find more examples of methods not in your comprehensive list.  For example, two methods I've seen in mineral potential have been backward stepwise regression and weights of evidence modelling.  I'm not a statistician; perhaps these would be considered covered in the list under linear regression and Bayesian methods.";;;
4916;2;2015-01-21T07:50:53.953;;"I agree with @geogaffer. This is a very good list, indeed. However, I see some issues with this list as it is currently formulated. For example, one issue is that suggested solutions are of different granularity levels - some of them represent approaches, some - methods, some - algorithms, and some other - just concepts (in other words, terms within a topic's domain terminology). In addition, - and I believe that this is much more important than the above - I think that it would be much valuable, if all those solutions in the list were arranged within a unified thematic statistical framework. This idea was inspired by reading an excellent book by Lisa Harlow ""The essence of multivariate thinking"". Hence, recently I've initiated a corresponding, albeit currently somewhat limited, discussion on the StackExchange's Cross Validated site. Don't let the title confuse you - my implied intention and hope is for building a unified framework, as mentioned above.";;;
4917;1;2015-01-21T09:15:54.750;Visualize performance, % of goal implementation;"I want to visualize goal achievment progress.This is my first idea:use area chart to show progress in current metricuse horizontal band to show the goal valuecolorize areas under/above the band into ""positive"" and ""negative"" colorsIs this approach informative enough? Are there better choises?Additional info:charts made in Tableautwo data sources: metric progress & goals";[education, open-source];57;
4918;1;2015-01-21T13:30:45.307;Optimization using existing GLM. When predicting new output, need to switch input vars for models;Firstly I'm creating a model for dependent variable y in [R] (e.g. via lm or glm) on a training set dataframe based on historic input variables (the x's).I then want to be able to predict future y using this model. The complexity I have is that I have additional models for some of these input variables that I used in training (in training they were static values), and these input vars can have dependency on an external variable z.I want to run through an optimization routine to minimize y with respect to only changing variable z. I'm looking to build a solution that scales well with complexity of the given functions (e.g. with hundreds of input variables in a glm). The predict function scales (esp. in terms of code complexity) brilliantly with complexity on static datasets - but I don't think I can tap into this in this instance.Bonus if this can support other machine learning models rather than just regression based models.;[education, open-source];26;
4919;1;2015-01-21T13:48:26.363;Spatial Co-location: Choosing optimal window size;May I know is there any method to automatically choose the window size to identify spatial co-location pairs that appear frequently. And how to choose the optimal threshold automatically? ;[education, open-source];23;
4920;2;2015-01-21T14:17:17.970;;"I think your approach is a little backwards.""What is the mean of a Gaussian distribution fitted to this data?"" is never the problem statement, so ""how do I fit a Gaussian?"" is never the problem you actually want to solve.The difference is more than semantic. Consider the question of ""how do I construct new features?"" If your goal is to develop an index, you might use some type of factor analysis. If your goal is to simply reduce the feature space before fitting a linear model, you might skip the step entirely and use elastic net regression instead.A better approach would be to compile a list of actual data analysis tasks you would like to be able to tackle. Questions like: How do I predict whether customers will return to my shopping website? How do I learn how many ""major"" consumer shopping patterns there are, and what are they? How do I construct an index of ""volatility"" for different items in my online store?Also your list right now includes an enormous amount of material; far too much to ""review"" and gain more than a surface-level understanding. Having an actual purpose in mind can help you sort out your priorities.";;;
4921;1;2015-01-22T00:52:51.710;"Could someone please offer me some guidance on some kind of particular, SPECIFIC project that I could attemp, to ""get my feet wet, so to speak""";"I am COMPLETELY new to the field of Data Science, mainly because every employer I have worked for, simply COULDN'T sell any customers anything that would use techniques learned in this field.Of particular interest to me is machine learning/Predictive Analysis.I have attempted many ""test projects"" myself, but I seem to NEED some sort of outside ""catalyst"" to tell me a specific goal, and a specific set of guidelines, when I am trying to learn something.Otherwise, I tend to lose focus, and jump from one interesting topic to the next, without ever gaining any experience.Thank you!!";[education, open-source];68;
4922;2;2015-01-22T10:44:37.240;;I would suggest Kaggle learning projects - http://www.kaggle.com/competitionsLook for the ones in the 101 section that offer knowledge. There's many pre-made solutions ready, which you can ingest and try variations of.Also, I have bookmarked a Comprehensive learning path – Data Science in Python, which among other things gives a few answers to your specific question.;;;
4923;1;2015-01-22T13:36:24.967;Bag of Words creation in image;In SIFT feature extraction how the key points will be generated and how the features will be stored in the database. In image will the bag of visual words be images or text words?;[education, open-source];62;
4924;2;2015-01-22T17:25:40.690;;You can use some public data sets to play around with. The iris flower data set is great for classification problems. As an example you can try to classify the flower species with k-nearest neighbor model or a decision tree (CART) model.  You can find this and other sample machine learning datasets here: http://archive.ics.uci.edu/ml/datasets.htmlThis list identifies what type of machine learning tasks can be performed with each dataset. ;;;
4925;1;2015-01-22T21:34:57.443;VM image for data science projects;As there are numerous tools available for data science tasks, and it's cumbersome to install everything and build up a perfect system. Is there a linux/Mac OS image with Python, R and other open-source data science tools installed available for people to use? Ideally an Ubuntu or a light weight OS with latest version of python , R (IDEs too), and other opensource data visualization tools installed will be ideal.  I haven't come across one in my quick search on google. Please let me know if there are any or if someone of you have created one for yourself? I assume some universities might have their own VM images. Please share the links. I thank everyone of you out there for your active support in this regard. ;[education, open-source];562;2
4926;2;2015-01-23T02:08:59.950;;There is another choice which popular recently: docker(https://www.docker.com). Docker is a container and let you create/maintain a working environment very easily and fast. install essential tools for data science in python https://registry.hub.docker.com/u/ceshine/python-datascience/use r language to do data sciencehttps://github.com/rocker-org/rockerHope that would help you.;;;
4927;2;2015-01-23T02:26:23.773;;While Docker images are now more trendy, I personally find Docker technology not user-friendly, even for advanced users. If you are OK with using non-local VM images and can use Amazon Web Services (AWS) EC2, consider R-focused images for data science projects, pre-built by Louis Aslett. The images contain very recent, if not the latest, versions of Ubuntu LTS, R and RStudio Server. You can access them here.Besides main components I've listed above, the images contain many useful data science tools built-in as well. For example, the images support LaTeX, ODBC, OpenGL, Git, optimized numeric libraries and more.;;;
4928;2;2015-01-23T06:10:37.417;;How to calculate key points:Take your image $I(x,y)$ and convolve it using a Gaussian $G(x,y,k\sigma$) at different scales $k \sigma$ to obtain $L(x,y,k\sigma)$. This produces several versions (one for each scale $k\sigma$) of your image $I(x,y)$ with different degrees of blurring.Separate your blurred images according to octaves (an octave is usually taken as $2\sigma$). Within a given octave, take the difference between adjacent blurred images $L(x,y,k_{i}\sigma)$ and  $L(x,y,k_{j}\sigma)$. This difference is called a Difference of Gaussians (DoG) $D(x,y,\sigma) = L(x,y,k_{i}\sigma) - L(x,y,k_{j}\sigma)$. At this point, an image will be helpful (source: opencv):As you can see, you have the blurred/convolved/filtered images on the left and the differences between adjacent images on the right.Then, take one pixel on a DoG and compare it to its 8 neighbors in the same DoG, as well as to the 9 equivalent pixels of the DoG located in the next scale  and to the other 9 pixels in the previous DoG. In the drawing, this corresponds to the DoG's located above and below. If this pixel is a local extrema, then you have a candidate keypoint. Keep in mind that candidates can be discarded based on other criteria.Once you have these keypoints, you proceed to generate descriptors. For each keypoint, calculate an image gradient. As you should know, gradients tell you the direction of maximum rate of change. Therefore, you can construct a grid around each keypoint that is oriented according to the dominant gradient around that point. This grid has subregions (usually 16 subregions) and for each subregion, calculate an 8-bin histogram. Finally, concatenate every histogram you obtained from every subregion in your grid and that is the feature vector for that keypoint (actually, the full feature vector also includes the location and rotation angle.) A helpful illustration (from Solem's book Programming Computer Vision with Python):At this point, you have a feature vector with 132 values for each keypoint. As for your second question, I'm not sure about the best approach to store an array in a database. Maybe others can expand on this point. Of course, there are several options:Create an array datatype to store your dataSimply use a VARCHAR fieldStore it as a binary fileUse a database specifically designed to handle arrays.;;;
4929;2;2015-01-23T10:22:36.327;;If you are looking for a VM with a bunch of tools preinstalled, try the Data Science Toolbox.;;;
4930;2;2015-01-23T14:00:52.367;;Did you try Cloudera's QuickStart VM?:http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.htmlI found it very easy to run it and it includes open source software such as Mahout and Spark. ;;;
4931;2;2015-01-23T15:34:07.570;;Today i used this repository from https://github.com/sequenceiq/docker-spark and build it with docker. it is a docker image building spark based on hadoop image of the same owner. if you to use spark, it has a python api called pyspark http://spark.apache.org/docs/latest/api/python/;;;
4934;2;2015-01-23T23:16:54.220;;What information should be shown on the charts are based on what the viewer wants. From your sample charts, the x axis indicates 'date'. For time serial analysis, people usually want to know the trending of the metrics and to predict the performance in the future. Here are 2 suggestions:People may want to know how it will perform in the future. Add a trendline. Trendline could help people to predict the performance in the future.http://onlinehelp.tableausoftware.com/current/pro/online/mac/en-us/trendlines.htmlPeople may want to know how far away from goal. When people mouseover on the points on the metric line, you can try to show how far from current metric to the goal.Try to understand what information people want to know and visualise them. Hope my answer could help you.;;;
4935;1;2015-01-24T15:15:27.777;"Questions on ""Active Archive""";"In the big data world there is lot of talk about implementing an ""active archive"". I see cloudera talk about it a lot. The value prop is that you move the low value (and less used) data from EDW to Hadoop and then save on expensive EDW storage by using Hadoop.So in my company say we keep 10 years of data on EDW and say we don't use anything below 2 years very actively. So I move 8 years of data from EDW to Hadoop.I can setup an Impala (or equivalent) product to query the 8 years of data as well.But the problem is how do you order, sort a query which requires some data form EDW and some data from Impala?because this kind of grouping, sorting ordering etc will have to be done in memory and the apps which queried the EDW were not written for operations and don't have the capacity as well to sort, group and process so much of data.So how are people implementing the Active Archive?";[education, open-source];30;
4936;1;2015-01-24T15:52:28.727;What happens when we train a linear SVM on non-linearly separable data?;What happens when we train a basic support vector machine (linear kernel and no soft-margin) on non-linearly separable data? The optimisation problem is not feasible, so what does the minimisation algorithm return?;[education, open-source];114;
4937;2;2015-01-24T16:51:33.300;;I think the idea is that you have all data in HDFS, and query it with Impala, and also keep some small amount of data in your data warehouse. That is, keep all 10 years in Hadoop, and also 2 years in an EDW. The cost of also having the 2 years in Hadoop is small.;;;
4938;2;2015-01-25T01:29:53.470;;"About your question, I think basic support vector machine means hard-margin SVM. At first, let's review what is hard-margin SVM.What is hard-margin SVM: In shortly, we want to find a hyperplane with the largest margin which be able to separate all observations correctly in our training sample space. The optimisation problem in hard-margin SVM: Now, let's look at the above definition again and what is the optimisation problem which we need to solve. the largest margin hyperplane: We want max(margin)be able to separate all observations correctly: We need to optimise margin and also satisfy the constrain: No in-sample errorsBack to your question, since you mentioned the training data set is non-linearly separable, by using hard-margin SVM without feature transformations, it's impossible to find any hyperplane which satisfy ""No in-sample errors"". Normally, we solve SVM optimisation problem by Quadratic Programming, because it can do optimisation tasks with constrains. If you use Gradient Descent or other optimisation algorithms which without satisfying the constrains of hard-margin SVM, you should still get a result, but that's not a hard-margin SVM hyperplane.By the way, with non-linearly separable data, usually we choose hard-margin SVM + feature transformationsuse soft-margin SVM directly(In practical, soft-margin SVM usually get good results)Thank you for your question, and hope I do answer your question.";;;
4940;1;2015-01-25T14:30:31.187;Cross Domain correlated graph- Comparison of visual words/phrases with text words;How to correlate the visual words/phrases and text in cross domain correlated graph? How to construct the cross domain correlated graph?Reference:[Web multimedia object classification using cross domain correlation knowledge][1] [1]: http://users.cis.fiu.edu/~taoli/pub/Lu-06589200-TMM.pdf  pg: 1924;[education, open-source];29;
4941;2;2015-01-25T14:43:21.033;;Neo4j and Spark GraphX are meant for solving problem at different level and they are complimentary to each other.They can be connected by Neo4j's Mazerunner extension: Mazerunner is a Neo4j unmanaged extension and distributed graph  processing platform that extends Neo4j to do big data graph processing  jobs while persisting the results back to Neo4j. Mazerunner uses a message broker to distribute graph processing jobs  to Apache Spark's GraphX module. When an agent job is dispatched, a  subgraph is exported from Neo4j and written to Apache Hadoop HDFS. After Neo4j exports a subgraph to HDFS, a separate Mazerunner service  for Spark is notified to begin processing that data. The Mazerunner  service will then start a distributed graph processing algorithm using  Scala and Spark's GraphX module. The GraphX algorithm is serialized  and dispatched to Apache Spark for processing. Once the Apache Spark job completes, the results are written back to  HDFS as a Key-Value list of property updates to be applied back to  Neo4j. Neo4j is then notified that a property update list is available from  Apache Spark on HDFS. Neo4j batch imports the results and applies the  updates back to the original graph.Check out this tutorial to get an idea on how to combine the two:http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html;;;
4942;1;2015-01-25T22:52:23.437;High-dimensional data: What are useful techniques to know?;Due to various curses of dimensionality, the accuracy and speed of many of the common predictive techniques degrade on high dimensional data. What are some of the most useful techniques/tricks/heuristics that help deal with high-dimensional data effectively? For example,Do certain statistical/modeling methods perform well on high-dimensional datasets?Can we improve the performance of our predictive models on high-dimensional data by using certain (that define alternative notions of distance) or kernels (that define alternative notions of dot product)?What are the most useful techniques of dimensionality reduction for high-dimensional data?;[education, open-source];259;4
4943;1;2015-01-25T23:03:02.297;Intuition for the regularization parameter in SVM;How does varying the regularization parameter in an SVM change the decision boundary for a non-separable dataset? A visual answer and/or some commentary on the limiting behaviors (for large and small regularization) would be very helpful.;[education, open-source];98;
4944;1;2015-01-25T23:09:22.863;How to learn a classifier from a dataset with high imbalance;"What are the most useful techniques for learning a binary classifier from a dataset with a high degree of imbalance (i.e., a dataset with the ""target"" class being much rarer than the ""background"" class)? For example,Should one first down-sample the majority/background class to reduce its frequency and then readjust the probabilities reported by the learning algorithm? How should one do the readjustment?Should one use different approaches for different learning algorithms, i.e., are there different techniques for dealing with imbalance in SVM, random forests, logistic regression, etc.?";[education, open-source];113;
4945;2;2015-01-26T01:37:05.640;;"The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications. However, for non-separable problems, in order to find a solution, the miss-classification constraint must be relaxed, and this is done by setting the mentioned ""regularization"".So, intuitively, as lambda grows larger they less the wrongly classified examples are allowed (or the highest the price the pay in the loss function). Then when lambda tends to infinite the solution tends to the hard-margin (allow no miss-classification). When lambda tends to 0 (without being 0) the more the miss-classifications are allowed.There is definitely a tradeoff between this two and normally smaller lambdas but not too small use to generalize well. Below three examples for linear SVM classification (binary).For non-linear-kernel SVM the idea is the similar.Given this, for higher values of lambda there is a higher possibility of overfitting, while for lower values of lambda there is higher possibilities of underfitting. The images below show the behavior for RBF Kernel, letting the sigma parameter fixed on 1 and trying lambda = 0.01 and lambda = 10You can say the first figure where lambda is lower is more ""relaxed"" than the second figure where data is intended to be fitted more precisely.(Slides from Prof. Oriol Pujol. Universitat de Barcelona)";;;
4946;2;2015-01-26T08:00:14.630;;"This is very broad question, which I think it's impossible to cover comprehensively in a single answer. Therefore, I think that it would be more beneficial to provide some pointers to relevant answers and/or resources. This is exactly what I will do by providing the following information and thoughts of mine.First of all, I should mention the excellent and comprehensive tutorial on dimensionality reduction by Burges (2009) from Microsoft Research. He touches on high-dimensional aspects of data frequently throughout the monograph. This work, referring to dimensionality reduction as dimension reduction, presents a theoretical introduction into the problem, suggests a taxonomy of dimensionality reduction methods, consisting of projective methods and manifold modeling methods, as well as provides an overview of multiple methods in each category.The ""projective pursuit"" methods reviewed include independent component analysis (ICA), principal component analysis (PCA) and its variations, such as kernel PCA and probabilistic PCA, canonical correlation analysis (CCA) and its kernel CCA variation, linear discriminant analysis (LDA), kernel dimension reduction (KDR) and some others. The manifold methods reviewed include multidimensional scaling (MDS) and its landmark MDS variation, Isomap, Locally Linear Embedding and graphical methods, such as Laplacian eigenmaps and spectral clustering. I'm listing the most of the reviewed methods here in case, if the original publication is inaccessible for you, either online (link above), or offline (References).There is a caveat for the term ""comprehensive"" that I've applied to the above-mentioned work. While it is indeed rather comprehensive, this is relative, as some of the approaches to dimensionality reduction are not discussed in the monograph, in particular, the ones, focused on unobservable (latent) variables. Some of them are mentioned, though, with references to another source - a book on dimensionality reduction.Now, I will briefly cover several narrower aspects of the topic in question by referring to my relevant or related answers. In regard to nearest neighbors (NN)-type approaches to high-dimensional data, please see my answers here (I especially recommend to check the paper #4 in my list). One of the effects of the curse of dimensionality is that high-dimensional data is frequently sparse. Considering this fact, I believe that my relevant answers here and here on regression and PCA for sparse and high-dimensional data might be helpful.ReferencesBurges, C. J. C. (2010). Dimension reduction: A guided tour. Foundations and Trends® in Machine Learning, 2(4), 275-365. doi:10.1561/2200000002";;;
4947;2;2015-01-26T09:39:41.527;;A common strategy for dealing with imbalance is to penalize harder the missclassifications that select the class with higher frequency. In a binary classification problem you could penalize by dividing 1/n where n is the number of examples of the opposite class.See the following from Prof. Jordi VitriáThis is the loss function for structured output SVM. The problem you mention is common in object recognition and object classification in images where much more background images are used than images containing the object. A stronger case happens with exemplar SVM's where just a single image of the object is used.;;;
4948;2;2015-01-26T14:45:43.053;;Some good answers have already been posted at this site:Quick guide into training highly imbalanced data setsAnd on Stats SE:http://stats.stackexchange.com/questions/81111/classification-problem-using-imbalanced-datasethttp://stats.stackexchange.com/questions/16050/how-to-handle-data-imbalance-in-classificationhttp://stats.stackexchange.com/questions/60180/testing-classification-on-oversampled-imbalance-data?rq=1;;;
4949;1;2015-01-26T15:45:51.317;Where did this NY Times op-ed get his Google Search data?;"I hope this is a question appropriate for SO.The article in question: http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.htmlAs far as I can tell, the only publicly available data from Google Search is through their Trends API.  The help page states that The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don't represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100. However in the article, the author reports (absolute) ""average monthly searches"".  The source is stated as: All monthly search numbers are approximate and derived from anonymous and aggregate web activity. Source: analysis of Google data by (author)So, how did he get this ""anonymous and aggregate web activity""?";[education, open-source];124;
4950;1;2015-01-26T17:45:02.203;semi-structured text parsing using machine learning;I am looking for a method to parse semi-structured textual data, i.e. data poorly formatted but usually having a visual structure of a matrix which may vary a lot in content and number of items in it, which may have headers or not, which may be interpreted sometimes column-wise or row-wise, and so on.I have read about the WHISK information extraction paper : https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdfbut unfortunately, it is not very detailed and I have not been able to find a real-system implementing it, or even snippets of code.Has anybody have an idea where I can find such help? Or suggest an alternative approach which may be suited to my problem?Thank you in advance for your reply!;[education, open-source];89;1
4951;1;2015-01-26T18:20:58.203;Can distribution values of a target variable be used as features in cross-validation?;"I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:The author built a model for each gesture of each player to guess which gesture would be played next. Calculating over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable for cross-fold validation.Is that legitimate? That seems like cheating. I would think you would have to exclude the target variables from your test set when calculating features in order to not ""cheat"".";[education, open-source];41;
4952;2;2015-01-26T19:24:51.783;;"There is nothing necessarily wrong with this.  If you have no better information, then using past performance (i.e., prior probabilities) can work pretty well, particularly when your classes are very unevenly distributed.Example methods using class priors are Gaussian Maximum Likelihood classification and Naïve Bayes.[UPDATE]Since you've added additional details to the question...Suppose you are doing 10-fold cross-validation (holding out 10% of the data for validating each of the 10 subsets). If you use the entire data set to establish the priors (including the 10% of validation data), then yes, it is ""cheating"" since each of the 10 subset models uses information from the corresponding validation set (i.e., it is not truly a blind test). However, if the priors are recomputed for each fold using only the 90% of data used for that fold, then it is a ""fair"" validation.An example of the effect of this ""cheating"" is if you have a single, extreme outlier in your data. Normally, with k-fold cross-validation, there would be one fold where the outlier is in the validation data and not the training data. When applying the corresponding classifier to the outlier during validation, it would likely perform poorly. However, if the training data for that fold included global statistics (from the entire data set), then the outlier would influence the statistics (priors) for that fold, potentially resulting in artificially favorable performance.";;;
4953;2;2015-01-26T21:09:16.850;;I would also suggest you to try an idea of 'Anomaly detection' using Gaussian distribution. In some cases it works really good - especially if you have a VERY skewed classes (say, among a million of examples only 10-20 are '1' (in class) and all the rest a 0's).  You may look up it in this video by prof. Andrew Ng.http://www.youtube.com/watch?v=h5iVXB9mczoOr in text:http://www.holehouse.org/mlclass/15_Anomaly_Detection.htmlNotice, that this is not a classification problem, it is not using a classification algorithm.;;;
4954;2;2015-01-27T01:37:13.737;;Google AdWords.  That has absolute search volumes. ;;;
4955;1;2015-01-27T10:31:10.233;How to extract features and classify alert emails coming from monitoring tools into proper category?;"My company provides managed services to a lot of its clients. Our customers typically uses following monitoring tools to monitor their servers/webapps:OpsViewNagiosPingdomCustom shell scriptsWhenever any issue is found, an alert mail comes to our Ops team so that they act upon rectifying the issue.As we manage thousands of servers, our Ops teams' inbox is flooded with email alerts all the time. Even a single issue which has a cascading effect, can trigger 20-30 emails.Now, what I want to do is to implement a system which will be able to extract important features out of an alert email - like server IP address, type of problem, severity of problem etc. and also classify the emails into proper category, like CPU-Load-Customer1-Server2, MySQL-Replication-Customer2-DBServer3 etc. We will then have a pre-defined set of debugging steps for each category, in order to help the Ops team to rectify the problem faster. Also, the feature extractor will provide input data to the team for a problem.So far I have been able to train NaiveBayesClassifier with supervised learning techniques i.e. labeled training data(cluster data), and able to classify new unseen emails into its proper cluster/category. As the emails are based on certain templates, the accuracy of the classifier is very high. But we also get alert emails from custom scripts, which may not follow the templates. So, instead of doing supervised learning, I want to try out unsupervised learning for the same. I am looking into KMeans clustering. But again the problem is, we won't know the number of clusters beforehand. So, which algorithm will be best for this use case? Right now I am using Python's TextBlob library for classification.Also, for feature extraction out of an alert email, I am looking into NLTK (http://www.nltk.org/book/ch07.html) library. I tried it out, but it seems to work on proper English paragraphs/texts well, however, for alert emails, it extracted a lot of unnecessary features. Is there already any existing solution for the same? If not, what will be the best way to implement the same? Which library, which algorithm?PS: I am not a Data Scientist.Sample emails: PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.10.0.100  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: CRITICAL  Date & Time: Sat Oct 4 07:02:06 UTC 2014    Additional Information:     CRITICAL - load average: 41.46, 40.69, 37.91RECOVERY: OK - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.1.1.100  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: OK  Date & Time: Sat Oct 4 07:52:05 UTC 2014    Additional Information:     OK - load average: 0.36, 0.23, 4.83PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.100.10.10  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: CRITICAL  Date & Time: Sat Oct 4 09:29:05 UTC 2014    Additional Information:     CRITICAL - load average: 29.59, 26.50, 18.49Classifier code:(format of csv - email, <disk/cpu/memory/mysql>) from textblob import TextBlobfrom textblob.classifiers import NaiveBayesClassifierimport csvtrain = []with open('cpu.txt', 'r') as csvfile:    reader = csv.reader(csvfile, delimiter=',', quotechar='""')    for row in reader:        tup = unicode(row[0], ""ISO-8859-1""), row[1]        train.append(tup)// this can be done in a loop, but for the time being let it bewith open('memory.txt', 'r') as csvfile:    reader = csv.reader(csvfile, delimiter=',', quotechar='""')    for row in reader:        tup = unicode(row[0], ""ISO-8859-1""), row[1]        train.append(tup)with open('disk.txt', 'r') as csvfile:    reader = csv.reader(csvfile, delimiter=',', quotechar='""')    for row in reader:        tup = unicode(row[0], ""ISO-8859-1""), row[1]        train.append(tup)with open('mysql.txt', 'r') as csvfile:    reader = csv.reader(csvfile, delimiter=',', quotechar='""')    for row in reader:        tup = unicode(row[0], ""ISO-8859-1""), row[1]        train.append(tup)cl = NaiveBayesClassifier(train)cl.classify(email)Feature extractor code taken from: https://gist.github.com/shlomibabluki/5539628Please let me know if any more information is required here.Thanks in advance.";[education, open-source];253;
4956;2;2015-01-27T11:15:04.250;;To add to a possibly never ending list:as mentioned by cyndd, there is Wikidata,and for curated structured knowledge, Wolfram Alpha.;;;
4957;1;2015-01-27T15:13:09.157;Machine learning toolkit for Excel;Do you know of any machine learning add-ins that I could use within Excel? For example I would like to be able to select a range of data and use that for training purposes and then use another sheet for getting the results of different learning algorithms.;[education, open-source];715;1
4958;2;2015-01-27T22:01:26.237;;Recently I am working at a similar analysis. I wrote some functions to test any possible combinations between variables, however it is specifically used for my own data set which definitely is different from your one.This is a fairly small job so I can not say any package dealing with such tests. And you have already worked out some combinations. Just keep going for a ideal function, maybe will be done in a couple of days.I add a link here, which partially answers your question and code is included: http://stats.stackexchange.com/questions/4040/r-compute-correlation-by-group ;;;
4959;2;2015-01-28T03:55:58.367;;"The idea you have in mind is called ""feature selection"" or ""attribute selection"". The fact that you have a categorical dependent variable and continuous independent variables is mostly irrelevant because you're expected to use an algorithm or statistical method that is suitable for your requirements.As for feature selection methods, there are several options:Find the subset of features that achieves better performance (usually in cross validation)Find the subset of features that correlates highly with the target variable and low with each other (although other criteria can be used)Use an algorithm that includes a built-in feature selection mechanism (e.g. decision trees, hierarchical bayesian methods)Furthermore, there are several methods aimed at obtaining a good compromise between a thorough search and a reasonable time execution (e.g. best first, steepest ascent search, etc)This question in particular provides very good suggestions for R packages. ";;;
4960;2;2015-01-28T04:34:30.777;;"Weka can import CSV files, and allows you to choose which columns and rows you want to use in your analysis.  It's not an ""add-in"" for Excel per-se, but it might work for you.";;;
4961;2;2015-01-28T04:42:19.780;;First of all, let me tell you that Excel shouldn't be used for machine learning or any data analysis complicated enough that you wouldn't be comfortable doing it on paper. Why? Here is a list of resources to tell you why:You shouldn’t use a spreadsheet for important work (I mean it)Destroy Your Data Using Excel With This One Weird Trick!Using Excel for Statistical Data Analysis - CaveatsProblems with ExcelSpreadsheet AddictionNow, if you really really want to do heavy calculations without exporting your data, I suggest using xlwings. Basically, this allows two-way communication between Excel and Python. Watch the video in the homepage for a quick introduction. In this way, you would be able to use numpy, pandas and scikit-learn (or other machine learning library that you may prefer) without exporting your data first.;;;
4962;2;2015-01-28T05:53:28.083;;"Nobody does serious machine learning in Excel; that's not what it's for. Fortunately, you can directly import Excel files into better platforms like python. In particular, there's a great package called pandas, which makes work very pleasant. Here's a demo.";;;
4963;1;2015-01-28T05:57:56.090;What is advantage of using Dryad istead of Spark?;I found that Apache-Spark very powerful in Big-Data processing. but I want to know about Dryad (Microsoft) benefits. Is there any advantage for this framework than Spark?Why we must use Dryad instead of Spark?Thanks;[education, open-source];113;2
4964;2;2015-01-28T06:41:28.650;;Dryad is an academic project, whereas Spark is widely deployed in production, and now has a company behind it for support. Just focus on Spark.;;;
4965;2;2015-01-28T09:24:14.097;;"I would suggest to consider using latent variable modeling (LVM) or similar structural equation modeling (SEM) as an approach to this problem. Using this approach is based on recognizing and analyzing latent variables - constructs (factors), measured not directly, but through sets of measured variables (indicators). Note that a closely related term latent feature is frequently used within the machine learning domain. It seems to me that latent variables resemble what you call ""combinations/subsets/segments of the IVs"".By hypothesizing - usually, based on theory or domain knowledge - the latent structure of factors, LVM or SEM are able to automatically confirm or decline those hypotheses. This is done by using a combination of exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) (see my answer ). While EFA is frequently performed independently (and maybe that's enough for your purposes), doing it along with CFA represents a large part of LVM/SEM methodology, which is usually completed by performing path analysis, which is concerned about relationships between latent variables.The R ecosystem offers a variety of packages for performing LVM/SEM in its entirety or for performing EFA, CFA and path analysis. The most popular ones for EFA are psych, GPArotation and Hmisc. The most popular packages for CFA, path analysis and LVM are sem (the first R package for SEM), lavaan, OpenMx, semPLS, plspm. Various supplementary SEM-focused packages are also available.";;;
4966;2;2015-01-28T10:33:38.880;;As far as I know, currently there not that many projects and products that allow to perform serious machine learning (ML) work from within Excel. However, the situation seems to be changing rapidly due to active Microsoft's efforts in popularizing its ML cloud platform Azure ML (along with ML Studio). The recent acquisition of R-focused company Revolution Analytics by Microsoft (which appears to me as more of acqui-hiring to a large extent) is an example of the company's aggressive data science market strategy.In regard to ML toolkits for Excel, as a confirmation that we should expect most Excel-enabled ML projects and products to be Azure ML-focused, consider the following two projects (the latter is an open source):Excel DataScope (Microsoft Research): http://research.microsoft.com/en-us/projects/exceldatascopeAzure ML Excel Add-In (seems to be Microsoft sponsored): https://azuremlexcel.codeplex.com;;;
4967;1;2015-01-28T13:42:15.430;Quasi-categorical variables - any ideas?;"Let's say I'm trying to predict a person's electricity consumption, using the time of day as a predictor (hours 00-23), and further assume I have a hefty but finite amount of historical measurements.Now, I'm trying to set up a linear model akin to power_used = hr_of_day * alpha_hr + temperature * betaProblem: using the hr_of_day as a numerical value is a very bad idea for many reasons, the fact that 23 and 0 are actually quite close values is one problem that can be solved with a simple transformation [1]. The fact that electrical consumption is often bi-modal is another problem which isn't solved by a simple transformation. A possible solution that works rather well is to treat the time of day as a categorical variable. That does the trick, but it suffers from a significant drawback in that there's no information sharing between neighbouring hours.So what I'm asking is this: does anyone know of a ""soft"" version of categorical values? I'm suggesting something quite loosely defined: Ideally I would have some parameter alpha that reduces the regression to numerical regression where alpha = 1 and reduces to categorical regression where alpha = 0, and behaves ""in between"" if it's some other number. Right now the only answer I can think of is to alter the weights in the regression in such a way that they tend towards zero the further away the quasi-categorical value is from the desired value. Surely there are other approaches?[1] introduce the hour variable as two new variables: Cos(time_of_day/24) and Sin(time_of_day/24)";[education, open-source];78;2
4968;1;2015-01-28T17:40:32.450;What are the differences between Apache Spark and Apache Flink?;Both Apache-Spark and Apache-Flink projects claim pretty much similar capabilities.what is the difference between these projects. Is there any advantage in either Spark or Flink?Thanks;[education, open-source];1917;
4969;2;2015-01-28T18:42:06.763;;Current size limit for Amazon Redshift is 128 nodes or 2 PBs of compressed data. Might be circa 6PB uncompressed though mileage varies for compression. You can always let us know if you need more. anurag@aws (I run Amazon Redshift and Amazon EMR);;;
4970;2;2015-01-28T19:02:15.203;;If you really want to use an Excel add-in, the XLMiner package is a solid option, although expensive:http://www.solver.com/xlminer-data-mining;;;
4971;1;2015-01-28T19:24:07.543;How to access maximum volume of tweets using Twitter Streaming API, without firehose access?;"Twitter is a popular source of data for many applications, especially involving sentiment analysis and the like.  I have some things I'm interested in doing with Twitter data, but here's the issue:  To get all Tweets, you have to get special permission from Twitter (which, as I understand it, is never granted) or pay big bucks to Gnip or the like.OTOH, Twitter's API documentation says:Few applications require this level of access. Creative use of a combination of other resources and various access levels can satisfy nearly every application use case.Using the filter api with keyword tracking seems like something that would be a big part of this, but you obviously can't enumerate every keyword.  Using a User stream on many User accounts that follow a lot of people might be an option as well, and I'm not sure if it makes sense to think about using the search API in addition.  So here's the question ""What combination of other resources and access levels is the best way to get the maximum amount of data from Twitter""?";[education, open-source];138;
4972;2;2015-01-28T21:56:23.980;;I would suggest you to use the idea of so-called 'fuzzy clustering', where you put each of your hours of the day value into several clusters at the same time. Details in paper: http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.htmlThe idea is trivial:You decide how many clusters you want to have. For example, 4 (so you divide your day hours into 4 cathegories). Instead of computing just 1 number (which defines cluster membership) for each of your day hours you compute 4 numbers which represent the degree of membership to each of 4 clusters. So for example if you 4 clusters will contain periods 12 AM-6 AM, 6 AM- 12 PM, 12 PM - 6 PM and 6 PM - 12 AM then you would replace for example 4 AM hour in original data with vector of 4 numbers, first one is the biggest, second is smaller, third one is the smallest one etc.Then you could use these 4 numbers in your model to fit a regression line.Of course, if you want you could use 24 clusters and in such case each your day of hour would have a high 'relation' with nearby hours and almost 0 with the distant hours.;;;
4973;2;2015-01-28T22:27:18.587;;Did you know about the PUMA Benchmarks and dataset downloads? https://sites.google.com/site/farazahmad/pumadatasets It does include the following:TeraSortWikipediaList itemSelf-JoinAdjacency-ListMovies-databaseRanked-Inverted-Index ;;;
4974;1;2015-01-28T23:19:37.187;Partitioning Weighted Undirected Graph;I have an array of edges and weights:[['a', 'b', 4], ['a', 'c', 3], ['c', 'a', 2],  ...]I have about 100,000 edges and weights are between 1 and 700, most around 100.I am thinking of using Markov Cluster Algorithm however wanted to reach out to see if this is the best to use. What about Affinity Propagation? In either case, what is the workflow? Do you typically have a way to measure how well clustered the results. Is there an equivalent to a silhouette score? Is there a way to visualize the clusters?;[education, open-source];32;
4975;2;2015-01-29T00:57:39.603;;Even a simple Internet search reveals numerous papers on graph clustering approaches and algorithms. This paper is most likely the best starting point, as it presents a rather comprehensive overview of the topic in terms of the problem as well as approaches, methods and algorithms for solutions. The rest you can find easily via online search. In regard to graph clustering visualization, I recommend you to check my relevant answer - I'm pretty sure that the tools I reference there are able to visualize graph clusters as well.;;;
4976;2;2015-01-29T04:19:16.007;;Flink is the Apache renaming of the Stratosphere project from several universities in Berlin. It doesn't have the same industrial foothold and momentum that the Spark project has, but it seems nice, and more mature than, say, Dryad. I'd say it's worth investigating, at least for personal or academic use, but for industrial deployment I'd still prefer Spark, which at this point is battle tested. For a more technical discussion, see this Quora post by committers on both projects.;;;
4977;1;2015-01-29T07:32:19.207;What is the best Big-Data framework for stream processing?;I found that Apache-Storm, Apache-Spark, Apache-Flink and TIBCO StreamBase are some powerful frameworks for stream processing. but I don't know which one of them has the best performance and capabilities.I know Apache-Spark and Apache-Flink are two general purpose frameworks that support stream processing. but Apache-Storm and TIBCO StreamBase are built for stream processing specially. Is there any considerable advantage between these frameworks?Thanks;[education, open-source];331;1
4978;1;2015-01-29T09:04:56.190;Can 2 dimensional input be applied to SVM?;When considering Support Vector Machine, in an take in multiple inputs. Can each of these inputs be a vector??What i am trying to say is, can the input be a 2 dimensional vector??;[education, open-source];31;
4979;1;2015-01-29T11:35:07.657;Matlab simulation through FIS and Markov Process;I need to simulate for an academical project how the traffic fluxes (input/output with respect to a monitored area, measured in number of cars) of a city area evolves in correspondence of an event (i.e. the opening of a restricted traffic area to decongest the traffic).I have some simulated sensors that provide the data: I was thinking to use a combination of a fuzzy system (to assign a membership function to each type of data, e.g. PM10 value and CO2 value) and a markov process: I would need to modify the probability to decrement the number of car in the monitored area (simulating that a car is going out the congested area, towards the new opened area) basing on decisions made by means of a fuzzy system.So my questions are:It is a good way to interpret the problem or there are better ideas that I have not taken into account yet?How to implement such a combination of markov chain and fuzzy systems in matlab?Thanks;[education, open-source];31;
4980;1;2015-01-29T12:13:01.677;Help regarding NER in NLTK;I have been working in NLTK for a while using Python. The problem I am facing is that their is no help available on training NER in NLTK with my custom data. They have used MaxEnt and trained it on ACE corpus. I have searched on the web a lot but I could not find any way that can be used to train NLTK's NER. If anyone can provide me with any link/article/blog etc which can direct me to Training Datasets Format used in training NLTK's NER so I can prepare my Datasets on that particular format. And if I am directed to any link/article/blog etc which can help me TRAIN NLTK's NER for my own data.This is a question widely searched and least answered. Might be helpful for someone in the future whose working with NER.;[education, open-source];182;
4981;2;2015-01-29T12:42:05.757;;If I understand your question correctly. Yes, SVM can take multiple inputs. My suggestion for handling a vector as a feature would be to expand it out. For example,x0 = (1,2)       x0 = 1x1 = .4  ----->  x1 = 2x2 = 0           x2 = .4                 x3 = 0If this does not capture all of the characteristics of the vector that are important, then you may want to add other features (like magnitude of the vector) as well. ;;;
4982;2;2015-01-29T12:48:41.620;;It really depends on what you are looking to do. I love Apache Spark, but Storm has some history. I am sure as the streaming capability in Spark is built out that it will become a competitive solution. However, until Spark has some heavy hitting users (for streaming) there will remain unknown bugs. You can also consider the community. Spark has a great community. I am not sure the level of the Storm community as I am usually the one receiving the data not handling the ingest. I can say we have used Storm on projects and I have been impressed with the real-time analysis and volumes of streaming data.;;;
4983;2;2015-01-29T12:51:45.280;;"I agree that there is nothing wrong with using these type of features.  I have used for inter-arrival times for example in modeling work.  I have noticed however that many of these kind of features have ""interesting"" covariance relationships with each other, so you have to be really careful about using multiple distribution features in a model.";;;
4984;2;2015-01-29T12:58:28.257;;R is great for a lot of analysis. As mentioned about, there are newer adaptations for big data like MapR, RHadoop, and scalable versions of RStudio.However, if your concern is libraries, keep your eye on Spark. Spark was created for big data and is MUCH faster than Hadoop alone. It has vastly growing machine learning, SQL, streaming, and graph libraries. Thus allowing much if not all of the analysis to be done within the framework (with multiple language APIs, I prefer Scala) without having to shuffle between languages/tools.;;;
4985;1;2015-01-29T13:54:24.940;Pre-processing (center, scale, impute) among training sets (different forms) and the test set - what is a good approach?;I am currently working on a multi-class classification problem with a large training set. However, it has some specific characteristics, which induced me to experiment with it, resulting in few versions of the training set (as a result of re-sampling, removing observations, etc).I want to perform pre-processing of the data, that is to scale, center and impute (not much imputation though) values. This is the point where I've started to get confused.I've been taught that you should always pre-process the test set in the same way you've pre-processed the training set, that is (for scaling and centering) to measure the mean and standard deviation on the training set and apply those values to the test set. This seems reasonably to me. But what to do in case when you have shrinked/resampled training set? Should one focus on characteristics of the data that is actually feeding the model (that is what would 'train' function in R's caret package suggest, as you can put the pre-processing object in there directly) and apply these to the test set, or maybe one should capture the real characteristics of the data (from the whole untouched training set) and apply these? If the second option is better, maybe it would be worth it to capture the characteristics of the data by merging the training and test data together just for pre-processing step to get as accurate estimates as possible (I've actually never heard of anyone doing that though)?I know I can simply test some of the approaches specified here, and I surely will, but are there any suggestions based on theory or your intuition/experience on how to tackle this problem?I also have one additional and optional question. Does it make sense to center but NOT scale the data (or the other way around) in any case? Can anyone present any example where that approach would be reasonable?Thank you very much in advance.;[education, open-source];152;
4986;1;2015-01-29T17:21:07.070;Normalize weekly data - Python;I have a weekly dataset and I have to normalize this data. Data is something like this : 1. week   502. week   513. week   504. week   545. week   1506. week   1557. week   ...The important thing is, the difference between week 3 and week 4 (50-54) is not same with week 5 and week 6. And also there is a huge different between week 4 and week 5. My question is how can i handle all of this things ?Is the standard normalization functions(for example scikit normalization) can do it for me and should I normalize this data 0-1 or -1 to 1 ? Sklearn normalization pageNOTE I am working with python and generally scikit-learn library.Any help is appreciated.;[education, open-source];148;
4987;2;2015-01-29T17:28:51.800;;I would find the unit variance of the all the weeks and then divide by that. Scikit can do this for you using scale.;;;
4988;2;2015-01-29T20:36:25.567;;This looks a well structured dataset. You can read more about database design in this section of wikipedia. Your data are well structured so querying is easy. As Jake C says, you'll want to transform it for specific tasks. Packages like dplr and reshape2 are excellent for this. You could also consider writing your data to a specific database. This is particularly useful if your dataset is so large that R runs out of RAM. I've written an example with SQLite here: https://scottishsnow.wordpress.com/2014/08/14/writing-to-a-database-r-and-sqlite/;;;
4989;2;2015-01-29T21:03:27.097;;"As other answers have noted, R can be used along with Hadoop and other distributed computing platforms to scale it up to the ""Big Data"" level.  However, if you're not wedded to R specifically, but are willing to use an ""R-like"" environment, Incanter is a project that might work well for you, as it is native to the JVM (based on Clojure) and doesn't have the ""impedance mismatch"" between itself and Hadop that R has.  That is to say, from Incanter, you can invoke Java native Hadoop / HDFS APIs without needing to go through a JNI bridge or anything.";;;
4990;2;2015-01-29T21:31:50.280;;"As bogatron and Paul already said, there is nothing wrong with using the prediction from one classifier as a feature in another classifier. Actually, so-called ""Cascading classifiers"" work that way. From Wikipedia: Cascading is a particular case of ensemble learning based on the concatenation of several classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade.This can be helpful not only to inform posterior classifiers using new features but also as an optimization measure. In the Viola-Jones object detection framework, a set of weak classifiers is used sequentially in order to reduce the amount of computation in the object recognition task. If one of the weak classifiers fails to recognize an object of interest, others classifiers don't need to be computed.";;;
4991;2;2015-01-30T10:44:58.467;;Is this article good enough?http://www.succeed-project.eu/wiki/index.php/NLTK#Input_format_for_trainingThere is explanation about how corpus should look like.Your data needs to be in IOB format (word tag chunktag) to make it work.Eric NNP B-PERSONis VB Othe AT B-NPCEO NN I-NPof IN OGoogle NNP B-ORGANIZATION ;;;
4992;1;2015-01-30T11:08:37.280;Amplifying a Locality Sensitive Hash;"I'm trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I'm trying to dice it quite finely and pick things with cosine similarity 0.1 and above.I've been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I'm struggling to see how I implement this practically.What I have so far is as followsI have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user's score)I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits.Next I look for similar sketches by doing the followingI split the sketch vector into b bands of r bitsEach band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket.I then look in each bucket. Any movies that are in the same bucket are candidate pairs.Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets.The book suggests I can ""amplify"" my results by adding more AND and OR steps, but I'm at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers.Can anyone help me understand how to do this?";[education, open-source];73;1
4993;2;2015-01-30T12:11:41.293;;After speaking with some experienced statisticians, this is what I got. As for technical issues regarding the paper, I'd be worried about data leakage or using future information in the current model. This can also occur in cross validation. You should make sure each model trains only on past data, and predicts on future data. I wasn't sure exactly how they conducted CV, but it definitely matters. It's also non-trivial to prevent all sources of leakage. They do claim unseen examples but it's not explicit exactly what code they wrote here. I'm not saying they are leaking for sure, but I'm saying it could happen.;;;
4994;2;2015-01-30T16:33:23.683;;I would call a mapping between N dimensional input and N dimensional output a regression problem. If you add more constraints about the relation between the input and output it might be called different names: linear filtering, nonlinear filtering, etc...some examples on common techniques for that would be: neural networks, regression trees, regularised regressions...;;;
4995;1;2015-01-30T18:55:46.173;Can we access HDFS file system and YARN scheduler in Apache Spark?;We can access HDFS file system and YARN scheduler In the Apache-Hadoop. But Spark has a higher level of coding. Is it possible to access HDFS and YARN in Apache-Spark too?Thanks;[education, open-source];694;1
4996;2;2015-01-30T20:15:09.003;;Apache Storm and Apache Spark are more popular than the other ones, there are already many discussions on Quora(Storm vs Spark, Use cases for comparison).Personally, I think Spark is a better choice.;;;
4997;2;2015-01-30T20:26:42.293;;Data SetsAcademic TorrentsQuorahadoopilluminated.comdata.govQuandlfreebase.comusgovxml.comenigma.iodatahub.ioaws.amazon.com/datasetsdatabib.orgdatacite.orgquandl.comfigshare.comGeoLite Legacy Downloadable DatabasesQuora's Big Datasets AnswerPublic Big Data SetsHouston Data PortalKaggle Data SourcesA Deep Catalog of Human Genetic VariationA community-curated database of well-known people, places, and thingsGoogle Public DataWorld Bank DataNYC Taxi dataOpen Data Philly Connecting people with data for PhiladelphiaA list of useful sources A blog post includes many data set databasesData Sets From awesome-datascience;;;
4998;1;2015-01-30T21:04:30.173;Which Big-Data Frameworks have most simple interfaces?;I found that Apache-Spark has pretty much simple interface and easy to use. But I want to know about other interfaces.Can anyone give me a ranking of Big-Data frameworks in base of simplicity of their interfaces. also this is useful to express most simple and complex interfaces in base of your experiences.Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. Detailed comparison is so lengthy and this is not my purpose. Just a selection or ranking on your opinions is sufficient.Thanks;[education, open-source];143;
4999;2;2015-01-30T21:14:25.563;;N_dimension input - n_dimension output is a too general description. You could think of it as a regression problem where you predict multi-dimensional output. But also it could be the case that you are solving multiclass-classification problem:input: n featuresoutput: vector which defines class membership - either 0's and 1's or the real value which defines degree of membership to the classOr you could also think of it as of multilabel classification problem:input: n featuresoutput: vector of 0 and 1 which define which labels are associated with the input.So in general multi-dimensional output is not telling anything about the matter of task.You could try 2 approaches to solve the task which involves multi-dimensional output:1) One-vs-rest or one-vs-one strategies (or their variations) where for each 'part' (dimension) of the output you train separate classifier or separate regressor.2) Neural network with multiple output neurons. I would suggest to try it after trying #1, neural networks are complicated, computing-expensive and maybe somewhat clumsy - so far, I wasn't able to construct neural network which would outperform other models in specific tasks I tried to solve.  But of course, this is my personal opinion about NN. In your case they may really shine.;;;
5000;1;2015-01-30T21:30:18.077;Proper way of fighting negative outputs of a regression algorithms where output must be positive all the way;Maybe it is a bit general question. I am trying to solve various regression tasks and I try various algorithms for them. For example, multivariate linear regression or an SVR. I know that the output can't be negative and I never have negative output values in my training set, though I could have 0's in it (for example, I predict 'amount of cars on the road' - it can't be negative but can be 0). Rather often I face a problem that I am able to train relatively good algorithm (maybe fit a good regression line to my data) and I have relatively small average squared error on training set. But when I try to run my regression algorithm against new data I sometimes get a negative output. Obviously, I can't accept negative output since it is not a valid value. The question is - what is the proper way of working with such output? Should I think of negative output as a 0 output? Is there any general advice for such cases?;[education, open-source];89;1
5001;1;2015-01-30T23:39:04.687;Data available from industry;I'm going to start my degree thesis and I want to do a fault detector system using machine learning techniques. I need datasets for my thesis but I don't know where I can get that data. I'm looking for historical operation/maintenance/fault  datasets of any kind of machine in the oil&gas industry (Drills, steam injectors etc) or electrical companies (transformators, generators etc). Thank you for your help.;[education, open-source];96;1
5002;2;2015-01-30T23:44:42.127;;"I think that it is impossible to answer this question comprehensively, at least for the following reasons:big data frameworks have different goals and target different knowledge domains, so the comparison simply doesn't make much sense;most big data frameworks (and other programming frameworks, for that matter) have multiple interfaces, and frequently those sets of interfaces are significantly different (the intersection is small), so there is a risk of comparing apples and oranges;trying to compare anything (in this case, interfaces), using simplicity as a criterion, involves a significant amount of subjectivity - what one person perceive as very simple, another person might find quite complex;the variety and the number of big data frameworks is mind-boggling (for example, see https://github.com/onurakpolat/awesome-bigdata); the same applies to a related topic of machine learning frameworks (for example, see https://github.com/josephmisiti/awesome-machine-learning);corollary from the points above: a comprehensive comparison (considering all the above-mentioned issues) would go far beyond the scope of a single answer on this site, in volume and effort - it would be more like a long research paper, a book chapter or even a book.";;;
5004;2;2015-01-31T01:18:10.307;;"A standard trick is to estimate the logarithm of the desired quantity, then take its exponential, which is always positive. The drawback is that the error is optimized for the log, which treats differences in order of magnitude as equal. Another option is to do your regression as usual then project onto the feasible set (use the positive part of the output; $max(0, \cdot)$)";;;
5005;2;2015-01-31T02:22:36.000;;"Yes.There are examples on spark official document: https://spark.apache.org/examples.htmlJust put your HDFS file uri in your input file path as below (scala syntax).val file = spark.textFile(""hdfs://train_data"")";;;
5006;2;2015-01-31T12:29:35.733;;"HDFSSpark was built as an alternative to MapReduce and thus supports most of its functionality. In particular, it means that ""Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc.""1. For most common data sources (like HDFS or S3) Spark automatically recognizes schema, e.g.: val sc = SparkContext(...)val localRDD = sc.textFile(""file://..."")val hdfsRDD  = sc.textFile(""hdfs://..."")val s3RDD    = sc.textFile(""s3://..."")For more complicated cases you may need to work with lower-level functions like newAPIHadoopFile: val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],       classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],      classOf[org.apache.hadoop.hbase.client.Result])val customRDD = sc.newAPIHadoopRDD(conf, classOf[MyCustomInputFormat],       classOf[MyCustomKeyClass],      classOf[MyCustomValueClass]) But general rule is that if some data source is available for MapReduce, it can be easily reused in Spark.YARNCurrently Spark supports 3 cluster managers / modes: StandaloneMesosYARNStandalone mode uses Spark's own master server and works for Spark only, while YARN and Mesos modes aim to share same set of system resources between several frameworks (e.g. Spark, MapReduce, Impala, etc.). Comparison of YARN and Mesos may be found here, and detailed description of Spark on YARN here. And, in best traditions of Spark, you can switch between different modes simply by changing master URL. ";;;
5007;1;2015-01-31T13:28:45.590;How to proceed 2 executions in 1 step in hive?;"I am wondering if there is a way to proceed 2 exectuions in 1 step in hive.For example:SELECT * FROM TABLE1SELECT * FROM TABLE2;Do this in one window, and do not have to open 2 hive windows to execute each line separetly.Can it be done on HUE?";[education, open-source];24;
5008;2;2015-01-31T14:27:15.657;;A huge list of open data sets is listed here:Publicly available datasetsIncluding Amazon, KDnuggets, Stanford, Twitter, Freebase, Google Public and more.;;;
5009;2;2015-01-31T16:55:53.350;;"You can use HiveCLI Tool to run HiveQL with a given sql file. $HIVE_HOME/bin/hive -f /home/my/hive-script.sqlPlease see official document: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+CliWhat you need to do is toPut your HiveQLs in a file as belowSELECT * FROM TABLE1;SELECT * FROM TABLE2;Use HiveCLI and run with above file";;;
5010;2;2015-01-31T17:31:09.967;;"You can separate each query with a semi colon (;)select column1 from table1;select column2 from table2;This command can be executed from command line via inline queries or a file. Usage of Hive CLI is not recommended. You must use beeline to execute queries configured via hive server 2 so that all/any underlying security control measures are honored.you may invoke beeline with the command:beeline";;;
5011;2;2015-02-01T01:34:13.597;;"I thought about it this way: the training and test sets are both a sample of the unknown population. We assume that the training set is representative of the population we're studying. That is, whatever transformations we make to the training set are what we would make to the overall population. In addition, whatever subset of the training data we use, we assume that this subset represents the training set, which represents the population.So in response to your first question, it's fine to use that shrinked/resmpled training as long as you feel it's still representative of that population. That's assuming your untouched training set captures the ""real characteristics"" in the first place :)As for your second question, don't merge the training and testing set. The testing set is there to act as future unknown observations. If you build these into the model then you won't know if the model wrong or not, because you used up the data you were going to test it with.";;;
5012;1;2015-02-01T05:58:00.343;physionet arrhythmia database (MIT-BIH) - Labels of an ECG signal;"I am trying to train ECG signal downloaded from Physionet using LIBSVM svmtrain, The code is as follows: model=svmtrain(labels,features_sparse,'-t 2 -s 0');I have created features_sparse, but I am confused in making/retrieving label vector from '.atr' file. The only thing I know is that this must be from '.atr' file. But I don't know how to make this matrix/vector.Any help is appreciated.NOTE : I used rddata.m to extract ECG matrix from 100.dat, 100.atr and 100.hea";[education, open-source];66;
5013;1;2015-02-01T09:35:16.113;Machine learning for Point Clouds Lidar data;Our main use case is object detection in 3d lidar point clouds i.e. data is not in RGB-D format. We are planning to use CNN for this purpose using theano. Hardware limitations are CPU: 32 GB RAM Intel 47XX 4th Gen core i7 and GPU: Nvidia quadro k1100M 2GB. Kindly help me with recommendation for architecture. I am thinking in the lines of 27000 input neurons on basis of 30x30x30 voxel grid but can't tell in advance if this is a good option.Additional Note: Dataset has 4500 points on average per view per point cloud;[education, open-source];122;
5014;1;2015-02-02T14:53:51.810;How to compute F1 score?;Recently I read about path ranking algorithm in a paper (source: Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion). In this paper was a table (Table 3) with facts and I tried to understand how they were calculated.F1 (harmonic mean of precision and recall) = 0.04P (precision) = 0.03R (recall) = 0.33W (weight given to this feature by logistic regression)I found a formula for F1 via Google which is$F1 = 2 * \frac{precision * recall}{precision + recall}$The problem is that I get the result of 0.055 with this formula, but not the expected result of 0.04.Can someone help me to get this part?Also, does someone know how 'W' can be calculated?Thanks.;[education, open-source];49;
5015;2;2015-02-02T15:55:49.363;;First you need to learn about Logistic Regression, it is an algorithm that will assign weights to different features given some training data. Read the wiki intro, is quite helpful, basically the Betas there are the same as the Ws in the paper. The formula you have is correct, and those value do seem off. It also depends on the number of significant figures you have, perhaps they are making their calculations with more than the ones they are reporting.But honestly, you can't understand much of the paper unless you understand LR;;;
5016;2;2015-02-02T15:59:40.643;;First, CNNs are great for image recognition, where you usually take sub sampled windows of about 80 by 80 pixels, 27,000 input neurons is too large and it will take you forever to train a CNN on that.Furthermore, why did you choose CNN? Why don't you try some more down to earth algorithms fisrst? Like SVMs, or Logistic regressions.4500 Data points and 27000 features seems unrealistic to me, and very prone to over fitting.Check this first.http://scikit-learn.org/stable/tutorial/machine_learning_map/;;;
5017;2;2015-02-02T16:08:50.580;;I don't really get why would you mix Fuzziness and Probabilities. HMMs already can give you probabilities without the need of adding Fuzzy systems into the mix.I would just do a random walk with probabilities of transitions defined by the state of the lights.;;;
5018;2;2015-02-02T17:00:51.307;;"Yes, you can execute multiple HQL's using Hue as long as each individual HQL is separated by a semi colon (;)";;;
5019;2;2015-02-02T20:20:08.503;;The problem is your model choice, as you seem to recognize. In the case of linear regression, there is no restriction on your outputs.  Often this is fine when predictions need to be non-negative so long as they are far enough away from zero.  However, since many of your training examples are zero-valued, this isn't the case.If your data is non-negative and discrete (as in the case with number of cars on the road), you could model using a generalized linear model (GLM) with a log link function.  This is known as Poisson regression and is helpful for modeling discrete non-negative counts such as the problem you described. The Poisson distribution is parameterized by a single value $\lambda$, which describes both the expected value and the variance of the distribution.This results in an approach similar to the one described by Emre in that you are attempting to fit a linear model to the log of your observations.;;;
5020;2;2015-02-02T20:58:47.970;; I want to try out unsupervised learning for the same. I am looking into KMeans clustering. But again the problem is, we won't know the number of clusters beforehand. So, which algorithm will be best for this use case?When you don't know the number of clusters beforehand, it is still possible to do unsupervised learning using a Dirichlet process to sample parameters associated to clusters/groups and then cluster your tokens according to those parameters. The general idea is to use a Dirichlet distribution to generate probabilities over words for each cluster and a Dirichlet process uses these probabilities to assign a cluster to each word in your vocabulary. If you want to share clusters between emails, then you use Hierarchical Dirichlet Processes. Here you can find a nice blog post of how this works.The most popular library for clustering is gensim, but notice their warning regarding the Hierarchical Dirichlet Process implementation: gensim uses a fast, online implementation based on [3]. The HDP model is a new addition to gensim, and still rough around its academic edges – use with care.As for feature extraction, your question doesn't say exactly what kind of unnecessary features you are getting, but if that's the case, you need to filter your tokens before or after processing them with NLTK. In general, you can't expect excellent results for very specific applications.;;;
5021;1;2015-02-03T04:23:37.463;Rank players of any given sport;I've recently become interested in possibly of developing some sort of method for ranking athletes of sports such as American football and determining which players are better than others in terms of specific statistics. My thoughts are that there are two ways to go about doing this. The first would be some sort of mathematical formula which would take in the statistics of a given player and provide some sort of standardized score which could be compared with other players to determine which is better.My other idea would be to have some machine learning algorithm go through historical data and determine the patterns which indicate how well a certain combination of statistics would perform in the following week of play by using the patterns it recognizes over time.I'm not sure which approach would be more effective and so I'm hoping that someone has an idea or any advice as to which would be best to look into. Thanks!;[education, open-source];62;2
5022;2;2015-02-03T08:09:44.953;;PredictionIf the main goal is predicting anything, say, the statisitcs of the player in the next game, game result, then I would not recommend to do any scoring. Better way to go is using the pure statistics data as an input to the model. Any scoring/rankning - is information loss.Ranking If the goal is ranking itself, than you still need to have some target variable to predict. As you may want to check real predictive value of those ranks. That could be, again, playser stats in the next game or game result itself.ReferencesSport scores prediction and RFM scoring are probably the next directions for you to look at.;;;
5023;1;2015-02-03T08:52:42.133;What is a discrimination threshold of binary classifier?;"With respect to ROC can anyone please tell me what the phrase ""discrimination threshold of binary classifier system"" means? I know what a binary classifier is.";[education, open-source];77;
5024;1;2015-02-03T10:14:53.543;Is the GA R package the best Genetic Algorithm package?;I am learning about Data Science and I love the Healthcare part. That's why I have started a blog and my third entry is about using Genetic Algorithm for solving NP-Problems. This post is https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/I have some expertise with GA package solving problems like the TSP, but do you know any most powerful R package?Thanks so much!;[education, open-source];770;
5025;2;2015-02-03T11:37:06.760;;For such questions, I like to go to the Task Views on CRAN, since the packages noted there are, to a degree, pre-vetted by the R community. I'd trust those a tiny bit more than just googling myself.The Machine Learning Task View at CRAN says: Packages rgp  and  rgenoud  offer optimization routines based on genetic algorithms. The package  Rmalschains  implements memetic algorithms with local search chains, which are a  special type of evolutionary algorithms, combining a steady state  genetic algorithm with local search for real-valued parameter  optimization.;;;
5026;2;2015-02-03T16:00:05.673;;Classifiers often return probabilities of belonging to a class. For example in logistic regression the predicted values are the predicted probability of belonging to the non-reference class or $\text{Pr}(Y = 1)$. The discrimination threshold is just the cutoff imposed on the predicted probabilities for assigning observations to each class.;;;
5027;2;2015-02-03T19:01:59.827;;Amazon Kinesis might be another choice for stream processing, if you don't want to set up the clusters by yourself.;;;
5028;2;2015-02-03T19:53:47.233;;From what I understand of the objectives of the lambda architecture your point: Your batch layer is probably a map reduce job or a HIVE query. Is not what was intended. The batch layer is not meant to be directly queried against, but rather feeds a serving layer, possibly a simple key-value store, for low latency queries.Check out http://lambda-architecture.net/ for a more full explanation.;;;
5029;2;2015-02-03T20:05:59.917;;One other data source I didn't see listed is The GDELT Project. From the site: GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world.;;;
5030;2;2015-02-03T20:24:04.800;;As @Sidhha mentioned GridSearchCV is a great way to estimate parameters. Here is an example showing how to use it. Good luck.;;;
5031;2;2015-02-03T22:51:57.083;;If you have an imbalanced dataset you usually want to make it balanced to begin with, since that will artificially affect your scores.Now, you want to be measuring precision and recall, since those can capture a bit better the imbalanced dataset biases.L1 or L2 won't perform particularly better in a balanced or unbalanced dataset, what you want to do is call elastic nets (which is a combination of the two) and do cross validation over the coefficients of each of the regularizers. Also, doing grid search is very odd, you are better using just cross validation and see what parameters work better.They even have ElasticNetCV, which does that part for you (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV);;;
5032;2;2015-02-03T22:56:04.747;;Just to add a bit.Like it was mentioned before, if you have a classifier (probabilistic) your output is a probability (a number between 0 and 1), ideally you want to say that everything larger than 0.5 is part of one class and anything less than 0.5 is the other class.But if you are classifying cancer rates, you are deeply concerned with false negatives (telling some he does not have cancer, when he does) while a false positive (telling someone he does have cancer when he doesn't) is not as critical. So you might artificially move that threshold from 0.5 to higher or lower values, to change the sensitivity of the model in general.By doing this, you can generate the ROC plot for different thresholds.;;;
5033;1;2015-02-03T23:49:30.013;Format of CSV file;"I'am trying t create a regression based prediction (like booking website): predict the number of clicks for each hotel.I have to generate a .csv file containing two columns: hotel_id, predicted_number_of_clicks for all hotel_ids.My first question question is : should I put the Id_hotel as feature in the predictive model. I think that I must to drop it no?Second question is : how can write in the csv file only this 2 columns ""hotel_id"", ""predicted_number_of_clicks"" if I drop hotel_id from the model feature?Thank you very much for your reply in advance";[education, open-source];87;
5034;2;2015-02-04T00:16:15.323;;The hotel_id should not be a feature.Let's see if I understand you correctly.At testing time you give your model a whole set of feature values for a particular hotel you are interested in. This hotel has an id, which is known to you.Your model should be able to take both an id and a set of feature values as input, so it can print both to an output file.Does that answer your question?;;;
5037;2;2015-02-04T02:54:52.773;;I've found an interesting project With tons of data available. It's a real data benchmark executed over an industrial valve. This is the website.Industrial Actuator Real Data Benchmark Study.;;;
5038;1;2015-02-04T04:51:28.063;minimization with a negative cost function: works in MATLAB, not in Python;I'm trying to use a particular cost function (based on doubling rate of wealth) for a classification problem, and the solution works well in MATLAB. See https://github.com/acmyers/compareCostFXsWhen I try to do this in Python 2.7.6 I don't get any errors, but it only returns zeros for the theta values.Here is the cost function and optimization method I've used in Python:def costFunctionDRW(theta, X, y):    # Initialize useful values    m = len(y)    # Marginal probability of acceptance    marg_pA = sum(y)/m    # Marginal probability of rejection    marg_pR = 1 - marg_pA    # =============================================================    pred = sigmoid(np.dot(X,theta))    final_wealth_individual = (pred/marg_pA)*y + ((1-pred)/marg_pR)*(1-y)    final_wealth = np.prod(final_wealth_individual)    final_wealth = -final_wealth    return final_wealthresult = scipy.optimize.fmin(costFunctionDRW, x0=initial_theta, \                   args=(X_array, y_array), maxiter=1000, disp=False, full_output=True )Any advice would be much appreciated!;[education, open-source];73;
5039;2;2015-02-04T05:33:59.907;;If your number of cases is large, you may be running into a problem of numerical underflow in the line  final_wealth = np.prod(final_wealth_individual)If each value of final_wealth_individual is between 0 and 1, multiplying them all together can lead to a result that is too small to represent as a floating point number, resulting in a value of 0.To address this issue, take the log of final_wealth_individual and add them together instead of multiplying.  Note that this will cause final_wealth to be negative, so you will not need to multiply it by -1 as you are currently.;;;
5040;1;2015-02-04T06:10:34.893;How to determine whether a bad performance is caused by data quality?;I'm using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y$, which is a continuous value from zero to one.At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.Then, I have tried different models, e.g., decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and etc. However, all of these models also do not perform well. (RMSE $\approx $0.35, there is not significant difference with linear regression)I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.My question is: how can I examine whether it is caused by bad data quality?BTW, for the size of data set, there are more than 10K data points, each of which associated with 105 features.I have also tried to investigate importance of each feature by using decision-tree-based regression, it turns out that, only one feature (which should not be the most outstanding feature in my knowledge to this problem) have an importance of 0.2, while the rest of them only have an importance less than 0.1.;[education, open-source];55;1
5041;2;2015-02-04T06:15:52.873;;How many features do you have?Is quite unlikely that ALL the features are bad. So you could regress with a different number of features.For example, do one pass with all the features, then take one out (usually X_m) so you have m-1 features. Keep doing this so you can take out uninformative features.Also, I would recommend you calculating P-Values to see whether your regessors are significative are informative.;;;
5042;2;2015-02-04T06:18:04.207;;Trying to classify using hotel ID is the same as trying to determine if a student is going to perform well on a test based on their last name. You should get additional things, like number of rooms, amenities, location, staff, etc. Those are informative features that you can use in a classifier;;;
5043;2;2015-02-04T06:22:23.377;;"First, it sounds like your choice of model selection is a problem here.  Your outputs are binary-valued, not continuous. Specifically you may have a classification problem on your hands rather than a traditional regression problem.  My first recommendation would be to try a simple classification approach such as logistic regression or linear discriminant analysis.Regarding your suspicions of bad data, what would bad data look like in this situation? Do you have reason to suspect that your $X$ values are noisy or that your $y$ values are mislabeled? It is also possible that there is not a strong relationship between any of your features and your targets.  Since your targets are binary, you should look at histograms of each of your features to get a rough sense of the class conditional distributions, i.e. $p(X_1|y=1)$ vs $p(X_1|y=0)$. In general though, you will need to be more specific about what ""bad data"" means to you.";;;
5044;2;2015-02-04T09:28:16.910;;When training the model you don't need to use the hotel id. The model needs to learn from examples. It only needs feature values and number of clicks, so it can learn the relationship between these.Once you've trained your model, you can use it for unseen examples.These would be hotels for which you have both an id and a set of feature values.Your model should take the id and the feature values as input, but it should only use the feature values for the prediction. The id should just be kept on the side, so it can print it together with the prediction to the output csv file.I hope this helps!;;;
5045;2;2015-02-04T11:22:53.063;;Supervised learning should try to 'understand' what makes a hotel to have more clicks than other. As a consequence learning tries to define which are the characteristics of some given hotels which make them attractive or not. So it uses some kind of similarities, because it is supposed that similar hotels behaves in a similar way. Now if you restrict the similarity to identity than you learn nothing new because hotels are unique. In fact such kind of learner exists and is called Rote learner, and it consists of one-to-one mapping from inputs to outputs. It is also called memoisation. And this happens if you will add hotel_id in the features. However I think you hope to use that to predict the number of clicks for new hotels (which does have a different hotel_id than any from training set). On the other hand, in order to use hotel_id to store prediction you only have to save a copy of the original data set. At learning time you have a train data set from which you remove hotel_id, and use that for learning. At prediction time you make a copy of the data set for later use. From the original data set remove order_id, use that for prediction and get the results. Now the predicted results have the same order of instances as the copied data set. This happens for sure in python (scikit learn), java (weka), R. In fact I am not aware of a system which does not preserve positions.Now using positions from the copy of the original and prediction you can associate each hotel_id to each prediction with no problem. ;;;
5046;2;2015-02-04T12:51:31.943;;Your understanding is correct. The point is that equation (8)$$y_i(<\textbf{w}, \phi_i> + b) - 1 = 0$$is not exactly an equation, but a system of equations, one for each $i$ index of the support vectors (those for each $0<\alpha_i<C$. The point is that you cannot compute $b$ during the optimization of the dual problem since it does not matter for optimization, you have to go back and compute $b$ from all the other equations you have (one possible way is (8)). Vapnick suggestion is to not use only one of those equations, but two of them, specifically one support vector for a negative observation and one for a positive observation. In other words two support vectors which have opposite signs for $y_i$. Let's name $A$ the index of one support vector and $B$ the index of a suport vector of opposite side, baiscally you select from the system of equations at (8) only two of them. Evaluate both of them and take the mean.From:$$y_A(<\textbf{w},\phi_A>+b)=1$$$$y_B(<\textbf{w}, \phi_B>+b)=1$$We get:$$b_A=\frac{1}{y_A}-<\textbf{w},\phi_A>$$$$b_B=\frac{1}{y_B}-<\textbf{w},\phi_B>$$Where $b_A$ and $b_B$ are two estimations, then the mean is$$b = (b_A+b_B)/2 = -\frac{1}{2}(<\textbf{w},\phi_A>+<\textbf{w},\phi_B>)=-\frac{1}{2}\sum_{i=1}^{n}y_i\alpha_i(<\phi(x_i),\phi(x_A)>+<\phi(x_i),\phi(x_B)>)$$;;;
5047;1;2015-02-04T22:03:48.507;OCR / Text Recognition and Recovery Problem;I am working on a research project that deals with American military casualties during WWII. Specifically, I am attempting to construct a count of casualties for each service at the county level. There are two sources of data here, each presenting their own challenges.1. Army and Air Force data. The National Archives hosts lists of Army and Air Force servicemen killed in action by state and county. There are .gif images of the report available online. Here is a sample for several counties in Texas. I DO NOT need to recover the names or any other information. I simply need to count the number of names (each on its own line, and listed in groups of five) under each County. There are hundreds of these images (50 states - 30-100 for each state). I have been unable to find an OCR program that can tackle this problem adequately. How would you suggest I approach this challenge? (I have some programming expertise in Python and Java, but would prefer to use any off-the-shelf solutions that may exist).2. Navy and Marine Core data. This data is organized differently. Each state has long lists of casualties with the address of their next of kin. Here is a sample for Texas again. For these images, I need to BOTH count the number of dead and recover their hometown, which is typically the last word in each entry. I can then match these hometowns to counties and merge with database 1.Again, the usual OCR programs have proved inadequate. Any help on this (admittedly more difficult) problem would be very much appreciated.Thank you in advance experts!;[education, open-source];130;
5048;2;2015-02-04T23:58:53.323;;In what way have the usual OCR programs proved inadequate?Do you have some example output that you find you can't work with?I can see how the columns complicate things.I'd say for data set 1:OCR the images, then read the files line by line and match on for instance a sequence of at least five numbers. So you get 0, 1, 2 or 3 per row.You may miss a couple due to the OCR accidentally recognizing a number as a letter for instance, but I expect this to work reasonably well.How precise do you have to be?Data set 2 seems more difficult.Maybe counting can be done by matching on capitalized sequences followed by a comma.Placename... very tricky. Once again, do you have some OCR output we can look at?;;;
5049;2;2015-02-05T00:49:11.713;;I'm surprised one has not mentioned this, as it seems fairly obvious: http://www.kaggle.com consistently has new and very interesting datasets. Information is considered an asset, so often companies don't want to release that data (plus privacy concerns). Kaggle gives you data and they hope you solve business problems with it in exchange.;;;
5050;1;2015-02-05T01:29:00.997;Word Frequency Analysis of Document Sets;I'm doing some work trying to extract commonly occurring words from a set of human classified documents and had a couple questions for anyone who might know something about NLP or statistical analysis of text.We have a set of a bunch of documents, and users have classified them as either good or bad. What I'd like to do is figure out what words are common to the good documents, but not necessarily the other ones.I could, for example, use the (frequency within good documents / total frequency) which would essentially normalize the effect of a word being generally common. This, unfortunately, gives very high precedence to words that occur in only a few good documents & not at all in the other documents. I could add some kind of minimum threshold for # of occurrences in good docs before evaluating the total frequency, but it seems kind of hacky.Does anyone know what the best practice equation or model to use in this case is? I've done a lot of searching and found a lot of references to TF-IDF but that seems more applicable for assessing the value of a term on a single document against the whole set of docs. Here I'm dealing with a set of docs that is a subset of the larger collection.In other words, I'd like to identify which words are uniquely or more important to the class of good documents.;[education, open-source];140;
5051;2;2015-02-05T02:25:51.627;;Your (frequency within good documents / total frequency) seems reasonable to me.It could be that most words that occur in many good documents, simply also occur in many bad documents.How about you make a list of all the words appearing in the good documents.Then you count their appearance in the good documents and their appearance in the bad documents and compare those two numbers.The words that appear more often in the good ones, with a difference higher than a certain threshold are the ones of interest to you (if they exist).;;;
5052;1;2015-02-05T05:44:05.497;How do PCFG parsers actually work?;I'm new to NLP. I have a few doubts about PCFG parser (NLTK). My understanding is that PCFG parser will return most probable parse tree. So if I'm parsing one sentence with PCFG parser, I'll be getting one parse tree. Is my assumption right?Moving further, PCFG parser is trained using corpus consisting of 1000 sentences, will I be getting 1000 parse tree (1 per sentence) or only one parse tree? If it is different for each sentence, are the production rules for one parse tree independent of another parse tree?Why does the NLTK PCFG parser expect input in Penn treebank format? (I mean Penn treebank is also a parsed tree, isn't it?)Do we have to define production rules and assign its probability explicitly? If no, how to do it programmatically?;[education, open-source];73;
5053;2;2015-02-05T06:54:16.023;;I guess what you are looking for is Differential Word Usage. This method takes two text corpora as input and you can get the list of words which are being used more in one text corpus over the other.Basically what you need to do is to build a common Term Document Matrix for the corpora you are using and then divide this TDM into two TDMs such that all the document columns from corpus 1 fall in one TDM and all the documents columns form corpus 2 fall in the second TDM.  For example, you have 2 corpora, the first one containing 10 documents and the second one containing 15 documents. You first, combine both these corpora and form 25 document corpus and then form the TDM, where terms become the rows (let's say there are 300 terms) and the 25 documents become the 25 columns. Here the first 10 columns represent the documents of first corpus and the remaining 15 belong to the second corpus. So, you divide this TDM of dimensions 300 x 25 to two TDMs of dimensions 300 x 10 and 300 x 15. Then you can use Chi-square difference over these TDMs to determine which words are occurring more in one corpus than the other.A wonderful example has been given regarding this approach by Vik in his blog using Wikileaks corpus and R here: http://www.vikparuchuri.com/blog/finding-word-use-patterns-in-wikileaks/;;;
5054;2;2015-02-05T07:26:04.347;;There are many algorithm to do classification: Naïve Bayes, logistic regression, SVM, decision tree..etc. My suggestion is to try Naïve Bayes first by calculating below probabilities which a new document belongs to $class_{good}$ or $class_{bad}$. (https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)$$P(Class_{good} \vert document_{new}) = \frac{P(document_{new} \vert Class_{good}) \cdot P(Class_{good}) }{P(document_{new})}$$$$P(Class_{bad} \vert document_{new}) = \frac{P(document_{new} \vert Class_{bad}) \cdot P(Class_{bad}) }{P(document_{new})}$$And generally, when we are doing text mining questions, we will do several preprocess on one document: Tokenization(1-gram/bigram/...etc)remove stop words ('a', 'the' 'at', ... etc)Stemming: transforms a word into its root form. (studied => study)My suggestion is to do above preprocesses and try more features not just the words in one document, if there are some metadataes.;;;
5055;1;2015-02-05T07:41:10.443;machine learning on athlete performances to predict the time in a future race;Example DataI have a dataset (in R as a data frame) of race results for athletes.athlete racedistance time    location tracktype       date    coachA          100       10.0       UK     typeA       2014-01-01  carlosA          200       20.0       US     typeB       2014-02-01  carlaA          100        9.5      AUS     typeC       2014-03-01  chrisB          100       11.0       UK     typeA       2014-01-01  carlaB          200       21.0       US     typeB       2014-02-01  carlosB          400       61.0      AUS     typeC       2014-03-01  carlaB          100       10.5      GER     typeA       2014-04-01  cliveC          100        9.5       UK     typeA       2014-01-01  cliveC          200       21.5       US     typeB       2014-02-01  chrisQuestionIs there an appropriate machine learning algorithm or method that can use the previous results of each athlete as a feature, when trying to predict the time for an athlete in a future race? For example, athlete A has three races, with one month rest between them. In the third race he performs slightly better than the first race over the same distance. Can an algorithm learn that the second race had an effect on the athlete, which meant he performed better in the third race?From what I've read on the subject and the training examples I've completed it would appear that each 'row' of data should be independent, is this the case for all ML algorithms? Is there another prediction technique I should be considering to solve this type of problem?;[education, open-source];150;1
5056;2;2015-02-05T10:02:08.827;;By working with your features you could make the ML algorithm (maybe regression or SVR or whatever) to learn this fact (that sequental races are increasing the performance of athlete). To do this you may want to drop out the date column and introduce some new column, maybe 'race number' with 0 for first race, 1 for second, 2 for third etc.In such case regression model will be able to learn what you say 'that the second race had an effect on the athlete, which meant he performed better in the third race'. It is all about feature selection.;;;
5057;1;2015-02-05T14:39:25.573;Image oriented index generation;Can anybody tell how the index table will be generated to place the patches from the source texture to perform texture synthesis?How the index table is generated in the following image?;[education, open-source];8;
5058;2;2015-02-05T17:22:04.123;;Ok, in this case, time is your dependent variables, and all the other ones are your features.You should use linear regression (since more complex stuff needs more expertise), any machine learning library has that implemented.Do not use the date as a feature, those are usually lousy estimators.;;;
5060;2;2015-02-05T20:40:51.363;;I think I've worked something out. Basically I'm looking for an approach that works in a map/reduce type environment and I think this approach does it.So,suppose I have b bands of r rows and I want to add another AND stage, say another c ANDs.so instead of b * r bits I need hashes of b * r * c bitsand I run my previous procedure c times, each time on b * r bitsIf x and y are found to be a candidate pair by any of these procedures it emits a key value pair ((x, y), 1), with the tuple of IDs (x,y) as the key and the value 1At the end of the c procedures I group these pairs by key and sumAny pair (x,y) with a sum equal to c was a candidate pair in each of the c rounds, and so is a candidate pair of the entire procedure.So now I have a workable solution, and all I need to do is work out whether using 3 steps like this will actually help me get a better result with fewer overall hash bits or better overall performance...;;;
5061;1;2015-02-05T21:18:01.527;How much of a background in programming is necessary to become a data scientist?;I once knew some Java, but that was close to 10 years ago.  Assuming I can learn a language to get into data analytics.... what language do you recommend?  ;[education, open-source];349;1
5062;2;2015-02-05T23:22:08.800;;Based on this infographic and other things I've read, it sounds like you need to know some coding to be a true data scientist. http://blog.datacamp.com/how-to-become-a-data-scientist/ But you could still be a data analyst without compsci - basically a statistician. ;;;
5063;2;2015-02-05T23:23:04.180;;The R language is the best place to start. Grab some open datasets, and start programming with r. R has many different analytical functions that you can learn a lot with it.;;;
5064;1;2015-02-05T23:57:05.757;Where in practice can we see following information propagation model;I am not certain, if this is the right place to ask the following question. I am looking for some practical scenarios in social networks where the following information propagation model can arise:Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender.;[education, open-source];42;
5065;2;2015-02-06T00:05:02.313;;Data Scientists code every day. However, just because you don't have background doesn't mean you can't pick it up! The level of programming you need to know to start doing Data Science isn't very high, but you will at least need:the logical mindset to phrase the solution to your problem inprocedural code  to know the programming language, functions, and libraries needed in this field.1st point is the most difficult of the two. Hopefully you have taken enough math and physics by now to wire your mind to think programmatically. If so then yes, you absolutely can learn a language! There are guides out there that teach out the syntax and functions. For example:  R - http://tryr.codeschool.com/General Python - http://www.codecademy.com/en/tracks/pythonDataSci Python - https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-pythonPersonally I would recommend Python first. To me the language places more emphasis on readability and cleanliness, making it a great first language. It's also a general purpose language so it's good to know. I did start with R though and it's also good, but is more function-over-form IMO. Try both out and see which feels best first, since you'll likely have to pick up both if you delve into this field anyways.;;;
5066;2;2015-02-06T00:47:49.187;;"It sounds to me like you have a binary classification problem (classifying good vs. bad documents for some definitions of good and bad) and the words are being used as features or ""signals"" for what predicts good vs. bad documents. One thing you might try is to measure some type of correlation statistic between unigrams and each class you're interested in. This preserves your requirement of measuring occurrences of words given a target class over groups of documents. So, to be a bit more concrete, you could split your documents into two sets (good and bad), and then tokenize your documents to obtain individual terms. From here you could really choose whichever term weighting scheme you like (TF, TF normalized against the length of the document, TF-IDF) and measure your correlation statistic between all these unigrams and the class of interest. You could then produce a ranking based on the correlation coefficients for each term, and take the top-k terms. Some correlation statistics you might try could be Chi-squared (which would measure ""lack of independence"" between terms and a class). There's also a nice implementation of Chi-squared test for feature selection in Python's Scikit-Learn machine learning library that may be a place to start for this task. Hopefully, that helps!";;;
5067;2;2015-02-06T05:51:56.090;;Johns Hopkins University as a set of course on Coursea that is gear on Data Science. Here is the link to the classes https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop. You can also take the classes for free.This is a set of 9 classes that would give you a good foundation to build-on and to start a career.;;;
5068;2;2015-02-06T06:11:35.397;;Data science, being a new term, covers a broad spectrum of jobs. At one end you are expected to write production code. At the other end you do statistics in packaged software. They also call such people statisticians or analysts. So decide what you enjoy doing before you leap. If you just want to analyze data, you could definitely get by with R or python as long as you're mathematically proficient. I find that in these kind of jobs, your communication and social skills matter too, since you have to explain the data to executives and the like.;;;
5070;2;2015-02-06T09:06:02.607;;"First of all, the fact that you have known some Java, even ten years ago, already means that you don't ""know nothing about programming"" (I suggest you update the title of your question to reflect that - change ""nothing"" to ""a little""). I'd like to make several points, which I hope will be useful to you.In terms of the level of programming proficiency, which is expected (needed) for a data scientist, the following popular definition says it all: A data scientist is someone who is better at statistics than any  software engineer and better at software engineering than any  statistician.Another perspective on the role of programming abilities in a data scientist's skill set can be found in a popular visual representation of data science, using Venn diagrams. The original data science Venn diagram was presented by data scientist Drew Conway (see this blog post):Since its original introduction, the original diagram was modified by various people for various reasons. The two interesting adaptations are for data science in the social sciences domain (http://www.datascienceassn.org/content/fourth-bubble-data-science-venn-diagram-social-sciences), as well as data science Venn diagram V2.0, where data science is represented not as an intersection of knowledge domains, but as their union (http://www.anlytcs.com/2014/01/data-science-venn-diagram-v20.html). Another very interesting and useful visual perspective of data science skill set, also based on Venn diagram, is the following Gartner's diagram, mapping specific skills to business intelligence (BI) or business analytics knowledge domains:An alternative perspective for a data scientist's skill set and domain knowledge is a taxonomy of data scientists, such as this taxonomy, which classifies data scientists, according to their focus (or the strongest skill set): mathematics, data engineering, machine learning, business, software engineering, visualization, spacial data (GIS) or others.If you're curious about the meaning of the ""Danger Zone"" in the original data science Venn diagram, this Quora discussion, containing, among other nice answers, also an answer by the original diagram's author, can be very helpful.If you're interested in learning about a range of skills and knowledge domains, useful for a data scientist, check this open source curriculum for learning data science: http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go. Of course, popular and research papers, lectures on YouTube, MOOC courses, online and offline bootcamps as well as a wealth of other resources is only an Internet search away.Finally, a note on programming languages for data science. I think that it is important to understand that this aspect is really of secondary importance. The focus should be on two words, which the term ""data science"" consists of: data and science. Focus on data means that it is important to think about data science (or BI, or analytics) tasks in terms of the corresponding domain knowledge as well as to pay attention to data quality and representativeness. Focus on science means adhering to scientific approaches to data collection and analysis, of which reproducibility plays an important role. A programming language for data science is just a tool and, therefore, should be chosen to match the task at hand. Python and R represent very good and the most popular programming languages and environments for a data scientist, however, you should be aware of other options (tool set).";;;
5071;2;2015-02-06T09:41:54.033;;The problem is with division on python 2.x.  In python 2.x, division involving two integers produces an integer result.  So 1/2==0.  Python 3.x does not have this problem, 1/2==.5.There are two ways to avoid this.  First, you can always convert one value you a float.  So 1./2==0.5 and 1/2.==0.5.  However, you have to remember to do this everywhere, and if you forget it can lead to hard-to-find errors.The more reliable method is to always put this at the top of your code: from __future__ import division.  This will switch python 2.x to the python 3 behavior, so 1/2==.5.  In python 3.x it does nothing, so it also makes your code python 3.x compatible in this regard.;;;
5073;1;2015-02-06T17:52:05.363;Question and Answer Chatbot for Customer Support;"I want to build a chatbot that serves as a first line customer support on a  retail website. I have a large log of chat sessions between customers and support professionals that I can use. I am wondering what is the best way to go about building this chatbot. Here are some ideas I have looked into;The first thing I did was to breakdown the chat logs into Q&A pairs. Treat each question as a document, compute a term-document matrix and use it to retrieve the most similar question when a customer asks sometihng. The idea was to then simply pick the answer given to the question by the human (with some modifications). However, this gives really abysmal results and does not match to previous questions very well. Even if I get this approach to work well, we would be limited to the questions that already been asked. Another thing I thought about was to modify something like ALICE and write some AIML for customer support related QA. However, that would take a lot of very precise AIML writint to work well. This solution would not be able to scale to other languages, which is something I want to do.Another idea I have is to try and cluster the answers given by the customer support staff (they are more likely to be aligned compared to the questions asked by customers). This would give me a sense of what questions are similar. Then I can use something like LSA on the questions and fall back to the first approach.Yet another way is to build an ontology of the domain specific knowledge and given a question decide which category the question falls into. The questions from the chat logs can then be mapped to the ontology and I could train classifiers for mapping questions to different knowledge areas. Once I reach the leaf nodes, I can give back pre-defined answers. However, my reservation is that mapping chat logs to the ontology would be very tedious. Is there a good way to map the existing QA to the ontology?";[education, open-source];151;2
5074;1;2015-02-06T18:13:16.210;MPI, MapReduce, or Spark for complex datasets and processing;"I have 2 data files: the first one is a database, potentially very large; the second one contains queries I want to answer. My program pipeline is processing the database to get some information first, and then use that same information to answer the queries. Although the number of queries is not big, processing each query takes a long time. So I want to give each worker some queries to answer, then combine all the answers together into one.This sounds like a MapReduce job. But to answer each query, the worker also needs to use the processed information from the database, and I'm not sure how to do this with MapReduce. I'm new to parallel programming and just heard of MPI and Spark.Can you help me to choose an appropriate one?Please ask if the description is not clear.";[education, open-source];74;
5075;1;2015-02-06T22:30:53.073;Why does Slater's condition hold for the SVM quadratic programming problem?;Slater's condition is that there exists an x such that the optimisation constraints are strictly feasible. The KKT conditions are always sufficient, but are also necessary when Slate's condition holds. Why does the condition hold for the SVM optimisation problem?;[education, open-source];35;
5077;2;2015-02-07T02:41:06.783;;If I understand your question correctly. What you want to do is Write a sql query against to a database to get the processed/aggregated results first.Execute several queries in parallel.The first thing first, if you want to run MapReduce programs on your data, you need to distribute your data. In addition, MapReduce doesn't run your queries to different nodes separately. If your $query_{A}$ runs on $data_{A}$, what we do here is to divide $data_{A}$ to different nodes and run $query_{A}$ on these nodes on $PartOfData_{A}$ in parallel. By the way, you can use Sqoop to migrate the data from your DB to HDFS(it is used to store data separately).Here are steps for you:migrate all/part of your data to HDFStransform your queries to MapReduce version program;;;
5078;2;2015-02-07T07:37:14.040;;The reason your OCR program is not recognizing anything is that the font size is very small. If you resize your image to increase its size, you will obtain better results. For example, using ImageMagick to apply a fixed threshold to remove the background on your first image and increase its size:convert -density 500 -threshold 40% 29-1891a.gif -resize 250% output.tiffAfter this, tesseract does a reasonable job:tesseract output.tiff output test.config I included a configuration file test.config to tesseract in order to restrict the permitted characters. In this way, we don't obtain mistaken unicode characters in our text.test.configtessedit_char_whitelist ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-This is the result:HOWARD COUNTY 10011607NALL JOHN E 0-686196 1 LT F05 DALE MARVIN E 30037267 PVT DON SANSJNG DAVID 10505969 PFCNIXSON EONOND R 10077072 CPL KIA DANIEL NILBORN N 30637064 PVT FOB SAUNDER6 NEWTON L 0-401709 CAPTNOBLES STEPNEN E 30502791 PVT 0N0 DAVIS IR D 30433764 PVT 0N0 SNELTON CHARLES R 14001742 VTOSBURN SILLY F 02008518 2 LT DNB BAVIS JINKIE L 18214208 5 SC KIA SHELTON CHARLIE 33423953 7335PELTON J L 30105473 5 SC RNA 0 DEAN CHARLES 02066314 3 LT 0N0 SMERAN CARLTON A JR 0-510441 2 LTGUEZADA FRANK 5 10014939 PFC KIA IDIXON JOHN N 6202114 PVT 00R SIBLEV JESSE 30037241 PFCRAMIREZ JOSE T 30347914 PPC KIA DREADIN RAYMOND 6950964 PVT KIA SLATER JERONE E JR 0-605266 1 LTOSE THO S 0 30345100 PPC NTA - DUNN GARLAND J 30037202 PVT 0N0 SMITH 00 SE 30509104 PPCR035 ORREN C 16015376 PVT ONE 1 ELDER GERALD P 0-387706 2 LT KIA SNITN JASPER 7 33037355 7 SCRUTLEDCE CARL R 38107116 PVTELPNAA JEARHER MARVIN R 30279097 CPL DON SMITN JERALO D 10005000 CPLSCUDDAY BERNIE L 0-603906 1 LT KIA - FINDLEY PAUL A 50536010 PFC OMB SMITH NELDON A 5550735 PFCSNITH ROBERT L 10015524 507 ONE- ELINC ROY T I 0-724730 CAPT F05 S THEY 4295431 5 5SNITH TRAVIS L 30670530 PVT IDUN FORD HERRELL E 0-665672 CAPT E00 STEPN NSON JESSIE P 10006200 PVTSNEED ROY A 13076927 S 80 ONE GARRETT TOMMY 30220694 PFC KIA SULLIVAN 31 01525125 2 LSOUTN CARL 0 JR 30343379 PVT OMB CATLOR R T 20012665 SOT KIA SNINDELL VERL O 30037252 7005STEVENS JANESCO 10015557 PVT DNR CLASSCOCN CNARLES J 6571000 5 SO ONO TAYLOR ARTHUR V 33531479 PFCSTENA D JO 0-723036 2 LT POL 7 00 A 10007027 PVT DOM ITOHN E N A JR 30035251 5 SOUTTON ARVI 6360431 AV C 0N6 COSSETT JANES N 30436055 CPL KIAI ALKER J NEE H 30117597 SOTIALBOTT CHARLES 9 38342083 PFC KIA GRESN J 18126923 5 SO KIA NALKER RALPH L 203126071PPCTUCKER JAMES 30848763 PVT MIA CRIFFIS WILLIAM J 6270119 PFC 0N0 NALLACE JESSE A 30043713 T SC-TUCKER STERLING P 33341143 RPCI KIA CROSS ELERY C 0-390713 3 LT DNR NARREN CHARLES D 30110253 5 50NAOSNORTN PAUL P 30345079 5 SC ONE HAKNONBS ROBERT M 38433940 3 SC KIA MASHINCTON HENRY 30299100 PFCNALNER JAN S H JR 0-696023 2 LT P0 HANDLSY JAMES J JR 0-417955 1 LT ONE EEMS NINPRED E 20017930 CPLNEBB GLEN 30343104 PVT -K1 7 NANET FARRELL 8 30012597 SCT KIA NNITE DE NIS 10124415 PFCNRAT JAMES H 30067743 TE05 KIA NARCIE PRANCNARD 6370053 1 SC NIA NMITE MARVIN J 30430974 PFCNRIOHT NAILAND 0 30609570 PFC KIA NARKEY VEWCEN 30424190 PVT KIA NOODARD BILLY E 510217472 T 5HARRIS PRBS 38111537 PFC OKIA NRICNT EILL 0-725355 CAPTI HARRISO DUKE N JR 18055432 AV 0 ONE YOST TNURNAN R 6273153 PVTHENDRIX JOHN N JR 10217563 PVT 0N8HICKERSDN JACK 04431540 1 LT 9N8HUDSPETH COUNTY 3536305354 A CHEW 55555 W9 0776 A -- JOHNSTON LONNIE O 0-754906 3 LT KIA H INSON COUNTYONES GEORBE N 01173315 1 LT ONE UTCHJUMPER ISAAC H 30605960 PVT KIA IA9515 HAN C -7 7 3 LT N5 LONG OSCAR D 30812573 5 SC KIA ALEXANDER BOYD A 0-690739 2 LTEARDNERNE3SEPN H 3L5E3355 3 L7 335 LVTLE JOHN E 30433946 PVT KIA EALDNIN JANES H 0-407114 CAPTNORALES ALFREDO L 18015539 3 CG P05 MACK HULET 00057345 1 LT NIA EICCERSTAPP C N 34530001 PC54N1952 LOCAS N 39441440 PFC K14 NAJORS TRUSTT J 00410046 2 LT KIA BRITTON JAMES H 18036279 8 SCROBLES VICENTE 30570733 PFC 0N5 MASON DICE 30203437 CPL NIA EULLARD CAR 5 30607623 PVTSANCNE2 ANGEL R 38310341 PVT ONE MASON HALTER P 0-671673 3 LT NIA C IN ROBE T E 33105799 TECSVALLES RENICIO 0 30441430 PIC DON NASSEV JESSE D 30435424 PVT KIA COHA K LL03 N 10077256 PVTNC CLENDON J H 30117530 PPC ONE EVANS L 0 37392406 TECSNC HNORTER C R 30300391 PVT KIA FIRLEY MARVIN L 38572402 PVT4 HILLNAN ODEAN R T-000345 FL 0 DNB IPIELDS JAKES 35711546 PVTHIL10N JACKSON 7 13030591 PFC 933 ORADDY ROY L 39112920 PFCHUNT COUNTY HOORE ARLON D 6295620 9 SC KIA GRANT 30572401 PVTMORRISON DURHARD D 0-519411 3 LT KIA HANNA EVERETT T 10104243 S SC- LNULLINS GERALD D 6295622 T 50 NIA HANSARD SAMUEL 2 04754619 1 LTNEAL HOMER M 30609200 PVT KIA THARVEY JOHN B 0-602116 2 LTALANIS VICENTE 30894642 VT KIA MEAL RAVN 39 9 1759550 3 NECOAL MARTIN J 6396940 PFCALLEN TRUHAN L 0-123936 1 LT KIA NELSON 1 0 30431073 PPET ONE HILL RAYMONO 8 30050142 PVTALLEY NILBUR K 01703993 2 LT KIA NELSON TRAVIS C 38204649 PVT DOH HOP JACK H D 30342393 PFCBENCH CLARENCE A 30003600 PFC KIA INEWLAND OTIS T T-000346 FL 0 F051 HDFF ROBERT C 30345133 T 50BENNETT EVERETT N 0-692130 LT 0N5 NICNOLSON DALE- 10005042 PVT KIA JONES JANES 0 30343505 PVTI IA NIXON LOYD 5041674 TECS KIA KAPPELMN MC 0-360687 CAPTBLACKHELL E C 38357321 307 KIA PARKER N 33130N V 52959524 3 L7 KIA KECANS TIM JR 38342898 FCERITT BASIL JR 36002307 PFC K14 PATTERSON THOMAS H 18136913 SCT 9N5 KENNINER EARL 0 30401408 SGTBROHN SHOE 8 38634396 PFC KIA PERRI JA 5 0-562333 1 LT KIA LANTRON EDWARD L 19190302 5 SOBURNS VIRCIL P 38035701 PRC KIA PETTICREH FRED 0 16215700 S 50 NIA LESNER ROLLIE H 30050735 AV CBUTLER GEORGE A 37259433 1 SC KIA PHILLIPS C L JR 0-431139 CAPT KIA NC CARTY TOURNAN P 30711042 PFCCAMERON VANDSLL C 30634570 PPCI KIA PILCRTN CLYDE 30115026 V NIA NC GLENDON JACK 30330395 PFCCARTER LEO RD K 30119959 AV 0 ONE PO D EUCENE N 4 30431074 PRC KIA NC NINNET K E 30340151 PVTCARTER OILLIAH F 38685535 PFC 0N8 PRESLEY NILLIAN H 30037547 PVT 0N5 NC QUEEN JAMES Y 02055059 2 LTCREEK LOTD 6379416 A SC DNB PRICE PREDRICK P 30409331 CPL 100W NOTEN GEORGE N 30304695 PFCCLARK JOHN 18317907 CPL KIA PURCELL SAMUEL N 30017905 PVT 0N5 PANNELL NOLENIA 6227623 1 SCCOLLINS RAYMOND N 30137040 PFC OOH RAILIPF NARDEN A 30037006 CPL 00H PIERCE FELIX 0 30335390 5 SCCREAKER SAH BL N 18006101 PVT KIA HAYNES NILLIAN T 7-954739 PL 0 9N5 PIETZSCH JAI E 0-426961 2 LTCRIDER NARLAN 0 10154794 SCT KIA REED JDRN L 30529414 PVT POL RTER NALLACE N JR 38193560 CPLDALE CHARLEY 8 I 38049500 3 SC KIA ROI NILLIS 5 01393956 1 LT KIA PRESCOTT DAVID L JR 0-707191 2 LT05 231D KZXXN DNNNNH 2XXLA-A- Q233UR R205535As you can see, there are many mistakes, but at least we are obtaining reasonable results. In any case, you don't want to do this because even if you were able to get perfect recognition, some of your lines would be placed in the wrong county because tesseract is looking at your document as a big paragraph. I recommend you to use the vertical lines to segment your image into 3 parts and then preprocess each one of them. You can even try to concatenate the parts vertically and perform OCR in a single page. This also applies to the second image.By the way, the resolution of your image is not great, so if you can get a better image, that is going to make a big difference. ;;;
5079;1;2015-02-07T14:21:42.543;Item based recommender using SVD;"I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):1.00 0.88 0.96 0.99 0.88 1.00 0.99 0.96 0.96 0.99 1.00 0.86 0.99 0.96 0.86 1.00 I need to implement recommender which, for a set of items, recommends a new set of items.I was thinking about using SVD to reduce the items to n-dimensional space, let's say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.For a base set of items (which can get quite big), I hope I could calculate an average of their vectors, and use it for search.Is this a good idea? What is this procedure called? And can it be done in Mahout?EDIT: This is my code so far:ItemSimilarity similarity = new LogLikelihoodSimilarity(model);Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);// copy similarities to a matrixfor (int i = 0; i < NUM_ITEMS; i++) {        double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));        for (int j = 0; j < NUM_ITEMS; j++) {            m.setQuick(i, j, similar[j]);    }}Matrix v = new SingularValueDecomposition(m).getV();Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);The problem is, SVD is taking forever for NUM_ITEMS > 30. I don't know if there is an issue with data, or with SVD implementation I'm using. The matrix m is symmetrical, could that be an issue? I tried googling ""demean matrix mahout"" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?EDIT 2:The problem was the matrix contained a few NaN values, that's why SVD was taking infinite time. After replacing these with 0.0 it works fine for 1000 x 1000 matrix. And my recommendations are working like a charm. I'll still need compute SVD of 20x more rows and columns. If anyone knows what's the easiest way to compute (approximate) SVD of 20.000 x 20.000 dense matrix, probably through some cloud parallel service (?), please let me know.PS. Thanks for help!";[education, open-source];135;1
5080;1;2015-02-07T15:37:47.327;Working as data scientist for a nonprofit company;Hello I work as data scientist for a private company.I am interested in working for a nonprofit company, such as a research institute (public or private) or a company that takes care of  issues such as  environment, public health,  social improvements. Even an internet company like Wikipedia can be interesting.Does anybody know if nonprofit companies hire data scientists?;[education, open-source];356;1
5081;2;2015-02-08T01:47:57.963;;I see at least five ways to approach this problem of finding a data scientist position/work specifically at non-profit, non-governmental or similar organizations, as I describe below. I hope that this is helpful.First, and the most obvious, way is to search major job portals, such as indeed.com, dice.com, monster.com, CareerBuilder, Glassdoor and others, for data scientist or similar positions, such as data analyst, data engineer, quantitative analyst, statistical modeler, or even market researcher.The second and also obvious way is to tap your professional social networks and research or inquire your contacts for any potential data science work opportunities in the areas of your interests.The third way is to search web sites, focused on the non-profit and related topics (they usually have a job listings or similar section) as well as specialized non-profit job portals. For example: http://encore.org, http://idealist.org, http://bridgespan.org, http://cgcareers.org, http://opportunityknocks.org, http://foundationcenter.org, http://thenonprofittimes.com, http://philanthropy.com, http://ynpn.org, http://philanthropyjournal.org, http://nonprofit-jobs.org, CoF Jobs (section isn't easy to find), http://careers.councilofnonprofits.org, http://nonprofittalentmatch.com.The fourth way is to perform Internet search and research on non-profit or similar organizations that you might be interested in working with (criteria might vary from organization's size to industry focus or geographical locations). Based on the information presented  on their websites, make notes and then approach those organizations, directly inquiring or applying for positions of interest.The fifth way is to consider various non-profit, for-profit and otherwise social good-themed data science-focused organizations, initiatives and Kaggle-like competitions, such as DataKind, DrivenData, DataLook and Data Science for Social Good.;;;
5082;2;2015-02-08T03:29:06.623;;" I need to implement recommender which, for a set of items, recommends a new set of items. Is this a good idea?Have you looked into Association Rules Mining? If you're open to other procedures this one is first one that came to my mind for recommenders based on sets of items. For those not familiar, this is simple method for retail shops to determine ""75% of customers who bought A, B also bought C"". Within these algos, the Apriori algo is straight-forward, easy to implement, and may get you what you need.";;;
5084;2;2015-02-08T16:05:58.517;;Assuming the central node is at level 0 (root), this graph becomes a tree. Now it is easy reason about the different types of trees that can be formed and different types of social networks that can be modelled. In order for the graph to be a tree, communication between nodes has to be restricted, such that the tree structure is not violated (e.g. two nodes on the same level cannot communicate). I can't think of any social network where communication is naturally restricted in this way, but such a social network can be created artificially.Examples of such artificial social networks would commonly follow a chain-of-command structure. For example in a military setting, the official information flows would have a tree structure.   ;;;
5085;1;2015-02-08T18:05:34.020;Should I use epochs > 1 when training data is unlimited?;If I have virtually endless training data (it's synthesized) is there still purpose in having epochs? I.e. training on the same samples multiple times?;[education, open-source];33;
5086;2;2015-02-08T21:03:52.657;;If the data is unlimited, how would you have an epoch to begin with? For example, if you are analyzing tweets, you could never finish an epoch will all the tweets, since there will be an endless supply of new tweets. A much better approach will be to do some online or streaming learning.Would it make sense to create a subset by ignoring new incoming tweets or data in general?That really depends on your problem. For example if you have a datastream of tweets, or any other where new data is being produced in real time, you would miss out any trends or patterns that emerged after you sampled from the stream. Are those missed patterns relevant to your problem? They may or may not be.    ;;;
5087;2;2015-02-08T21:17:50.990;;"This was intended as a comment.I think this question is not receiving enough attention because it is too difficult to answer without trying several approaches first. Your third idea is a nice one, but LSA is most likely not going to be able to help you choose clusters with the necessary granularity. For example, you may get a cluster of ""broken things"" but not to the specificity that is required to be helpful to the customer. A similar approach would be using Hierarchical Dirichlet Processes, but again, you may not get the clusters that you want.I wouldn't be so eager to try your fourth option because it requires a lot of work and I'm not sure you're going to get great results unless your customer support deals with very well-defined questions and in that case, you're going to have to build your own ontology from scratch.In your situation, I would be curious if I can get information about the inner workings of Watson (obviously, the purpose is only to have a general idea of some steps that are sensible enough to try.) Here is a descriptive image that could be helpful from Wikipedia:The main idea is to segment questions into its components (using a regular chunking), then come up with several possibilities (hypothesis generation) of what the question might refer to (maybe this step can be implemented as a clustering). The third stage is rating the previous hypothesis based on available information. Finally, you choose the more likely hypothesis. There is a feedback involved in this process using sample questions and their answers. It would be interesting to see if the rating stage can be modeled in terms of a Bayesian or an incremental approach to allow for a feedback.";;;
5088;1;2015-02-09T00:18:46.427;Error when using MAX in Apache Pig (Hadoop);"I am trying to calculate maximum values for different groups in a relation in Pig. The relation has three columns patientid, featureid and featurevalue (all int). I group the relation based on featureid and want to calculate the max feature value of each group, heres the code:grpd = GROUP features BY featureid;DUMP grpd;temp = FOREACH grpd GENERATE $0 as featureid, MAX($1.featurevalue) as val;Its  giving me  Invalid scalar projection: grpd Exception. I read on different forums that MAX takes in a ""bag"" format for such functions, but when I take the dump of grpd, it shows me a bag format. Here's a small part of the output from the dump:(5662,{(22579,5662,1)})(5663,{(28331,5663,1),(2624,5663,1)})(5664,{(27591,5664,1)})(5665,{(30217,5665,1),(31526,5665,1)})(5666,{(27783,5666,1),(30983,5666,1),(32424,5666,1),(28064,5666,1),(28932,5666,1)})(5667,{(31257,5667,1),(27281,5667,1)})(5669,{(31041,5669,1)})Whats the issue ?";[education, open-source];81;
5089;2;2015-02-09T00:34:26.520;;"The issue was with column addressing, heres the correct working code:grpd = GROUP features BY featureid;temp = FOREACH grpd GENERATE group as featureid, MAX(features.featurevalue) as val;";;;
5091;1;2015-02-09T17:28:54.390;Extract company names/job titles from free text;"I have a complete Hadoop platform with HDFS, MR, Hive, PIG, Hbase, etc., Python, R, Java. All data sets have a large size.The data set A, describing the jobs of people working in a company, is composed of the following fields:Id Person: a unique alphanumeric identifier per person.Start Date: a date format iso entry in the post  End Date: iso size release date of the position. If the date is not given, it is the current positionJob Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with Google, Commercial Manager at [missing text] , Manager at googole ...My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?Thank you in advance";[education, open-source];151;
5092;1;2015-02-09T23:22:05.547;design pattern for extracting features;I am looking for a design pattern that is relevant to a module that extracts features.I want to define a certain number of features over my data points, and then according to the performance and the feature selection, I may want to remove some of them and add others, and also I may want to consider any subsets of them to test.What is a good design pattern to do that? Did I miss something obvious? I am neither an engineer nor a developer, so I never study such things but I understand that it could help me a lot!Thanks for any help,;[education, open-source];40;
5093;1;2015-02-10T01:04:15.963;InterquartileRange takes up most instances in data set;I'm very new to this community, so please overlook my noobness.I have a data set with 2948 instances and I tried to remove outliers using InterquartileRange filter in Weka. The issue is that the number of 'YES' instances in ExtremeValues and Outliers takes up to 2947 and 2946 respectively. In other words, all my data are considered outliers. What does this say about my data set? Or am I not meant to perform IQR on this data, if so, is there other algorithms to identify outliers other than IQR?  And how would one perform regression on such a data set?Thank you.;[education, open-source];56;
5094;2;2015-02-10T02:06:19.957;;"I think what you want is to extract company names from ""Job Title"". In natural language process, we call this kind of research as ""Name Entity Recognition(NER)"". You can try to use  Stanford Named Entity Recognizer (NER)[http://nlp.stanford.edu/software/CRF-NER.shtml]. Stanford NER performs very well on English contents and there are lots packages for many programming language: UIMA: Florian Laws made a Stanford NER UIMA annotator using a modified version of Stanford NER, which is available on his homepage. [Old version.] Perl: Kieren Diment has written Text-NLP-Stanford-EntityExtract, a Perl module that provides an interface to Stanford NER running as a server. Ruby: tiendung has written a Ruby Binding for the Stanford POS tagger and Named Entity Recognizer. Python: Dat Hoang wrote pyner, a Python interface to Stanford NER. [Old version.] NLTK (2.0+) contains an interface to Stanford NER written by Nitin Madnani: documentation (note: set the character encoding or you get ASCII by default!), code, on Github. F#/C#/.NET: Sergey Tihon has ported Stanford NER to F# (and other .NET languages, such as C#), using IKVM. See also pages on: GitHub and NuGet. PHP: PHP-Stanford-NLP. Supports POS Tagger, NER, Parser. By Anthony Gentile (agentile).If you are not satisfied with the performance of Stanford NER, you can also train you own models to extract company names by crawl company names from several popular sites with company names, such as Linkedin/Facebook/Glassdoor...etc";;;
5095;2;2015-02-10T05:36:05.493;;"While @Ben's answer is nice and partially introduces what should be done first with a newly cleaned data set, I feel that the approach is important enough to have its name presented loud and clear: exploratory data analysis (EDA). Therefore, the short answer to your question is that the first step should be EDA.This suggestion is supported by most researchers, regardless of their knowledge domain or type of study. Here is how the father of EDA presents his thoughts on the subject (Tukey, 1977, p. 1-3; emphasis mine): Exploratory Data Analysis (EDA) is detective work – numerical  detective work – or counting detective work – or graphical detective  work ... unless exploratory data analysis uncovers indications,  usually quantitative ones, there is likely to be nothing for  confirmatory data analysis to consider ... [it] can never be the whole  story, but nothing else can serve as the foundation stone - as the  first step.There is an enormous amount of information on approaches, guidelines and procedures for performing EDA. Potential starting points might include EDA page on the NIST's Engineering Statistics Handbook website, EDA pages on the EPA's website, corresponding chapter  from the book ""Experimental Design for Behavioral and Social Sciences"" and a survey research paper on EDA by Begrens (1997), among many others. It is interesting to note that some sources include less traditional methods into EDA toolset, such as dimensionality reduction and clustering (for example, see the description of this research seminar). While some of the EDA approaches and methods are relatively simple, overall EDA is both art and science, as it combines unstructured (creative) and structured approaches. This aspect is especially important to recognize, as big data exponentially increases complexity of data analyses, including EDA.ReferencesBehrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131-160.Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.NOTES: For those interested in the Tukey's classic, it is available on Amazon. Various MOOCs on EDA are also available, for example this one and this one (both are R-focused).";;;
5096;2;2015-02-10T07:09:28.237;;When I think about how to implement a certain ML or data mining process in an OOP (like Java), I usually go to see how smarter people than me designed their system.In this case, I'd see how Weka, RapidMiner, JAVA ML or others decided to tackle this problem.In your case of feature/attribute selection, i'm adding a link to Weka's API.If you want to better understand how it's done, you should download Weka and play with the source code.http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/AttributeSelectedClassifier.htmlWithout getting into specific implementations, I think that a linked list of any sort would do the trick, since you want to select a subset of all features.;;;
5097;1;2015-02-10T12:36:48.250;Python for data analytics;What are some data analytic package & feature in python which helps do data analytic?;[education, open-source];56;1
5098;2;2015-02-10T15:59:19.010;;You're looking for this answer: https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists;;;
5099;2;2015-02-10T16:26:19.457;;I think the key here is what you believe to be dependent variables. You mentioned, for instance, a three month rest. Encoding the rest between races is likely to be a better idea than simply encoding the date of the rest as a lot of the date is redundant to what is actually being said. As with any machine learning algorithm, representation of the data is key and in many ways, the actual algorithm applied is less important.  ;;;
5101;2;2015-02-10T21:13:37.107;;It might be a good idea to start with voluntary work and see if that leads to payed position. DataKind, that has been mentioned above, is where I would start, especially if you live in England, since you can register your interest online and even do work with them as a data scientist for a weekend in a meetup event. There are also various meetups in many cities where it's likely to find people working on nonprofit companies and expand your network. ;;;
5102;1;2015-02-10T21:20:38.437;Extraction string using Pig Regex;I have a complete Hadoop platform with HDFS, MR, Hive, PIG, Hbase, etc., Python, R, Java. All data sets have a large size.The data set A, describing the jobs of people working in a company, is composed of the following fields:Id Person: a unique alphanumeric identifier per person.Start Date: a date format iso entry in the postEnd Date: iso size release date of the position. If the date is not given, it is the current positionJob Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with Google, Commercial Manager at [missing text] , Manager at googole ...My question is how can I extract copany name from job Title using Pig Regex?Thank you in advance;[education, open-source];96;
5103;2;2015-02-10T23:18:18.657;;Both Storm and Spark are great tools. It depends on your use-case. Do you want to quickly parse huge stream of data and store it into a database? Use Storm (e.g. counting tweets).Training a classifier on a stream of data would be a task suitable for Spark. There's a data window on which you're working and it will take a while.I haven't tried Flink, but it looks more similar to Spark. Spark has general concept of RDD (Resilient Distributed Datasets) that can be used also for graphs, huge matrices etc.If you want to write a word count, you any of them. But who wants a word count (except from hello world tutorials)? ;;;
5104;2;2015-02-11T06:13:12.223;;"Without a sample of your data, it's unclear what's the structure of your data and what tool is suitable to process it.Here are some blind recommendations based on my experience:If you just need some flexibilty parsing the text record, such as variable repeat number of certain field, or conditional parsing of fields, then you should check out this python library: http://construct.readthedocs.org/en/latest/it allows you to first define a hirachcal structure of your data, and then apply this structure to parse information from a text file. It's especial useful when parsing binary files.If you're looking for an algorithm that can actually ""understand"" your text data and ""infer"" the structure in a smart way. Then you might want to try graph based approach: http://kavita-ganesan.com/opinosis";;;
5106;2;2015-02-11T07:03:08.857;;I am also a newbie to data analytic, though I have done some analytic with weka. I can suggest you some things.1) Have you checked your data distribution for the field you are applying IQR for? If the distribution is normal then only IQR will be effective.2) You haven't explained about your data. Such as whether it's labelled or unlabelled. If it's unlabelled you can apply clustering & see what data lie outside.I hope it helps :).;;;
5107;2;2015-02-11T07:56:48.717;;No, no purpose other than saving data. A fresh sample is always better than a used one.The only situation I can think of in which having epochs would make sense would be if the synthesis process would be really time consuming per example.;;;
5108;1;2015-02-11T08:00:52.377;Reasons and prevention of trivial (and less trivial) misclassification errors?;"I was not sure about posting this question with mentioning the name of the company, which I quite respect and admire. However, I've figured that a wider exposure might help the team to fix this and similar problems faster as well as increase the quality of the machine learning (ML) engine of their website.The problem exposes itself by too many occurrences of a quite trivial misclassification error on Amazon's book categories classification (which I'm a frequent visitor of). In the following example, the underlying reason of such behavior is quite clear, but in other cases the reasons might be different. I am curious about what could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems. Without much further ado, here's how the problem appears in real life.I was reviewing some books, related to transitioning from graduate programs (Ph.D., in particular) to work environment in academia. Among several other books, I ran across the following one:So far, so good. However, let's scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out categories, relevant to the book's discipline, topic and contents. How surprised was I (and that's an understatement!) to see the following result of Amazon.com's sophisticated ML engine and algorithms:Clearly, the only fuzzy fact that connects this book with the subject ""Audiology and Speech Pathology"" (!) is IMHO the author's last name (Boice), which, is close to the word ""voice"". If my guess is correct, Amazon's ML engine, for some reason, decided to take into account the book's lexicographical attribute instead of the book's most important and most relevant attributes, such as title, topic and contents. I've seen multiple occurrences of similar absolutely incorrect ML-based decision making on Amazon.com and some other websites. So, hopefully my question makes sense as well as interesting and important enough to spark a discussion: What could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems? (Any related thoughts will also be appreciated.)";[education, open-source];44;
5109;1;2015-02-11T10:48:51.903;Machine Learning on financial big data;"Disclaimer: although I know some things about big data and am currently learning some other things about machine learning, the specific area that I wish to study is vague, or at least appears vague to me now. I'll do my best to describe it, but this question could still be categorised as too vague or not really a question. Hopefully, I'll be able to reword it more precisely once I get a reaction.So,I have some experience with Hadoop and the Hadoop stack (gained via using CDH), and I'm reading a book about Mahout, which is a collection of machine learning libraries. I also think I know enough statistics to be able to comprehend the math behind the machine learning algorithms, and I have some experience with R.My ultimate goal is making a setup that would make trading predictions and deal with financial data in real time.I wonder if there're any materials that I can further read to help me understand ways of managing that problem; books, video tutorials and exercises with example datasets are all welcome.";[education, open-source];1122;5
5110;1;2015-02-11T11:53:30.890;Graph layout for a network of molecules;I am looking for a suitable graph representation where the nodes/vertices are molecules with 2 main variables: structural characteristic and a real number property (for example biological potency). The network is build up by molecules by matching structural properties and it can be rather complex with many clusters of high degree vertices (ie molecules that are structurally similar to each other). However the representation should give a clear picture of potencies, although high detail accuracy is not required: some sort of classification is adequate. I played with JUNG with a graph having edge weights as the change in potency and the ISOM layout thinking that it reserves (as much as possible) the edge weights but with not good results. Something like radial layout could be ideal, as it gives a clear picture of potency distribution, but this obviously would suffer from crowding of vertices that are closer to the center.  As, obviously, my knowledge is restricted, is there some layout that you would suggest and is practical for a non expert to implement? ;[education, open-source];39;
5111;2;2015-02-11T12:25:31.077;;"There are tons of materials on financial (big) data analysis that you can read and peruse. I'm not an expert in finance, but am curious about the field, especially in the context of data science and R. Therefore, the following are selected relevant resource suggestions that I have for you. I hope that they will be useful.Books: Financial analysis (general / non-R)Statistics and Finance: An Introduction;Statistical Models and Methods for Financial Markets.Books: Machine Learning in FinanceMachine Learning for Financial Engineering (!) - seems to be an edited collection of papers;Neural Networks in Finance: Gaining Predictive Edge in the Market.Books: Financial analysis with RStatistical Analysis of Financial Data in R;Statistics and Data Analysis for Financial Engineering;Financial Risk Modelling and Portfolio Optimization with RStatistics of Financial Markets: An Introduction (code in R and MATLAB).Academic JournalsAlgorithmic Finance (open access)Web sitesRMetricsQuantitative Finance on StackExchangeR Packagesthe above-mentioned RMetrics site (see this page for general description);CRAN Task Views, including Finance, Econometrics and several other Task Views.CompetitionsMODELOFF (The Financial Modeling World Championships)Educational ProgramsMS in Financial Engineering - Columbia University;Computational Finance - Hong Kong University.Blogs (Finance/R)Timely Portfolio;Systematic Investor;Money-making Mankind.";;;
5112;2;2015-02-11T13:25:28.473;;By looking at the ratings of the book, it looks like they are doing some sort of hierarchical clustering.First of all it's a book, then a text book, then in medicine & health science.This is a pretty difficult problem because:You don't know how many and which topics you have, and the topics are constantly changing.Since this is a clustering problem (or a semi-supervised problem in the better case), a closed feedback loop for these errors cannot be applied. Clustering is always more difficult than a supervised learning problem.Apparently they're doing a pretty good job in general. This book in particular is a tough one...;;;
5113;1;2015-02-11T13:26:27.753;Dividing percentage;"A book I'm now reading, ""Apache Mahout Cookbook"" by Pierro Giacomelli, states that  To avoid [this], you need to divide the vector files into two sets called the 80-20 split <...>  A good dividing percentage is shown to be 80% and 20%.Is there a strict statistical proof of this being the best percentage, or is it a euristic result?";[education, open-source];68;
5114;2;2015-02-11T13:56:30.847;;"If this is about splitting your data into training and testing data, then 80/20 is a common rule of thumb. An ""optimal"" split (which would need to be operationalized) would likely depend on your sample size, distributions and relationships between your variables.It is also common to split your data three ways (e.g., 60/20/20 - again rules of thumb), into a training set that you train your models on and a test set which you test your model on. You will iterate training and testing until you like the result. Then, and only then you apply the final model (trained on both the training and test set) on the third validation set. This avoids ""overfitting on the test set"".However, cross-validation is much better than a simple data split. Your textbook should also cover cross-validation. If it doesn't, get a better textbook.";;;
5115;1;2015-02-11T16:46:38.800;Calculating entropies of attributes;"Can you please show the step by step calculation of Entropy(Ssun)?  I do not understand how 0.918 is arrived at.I tried but I get the values as 0.521089678, 0.528771238, 0.521089678 for Sunny, Windy, Rainy.I was able to calculate the target entropy (Decision) correctly as = -(6/10)*log(6/10) + -(2/10)log(2/10) + -(1/10)log(1/10) + -(1/10)log(1/10) = 1.570950594I am totally stuck at the next step.  Request your help.Reference: http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.htmlPlease search for ""The first thing we need to do is work out which attribute will be put into the node at the top of our tree:"" to reach the line I am referring to.";[education, open-source];69;
5116;1;2015-02-11T18:31:31.193;How do I use Hierarchical Dirichlet Process (HDP) implementations (hdpfaster by C.Wang or hca by W.Buntine at mloss.org) to discover number of topics?;"Context of question:I want to find semantically similar documents in corpora. For that, I'm first trying Latent Dirichlet Allocation (LDA) with divergences (Hellinger, Kullback-Leibler, Jensen-Shannon) on the per document topic distributions. However, to find # of topics for my corpus ( a 948 document dataset, extracted from larger collection, where docs about the same story are humanly annotated), it was suggested to use HDP. Unfortunately, after MANY tries, I'm still unsure I'm using the packages correctly.More detailed:1) HDPFASTER continually increases the number of topics (in train.log file).  What is the stopping criterion (e.g. difference in avg. likelihood between successive iterations smaller than x? which x?) and/or how many cycles should I let it run? Afterwords, do I take as the correct # of topics the last line of ""train.log"" OR should I also test for minimum perplexity (It has also been suggested that Bayesian Information Criterion (BIC) might be a better measure) and choose the smallest? If the latter, should I test for ALL iterations?!  Is HDPFASTER's test feature the appropriate tool for this? Last, should I --sample_hyper? Do I just use a(alpha) as input to LDA's prior alpha, perhaps divided by a number (e.g. # of topics)? What about LDA's prior beta? 2) HCA continually decreases  the number of topics (in *.log file). When to stop? Do I accept the final # of topics in .log ( exp.ent =number0) OR do I search for the minimum perplexity of test set (as I assume it appears as number2 in ""log_2(perp)=number1,number2"" in .log throughout iterations), which ALWAYS appears VERY close to my initial max number of topics? Hyperparameters alpha,beta: do I sample using -D , -E? Which do I put as input to LDA's priors? Is it generally worth it?Generally: Is it possible that my dataset is just too small and/or 'biased/fragmented/incomplete', in the sense that each story has only 1-3 examples, while the diversity of the topics these stories discuss can be quite large? Should I just try and augment it with a larger 'homogeneous' dataset? ";[education, open-source];81;
5117;2;2015-02-11T18:32:14.230;;Consider the formula for Entropy:$E(S) = \sum_\limits{i=1}^{n}-p_{i}\log_2p_{i}$Expanding that summation for the four concept decision attributes for your problem gives$E(S) = -p_{cinema}\log_2\left(p_{cinema}\right) - p_{tennis}\log_2\left(p_{tennis}\right) - p_{stayin}\log_2\left(p_{stayin}\right) - p_{shopping}\log_2\left(p_{shopping}\right)$There are three observations where Weather=Sunny. Of those three, one has Decision=Cinema and two have Decision=Tennis. So for Weather=Sunny you have $p_{cinema}=\frac{1}{3}$, $p_{tennis}=\frac{2}{3}$, and other decision probabilities are zero. Plugging those values into the equation above gives$\begin{align}E(S_{Sun}) &= -\frac{1}{3}\log_2\left(\frac{1}{3}\right)-\frac{2}{3}\log_2\left(\frac{2}{3}\right) - \left(0\right)\cdot\log_2\left(0\right) - \left(0\right)\cdot\log_2\left(0\right) \\           &= 0.38998 + 0.52832 + 0 + 0 \\           &= 0.918\end{align}$;;;
5118;2;2015-02-11T23:31:20.127;;"Let me first explain the concept of entropy for decision trees:Entropy is a so called impurity measure for a set of elements. Impurity - being the opposite of purity - is referring to the distribution of the decision categories (or class labels) within the set. Initially, each row in the table is one element and your set of elements are all rows of your table. This set is called pure if it just consists of the same class label and it is called impure if it contains all different class labels in the same proportion. Translating into the values of entropy (remember being an impurity measure, the set has in the latter case the highest possible value and in the former case (just the same class label) the lowest value. The lowest possible value for entropy is 0.   So let's choose a visual respresentation to illustrate what really counts and what does not matter for the entropy. 1) You start off with the initial table which contains the following class labels:Please note that for computing the entropy none of the remaining attributes, attribute values matter, also the order of the rows don't matter (entropy is a measure for a set of elements)2) You convert the different class labels to different colors3) So instead of this table with the colored class labeled, you just draw a circle (""ball"") for each class label (=element)Now you throw all the balls in a bag. There you go! Your elements of a set are represented as balls in a bag.  If you open the bag and look into it, you might see just balls of the same color --> one pure color. The entropy says, it's not impure at all, I give it a zero! entropy = 0the same amount of balls in different colors --> that doesn't look pure at all, it's impure! The entropy assigns the highest possible value to this set (bag). The highest possible entropy value depends on the number of class labels. If you have just two, the maximum entropy is 1. In your case, you have 4 different class labels, so the maximum entropy would be e.g. if you had 12 balls and 3 balls of each color. The maximum entropy of a 4 class set is 2.none of the above. Then your entropy is between the two values. If one color is dominant then the entropy will be close to 0, if the colors are very mixed up, then it is close to the maximum (2 in your case).How does a decision tree use the entropy?Well, first you calculate the entropy of the whole set. That impurity is your reference. What your decision tree tries to achieve is to reduce the impurity of the whole set. So the information Gain for a given attribute is computed by taking the entropy of the whole set and subtracting it with the entropies of sets that are obtained by breaking the whole set into one piece per attribute category. To make sure a set with just a few elements don't get the same weight as a set with many elements the entropies of sets are multiplied by their relative frequency.Coming back to the example, for computing the Gain (S, parents) you need to compute the weighted entropies of the two sets: set 1 = all rows with category(parent) = yes and set 2 = all rows with category(parent) = no).Set 1 refers to five rows, all having the decision category = cinema(just 5 orange balls!)So the weight is 5 rows out of 10 rows = 5/10 = 0.5. And the entropy is 0 (totally pure) or by applying the formula:You have a sum over all class labels so n = 4. So you assign a class label to each of the i's. E.g.For i = 1 being cinema: - (5/5 * log(3/3) = - (1 * log(1)) = - (1*0) = 0For i = 2 being tennis: - (0/5 * log(0/3) = - (0 * log(0)) = 0For i = 3 being stay in: - (0/5 * log(0/3) = - (0 * log(0)) = 0For i = 4 being shopping: - (0/5 * log(0/3) = - (0 * log(0)) = 0So the sum of all four is 0. 0 times the weight 0.5 equals 0.Set 2 also refers to five rows, having the decision categories = 1*cinema, 2*tennis, 1*stay in, 1*shoppingSo the weight is also 5 rows out of 10 rows = 5/10 = 0.5. Applying the formula:For i = 1 being cinema: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46For i = 2 being tennis: - (2/5 * log(2/5) = - (0.4 * -1,32) = 0.53For i = 3 being stay in: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46For i = 4 being shopping: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46So the sum of all four is 1.92. 1.92 times the weight 0.5 = 0.96.Thus, the Gain for this attribute is: Initial entropy - weighted entropies for all attribute categories:1.57 - 0 - 0.96 = 0.61.For the other attributes you do the same calculation.";;;
5119;1;2015-02-12T07:00:38.160;Kaggle Titanic Survival Table an example of Naive Bayes?;"is the survival table classification method on the Kaggle Titanic dataset an example of an implementation of Naive Bayes ? I am asking because I am reading up on Naive Bayes and the basic idea is as follows:""Find out the probability of the previously unseen instancebelonging to each class, then simply pick the most probable class""The survival table (http://www.markhneedham.com/blog/tag/kaggle/)seems like an evaluation of the possibilities of survival given possible combinations of values of the chosen features and I'm wondering if it could be an example of Naive Bayes in another name. Can someone shed light on this ?";[education, open-source];265;
5120;1;2015-02-12T07:07:56.257;How to start the process of coming up with the predicted math score?;"I am working on a problem(non competition) from hacker rank https://www.hackerrank.com/challenges/predict-missing-gradeBasically you're given test data of a bunch of students of their scores in other subjects but math and you are to predict their score in math based off all their other test scores. Say you were passed data of{""SerialNumber"":1,""English"":1,""Physics"":2,""Chemistry"":3,""ComputerScience"":2}How would you go about generating that student's score in mathematics or coming up with a prediction engine to generate the math score? I know that's the whole point of this question but can someone give me a hint or a resource to go to so I can have a chance of figuring this out and actually get started? I really want to learn.";[education, open-source];51;
5121;1;2015-02-12T07:08:16.537;Applications and differences for Jaccard similarity and Cosine Similarity;Jaccard similarity and cosine similarity are two very common measurements while comparing item similarities. However, I am not very clear in what situation which one should be preferable than another.Can somebody help clarify the differences of these two measurements (the difference in concept or principle, not the definition or computation) and their preferable applications?;[education, open-source];168;
5122;1;2015-02-12T10:46:41.280;Best Python library for statistical inference;I'm curious if anyone has Python library suggestions for inferential statistics. I'm currently reading An Introduction to Statistical Learning, which uses R for the example code, but ideally I'd like to use Python as well. Most of my data experience is with Pandas, Matplotlib, and Sklearn doing predictive modeling. So far I've found statsmodels. Is this what is recommended or is there something else?Thanks!;[education, open-source];127;
5124;2;2015-02-12T11:10:53.967;;What you are looking for is a machine learning algorithm. Although the easiest way would be to take the average scores and use that, there are much more accurate ways to make predictive models.This was the first data science tutorial I did. It's perfect for getting started. Here is it in R and in Python.If you're looking for a short answer, something you can just look up how to implement, I'd check out Random Forests.;;;
5125;2;2015-02-12T11:23:23.487;;statsmodels is a good, and fairly standard, package to statistics.For Bayesian interference you can go with PyMC - see as in Cam Davidson-Pilon, Probabilistic Programming & Bayesian Methods for Hackers.;;;
5127;1;2015-02-12T14:40:43.000;Matching two linked but varied observations using two unique identifiers;I have two dataframes. One df contains an email and a timestamp of when someone opened an email. The other df contains a unique id and a timestamp of when someone clicked on the email. However, this data is captured in two different servers and their timestamps are off by a varied amount i.e (some click timestamps are possibly before the open timestamp and some possibly after). I want to either 1) run tests to see if these two datasets are off by some constant and calculate the match rates for each test or 2) run statistical tests that calculate the probability that a unique id/timestamp combination belong to an email/timestamp combination and then the highest probability would create a match. The problem is that I have more click/timestamp observations than email/timestamp observations. Also, two different email addresses cannot have the same unique id but the same email address can have multiple timestamps. What methods do you advise?;[education, open-source];20;
5128;2;2015-02-12T15:47:15.277;;Jaccard Similarity is given by $s_{ij} = \frac{p}{p+q+r}$where, p = # of attributes positive for both objects q = # of attributes 1 for i and 0 for j r = # of attributes 1 for i and 0 for j Whereas, cosine similarity = $\frac{A \cdot B}{\|A\|\|B\|}$ where A and B are object vectors.Simply put, in cosine similarity, the number of common attributes is divided by the total number of possible attributes. Whereas in Jaccard Similarity, the number of common attributes is divided by the number of attributes that exists in at least one of the two objects. And there are many other measures of similarity, each with its own eccentricities. When deciding which one to use, try to think of a few representative cases and work out which index would give the most usable results to achieve your objective.For example, if you have two objects both with 10 attributes, out of a possible 100 attributes. Further they have all 10 attributes in common. In this case, the Jaccard index will be 1 and the cosine index will be 0.001. Now consider another scenario where object A has 10 attributes, and object B has 50 attributes, but B has all 10 attributes that A has. Here, Jaccard index will be 0.2 and cosine index will still be 0.001. So the key question is if that extra bit of information reflected, in this case, in the Jaccard index useful or does it hurt or does it not matter. Your choice will depend on which is the best solution for your problem. The Cosine index could be used to identify plagiarism, but will not be a good index to identify mirror sites on the internet. Whereas the Jaccard index, will be a good index to identify mirror sites, but not so great at catching copy pasta plagiarism (within a larger document). Of course, these are toy examples to illustrate a point. When applying these indices, you must think about your problem thoroughly and figure out how to define similarity. Once you have a definition in mind, you can go about shopping for an index.    ;;;
5129;1;2015-02-12T16:11:30.087;R Programing beginner;Hi I am new to Data analytics.I am planning to learn R by doing some real time projects. How should I stream line (set goals) my time in learning R and also I have not learnt statistics till data. I am planning to learn both side by side. I am mid level data warehouse engineer who has experience in DBMS Data-Integration. I am planning to learn R so that I can bring out useful analysis from the Integrated data. If I be specific, I am beginning in R, so what are the basic statistical concepts I should know and implement it in R. If I want to be an expert or above average person in R how should I plan strategically to become one. Say if I can spend 2 hrs a day for 1 year what level I should reach. FYI am working for a SaaS company. What are the way s in which I can utilize R knowledge in a SaaS environment ;[education, open-source];114;1
5130;2;2015-02-12T16:46:29.453;;I would start off with All Of Statistics by Larry Wasserman. This quickly gets you upto speed with statistics assuming you have some mathematical background. I think all it needs is introductory calculus and linear algebra. R is pretty straightforward to pick up and there are a number of resources you can use. The R Programming course on coursera is an excellent short course to get you familiarized with R. Besides that there are a number of books and tutorials on the subject.I would recommend you start working through All of Statistics and start playing around in R along with that book.   If you are at a SaaS company, there are a number of data sciency roles and responsibilities that should be available. Most SaaS companies will have analytical tools to provide basic insight. Once you start learning statistics and data science, you will be able to identify the gaps in the pre baked tools and ways to improve them. I would also read The Field Guide to Data Science, it's a short book that will give you a high level idea of data science and its utility. ;;;
5131;2;2015-02-12T19:09:05.900;;No one seems to have posted the XKCD overfitting comic yet. ;;;
5132;2;2015-02-13T00:33:59.857;;"This issue is caused because the following environment variables are not set correctlyPKG_CONFIG_PATH and LD_LIBRARY_PATHfor my configuration, i should execute this:export PKG_CONFIG_PATH = /usr/local/lib export LD_LIBRARY_PATH = /usr/local/libalso set this commands on R console: Sys.setenv(HADOOP_HOME=""/usr/local/hadoop/"")Sys.setenv(HADOOP_BIN=""/usr/local/hadoop/bin"")Sys.setenv(HADOOP_CONF_DIR=""/usr/local/hadoop/conf"")";;;
5133;2;2015-02-13T05:57:36.840;;The answer to both data sets is an OCR application with some post-processing, but a more specialized program than a generic low-quality or an open source OCR. Essentially the harder the problem, the more capable and advanced tools need to be used to solve it.There will be two major stages in this task: digitizing the data (image to text, i.e. OCR) and processing the data (performing the actual count). Look at them separately in order to select the best method for each stage.The main challenges in these images and generic OCR are:a) images have low resolution. For example the # 1 image has resolution of about 72 dpi. Suggested resolution for such text quality is to scan at 300 to 400 dpi, but it is clear that re-scanning or controlling scan resolution is not applicable now. That’s why one option is to clean and increase the size using image pre-processing tools. This is what the original #1 image snippet looks like after adaptive binarization and zoomed at 300%. It is clear that each character has too few pixels and characters can be easily misread.b) GIF format in #1 is not supported by many OCR applications. Images need to be batch-converted to a different format, such as PNG or TIF.c) in these scans the backgrounds and bleed-through (shadow from the text on the other side of the paper) is visible. Good binarization needs to be used to remove background and bleed-through, but not remove vital parts of actual characters.After implementing specific pre-processing solutions for the items listed above, and then using a high quality OCR system, such as www.ocr-it.com API, highest possible results can be achieved.  Result is far from perfect, but it is as high accuracy as it could be achieved with a modern OCR engine on these images.Luckily for this project, the data needs to be counted, so the second stage has all necessary data for reliable data post-processing analysis. Contrary to other basic OCR engines, the OCR provided by www.ocr-it.com API, which I used to produce the above recognition.  OCR-IT API is free to develop with, and very inexpensive to use per page, so that may be a very economical but powerful solution for this project.  It returns formatted text layout, including preserving line breaks and overall format structure.  This makes text post-processing easier.  A simple algorithm can be run to count the number of lines, resulting in the necessary for the research count.The above describes a two-stage approach: getting best possible OCR result, and using an applicable method to process data for the required taskBat wait, there is more…There is a second option to use an even more specialized OCR application called FlexiCapture with FlexiLayout technology.  This powerful and intelligent data capture technology has built-in high-accuracy OCR, and it has a powerful rules and data analytics engine to perform very specialized user-defined chains of actions and tasks.The implementation of this method using FlexiCapture with FlexiLayout takes the following logical steps.First, full page OCR is performed and all objects are extracted, including characters, noise, black horizontal and vertical lines, white gaps, and objects (which could be pictures, logos, handwriting, etc.).  This produces objects upon which we can apply our search criteria.Next, the following constraints are applied to the post-OCR data analysis and search criteria: separate image into three vertical columns and run the following logic per column, use line-start as individual count, skip header/footer/indented lines (county names), assume each name to have at least three characters, find recursively every name starting from top to bottom in every column, exclude previously found lines.While the above logic sounds complex to setup, the actual setup takes just a few minutes and requires minimal work through user interface (UI) environment.  No coding or programming is necessary.   The following search elements and criteria have been created.RepeatingGroup consisting of a CharacterString search object.This setup produces the following search result for the first column of data:As the last step, FlexiCapture is instructed to return the number of total found elements that fit our search criteria, effectively producing the necessary data for the research task.There are other logic alternatives that can be setup in FlexiCapture, such as finding the number of white spaces between lines, or searching for the fixed-length fixed-placement 3-letter combinations at the end of every column linen.In conclusion, there are several options (which is always nice) how this task can be achieved with relative ease in effort and high quality, but the success depends on the quality of tools used and necessary knowledge how to use them.If you believe some of these tools and processes can be beneficial to your project, please contact my directly.  I specialize in these workflows.  Ilya @ WiseTREND.  My company may be able to help with the setup or guidance.  We have participated in various research initiatives, some through donations to a good cause.;;;
5134;2;2015-02-13T06:03:41.233;;The MOOCs like UDACITY, COURSERA, UDEMY and EDX  are a great place to learn high quality R programming courses free. You can also search on the MOOCTIVITY siter which is a MOOC aggregator. I have personally found great courses on R programming at  UDACITY, COURSERA and UDEMY and also EDX.Good Luck !!Gopinath SUbbegowda;;;
5135;2;2015-02-13T09:06:33.070;;"Naive Bayes is just one of the several approaches that you may apply in order to solve the Titanic's problem. The aim of the Kaggle's Titanic problem is to build a classification system that is able to predict one outcome (whether one person survived or not) given some input data. The survival table is a training dataset, that is, a table containing a set of examples to train your system with. As I mentioned before, you could apply Naive Bayes to build your classification system to solve the Titanic problem. Naive Bayes is one of the simplest classification algorithms out there. It assumes that the data in your dataset has a very specific structure. Sometimes Naive Bayes can provide you with results that are good enough. Even if that is not the case, Naive Bayes may be useful as a first step; the information you obtain by analyzing Naive Bayes' results, and by further data analysis, will help you to choose which classification algorithm you could try next. Other examples of classification methods are k-nearest neighbours, neural networks, and logistic regression, but this is just a short list. If you are new to Machine Learning, I recommend you to take a look to this course from Stanford: https://www.coursera.org/course/ml";;;
5136;2;2015-02-13T15:04:24.963;;I second saq7 and Gopinath, the R courses on Coursera are excellent. I really rate the Johns Hopkins ones: https://www.coursera.org/specialization/jhudatascience/1/courses. You should also keep an eye on the software carpentry site for courses they run in your area. If you can't wait, all the software carpentry learning material is online so you can follow it yourself.;;;
5137;1;2015-02-14T07:09:01.527;Will demand for data scientist decrease because of AYASDI?;I am pursing Data Analyst course at Udacity. I came across this video : https://www.youtube.com/watch?v=3Z73Wd2T1xE Watching it lead me to wonder if Ayasdi products would reduce the demand for data scientists. I wish to compete in Kaggle contests but after watching the video, I feel that many of those problems can be easily solved using their platform and that I will be at disadvantage since I do not have access to their tools. Also I feel such tools would reduce the need of experts in data sciences. Now I am worried if I should continue with Nano Degree in Data Analyst at Udacity.;[education, open-source];128;
5138;1;2015-02-14T17:13:31.473;Definition of likelihood function;I am confused by the definition of the likelihood function in different contexts.In the case of linear and logistic regression, it is defined as y given xIn the case naive bayes and LDA, it is defined over x and yIn the case of the EM algorithm (with observed variable x and unobserved variable z), it is defined over xHow do we know over what it should be defined?;[education, open-source];80;
5140;2;2015-02-14T21:45:05.987;;"I also found confusing the definition of likelihood the first time I encountered it. The only thing you need to remember is that likelihood refers to the probability of your data given a parameter. That is,$$L(\theta| X) = P(X|\theta)$$As you can see, the likelihood focuses on the parameter, so you're asking ""What is the likelihood of this parameter given some observations?"" In this case, the phrase ""given some observation"" assumes the observations $X$ are fixed and the parameter $\theta$ is not fixed. On the other hand, if you describe the same in terms of probability, you're trying to measure the probability of some observations $X$ given a fixed parameter $\theta$. It is only a matter of usage. In the case of logistic regression, the likelihood function is described as:$$L(\beta| y) = \prod_{i}^{N} \pi_{i}^{y_{i}}(1-\pi_{i})^{n_{i} - y_{i}}$$where $\beta$ is the vector of parameters $\{\pi_{i}| i=1,...,N \}$and $y$ is the vector of Bernoulli variables. As you can see, this is the same (by definition) as $P(y|\beta)$ and that is what you have in the equation above: The product of the probabilities for each observation $y_{i}$ given its parameter $\pi_{i}$. As usual, each probability $\pi_{i}$ is equal to the inverse of the logit function.In Naive Bayes, you have the same basic idea. The likelihood is defined as $L(p_{k}|x) = \prod_{i}^{N}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}$ where $p_{k}$ is a vector of parameters describing the probabilities of a class $k$ generating a word $x_{i}$).In the EM algorithm, the likelihood is again $L(\theta|X,Z)$, where $\theta$ are your parameters, $X$ your observations and $Z$ are latent variables. The fact that you have a latent variables shouldn't confuse you, because this variable will be ""integrated out""  in the expectation step of the algorithm.In the previous equations, the calculation you need to do is self-evident if you think about it starting from your observations: First, I have some observations. Then I want to calculate the probability of obtaining these observation using some model. Most models require parameters to be fitted. So I want to calculate the probability of these observations given those parameters. That is, $p(X|\theta)$. Now, if you need to talk about likeliihood, then you simply write $L(\theta|X)$.";;;
5141;2;2015-02-14T22:45:50.003;;"What got me to understand the problem about overfitting was by imagining what the most overfit model possible would be. Essentially, it would be a simple look-up table.You tell the model what attributes each piece of data has and it simply remembers it and does nothing more with it. If you give it a piece of data that it's seen before, it looks it up and simply regurgitates what you told it earlier. If you give it data it hasn't seen before, the outcome is unpredictable or random. But the point of machine learning isn't to tell you what happened, it's to understand the patterns and use those patterns to predict what's going on.So think of a decision tree. If you keep growing your decision tree bigger and bigger, eventually you'll wind up with a tree in which every leaf node is based on exactly one data point. You've just found a backdoor way of creating a look-up table.In order to generalize your results to figure out what might happen in the future, you must create a model that generalizes what's going on in your training set. Overfit models do a great job of describing the data you already have, but descriptive models are not necessarily predictive models.The No Free Lunch Theorem says that no model can outperform any other model on the set of all possible instances. If you want to predict what will come next in the sequence of numbers ""2, 4, 16, 32"" you can't build a model more accurate than any other if you don't make the assumption that there's an underlying pattern. A model that's overfit isn't really evaluating the patterns - it's simply modeling what it knows is possible and giving you the observations. You get predictive power by assuming that there is some underlying function and that if you can determine what that function is, you can predict the outcome of events. But if there really is no pattern, then you're out of luck and all you can hope for is a look-up table to tell you what you know is possible.";;;
5142;1;2015-02-15T04:51:21.167;Local Development for Apache Spark;I'm wondering how other developers are setting up their local environments for working on Spark projects. Do you configure a 'local' cluster using a tool like Vagrant? Or, is it most common to SSH into a cloud environment, such as a cluster on AWS? Perhaps there are many tasks where a single-node cluster is adequate, and can be run locally more easily.;[education, open-source];82;2
5145;1;2015-02-15T14:00:31.457;Comparing two ordered lists;Hi I would appreciate it if someone can point me in the right direction.  I'm looking for an algorithm or mathematical theory which I would use to compute the similarity between two ordered lists, where each list element can have n sub-elements.  I will explain with an example:Suppose I go to a baseball game and I record the sequence of strikes and balls for each of the first 30 players at bat.  My list looks like this, where P is a player, S is a strike and B is a ball. Order matters. L1: {P1=(S,S,S)}, {P2=(B,B,S)}, {P3=(B,B,S,S)}, ... My friend goes to a baseball game and does the same thing.  Later, we meet up and compare our lists.  We find that our lists are almost identical except that I recorded a strike for player 16 where my friend recorded a ball.  What are the chances we were at the same game and one of us made a mistake at player 16?Thanks in advance...;[education, open-source];83;
5146;2;2015-02-15T14:14:47.110;;What about a model stating that you have two vectors of size N (where N is the total number of players, maybe unknown to us) where each element belongs to a space of {B,S} sequences, maybe empty. If you then define a distance function between two arbitrary sequences (say, normalized Levenshtein distance for two non-empty ones and some fixed cost when one is missing), you can define cosine similarity between the vectors.(Obviously, you just consider your sequence a compact representation of sparse vector in this case.);;;
5147;1;2015-02-15T17:01:50.720;Data Science Methodologies;What are the best known Data Science Methodologies today? By methodology I mean a step-by-step phased process that can be used for framing guidance, although I will be grateful for something close too.To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist.A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? ;[education, open-source];120;1
5148;2;2015-02-15T17:35:11.930;;Can you elaborate what you mean by 'methodologies'?In the meantime, take a look at The Field Guide To Data Science by Booz Allen Hamilton. This guide talks about data science processes and frameworks.Data Science Design Patterns by Mosaic talks about, you guessed it, data science design patterns. This is quite useful to get a sense of common design patterns. They are also working on releasing a book on the same subject. Then there are several resources out there that will come up as results to more targeted searches, such as machine learning paradigms, recommender systems paradigms, etc. Data Science is a large and varied field, and you'll find many resources out there for each subsection of it. As far as I know, there isn't one book that covers it all. ;;;
5149;2;2015-02-15T17:46:34.887;;Alex's answer is spot on in how you would go about figuring out similarity. One more step required, to answer your question, is to come up with a threshold of similarity. I.e. some similarity threshold beyond which you can say that the discrepancies are probably errors.If you are looking for resources to learn more about this, I'd recommend Data Mining Concepts and Techniques by Han et all;;;
5150;2;2015-02-15T23:07:23.753;;I'm currently writing a book about Data Science in Higher Education, and the following methodologies are the ones I am including:For regression, we have:Simple Linear RegressionMultiple Linear RegressionFor classification, we have:Naive Bayes ClassifierDecision Tree InductionK-Nearest NeighborThese are some of the more elementary topics in statistical analysis (which you could argue is predictive analytics which you could argue is data science), and thus I would suspect they are also the more common. ;;;
5151;2;2015-02-15T23:16:47.210;;I will assume that you have not been working in statistics that long since you are talking about jumping ship because of a new software that's coming out. Let me give you a bit of reassurance: For the past fifty or so years computers have been solving complex mathematical problems for statisticians and mathematicians, and now with the type of computing power we have today we are able to process terabytes of data per second and develop extremely sophisticated predictive models in a matter of seconds. Software has constantly evolved and constantly made people's lives easier. Has that changed the fact that we still need people to interpret the data? The model? The results? No, it hasn't. However, you should know that when software comes out that eliminates a lot of the grunt work, well, you no longer need to employ undergrads/post-baccs to complete those grunt tasks. Now you can use a software to do it, and higher people of a stronger caliber to perform more complex sets of analyses. The more software evolves, the more training a data scientist will require before breaking into the field. So you see, it's not that completing the nano degree alone will or will not give you a competitive edge in the data science job market, it's that this nano degree is the first step in your journey toward becoming a data scientist. If anything, software like this should encourage all of us to hit the books and ensure that we're still up to date on our game. No amount of bangs and whistles will teach a computer judgement. Prediction? Yes. But not judgement. We don't have Skynet yet. ;;;
5152;2;2015-02-16T02:32:21.373;;Why restrict yourself to those two approaches? Because they're cool? I would always start with a simple linear classifier \ regressor. So in this case a Linear SVM or Logistic Regression, preferably with an algorithm implementation that can take advantage of sparsity due to the size of the data. It will take a long time to run a DL algorithm on that dataset, and I would only normally try deep learning on specialist problems where there's some hierarchical structure in the data, such as images or text. It's overkill for a lot of simpler learning problems, and takes a lot of time and expertise to learn and also DL algorithms are very slow to train. Additionally, just because you have 50M rows, doesn't mean you need to use the entire dataset to get good results. Depending on the data, you may get good results with a sample of a few 100,000 rows or a few million. I would start simple, with a small sample and a linear classifier, and get more complicated from there if the results are not satisfactory. At least that way you'll get a baseline. We've often found simple linear models to out perform more sophisticated models on most tasks, so you want to always start there.;;;
5153;2;2015-02-16T03:03:43.883;;I'm doing some similar research, and have found PluralSight, http://pluralsight.com, to be an invaluable resource.  They have video courses on Machine Learning, AWS, Azure, Hadoop, Big Data, etc.  Personally, I find that these video courses allow me to learn the material much faster and more easily than books.;;;
5155;1;2015-02-16T12:16:26.923;Assigning new items to existing similarity based clustering;Given some clusters created from similarity measures between items, is there a recommended way to assign a new item to an existing cluster based on similarity alone? (i.e. avoiding re-clustering)Measuring the similarity of a new item to all other items is fairly cheap, so I'm looking for a way of using this to assign it to the cluster it's most likely to belong to. It's also important for it to take cluster size into account (i.e. doesn't unfairly weight towards or against larger clusters).Basically, I'm trying to sacrifice some clustering accuracy in exchange for avoiding a complete re-clustering when the occasional new item is added.;[education, open-source];30;
5156;2;2015-02-16T13:50:32.200;;"I suggest you think about this in terms of ""data set"" and ""training set"" (technically, it is also recommended to have a separate test set). Once you have your clusters defined on the training set, your can start using them to classify any amount of new data without recalculating, by simply measuring similarity to cluster centroids, for example.(This doesn't prevent you from deciding to enlarge your training set and data set later, just try to not do that selectively to avoiding overfitting.)";;;
5158;2;2015-02-16T20:14:41.513;;While I have only had a brief look at it I think SageMathCloud looks quite promising. I have recommended it to at least one person previously, and they seemed to be quite happy with it. Bayond R support you also get access to Python, SAGE (as the name indicates) and a few other things. EDIT: Make sure to check the documentation on how to get an R (as opposed to a Python) session in a worksheet.;;;
5159;1;2015-02-16T21:10:06.893;Regression model of time dependent system response;I have two time-dependent coupled equations. One of which is several orders of magnitude more computationally demanding than the other. I am trying to use machine learning to reproduce the behavior of the more expensive equation. system 1input: c(t), a(t)output a(t+dt)system 2input: a(t)output: c(t+dt)So essentially I want to reconstruct the response of equation 2. Keep in mind that internally there are variables in equation 2 which retain 'memory' of the previous states. So the response depends on the history of input.some more details, this is a multiscale simulationsystem 1 is a simple finite difference equation$a(x,t+dt) = 2a(x,t) - a(x,t-dt) + \frac{dt^2}{dx^2} \left[ a(x+dx,t)-2a(x,t) + a(x-dx,t) \right] + dt^2 c(x,t)$for the second part at each x I have a time-dependent set U(t) to propogate to U(t+dt). This propagation depends on an input a(x,t) and produces c(x,t+dt) to be fed back into the first equation. The details of this part are a bit convoluted/involved, but the essential point is that I want to avoid explicitly storing or propagating U (very very very expensive e.g. 10,000+ cpu cores needed)Any advice on where to start or what methods have been developed for this type of system? OR if there is a more appropriate place to post this?;[education, open-source];27;
5160;1;2015-02-17T02:39:20.920;Where the following propagation model arise in practice;This is in continuation to a previos question. I would like to know where in practice will the following M-ary tree-structured social network propagation model arise. I am looking for some concrete examples in social media (twitter and youtube etc.). Eventually, I want to do some statistical analysis on such social networks.;[education, open-source];28;
5161;2;2015-02-17T06:43:03.820;;RStudio Server is definately one of the options, meant exactly for this. I've thought about using it with a cloud virtual machine, but haven't had the need yet. But when I (probably) need to prepare an intro data analysis class for the fall semester, then Rstudio Server is the first option I'll be trying out.;;;
5162;2;2015-02-17T07:02:21.497;;"In addition to other answers (and there's some good link in the comments) it depends on what the problem is or what kinds of questions you want to answer. As I can only suggest based on my own experience, then in case of a classification task, the possible methods can be severely limited based on class balance in dataset. Once you go to a larger than around 1:10 class imbalance, then most classification methods just stop working. You'll be left with methods based on random forest and maybe neural nets (haven't tried yet). I work with the class balance in the range of 1:500 to 1:1000 and have found that neither down- or upsampling works. Luckily my dataset is ""only"" 6mln observations by 200 variables and I'm able to run boosted trees on the whole set in reasonable time.So to directly answer your question:you should come up with a bunch of questions you would want to answer and in case of classification then check the class balances of the target variables.you should check the distribution (not in mathematical sense) of missing values in all of your data and document what you find. Some ML methods are fine with missing values while others are not and you need to look into data imputation (which has its own set of rules and guidelines and problems).";;;
5164;1;2015-02-17T18:07:28.333;Getting Scikit-Learn RandomForestClassifier to output Top N results;I'd like to see the top N results for a RandomForestClassifier prediction, ordered by descending probability.The answer may be predict_proba, but I have no idea how to interpret the results.Help appreciated!;[education, open-source];74;
5165;1;2015-02-17T19:10:05.547;When it is time to use Hadoop?;"Hadoop is a buzzword now. A lot of start-ups use it (or just say, that they use it), a lot of widely known companies use it. But when and what is the border? When person can say: ""Better to solve it without Hadoop""?";[education, open-source];91;0
5166;1;2015-02-17T21:50:10.210;training neural net with multiple sets of time-series data;I have the following data($x^1_i$, $y^1_i$) for $i=1,2,...N_1$($x^2_i$, $y^2_i$) for $i=1,2,...N_2$...($x^m_i$, $y^m_i$) for $i=1,2,...N_m$Is it possible to train a neural net to produce some $y_k$ where $k<=min(N)$ given a input ${x_1, x_2, ..., x_{k-1}}$?If so any suggestion of documentation/ library I can look at (preferably python)?;[education, open-source];57;
5167;2;2015-02-17T22:50:19.067;;"It's an economic calculation, really.  When you have a computing ""problem"" (in the most general possible sense) that you can't solve with one computer, it makes sense to use a cluster of commodity machines when doing so A. allows you to solve the problem, and B. is cheaper than a forklift upgrade to a bigger computer, or upgrading to specialized hardware.When those things are true, and you are going the ""commodity cluster"" route, Hadoop makes a lot of sense, especially if the nature of the problem maps (no pun intended) well to MapReduce.  If it doesn't, one shouldn't be scared to consider ""older"" cluster approaches like a Beowulf cluster using MPI or OpenMP.  That said, the newer YARN based Hadoop does support a form of MPI, so those worlds are starting to move closer together.";;;
5168;1;2015-02-17T22:55:26.300;What are the possible ways to handle class unbalance in a large scale image recognition problem with Deep Neural Nets?;"I have 22 classes of objects but they have very skewed distributions where max class has 100.000 images and the min class has 1600 images. In that setting I would like to hear some possible solutions to this balance problem.I have tried followings so far;multiply number of instances in the lower classes up to the max class by replicating the instances, possibly adding some noise as well.changing the learning regarding the class distribution in the given minibatch of the next epoch. (no implemented but in my mind)What are your suggestions?";[education, open-source];54;1
5169;2;2015-02-17T23:04:07.277;;From my perspective, for 5 million instances you need lots of trees to get a good generalization bound (a good model in the layman term). If this is not a problem then go for it,even the exact answer is relying on the nature of your problem. GBT is a good method especially if you have mixed feature types like categorical, numerical and such. In addition, compared to Neural Networks it has lower number of hyperparameters to be tuned. Therefore, it is faster to have a best setting model. One more thing is the alternative of parallel training. You can train multiple trees at the same time with a good CPU. If you are not satisfied with the results then go for Neural Nets since that means your model should be more extensive and should learn higher order information through your data. That is the due of NNs compared to other learning algorithms. ;;;
5170;1;2015-02-18T00:08:32.800;depth/complexity of decision trees;I have used the same methods/parameters to create two decision trees. The trees classify the presence or absence of a medical condition using the presence or absence of various symptoms. There is a tree for Medical Condition #1 and another tree for Medical Condition #2. Both trees are based on the same set of symptoms, rated by patients. If Medical Condition #1 resulted in a much simpler tree than Medical Condition #2, can that suggest Medical Condition #2 is a more complex disease? If so, can anyone point me to a reference that states the complexity/depth of a tree can be representative of a complex condition?;[education, open-source];39;
5171;1;2015-02-18T00:33:51.823;How to appropriately set weights for weighted KNN;I'm currently trying to predict a continuous variable using KNN. Instead of treating each neighbor equally I would like to use the weights to create a weighted average. The weights by themselves are not ideal, as the closer a neighbor the more I would like that neighbor to influence the final results. This lead me to consider the inverse of each of the distances, but this doesn't handle the case where an instance is the exact same -> with a distance of 0.Any recommendations on how to properly set the weights of each neighbor relative to their distance? Similar to how the inverse would handle this, but one that allows for 0 values.;[education, open-source];74;
5172;1;2015-02-18T04:24:51.153;perceptron implementation, preocedure for post-feature vecor: bag-of-words model;I have code to generate feature vectors in the style of the bag-of-words model, you can see it on my github page.It renders output of the form:/data/train/politics/p_0.txt, [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]/data/train/science/s_0.txt, [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]/data/train/atheism/a_0.txt, [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]/data/train/sports/s_1.txt, [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]I want to move on to the next step of the perceptron but I'm confused about what to do next. I guess the next thing I need to do is generate a feature vector for the 'test' data, isn't it?For instance, in this example what is 'teachingOutput' supposed to be, training data vectors? Such as the ones I have above?I also tried poking around in this implementation in an attempt to figure out what to do but, alas, to no avail. ;[education, open-source];21;
5173;2;2015-02-18T09:04:27.770;;Technically speaking the inverse of the distance should not pose any problem. Considering the case when you have to classify an observation (instance) whih is identical with one observation from the 'learned' data set, than the inverse distance is not defined by a direct formula since you have to divide by zero. But considering the math behind, inverse of the zero distance is positive infinity. Which means that, the weight for the identical point would dominate all other weights of the data instances. Thus the classification for that specific point could be taken as the class of the identical observation from the data set. On the other hand the inverse of the euclidean distance is only one type of distance. You can always apply a kernel function on that to weight non-uniformly the contributions. For example you can choose a normal density with mean equals zero and variance equals 1. Than, your weight would be the value of density function for the euclidean distance. ;;;
5175;1;2015-02-18T10:03:13.913;List of used aliases in Tableau;I often get the problem when this or that alias name is already used somewhere, and I can't easily find that variable or aggregation to release the name.Is there some place in Tableau where I can view/edit/reset full list of aliases?;[education, open-source];291;
5178;1;2015-02-13T10:09:25.177;How to deal with version control of large amounts of (binary) data;"I am a PhD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know svn and git fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find git also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn't fun, and cost me a lot of time and effort.I know that others in science seem to have similar problems, yet I couldn't find a good solution.I want to use the storage facilities of my institute, so I need something that can use a ""dumb"" server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So, I need a tool that can handle more than one remote location.Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.I have evaluated a lot of different solutions, but none seem to fit the bill:svn is somewhat inefficient and needs a smart serverhg bigfile/largefile can only use one remotegit bigfile/media can also use only one remote, but is also not very efficientattic doesn't seem to have a log, or diffing capabilitiesbup looks really good, but needs a ""smart"" server to workI've tried git-annex, which does everything I need it to do (and much more), but it is very difficult to use and not well documented. I've used it for several days and couldn't get my head around it, so I doubt any other coworker would be interested.How do researchers deal with large datasets, and what are other research groups using?To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me that almost everyone should have this problem, yet I don't know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?";[education, open-source];317;3
5179;2;2015-02-13T13:38:56.797;;"I have dealt with similar problems with very large synthetic biology datasets, where we have many, many GB of flow cytometry data spread across many, many thousands of files, and need to maintain them consistently between collaborating groups at (multiple) different institutions.  Typical version control like svn and git is not practical for this circumstance, because it's just not designed for this type of dataset.  Instead, we have fallen to using ""cloud storage"" solutions, particularly DropBox and Bittorrent Sync.  DropBox has the advantage that it does do at least some primitive logging and version control and manages the servers for you, but the disadvantage that it's a commercial service, you have to pay for large storage, and you're putting your unpublished data on a commercial storage; you don't have to pay much, though, so it's a viable option.  Bittorrent Sync has a very similar interface, but you run it yourself on your own storage servers and it doesn't have any version control.  Both of them hurt my programmer soul, but they're the best solutions my collaborators and I have found so far.";;;
5180;2;2015-02-18T14:07:39.487;;As there isn't too much code/context you're giving us I assume that you're working with a binary classification problem:import numpy as npX = ... # input for classification, shape (n_samples, n_features)y_pred = rf.predict_proba(X)[:, 1] # index slicing to retrieve a 1d-array of probabilitiesy_pred# array([  0.18, 0.21, 0.1, 0.2, 0.3])# Now lets see what the 2 biggest probabilities arenp.argsort(y_pred)# [2, 0, 3, 1, 4] => lowest probability is at index 2 (the 3rd probability as classified by the RandomForestClassifier)# Now lets see what the top 5 samples look like (remember, argsort sorts in ascending order, so we take the last 5 indices of argsort):X[np.argsort(y_pred)[-5:]]Now you should see the rows in X with the 5 most confident predictions.;;;
5181;2;2015-02-18T23:14:02.563;;"Imagine each node in your graph as a user, and each edge as an action such as 'share'. It may also be a bidirectional relationship 'share' and 'view'.Some social scientists and engineers estimate the probability that a message is 'shared' and 'viewed' given that a particular user decides to share it. This process is called ""information diffusion"", ""information propagation"", ""cascading"", etc.If you are interested in the details of how these calculations are performed, check out these papers:http://cs.stanford.edu/people/jure/pubs/netrate-netsci14.pdfhttp://cs.stanford.edu/people/jure/pubs/cascades-www14.pdfhttp://cs.stanford.edu/people/jure/pubs/infopath-wsdm13.pdfSimilar topics:http://snap.stanford.edu/papers.html";;;
5182;2;2015-02-19T06:30:45.190;;I have used Versioning on Amazon S3 buckets to manage 10-100GB in 10-100 files. Transfer can be slow, so it has helped to compress and transfer in parallel, or just run computations on EC2. The boto library provides a nice python interface.;;;
5184;2;2015-02-19T12:06:08.173;;Finally solved the issue.You should go to:Data –> your_data_source –> Edit Aliases –> Measure NamesAnd see full mapping between variables/aggregations and aliases.;;;
5185;2;2015-02-19T14:59:05.007;;A decaying exponential of the form $e^{-\alpha x}$ (where $x$ is the distance from the observation) is convenient in this situation. It has the nice feature that the weight is equal to $1$ when the observation lies exactly at one of your training points and decays to zero as $x\rightarrow \infty$.What you need to decide is the scaling factor, $\alpha$, which can greatly affect your results, since the decaying exponential is nonlinear. If $\alpha$ is too small or large, then all $k$ points will be weighted nearly equally (assuming none of the $k$ points have a distance very close to zero). One approach is to pick a global value of $\alpha$ that is suitable for your data set. Another approach is to compute a new value of $\alpha$ for each set of $k$ neighbors under consideration, which will guarantee variability in the weights (as long as all $k$ neighbors are not equidistant).;;;
5186;1;2015-02-19T16:23:15.093;Decision Tree Bayes rules / Maximax / Maximin;"I need to draw a decision tree about this subject :The research and development manager in an old oil company, which is considering making some changes, lists the following courses of action for the company:(i) Adopt a process developed by another oil company. This would cost €7 million in royalties and yield a net €20 million profit (before paying the royalty).(ii) Carry out one or two (not simultaneously) alternative research projects :(R1) the more expensive one has a 0.8 chance of success; net profit €16 millionand a further €6 million in royalties. If it fails there will be a net loss of €10 million. (R2) the alternative research programme is less expensive but only has a 0.7chance of success with a net profit of €15 million and a further €5 million in royalties. If it fails a net loss of €6 million will be incurred.(iii) Make no changes. After meeting current operating costs the company expects to make a net €15 million profit from its existing process.Failure of one research program would still leave open all remaining courses of action (including the other research programme).I need also to indicate the different payoffs. This is what I've done so far :I would like to be sure that I'm going in the right direction since I'm a beginner with decision trees. And then I need to decide the best course of action using Bayes, Maximax and Maximin rules.";[education, open-source];35;
5188;2;2015-02-19T22:17:45.773;;No, you can't infer that.I assume you have the same training set with the same predictors (symptoms). The only difference in the training set is the binary class label for each patient.The smaller tree just means:- with the given symptoms, it might be easier to distinguish between the people who have the condition 1 and the ones not having it. (since you have the same certainty with fewer information)This distinction seems harder with medical condition 2, that's way you have to consider more symptoms to be pretty sure about your classification. So if condition 2 is a very mild condition where it is hard to diagnose even for an expert if someone has it then it would result in a deep tree.;;;
5192;1;2015-02-20T12:21:31.560;Where can I find resources and papers regarding Data Science in the area of Public Health;I'm quite new to Data Science, but I would like to do a project to learn more about it.My subject will be Data Understanding in Public Health.So I want to do some introductory research to public health.I would like to visualize some data with the use of a tool like Tableau.Which path would you take to develop a good understanding of Data Science? I imagine taking some online courses, eg. Udacity courses on data science, but which courses would you recommend?Where can I get real data (secondary Dummy Data) to work with?And are there any good resources on research papers done in Data Science area with the subject of Public Health?Any suggestions and comments are welcome.;[education, open-source];54;3
5193;1;2015-02-20T14:09:23.483;Python or R for implementing machine learning algorithms for fraud detection;I was wondering which language can I use: R or Python, for my internship in fraud detection in an online banking system: I have to build machine learning algorithms (NN, etc.) that predict transaction frauds.Thank you very much for your answer.;[education, open-source];737;3
5196;1;2015-02-20T21:05:16.223;Scaling thousands of automated forecasts in R;Hello and thanks in advance!  I'd like some advice on a scalability issue and the best way to resolve.  I'm writing an algorithm in R to produce forecasts for several thousand entities.  One entity takes about 43 seconds to generate a forecast and upload the data to my database.  That equates to about 80+ hours for the entire set of entities and that's much too long.I thought about running several R processes in parallel, possibly many on a few different servers, each performing forecasts for a portion of total entities.  Though that would work, is there a better way?  Can Hadoop help at all?  I have little experience with Hadoop so don't really know if it can apply.  Thanks again!;[education, open-source];67;
5197;1;2015-02-20T21:56:45.690;Null-invariant measures of association in R;"A usual way to find association rules in R is the ""arules"" package, which easily let's use calculate some rules based on the apriori algorithm. However, for the data i'm using, I have a lot of NULL cases (baskets where no product A o B is present). This means that I need to calculate some null-invariant measures (kulcynski, for instance). Does anyone know of any package or workable code that let's me implement this as opposed to writing from scratch the entire algorithm?";[education, open-source];79;
5198;1;2015-02-20T23:16:18.767;How to visualize multivariate regression results;Are there commonly accepted ways to visualize the results of a multivariate regression for a non-quantitative audience? In particular, I'm asking how one should present data on coefficients and T statistics (or p-values) for a regression with around 5 independent variables.;[education, open-source];76;
5199;1;2015-02-21T01:16:34.387;What is a good hardware setup for using Python across multiple users;"This question is likely somewhat naive.  I know I (and my colleagues) can install and use Python on local machines. But is that really a best practice?  I have no idea.Is there value in setting up a Python ""server""?  A box on the network where we develop our data science related Python code.  If so, what are the hardware requirements for such a box?  Do I need to be concerned about any specific packages or conflicts between projects?";[education, open-source];67;
5200;2;2015-02-21T10:08:39.980;;"I would say that it is your call and purely depends on your comfort with (or desire to learn) the language. Both languages have extensive ecosystems of packages/libraries, including some, which could be used for fraud detection. I would consider anomaly detection as the main theme for the topic. Therefore, the following resources illustrate the variety of approaches, methods and tools for the task in each ecosystem.Python Ecosystemscikit-learn library: for example, see this page;LSAnomaly, a Python module, improving OneClassSVM (a drop-in replacement): see this page;Skyline: an open source example of implementation, see its GitHub repo;A relevant discussion on StackOverflow;pyculiarity, a Python port of Twitter's AnomalyDetection R Package (as mentioned in 2nd bullet of R Ecosystem below ""Twitter's Anomaly Detection package"").R EcosystemCRAN Task Views, in particular, Time Series, Multivariate and Machine Learning;Twitter's Anomaly Detection package;Early Warning Signals (EWS) Toolbox, which includes earlywarnings package;h2o machine learning platform (interfaces with R) uses deep learning for anomaly detection.Additional General InformationA set of slides, mentioning a variety of methods for anomaly detection (AD);A relevant discussion on Cross Validated;ELKI Data Mining Framework, implementing a large number of AD (and other) algorithms.";;;
5201;2;2015-02-21T11:59:31.860;;"Is installing Python locally a good practice?  Yes, if you are going to develop in Python, it is always a good idea to have a local environment where you can break things safely.Is there value in setting up a Python ""server""?  Yes, but before doing so, be sure to be able to share your code with your colleagues using a version control system.  My reasoning would be that, before you move things to a server, you can move a great deal forward by being able to test several different versions in the local environment mentioned above.  Examples of VCS are git, svn, and for the deep nerds, darcs.Furthermore, a ""Python server"" where you can deploy your software once it is integrated into a releasable version is something usually called ""staging server"".  There is a whole philosophy in software engineering — Continuous Integration — that advocates staging whatever you have in VCS daily or even on each change.  In the end, this means that some automated program, running on the staging server, checks out your code, sees that it compiles, runs all defined tests and maybe outputs a package with a version number.  Examples of such programs are Jenkins, Buildbot (this one is Python-specific), and Travis (for cloud-hosted projects).What are the hardware requirements for such a box? None, as far as I can tell.  Whenever it runs out of disk space, you will have to clean up.  Having more CPU speed and memory will make concurrent builds easier, but there is no real minimum.Do I need to be concerned about any specific packages or conflicts between projects?  Yes, this has been identified as a problem, not only in Python, but in many other systems (see Dependency hell).  The established practice is to keep projects isolated from each other as far as their dependencies are concerned.  This means, avoid installing dependencies on the system Python interpreter, even locally; always define a virtual environment and install dependencies there.  Many of the aforementioned CI servers will do that for you anyway.";;;
5202;2;2015-02-21T12:19:34.107;;"This is largely a subjective question.  Trying to list some criteria that seem objective to me:the important advantage of Python is that it is a general-purpose language.  If you will need to do anything else than statistics with your program (generate a web interface, integrate it with a reporting system, pass it on to other developers for maintenance) you are far better off with Python.the important advantage of R is that it is a specialized language.  If you already know that there is a technique you want to use, and it is not a usual suspect (like NN), then you probably will find a library in CRAN that makes life easier for you.And here is another, more subjective advice:both languages are not really performance-oriented.  If you need to process large quantities of data, or process very fast, or process in parallel, you will run into trouble ... but it is far easier to run into such trouble with R than with Python.  In my experience, you find the limits of R within some weeks, and the way to push them is quite arcane; while you can use Python for years, never really missing the speed that C developers always mention as the Holy Grail, and even when you do, you can use Cython to make up for the difference.  The only real trouble is concurrency, but for statistics, it is hardly ever an issue.";;;
5203;2;2015-02-21T12:33:53.923;;If you're working with R language, I would suggest first to try use R ecosystem's abilities to parallelize the processing, if possible. For example, take a look at packages, mentioned in this CRAN Task View.Alternatively, if you're not comfortable or satisfied with the approaches, implemented by the above-referred packages, you can try some other approaches, such as Hadoop or something else. I think that a Hadoop solution would be an overkill for such problem, considering the learning curve, associated with it, as well as the fact that, as far as I understand, Hadoop or other MapReduce frameworks/architectures target long-running processes (an average task is ~ 2 hours, I read somewhere recently). Hope this helps.;;;
5204;1;2015-02-21T14:02:39.170;Matrix factorization for like/dislike/unknown data;Most literature focus on either explicit rating data or implicit (like/unknown) data. Are there any good publications to handle like/dislike/unknown data? That is, in the data matrix there are three values, and I'd like to recommend from unknown entries.And are there any good open source implementations on this?Thanks.;[education, open-source];85;
5205;1;2015-02-22T11:57:57.840;"Split columns by finding ""£"" and then converting to minvalue, maxvalue";"I have a pandas dataframe with a salary column which contains values like: £36,000 - £40,000 per year plus excellent bene...,  £26,658 to £32,547 etcI isolated this column and split it with the view to recombining into the data frame later via a column bind in pandas.I now have an object with columns like the below. The columns I split the original data frame column I think are blank because I didn't specify them (I called df['salary']=df['salary'].astype(str).str.split())So my new object contains this type of information:[£26,658, to, £32,547],[Competitive, with, Excellent, Benefits]What I want to do is:Create three columns called minvalue and maxvalue and realvalueList items starting with £ (something to do with ""^£""?Take till the end of the items found ignoring the £ (get the number out) (something to do with (substr(x,2,nchar(x)))?If there are two such items found, call the first number ""minvalue"" and call the second number ""maxvalue"" and put it below the right column. If there is only one value in the row, put it below the realvalue column.I am very new to pandas and programming in general, but keen on learning, your help would be appreciated. ";[education, open-source];177;
5206;2;2015-02-22T13:14:36.200;;This is more of a general regex question, rather than pandas specific.I would first create a function that extracts the numbers you need from strings, and then use the pandas.DataFrame.apply function to apply it on the pandas column containing the strings. Here is what I would do:import redef parseNumbers(salary_txt):    return [int(item.replace(',','')) for item in re.findall('£([\d,]+)',salary_txt)]#testing if this workstestcases = ['£23,000 to £100,000','£34,000','£10000']for testcase in testcases:    print testcase,parseNumbers(testcase)Here, I just used re.findall, which finds all patterns that look like £([\d,]+). This is anything that starts with £ and is followed by an arbitrary sequence of digits and commas. The parenthesis tells python to extract only the bit after the £ sign. The last thing I do is I remove commas, and parse the remaining string into an integer. You could be more elegant about this I guess, but it works.using this function in pandasdf['salary_list'] = df['salary'].apply(parseNumbers)df['minsalary'] = df['salary'].apply(parseNumbers).apply(min)df['maxsalary'] = df['salary'].apply(parseNumbers).apply(max)Checking if this all works:import pandasdf = pandas.DataFrame(testcases,columns = ['salary'])df['minsalary'] = df['salary'].apply(parseNumbers).apply(min)df['maxsalary'] = df['salary'].apply(parseNumbers).apply(max)df    salary  minsalary   maxsalary0   £23,000 to £100,000 23000   1000001   £34,000 34000   340002   £10000  10000   10000The advantages of moving the parsing logic to a separate function is that:it may be reusable in other codeit is easier to read for others, even if they aren't pandas expertsit's easier to develop and test the parsing functionality in isolation;;;
5207;2;2015-02-22T15:44:39.550;;The goal of my analysis is separate from this question. I am trying to figure out all the conclusions I can draw or suggest from my analysis. Yes, I am interested in saying something about the complexity of condition A to B. When is a condition complex ? When there are many symptoms as the disease is diagnosed by the symptoms.If it's hard to diagnose ? YESif the symptoms are severe ? NOcan s.o. have both conditions ? NO;;;
5208;1;2015-02-22T20:30:30.630;Splitting binary classification into smaller susbsets;As an example. If you are tying to classify humans from dogs. Is it possible to approach this problem by classifying different kinds of animals (birds, fish, reptiles, mammals, ...) or even smaller subsets (dogs, cats, whales, lions, ...) Then when you try to classify a new data set, anything that did not fall into one of those classes can be considered a human.If this is possible, are there any benefits into breaking a binary class problem into several classes (or perhaps labels)? Benefits I am looking into are: accuracy/precision of the classifier, parallel learning.;[education, open-source];40;
5209;1;2015-02-23T08:00:03.360;Accuracy of Stanford NER;I am performing Named Entity Recognition using Stanford NER. I have successfully trained and tested my model. Now I want to know:1) What is the general way of measuring accuracy of NER model ?? For example what techniques or approaches are used ??2) Is there any built-in method in STANFORD NER for evaluating the accuracy ??;[education, open-source];66;0
5210;2;2015-02-23T08:17:30.120;;"I personally like dotcharts of standardized regression coefficients, possibly with standard error bars to denote uncertainty. Make sure to standardize coefficients (and SEs!) appropriately so they ""mean"" something to your non-quantitative audience: ""As you see, an increase of 1 unit in Z is associated with an increase of 0.3 units in X.""In R (without standardization):set.seed(1)foo <- data.frame(X=rnorm(30),Y=rnorm(30),Z=rnorm(30))model <- lm(X~Y+Z,foo)coefs <- coefficients(model)std.errs <- summary(model)$coefficients[,2]dotchart(coefs,pch=19,xlim=range(c(coefs+std.errs,coefs-std.errs)))lines(rbind(coefs+std.errs,coefs-std.errs,NA),rbind(1:3,1:3,NA))abline(v=0,lty=2)";;;
5211;2;2015-02-23T11:43:11.833;;If you try to get the best accuracy, etc... for a given question you should always learn on a training set that is labeled exactly according to your questions. You shouldn't expect to get better results if you are using more granular class labels. The classifier then would then try to pick up the differences in the classes and try to separate them apart. Since in practice your variables in the training set will not perfectly explain the more granular classification question you shouldn't expect to get a better answer for your less granular classification problem.If you are not happy with the accuracy of your model you try the following instead:review the explanatory variables. Think about what might influence the classification problem. Maybe there us a clever way to construct new variables (from your existing ones) that helps. It's nowpossible to give a general advise on that since you have to consider the properties of your classifierif your class distribution is very skewed you might consider over/undersampling you might run more different classifiers and then classify based on the majority vote. Note that you will most likely sacrifice explainability of your model.Also you seem to have some missunderstanding, when you write 'you would assign it to human if it doesn't fall into any of the granular classes'. Note that you always try to pick class labels covering the whole universe (all possible classes). This can be always defined as the complement of the other classes. Also you will have to have instances for each class in your training set.;;;
5212;1;2015-02-23T12:03:47.773;Big data and data mining for CRM?;"We are currently developing a customer relationship management software for SME's. What I'd like to structure for our future CRM is developing CRM with a social-based approach (Social CRM). Therefore we will provide our users (SME's) to integrate their CRM into their social network accounts. Also CRM will be enhance intercorporate communication of owner company.All these processes I've just indicated above will certainly generate lots of unstructured data. I am wondering how can we integrate big data and data-mining contepts for our project; especially for the datas generated by social network? I am not the expert of these topics but I really want to start from somewhere.Basic capabilities of CRM (Modules)-Contacts: People who you have a business relationship.-Accounts: Clients who you've done a business before.-Leads: Accounts who are your potential customers.-Oppurtunites: Any business opportunity for an account or a lead.-Sales Orders-Calendar-TasksWhat kind of unstructured data or the ways (ideas) could be useful for the modules I've just wrote above? If you need more specific information please write in comments.";[education, open-source];131;
5213;1;2015-02-23T15:22:56.400;Convolutional neural network always gives nearly zero test error but higher training error;"I am using a two-convolution-layer neural networks to test some RNA sequencing data. What I did is first mapping all sequences into matrices as input of CNN, then to predict whether its expression is high or low. Basically I want to classify matrices into two classes. I am using ""DeepLearnToolbox"" for MATLAB.I divided my data into training set and testing set in several ways. The strange thing is that I always got 20% training error but almost 100% (99.9%) prediction accuracy. Even if I divide the entire 14000 data points into 500 points for training and rest for testing, I still get almost 100% right prediction.  I cannot interpret the result, could anyone give any clues to find out why the prediction is always right? ";[education, open-source];37;
5214;1;2015-02-23T15:45:21.700;Going from report to feature matrix;I am starting to play around in datamining / machine learning and I am stuck on a problem that's probably easy.So I have a report that lists the url and the number of visits a person did. So a combination of ip and url result in an amount of visits.Now I want to run the k-means clustering algorithm on this so I thought I could approach it like this:This is my data:url      ip    visitsabc.be   123   5abc.be/a 123   2abc.be/b 123   2abc.be/b 321   4And I would turn in into a feature vector/matrix like so:abc.be  abc.be/a   abc.be/b   impressions   1       0          0          5   0       1          0          2   0       0          1          2   0       0          1          4But I am stuck on how to transform my data set to a feature matrix. Any help would be appreciated.;[education, open-source];37;
5215;2;2015-02-23T21:57:39.677;;You can check the data types of your columns by doing df.dtypes, and if 'salary' isn't a string, you can convert it using df['salary'] = df['salary'].astype(str). This is what you were already doing before splitting. From there, Ferenc's method should work!;;;
5216;2;2015-02-23T22:39:53.290;;"I don't understand what you mean by  So I have a report that lists the url and the number of visits a person did. So a combination of ip and url result in an amount of visits.Assuming that you equate an IP with a user, and you wish to cluster users by their URL visitation frequencies, your matrix, M, would have One row per IP (user)One column for each URL that you are tracking (your features)and the entries in M would be ""visits"" of a given URL by a particular IPGiven these assumptions, and your report, M would be:    abc.be  abc.be/a  abc.be/b123   5        2         2321   0        0         4";;;
5217;2;2015-02-24T02:11:45.787;;https://www.codeschool.com/ is very similar to https://www.datacamp.com/when I tried it I fell in love with R and then found datacamp.www.codecademy.com  is also console-based but R is not yet available.;;;
5219;1;2015-02-24T03:14:02.910;Number of cuboids in a data cube;Suppose there is a warehouse with 5 dimensions and each of the dimensions has 5 levels not including the apex. How many cuboids would be there in the data cube, including apex and base?T = Product(from i=1 to n) (Li + 1) - I'm not sure if this includes the apex. ;[education, open-source];34;
5220;2;2015-02-24T03:51:15.133;;The two modules where you can really harness data mining and big data techniques are probably Leads and Opportunities. The reason is that, as you've written yourself, both contain 'potential' information that you can harness (through predictive algorithms) to get more customers. Taking Leads as an example, you can use a variety of machine learning algorithms to assign a probability to each account, based on that account's potential for becoming your customer in the near future. Since you already have an Accounts module which gives you information about your current customers, you can use this information to train your machine learning algorithms. This is all at a very high level but hopefully, you're getting the gist of what I'm saying.;;;
5221;2;2015-02-24T04:07:29.860;;My advice is to begin with a thorough grounding in Statistics, for which a lot of classic and unambiguous material is available online. Topics that you should have a firm grasp on include regression, correlation, hypothesis testing and the bias-variance tradeoff. You don't have to go into too much theoretical depth into any of these topics but you should know what these are before you start studying machine learning. Your study in machine learning should include developing an understanding of hypothesis spaces, Bayesian analysis (can you explain the difference between MLE, MAP and optimal Bayes?), Expectation Maximization, logistic regression, clustering (especially k-means), max-margin classifiers (SVMs), overfitting (can you explain what it is in terms of bias and variance?) and feature selection. Linear algebra is helpful but can be done without in many cases.   As someone who does research in data mining and has worked closely with several companies, I can tell you that if you seriously want to go into machine learning, it is NOT enough to simply know how to code something using Weka or R. Those are easy enough to use once you know the concepts. When companies hire data scientists, they want someone who can take the raw data and do something useful with it. A good grasp of fundamentals is obviously essential, since each company's data has its own quirks (and will typically be too big for you to try 'everything'). Good luck!;;;
5222;2;2015-02-24T08:40:05.083;;So far the answers have focused on learning particular methods. They are fine, but they won't make you a Data Scientist. Being a Data Scientist is not solely or even primarily about having mastery of particular data analysis methods (ML or others).Most fundamental is problem solving and decision support. What ever data you collect, what ever analysis methods you apply, and however you improve those methods over time, these must support the over-arching goals of solving problems or making better decisions.You need to start getting first-hand experience with data in your field.  I don't mean Kaggle data (i.e. already cleaned).  I mean raw data or nearly raw. A good 50% of a data scientist's time is spent wrangling raw data and cleaning it to the point where it's usable in analysis.  You need to learn how to deal with missing data, erroneous data, ambiguous data, misformatted data, and so on.You should also get some experience with decisions that do not map neatly on to the data.  Recommender systems are easy in this regard.  For example, you might take on the challenge of evaluating software vulnerabilities to guide vulnerability management decisions.;;;
5223;2;2015-02-24T11:52:40.977;;"Yes, this is a straightforward application for neural networks.  In this case yk are the outputs of the last layer (""classifier""); xk is a feature vector and yk is what it gets classified into.  For simplicity prepare your data so that N is the same for all.  The problem you have is perhaps that in the case of time series you won't have enough data: you need (ideally) many 1000's of examples to train a network, which in this case means time series, not points.  Look at the specialized literature on neural networks for time series prediction for ideas on network architecture.Library: try Pylearn2 at http://deeplearning.net/software/pylearn2/  It's not the only good option but it should serve you well.";;;
5224;1;2015-02-24T11:59:36.033;How to prepare images for neural network?;"I would like to use a neural network for image classification.  I'll start with pre-trained CaffeNet and train it for my application.  My question is: how to prepare the input images?  In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don't know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn't be really the same scale (maybe 1.5x variation).Also: I've heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?";[education, open-source];91;2
5225;2;2015-02-24T22:13:46.327;;Data science is a new somewhat vague terminology.  You will find much more information by using keywords such as: epidemiology, population health, public health surveillance, public health, statistics, evidence based medicine, biostatistics, statistical epidemiology, clinical decision making, interventions etc.The question as posted is overly broad. On the other hand you're in luck since there is a substantial amount of information on this topic.  Many of the R/Data Science courses on Coursera from John Hopkins have a public health flavor. Also, R and S-PLUS before it were primarily used in health settings.;;;
5226;1;2015-02-25T01:07:14.717;strings as features in decision tree/random forest;I am new to machine learning!Right now I am doing some problems on application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, scikit-learn takes only numbers as parameters, but I want to inject the strings as well as they carry significant amount of knowledge.How do I handle such scenario, I can convert string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.Thanks for your support!;[education, open-source];444;
5227;1;2015-02-25T03:24:20.450;problem of choosing right statistical method for scheduler prediction;I am struggling to choose a right data prediction method for the following problem.Essentially I am trying to model a scheduler operation, trying to predict its scheduling without knowing the scheduling mechanism and having incomplete data. (1) There are M available resource blocks that can carry data, N data channels that must be scheduled every time instance i(2) Inputs into the scheduler:  Matrix $X_i$ size M by N, consisting of N column vectors from each data source.    Each of M elements is index from 1 to 32 carrying information about quality of data channel for particular resource block. 1 - really bad quality, 32 - excellent quality.Data which contains type of data to be carried (voice/internet etc)Scheduler prioritizes number of resource blocks occupied by each channel every time instant i.Given that I CAN see resource allocation map every time instantI DO have access to matrix $X_i$  I DON'T know the algorithm of scheduler andI dont have access to the type of data to be scheduled. I want to have a best guess (prediction) how the data will be scheduled based on this incomplete information i.e, which resource block will be occupied by which data channel. What is the best choice of prediction/modelling algorithm?Any help appreciated!;[education, open-source];50;
5228;1;2015-02-25T10:07:24.563;Graph modularity measure;"There are several metrics for the quality of a graph clustering, e.g. Newman modularity. These enable you to compare two candidate clusterings of the same graph. Does anyone know a metric that will answer the question ""how modular is this graph""? For example the first of these two graphs is more modular than the second:    o===o-----o====o    o----o===o-----oIt would be possible to choose a clustering algorithm, run it, and compute your preferred modularity metric for the best clustering found. But this is only a lower bound, so it doesn't seem very satisfactory.The question matters. For example, the work of life scientists will be easier if the molecular organisation of life is modular than if it is not. It would be good to have a robust test - some of the discussion so far seems to involve wishful thinking.My best attempt at this is:- a tree is more modular if the edges near leaves are higher weight- the modularity of a graph is the modularity of its min cut spanning treeDoes anyone know of an established answer to this question?";[education, open-source];42;1
5229;2;2015-02-25T10:10:55.683;;In most of the well established machine learning systems categorical variables are handled naturally. For example in R you would use factors, in weka you would use nominal variables. This is not the care in scikit learn. The decision trees implemented in sklearn uses only numerical features and these features are interpreted always as continuous numeric variables. Thus, simply replacing the strings with a hash code should be avoided, because being considered as a continuous numerical feature any coding you will use will induce an order which simply does not exists in your data. One example is to code ['red','green','blue'] with [1,2,3], would produce weird things like 'red' is lower than 'blue', and if you average a 'red' and a 'blue' you will get a 'green'. Another more sublte example maigh happen when you code ['low', 'medium', 'high'] with [1,2,3]. In the latter case it might happen to have an ordering which makes sense, however some sublte inconsistencies might happen when 'medium' in not in the middle of 'low' and 'high'.Finally the answer to your question lies in coding the categorical feature into multiple binary features. For example you might code ['red','green','blue'] with 3 columns, one for each category, having 1 when the category match and 0 otherwise. This is called one-hot-encoding, binary encoding, one-of-k-encoding or whatever. You can check documentation here for encoding categorical features and feature extraction - hashing and dicts. Obviously one-hot-encoding will expand your space requirements and sometimes it hurts the performance as well. ;;;
5230;2;2015-02-25T11:18:26.313;;Have you heard about the intersection graph? You can try to draw players as points, connections (team mates) as edges and teams as transparent coloured blobs on top.As for your original question, I cannot understand your goal. I think your formulation is not well defined / incomplete. Suppose you have teams A [1,2] B [2,3] and C [1,3]. What do you want to display? Do you want to list the parts of the Venn diagram? I think that for more than 3 sets this can become more cumbersome than the bipartite graph itself = simple listing of team compositions.;;;
5231;1;2015-02-25T14:10:04.777;How to test/validate unlabeled data in association rules in R?;I produced association rules by using the arules package (apriori). I'm left with +/- 250 rules. I would like to test/validate the rules that I have, like answering the question: How do I know that these association rules are true? How can I validate them? What are common practice to test it?I thought about cross validation (with training data and test data) as I read that it's not impossible to use it on unsupervised learning methods..but I'm not sure if it makes sense since I don't use labeled data.If someone has a clue, even if it's not specifically about association rules (but testing other unsupervised learning methods), that would also be helpful to me.I uploaded an example of the data that I use here in case it's relevant: https://www.mediafire.com/?4b1zqpkbjf15iuy;[education, open-source];44;
5232;2;2015-02-25T17:08:09.830;;"First, some caveatsI'm not sure why you can't use your preferred programming (sub-)paradigm*, Inductive Logic Programming (ILP), or what it is that you're trying to classify. Giving more detail would probably lead to a much better answer; especially as it's a little unusual to approach selection of classification algorithms on the basis of the programming paradigm with which they're associated. If your real world example is confidential, then simply make up a fictional-but-analogous example.Big Data Classification without ILPHaving said that, after ruling out ILP we have 4 other logic programming paradigms in our consideration set:AbductiveAnswer SetConstraintFunctionalin addition to the dozens of paradigms and sub-paradigms outside of logic programming.Within Functional Logic Programming for instance,  there exists extensions of ILP called Inductive Functional Logic Programming, which is based on inversion narrowing (i.e. inversion of the narrowing mechanism). This approach overcomes several limitations of ILP and (according to some scholars, at least) is as suitable for application in terms of representation and has the benefit of allowing problems to be expressed in a more natural way. Without knowing more about the specifics of your database and the barriers you face to using ILP, I can't know if this solves your problem or suffers from the same problems. As such, I'll throw out a completely different approach as well.ILP is contrasted with ""classical"" or ""propositional"" approaches to data mining. Those approaches include the meat and bones of Machine Learning like decision trees, neural networks, regression, bagging and other statistical methods. Rather than give up on these approaches due to the size of your data, you can join the ranks of many Data Scientists, Big Data engineers and statisticians who utilize High Performance Computing (HPC) to employ these methods on with massive data sets (there are also sampling and other statistical techniques you may choose to utilize to reduce the computational resources and time required to analyze the Big Data in your relational database). HPC includes things like utilizing multiple CPU cores, scaling up your analysis with elastic use of servers with high memory and large numbers of fast CPU cores, using high-performance data warehouse appliances, employing clusters or other forms of parallel computing, etc. I'm not sure what language or statistical suite you're analyzing your data with, but as an example this CRAN Task View lists many HPC resources for the R language which would allow you to scale up a propositional algorithm.";;;
5233;1;2015-02-25T18:28:27.903;training one SVM model for predicting more than one response variable;Please I want to know if there is any SVM R package that can handle more than one response variable (y) at a time. that is to train one model for predicting more than one response variable. it could be regression or multi class classification problem.Thanks for your help;[education, open-source];121;
5234;2;2015-02-25T22:41:45.663;;"I don't think that you will learn much about data science (meaning, acquire understanding and skills) by using software tools like Tableau. Such tools are targeting mainly advanced users (not data scientists), for example analysts and other subject matter experts, who use graphical user interface (GUI) to analyze and (mostly) visualize data. Having said that, software tools like Tableau might be good enough to perform initial phase of data science workflow: exploratory data analysis (EDA).In terms of data science self-education, there are several popular online courses (MOOCs) that you can choose from (most come in both free and paid versions). In addition to the one on Udacity that you've mentioned (https://www.udacity.com/course/ud359), there are two data science courses on Coursera: Introduction to Data Science by University of Washington (https://www.coursera.org/course/datasci) and a set of courses from Data Science specialization by Johns Hopkins University (https://www.coursera.org/specialization/jhudatascience/1). Note that you can take specialization's individual courses for free at your convenience. There are several other, albeit less popular, data science MOOCs.In terms of data sources, I'm not sure what do you mean by ""Dummy Data"", but there is a wealth of open data sets, including many in the area of public health. You can review corresponding resources, listed on KDnuggets (http://www.kdnuggets.com/datasets/index.html) and choose ones that you're interested in. For a country-level analysis, the fastest way to obtain data is finding and visiting corresponding open data government websites. For example, for public health data in US, I would go to http://www.healthdata.gov and http://www.data.gov (the latter - for corresponding non-medical data that you might want to include in your analysis).In regard to research papers in the area of public health, I have two comments: 1) most empirical research in that (or any other) area IMHO can be considered a data science study/project; 2) you need to perform a literature review in the area or on the topic of your interest, so you're on your own in that sense.Finally, a note on software tools. If you're serious about data science, I would suggest to invest some time in learning either R, or Python (if you don't know them already), as those are two most popular open source tools among data scientists nowadays. Both have a variety of feature-rich development environments as well as large ecosystems of packages/libraries and users/developers all over the world.You might also find useful some of my other related answers here on Data Science StackExchange site. For example, I recommend you to read this answer, this answer and this answer. Good luck!";;;
5235;2;2015-02-26T05:08:31.993;;Do you know if the scheduler has a memory?Let us assume for a moment that the scheduler has no memory.  This is a straightforward classification (supervised learning) problem: the inputs are X, the outputs are the schedules (N->M maps).  Actually, if every N gets scheduled and the only question is which M it gets, the outputs are lists which channel (or none) is scheduled to each block, and there is only a certain possible number of those, so you can model them as discrete outputs (classes) with their own probabilities.  Use whatever you like (AdaBoost, Naive Bayes, RBF SVM, Random Forest...) as a classifier.  I think you will quickly learn about the general behavior of the scheduler.If the scheduler has a memory, then things get complicated.  I think you might approach that as a hidden Markov model: but the number of individual states may be quite large, and so it may be essentially impossible to build a complete map of transition probabilities.;;;
5236;2;2015-02-26T11:19:35.900;;"I am not sure there is a clear answer to this, especially as the problem does not seem to be well-defined right now - your ""figure"" seems to indicate edge weights but you then mention node weights, something significantly different.If the question is whether you can find a way to split a graph into two smaller modules, then you might want to look into applying Sparsest Cut techniques - a cut with low cost would imply (?) high modularity. I believe these can be easily modified to account for either unlabeled, edge-labeled or node-labeled graphs.";;;
5237;2;2015-02-26T11:31:04.870;;This is very similar to the netflix problem, most matrix factorization methods can be adapted so that the error function is only evaluated at known points. For instance, you can take the gradient descent approach to SVD (minimizing the frobenius norm) but only evaluate the error and calculate the gradient at known points. I believe you can easily find code for this.Another option would be exploiting the binary nature of your matrix and adapting binary matrix factorization tools in order to enforce binary factors (if you require them). I'm sure you can adapt one of the methods described here to work with unknown data using a similar trick as the one above.;;;
5238;2;2015-02-26T12:10:17.283;;I think those two maybe help you:Introduction to Data ScienceDataquest.io;;;
5239;2;2015-02-27T00:45:39.740;;"You may want to consider using your own APparameter object to put ""significance"" constraints on the rules learned by Apriori. See page 13 of the arules documentation. This could reduce the number of uninteresting rules returned in your run.In lieu of gold standard data for your domain, consider bootstrap resampling as a form of validation, as described in this article.";;;
5241;2;2015-02-27T05:28:25.457;;"I think this line is wrong:ytr = X(ii(1:N/2),:);ytr should be the label of the training data. In this case, it should beytr = y(ii(1:N/2),:);";;;
5242;2;2015-02-27T08:13:53.847;;Check out the e1071 package, here is the manual http://cran.r-project.org/web/packages/e1071/e1071.pdf;;;
5243;1;2015-02-27T10:22:59.577;Interested in Mathematical Statistics... where to start from?;"I have been working in the last years with statistics and have gone pretty deep in programming with R. I have however always felt that I wasn't completely grasping what I was doing, still understanding all passages and procedures conceptually.I wanted to get a bit deeper into the math behind it all. I've been looking online for texts and tips, but all texts start with a very high level. Any suggestions on where to start?To be more precise, I'm not looking for an exaustive list of statistical models and how they work, I kind of get those. I was looking for something like ""Basics of statistical modelling""";[education, open-source];95;
5244;1;2015-02-27T10:42:35.407;R in production;"Many of us are very familiar with using R in reproducible, but very much targeted, ad-hoc analysis. Given that R is currently the best collection of cutting-edge scientific methods from world-class experts in each particular field, and given that plenty of libraries exist for data io in R, it seems very natural to extend its applications into production environments for live decision making.Therefore my questions are:did someone of you go into production with pure R (I know of shiny, yhat etc, but would be very interesting to hear of pure R);is there a good book/guide/article on the topic of building R into some serious live decision-making pipelines (such as e.g. credit scoring);I would like to hear also if you think it's not a good idea at all;";[education, open-source];253;2
5245;1;2015-02-27T12:44:45.723;Extract Relationship Between two Entities using StanfordCoreNLP;"the similar question has been asked here but i cant find any relevant answer of it so i am trying again. I am able to get NER and Dependency tree using the library. Now what i looking for is that i want to extract entities with the relationship between the entities. For example , ""flipkart has invested in myntra"" so i should be able to get entity1 as ""flipkart "" and entity2 as ""myntra"" and ""investor"" as the relation .or similar kind of structure.Till now i am able to perform ""entity linking"" part. my code is as below.public static void main(String[] args) throws IOException, ClassNotFoundException {// creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution Properties props = new Properties();props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");StanfordCoreNLP pipeline = new StanfordCoreNLP(props);// read some text in the text variable//String text = ""Mary has a little lamb. She is very cute.""; // Add your text here!String text = ""Matrix Partners along with existing investors Sequoia Capital and Nexus Venture Partners has invested R100 Cr in Mumbai based food ordering app, TinyOwl. The series B funding will be used by the company to expand its geographical presence to over 50 cities, upgrade technology and enhance user experience."";text+=""In December last year, it raised $3 Mn from Sequoia Capital India and Nexus Venture Partners to deepen its presence in home market Mumbai. It was seeded by Deap Ubhi (who had earlier founded Burrp) and Sandeep Tandon."";// create an empty Annotation just with the given textAnnotation document = new Annotation(text);// run all Annotators on this textpipeline.annotate(document);// these are all the sentences in this document// a CoreMap is essentially a Map that uses class objects as keys and has values with custom typesList<CoreMap> sentences = document.get(SentencesAnnotation.class);for(CoreMap sentence: sentences) {  // traversing the words in the current sentence  // a CoreLabel is a CoreMap with additional token-specific methods  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {    // this is the text of the token    String word = token.get(TextAnnotation.class);    //System.out.println("" word \n""+word);    // this is the POS tag of the token    String pos = token.get(PartOfSpeechAnnotation.class);   // System.out.println("" pos \n""+pos);    // this is the NER label of the token    String ne = token.get(NamedEntityTagAnnotation.class);      //System.out.println("" ne \n""+ne);  }  // this is the parse tree of the current sentence  Tree tree = sentence.get(TreeAnnotation.class);  System.out.println("" TREE \n""+tree);  // this is the Stanford dependency graph of the current sentence  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);  System.out.println("" dependencies \n""+dependencies);}// This is the coreference link graph// Each chain stores a set of mentions that link to each other,// along with a method for getting the most representative mention// Both sentence and token offsets start at 1!Map<Integer, CorefChain> graph = document.get(CorefChainAnnotation.class);System.out.println(""graph \n ""+graph);}I am not able to get correct tool for doing the same. I neee some guidelines guys, how to achieve this?. Thanks in advance";[education, open-source];89;
5248;2;2015-02-27T19:38:22.287;;"In Hive itself? Unfortunately, the answer is simply no -- as the language definition manual shows, that statistic is simply not built in. In addition to the language manual, you can get more information on statistics in development in Hive here and here.Having said that, there are plenty of ways to calculate Kendall's W on data that's in Hive. You could write out the data to a file or query it into R or a statistical package such as  SAS, Stat, MATLAB, Excel, etc then run your calculation and, if necessary, write your results back to Hive.In R, for instance, you could do something like this:install.packages(""RODBC"")require(RODBC)db   <- odbcConnect(""Hive_DB"")hql  <- ""select * from table A""data <- sqlQuery(db , hql)kenw <- cor(x = data$a, y = data$b, method=""kendall"")sqlSave(db, kenw, tablename = ""new_table_of_kendall_coef"")or (if using Linux or Unix) then you could use RHive without needing to use an ODBC name.Another way to go about it would be to take the functions that do exist in Hive (which you linked to) and calculate Kendall's coefficient yourself with a custom function. As to how to specifically implement that, well you'd probably want to post on Cross Validated (stats.stackexchange.com).";;;
5249;1;2015-02-28T00:44:04.353;Combining data sets without using row.name;"I start with a data.frame (or a data_frame) containing my dependent Y variable for analysis, my independent X variables, and some ""Z"" variables -- extra columns that I don't need for my modeling exercise.What I would like to do is:Create an analysis data set without the Z variables;Break this data set into random training and test sets;Find my best model;Predict on both the training and test sets using this model;Recombine the training and test sets by rows; and finallyRecombine these data with the Z variables, by column.It's the last step, of course, that presents the problem -- how do I make sure that the rows in the recombined training and test sets match the rows in the original data set? We might try to use the row.names variable from the original set, but I agree with Hadley that this is an error-prone kludge (my words, not his) -- why have a special column that's treated differently from all other data columns?One alternative is to create an ID column that uniquely identifies each row, and then keep this column around when dividing into the train and test sets (but excluding it from all modeling formulas, of course). This seems clumsy as well, and would make all my formulas harder to read.This must be a solved problem -- could people tell me how they deal with this? Especially using the plyr/dplyr/tidyr package framework?";[education, open-source];49;
5251;2;2015-02-28T17:10:41.147;;R and most of its CRAN modules are licensed using the GPL.In many companies, legal departments go crazy if you propose to use anything that is GPL in production... It's not reasonable, but you'll see they love Apache, and hate GPL. Before going into production, make sure it's okay with the legal department. (IMHO you are safe to use your modified code for internal products. Integrating R into your commercial product and handing this out to others is very different. But unfortunately, many legal departments try to ban all use of GPL whatsoever.)Other than that, R is often really slooow unless calling Fortran code hidden inside. It's nice when you are still trying to figure out what to do. But for production, you may want maximum performance, and full integration with your services. Benchmark yourself, if R is the best choice for your use case.On the performance issues with R (I know R advocates are going to downvote me for saying so ...): Morandat, F., Hill, B., Osvald, L., & Vitek, J. (2012). Evaluating the design of the R language. In ECOOP 2012–Object-Oriented Programming (pp. 104-131). Springer Berlin Heidelberg.(by the TraceR/ProfileR/ReactoR people from purdue, who are now working on fastR which tries to execute R code on the JVM?) states: On those benchmarks, R is on average 501 slower than C and 43 times slower Python.and: Observations. R is clearly slow and memory inefficient. Much more so than other dynamic languages. This is largely due to the combination of language features (call-by-value, extreme dynamism, lazy evaluation) and the lack of efficient built-in types. We believe that with some effort it should be possible to improve both time and space usage, but this would likely require a full rewrite of the implementation.Sorry to break the news. It's now my research, but it aligns with my observations.;;;
5252;2;2015-02-28T17:15:39.423;;As a fellow CS Ph.D. defending my dissertation in a Big Data-esque topic this year (I started in 2012), the best piece of material I can give you is in a link:http://www.rpajournal.com/dev/wp-content/uploads/2014/10/A3.pdfThis is an article written by two Ph.D.s from MIT who have talked about Big Data and MOOCs. Probably, you will find this a good starting point. BTW, along this note, if you really want to come up with a valid topic (that a committee and your adviser will let you propose, research and defend) you need to read LOTS and LOTS of papers. The majority of Ph.D. students make the fatal error of thinking that some 'idea' they have is new, when it's not and has already been done. You'll have to do something truly original to earn your Ph.D. Rather than actually focus on forming an idea right now, you should do a good literature survey and the ideas will 'suggest themselves'. Good luck! It's an exciting time for you.;;;
5253;2;2015-02-28T17:21:46.300;;I would actually try out regression. Also, don't make the mistake of using the serial number in your machine learning algorithms! The reason why I'm suggesting regression as opposed to 'better' machine learning algorithms is because you said you wanted to learn, and it's important to understand the algorithms (for the long run, and to truly be good at this stuff) that you're using. Regression is the easiest tool in the book that works quite well! Weka is so easy to use that you'll be able to plug and play different machine learning algorithms just for the sake of it. Another pointer that's won me several competitions is to do some feature selection before using regression/machine learning. For example, in your case, it is reasonable to assume that a student who scores high in Physics probably has a better chance of scoring high in Math as opposed to someone who scores high in English (but not necessarily Physics). If you have enough data, the algorithm itself will be able to deduce these positive/negative correlations and train the model accordingly. Sometimes, there isn't enough data, and you have to do some feature selection. Good luck! I'm a regular participant on Kaggle myself, and I think it's great that you're taking the 'hacker' route to learn more. It's the best way to get your hands dirty on real data and engineering problems. ;;;
5254;2;2015-02-28T21:42:31.383;;If you know your approach is working, you can try to implement it more efficiently. Identify the crucial points, try to vectorize them better, for example.The R interpreter isn't the fastest. There are some efforts underway, but they are not yet ready. Vectorization (which means less interpreter, more low-level code) often yields a factor of 2x-5x, but you can sometimes get a factor of 100x by implementing it e.g. in C. (And the R advocates are going to hate me for saying this...)Once you know that your approach is working, this may be worth the effort.;;;
5255;1;2015-02-28T22:10:58.473;What is an 'old name' of data scientist?;Terms like 'data science' and 'data scientist' are increasingly used these days.Many companies are hiring 'data scientist'. But I don't think it's a completely new job.Data have existed from the past and someone had to deal with data. I guess the term 'data scientist' becomes more popular because it sounds more fancy and 'sexy'How were data scientists called in the past?;[education, open-source];250;
5256;2;2015-02-28T23:21:40.430;;In reverse chronological order: data miner, statistician, (applied) mathematician.;;;
5257;1;2015-03-01T02:56:05.590;Good formula for turning star reviews into upvotes;I want to turn reviews of up to 5 stars and the number of reviews into upvotes. Whats a good algorithm for doing this?A venue with 10 reviews total with a 5 star average rating should obviously get more upvotes than a venue with 10 reviews total with a 3 star average rating. Also a venue with 60 ratings and a 4 star rating should probably get more upvotes than the one with 10 reviews and a 5 star rating.I need this rating to be based off of the total number of reviews and the average star rating, but I would also like the number to stay below a variable number. (like say upvotes stay below 100 but I can also plugin 200 and it will stay below 200);[education, open-source];39;
5259;2;2015-03-01T07:16:51.440;;"You need neither to use the row names or to create an additonal ID column. Here is an approach based on the indices of the training set.An example data set:set.seed(1)dat <- data.frame(Y = rnorm(10),                  X1 = rnorm(10),                  X2 = rnorm(10),                  Z1 = rnorm(10),                  Z2 = rnorm(10))Now, your steps:Create an analysis data set without the Z variablesdat2 <- dat[grep(""Z"", names(dat), invert = TRUE)]dat2#             Y          X1          X2# 1  -0.6264538  1.51178117  0.91897737# 2   0.1836433  0.38984324  0.78213630# 3  -0.8356286 -0.62124058  0.07456498# 4   1.5952808 -2.21469989 -1.98935170# 5   0.3295078  1.12493092  0.61982575# 6  -0.8204684 -0.04493361 -0.05612874# 7   0.4874291 -0.01619026 -0.15579551# 8   0.7383247  0.94383621 -1.47075238# 9   0.5757814  0.82122120 -0.47815006# 10 -0.3053884  0.59390132  0.41794156Break this data set into random training and test setstrain_idx <- sample(nrow(dat2), 0.8 * nrow(dat2))train_idx# [1]  7  4  3 10  9  2  1  5train <- dat2[train_idx, ]train#             Y          X1          X2# 7   0.4874291 -0.01619026 -0.15579551# 4   1.5952808 -2.21469989 -1.98935170# 3  -0.8356286 -0.62124058  0.07456498# 10 -0.3053884  0.59390132  0.41794156# 9   0.5757814  0.82122120 -0.47815006# 2   0.1836433  0.38984324  0.78213630# 1  -0.6264538  1.51178117  0.91897737# 5   0.3295078  1.12493092  0.61982575test_idx <- setdiff(seq(nrow(dat2)), train_idx)test_idx# [1] 6 8test <- dat2[test_idx, ]test#            Y          X1          X2# 6 -0.8204684 -0.04493361 -0.05612874# 8  0.7383247  0.94383621 -1.47075238Find my best model...Predict on both the training and test sets using this model...Recombine the training and test sets by rowsidx <- order(c(train_idx, test_idx))dat3 <- rbind(train, test)[idx, ]identical(dat3, dat2)# [1] TRUERecombine these data with the Z variables, by columndat4 <- cbind(dat3, dat[grep(""Z"", names(dat))])identical(dat, dat4)# [1] TRUEIn summary, we can use the indices of the training and test data to combine the data in the rows in the original order.";;;
5261;1;2015-03-01T14:28:22.123;Vectorization of for loop in sentiment analysis;"I'm struggling with for loop in R. I have a following data frame with sentences and two dictionaries with pos and neg words:library(stringr)library(plyr)library(dplyr)library(stringi)library(qdap)library(qdapRegex)library(reshape2)library(zoo)# Create data.frame with sentencessent <- data.frame(words = c(""great just great right size and i love this notebook"", ""benefits great laptop at the top"",                         ""wouldnt bad notebook and very good"", ""very good quality"", ""bad orgtop but great"",                         ""great improvement for that great improvement bad product but overall is not good"", ""notebook is not good but i love batterytop""), user = c(1,2,3,4,5,6,7),               number = c(1,1,1,1,1,1,1), stringsAsFactors=F)# Create pos/negWordsposWords <- c(""great"",""improvement"",""love"",""great improvement"",""very good"",""good"",""right"",""very"",""benefits"",          ""extra"",""benefit"",""top"",""extraordinarily"",""extraordinary"",""super"",""benefits super"",""good"",""benefits great"",          ""wouldnt bad"")negWords <- c(""hate"",""bad"",""not good"",""horrible"")And now I'm gonna to create replication of origin data frame for big data simulation:# Replicate original data.frame - big data simulation (700.000 rows of sentences)df.expanded <- as.data.frame(replicate(100000,sent$words))    sent <- coredata(sent)[rep(seq(nrow(sent)),100000),]    sent$words <- paste(c(""""), sent$words, c(""""), collapse = NULL)rownames(sent) <- NULLFor my further approach, I'll have to do descending ordering of words in dictionaries with their sentiment score (pos word = 1 and neg word = -1).# Ordering words in pos/negWordswordsDF <- data.frame(words = posWords, value = 1,stringsAsFactors=F)wordsDF <- rbind(wordsDF,data.frame(words = negWords, value = -1))wordsDF$lengths <- unlist(lapply(wordsDF$words, nchar))wordsDF <- wordsDF[order(-wordsDF[,3]),]wordsDF$words <- paste(c(""""), wordsDF$words, c(""""), collapse = NULL)rownames(wordsDF) <- NULLThen I have a following function with for loop. 1) matching exact words 2) count them 3) compute score 4) remove matched words from sentence for another iteration:scoreSentence_new <- function(sentence){  score <- 0  for(x in 1:nrow(wordsDF)){    sd <- function(text) {stri_count(text, regex=wordsDF[x,1])} # count matched words    results <- sapply(sentence, sd, USE.NAMES=F) # count matched words    score <- (score + (results * wordsDF[x,2])) # compute score    sentence <- str_replace_all(sentence, wordsDF[x,1], "" "") # remove matched words from sentence for next iteration  }  score}When I call that functionSentimentScore_new <- scoreSentence_new(sent$words)    sent_new <- cbind(sent, SentimentScore_new)    sent_new$words <- str_trim(sent_new$words, side = ""both"")it resulted into desired output:                                                                             words user     SentimentScore_new                             great just great right size and i love this notebook    1                  4                                                 benefits great laptop at the top    2                  2                                               wouldnt bad notebook and very good    3                  2                                                                very good quality    4                  1                                                             bad orgtop but great    5                  0 great improvement for that great improvement bad product but overall is not good    6                  0                                       notebook is not good but i love batterytop    7                  0In real I'm using dictionaries with pos/neg words about 7.000 words and I have 200.000 sentences. When I used my approach for 1.000 sentences it takes 45 mins. Please, could you anyone help me with some faster approach using of vectorization or parallel solution. Because of my beginner R programming skills I'm in the end of my efforts :-( Thank you very much in advance for any of your advice or solutionI was wondering about something like that:n <- 1:nrow(wordsDF)score <- 0try_1 <- function(ttt) {sd <- function(text) {stri_count(text, regex=wordsDF[ttt,1])}results <- sapply(sent$words, sd, USE.NAMES=F)    score <- (score + (results * wordsDF[ttt,2])) # compute score (count * sentValue)    sent$words <- str_replace_all(sent$words, wordsDF[ttt,1], "" "")score}a <- unlist(sapply(n, try_1))apply(a,1,sum)But doesn't work :-(";[education, open-source];190;1
5263;2;2015-03-02T05:05:55.497;;"Try using Earth Mover's Distance (http://ai.stanford.edu/~rubner/papers/rubnerIjcv00.pdf). It measures how much it ""costs"" to optimally reshape one histogram into another, where an elementary transform (using the terminology of edit distance-like distance measures) is moving a unit of mass from a bin to a bin. If you have an implementation of EMD, then your distance is dist = EMD(H1, H2, D), where H1 = [ 14, 13, 40, 31 ]; -- histogram #1 (first image) H2 = [ 82, 8, 7 ]; -- histogram #2 (second image) D  = [ D11, D12, D13; ... ; D41, D42, D43 ]; -- (cross-bin) ground distanceDij is a distance from the i'th bin of the first histogram to the j'th bin of the second histogram. For example, D21 is a distance between 'White' and 'Pink'. It may be defined differently, based on which color space you want to use, and further, based on how you define what it means for two colors to be similar. If I use RGB, and I do not care much as to how human eye perceives colors, then D21 can be the \ell_2 norm of the difference [1, 1, 1] - [1, 0.05, 0.7]. (As far as I remember, the paper I mentioned uses LAB instead of RGB.)P.S. Besides color, you may include spatial information into comparison. EMD can handle it well, as long as you properly combine similarity of colors with spatial similarity in the definition of ground distance D.";;;
5264;1;2015-03-02T05:10:01.897;Does reinforcement learning always work on grid world?;Does reinforcement learning always need a grid world problem to be applied to? Can anyone give me any other example of how reinforcement learning can be applied to something which does not have a grid world scenario?;[education, open-source];60;
5265;2;2015-03-02T10:06:27.483;;"Reinforcement learning does not depend on a grid world.  It can be applied to any space of possibilities where there is a ""fitness function"" that maps between points in the space to a fitness metric.Topological spaces have a formally-defined ""neighborhoods"" but do not necessarily conform to a grid or any dimensional representation.  In a topological space, the only way to get from ""here"" to ""there"" is via some ""paths"" which are sets of contiguous neighborhoods. Continuous fitness functions can be defined over topological spaces.For what it is worth, reinforcement learning is not the be-all-end-all (family of) learning algorithms in fitness landscapes.  In a sufficiently rugged fitness landscape, other learning algorithms can perform better.  Also, if there are regions of the space where there are no well-defined fitness function at a given point in time, it may be indeterminate as to what learning algorithms are optimal, if any.";;;
5267;1;2015-03-02T21:15:34.480;Predicting Soccer: guessing which matches a model will predict correcly;I took on a project to predict the outcome of soccer matches but it turned out to be a very challenging task.I tried out different models but I only got 50-54% accuracy on my test dataset. Some of the models were created in sucha way that a certain model would predict if a team will win, draw, or loose a match. That same model would also predict ifthe opponent of that team will win, draw, or loose the match. Each model predicting with an accuracy of about 50% on each team distinctively. The second set of models I tried, takes the combination of data from both teams and predicts which class the match belongs to (home win, away win, draw). In the system,only 10 matches are given everyday to be predicted. Meaning, if I predict the 10 matches using the second model, I have a chance of predicting 5 correctly. In this project, I only need to predict 3 matches correctly out of the 10 matches given in a day. Is there a systemof knowing the 3 matches which my models have the best chance of predicting correctly? I only need to get 3 correct, I usuallyget 5 correctly but I don't know how to select my 3 best matches. Note: The first type of models use about 50 features for prediction while the second uses 101. I've tried ensembles, they still give me ~50% accuracy. I'm still about to setup a system that selects matches where the prediction for the home team does not contradict the prediction for the away team using the first type of model.;[education, open-source];255;
5268;1;2015-03-02T23:02:45.583;How to detect overfitting of a stock screener;"The project I am working on allows users to create Stock Screeners based on both technical and fundamental criteria.  Stock Screeners are then ""backtested"" by simulating the results of applying in over the last 10 years using Point-in-Time data. I get back the list of trades and overall graph of performance.(If that is unclear, I have an overview here and there with more details).Now a common problem is that users create overfitted stock screeners. I would love to give them a warning when the screen is likely to be over-fitted.Fields I have to work withAll trades made by the Stock ScreenerStock, Start Date, Start Price, End Date, End PriceS&P 500 performance for the same time frameMarket Cap, Sector, and Industry of each Stock";[education, open-source];89;
5269;2;2015-03-03T01:35:31.727;;The short answer is no! Reinforcement Learning is not limited to discrete spaces. But most of the introductory literature does deal with discrete spaces.As you might know by now that there are three important components in any Reinforcement Learning problem: Rewards, States and Actions. The first is a scalar quantity and theoretically the latter two can either be discrete or continuous. The convergence proofs and analyses of the various algorithms are easier to understand for the discrete case and also the corresponding algorithms are easier to code. That is one of the reasons, most introductory material focuses on them.Having said that, it should be interesting to note that the early research on Reinforcement Learning actually focussed on continuous state representations. It was only in the the 90s since the literature started representing all the standard algorithms for discrete spaces as we had a lot of proofs for them.Finally, if you noticed carefully, I said continuous states only. Mapping continuous states and continuous actions is hard. Nevertheless, we do have some solutions for now. But it is an active area of Research in RL. This paper by Sutton from '98 should be a good start for your exploration!;;;
5270;2;2015-03-03T05:59:08.820;;Training a model, related to information extraction, in general, and named entity recognition/resolution (NER), in particular, is described in detail in Chapter 7 of the NLTK Book, available online at this URL: http://www.nltk.org/book/ch07.html.Additionally, I think that you might find useful my related answer on Cross Validated site. It has a lot of references to relevant sources on NER and related topics as well as to various related software tools.;;;
5273;1;2015-03-03T16:12:13.860;How many possible hypothesis;Training set contains $p$ papers. Each paper is annotated as research or non-research. To develop the research paper filter, we consider the $W$ most frequent phrases in a paper. The research paper filter will use the presence/absence of these $W$ phrases to decide if the paper is indeed a research paper or not.How many possible hypothesis are there?I have $2^W$ possible hypothesis, but if we are including conjunctive hypothesis symbols, then I would say $4^W$. Is my logic correct?How many of them are consistent?For this question, I have no idea where to begin. Any insight?;[education, open-source];93;1
5275;2;2015-03-04T04:39:35.380;;I do think it is new job, basically data scientist has to apply mathematical algorithms on data with considerable constraint in terms 1) Run time of the application 2) Resource use of the application. If these constraints are not present, I would not call the job data science. Moreover, these algorithms are often need to be ran on distributed systems, which is another dimension of the problem.Of course, this has been done before, in some combination of statistics, mathematics and programming, but it was not wide spread to give rise to the new term. The real rise of data science is from the ability to gather large amounts of data, thus need to need to process it. ;;;
5276;2;2015-03-04T06:05:05.320;;Terms that covered more or less the same topics that Data Science covers today:Pattern RecognitionMachine Learning Data Mining Quantitative methods;;;
5277;1;2015-03-04T08:05:45.003;Do you have to normalize data when building decision trees using R?;So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.We learned normalization last week and it seems like you're supposed to normalize data when they have very different values. For decision trees, is the case the same?I'm not sure about this but would normalization affect the resulting decision tree from the same data set? It doesn't seem like it should but...;[education, open-source];103;
5278;2;2015-03-04T12:15:10.117;;Most common types of decision trees you encounter are not affected by any monotonic transformation. So, as long as you preserve orde, the decision trees are the same (obviously by the same tree here I understand the same decision structure, not the same values for each test in each node of the tree). The reason why it happens is because how usual impurity functions works. In order to find the best split it searches on each dimension (attribute) a split point which is basically an if clause which groups target values corresponding to instances which has test value less than split value, and on the right the values greater than equal. This happens for numerical attributes (which I think is your case because I do not know how to normalize a nominal attribute). Now you might note that the criteria is less than or greater than. Which means that the real information from the attributes in order to find the split (and the whole tree) is only the order of the values. Which means that as long as you transform your attributes in such a way that the original ordering is reserved, you will get the same tree. Not all models are insensitive to such kind of transformation. For example linear regression models give the same results if you multiply an attribute with something different than zero. You will get different regression coefficients, but the predicted value will be the same. This is not the case when you take a log of that transformation. So for linear regression, for example, normalizing is useless since it will provide the same result. However this is not the case with a penalized linear regression, like ridge regression. In penalized linear regressions a constraint is applied to coefficients. The idea is that the constraint is applied to the sum  of a function of coefficients. Now if you inflate an attribute, the coefficient will be deflated, which means that in the end the penalization for that coefficient it will be artificially modified. In such kind of situation, you normalize attributes in order that each coefficient to be constraint 'fairly'.Hope it helps;;;
5279;1;2015-03-04T14:19:00.713;How to select topology for neural network?;I was given a target function to design neural network and train: (y = (x1 ∧ x2) ∨ (x3 ∧ x4))The number of input and number of output seems obvious (4 and 1). And the training data can use truth table.However, in order to train as a multilayer artificial neural network, I need to choose number of hidden units. May I know where can I find some general guideline for this?Thank you!;[education, open-source];89;2
5280;1;2015-03-04T14:34:51.013;Computation of a column-stochastic matrix with target row sums;I want to generate an $N\times N$ matrix $A$ so as to target an $N$ vector of row sums and simultaneously all column sums should sum to 1. In addition to this, I have a prefixed number of elements which are set to zero. For example, beginning with:$$  \left[\begin{array}{rrr}    0 & 1 & 1 \\    1 & 0 & 0 \\    1 & 1 & 0  \end{array}\right]$$and the row sum vector $[1.5, 0.25, 1]^{T}$, I want to end up with$$  \left[\begin{array}{rrr}    0 & a_{12} & a_{13} \\    a_{21} & 0 & 0 \\    a_{31} & a_{32} & 0  \end{array}\right]$$under the following conditions:$a_{12} + a_{13}  = 1.5$$ a_{21} = 0.25$$a_{31}+a_{32} = 1$$a_{21}+a_{31} = 1$$a_{12}+a_{32} = 1$$a_{13} = 1$While this is simplistic, in general, I have $2N$ equations in $N^{2}-Z$ unknowns, where $Z$ is the number of elements fixed to zero. So, this system of equations could be overdetermined or underdetermined, but I would like to be able to generate matrices like this such that all nonzero elements lie in $(0,1]$.;[education, open-source];43;
5281;2;2015-03-04T15:53:46.530;;"Speed of code execution is rarely an issue.  The important speed in business is almost always the speed of designing, deploying, and maintaining the application.  An experienced programmer can optimize where necessary to get code execution fast enough.  In these cases, R can make a lot of sense in production. In cases where speed of execution IS an issue, you are already going to find an optimized C++ or some such real-time decision engine.  So your choices are integrate an R process, or add the bits you need to the engine.  The latter is probably the only option, not because of the speed of R, but because you don't have the time to incorporate any external process.  If the company has nothing to start with, I can't imagine everyone saying ""let's build our time critical real-time engine in R because of the great statistical libraries"".  I'll give a few examples from my corporate experiences, where I use R in production:Delivering Shiny applications dealing with data that is not/ not yet institutionalized.  I will generally load already-processed data frames and use Shiny to display different graphs and charts.  Computation is minimal.   Decision making analysis that requires heavy use of advanced libraries (mcclust, machine learning) but done on a daily or longer time-scale.  In this case there is no reason to use any other language.  I've already done the prototyping in R, so my fastest and best option is to keep things there.I did not use R for production when integrating with a real-time C++ decision engine.  Issues:An additional layer of complication to spawn R processes and integrate the resultsA suitable machine-learning library (Waffles) was available in C++The caveat in the latter case: I still use R to generate the training files.";;;
5282;2;2015-03-04T21:47:35.453;;You problem is linearly separable so you can use a single layer perceptron. Hidden units are necessary only for non linear problems (xor is a classic example). As for general guidelines, I don't think anyone has ever needed more than one hidden layer. Also the number of neurons in the hidden layer doesn't need to exceed the number of inputs. ;;;
5284;1;2015-03-05T18:28:43.367;How append works in hdfs? Where the newly created instance of file is placed?;I'm a Newbie to Hadoop!By web search and after going through hadoop guides, it is clear that hdfs doesn't allow us to edit the file but to append some data to existing file, it will be creating a new instance temporarily and append the new data to it.That being said,1.would like to know whether the new file or temp file is created in the same block or in a different block?2.What will happen if the revised file exceeds the previous  allocated block size?Any help would be greatly appreciated!;[education, open-source];77;
5285;2;2015-03-06T02:43:38.727;;Here are two ideas that I was thinking about.Bug PredictionBased on the previous activity future bugs can be predicted. There are a number of papers available on the net. I remember there was a paper about bug prediction from SVN data. ( Just google for software bug prediction )Error position from software tracesErrors may be like rare events or anomalies in the trace output. May be you can classify the error and pinpoint the procedure that caused the error. I am currently thinking about a system like that now ( Not sure about its success though ). I asked a question here: http://stats.stackexchange.com/questions/140232/error-position-in-software-trace-file;;;
5286;2;2015-03-06T03:46:41.693;;"When looking for texts to learn advanced topics, I start with a web search for relevant grad courses and textbooks, or background tech/math books like those from Dover.To wit, Theoretical Statistics by Keener looks relevant:http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-93838-7And this:""Looking for a good Mathematical Statistics self-study book (I'm a physics student and my class & current book are useless to me)""http://www.reddit.com/r/statistics/comments/1n6o19/looking_for_a_good_mathematical_statistics/";;;
5287;1;2015-03-06T04:09:56.030;Dataset to give same eigenvectors?;Any suggestion on what kind of dataset lets say $n \times d$ ($n$ rows, $d$ columns) would give me same eigenvectors?.I believe it should be the one with same absolute values in each cell. Like alternating +1 and -1. But it seems to work otherwise. Any pointers?;[education, open-source];58;
5288;2;2015-03-06T05:45:03.720;;"This problem, being more of a combinatorial rather than statistical nature, can be though of as a max-flow problem in a bipartite network. Each column of your matrix corresponds to a ""source node"" in the left part of the bipartite graph $BP(L, R)$; each row of the matrix corresponds to a ""destination node"" in the right part $R$. Capacity of each source node is the corresponding column's sum; similarly, capacities are defined for the destinations via row sums. Value $a_{ij}$ in the matrix describes flow of mass from $j$'th source to $i$'th destination along an existing edge $(j \to i)$ of $BP$. Your initially given 0-1 matrix describes which edges are present (e.g., the matrix in your example prescribes the edge $(3, 2)$ to be absent from the network, along with all self-loops). If you denote $F$ to be the sum of all column sums (or, alternatively, the total capacity of all sources), then your problem narrows down to finding a max-flow from $L$ to $R$ in $BP$ and checking whether the volume of the discovered flow equals $F$.Note 1 (Feasibility): Before plunging into finding best flows, you may want to check your row sums: a necessary condition for your problem to have a solution is that the sum of all column sums (which is $N$ in your particular case of a column-stochastic matrix) should be equal the sum of row sums, as both numbers describe the same -- the sum of all elements of the target matrix. (For example, for $N = 3$ and the vector of row sums from your example, the problem is unfeasible.)Note 2 (Upper bound): Since $a_{ij}$ describes the flow from $j$'th source, and the capacities of all sources are 1 in your setting, then each source cannot send more than 1 unit of mass to a single destination. Thus, each $a_{ij}$ will not exceed 1.Note 3 (Lower bound): To make $a_{ij}$ for each existing edge $(j \to i)$ strictly positive, you can augment the max-flow problem with lower bounds on edge capacities. In a general setting, edge capacity bounds are $0$ and $+\infty$. You may want to replace the lower bound with some positive number, small enough not to interfere with the discovery of the best flow and large enough to satisfy your edge saturation needs.";;;
5289;2;2015-03-06T06:11:10.997;;"The only such existing ""dataset"" is a 1-by-1 matrix with an arbitrary value of its sole item.If the eigenvectors, being the columns of matrix $V$, are ""the same"" (precisely up to a scalar constant factor), then matrix $V$ is singular (its columns are linearly dependent; its determinant is zero), and, hence, non-invertible $\Rightarrow$ such an eigendecomposition $V \cdot \Lambda \cdot V^{-1}$ cannot exist.";;;
5290;2;2015-03-06T07:56:19.903;;I will restrict myself to the finite-dimensional situation. First, if we are talking about normal eigenvalues and not generalized ones, we need a square matrix. If $M\in\mathbb{R}^{n\times n}$ (or $\mathbb{C}^{n\times n}$) is a diagonizable matrix, it has an eigendecomposition\begin{equation}M = V\Lambda V^{-1}\textrm{,}\end{equation}with $V:=\begin{bmatrix}v_{1},v_{2},\ldots,v_{n}\end{bmatrix}$ the matrix with its $n$ eigenvectors $v_{i}$ as columns and $\Lambda :=\operatorname{diag}\left(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)$ the diagonal matrix with the corresponding eigenvalues on its main diagonal.As you can see from this equation if you want to get matrices with the same eigenvectors, i.e. $V$ fixed, you can only change the eigenvalues $\lambda_{i}$. In general the relationship between the eigenvalues and the entries of the original matrix $M$ is non-trivial. The easiest situation arises if the eigenvectors $v_{i}$ form the standard basis, i.e. only $1$ at the $i$th position, $0$ otherwise. Then $M=I\Lambda I^{-1}=\Lambda $ ($I$ the identity matrix) and you can change every entry on the main diagonal without changing any eigenvector. EditI understood the question differently than victor. I thought the question asks what kind of different datasets can be described by the same eigenvectors. ;;;
5291;1;2015-03-06T10:11:02.133;Error using twitter R package's userTimlien;I am using twitteR package to retrievie timeline data. My request looks as follows:tweets <- try(userTimeline(user , n=50),silent=TRUE)and this worked quite well for a time, but now I receive this error message:Error in function (type, msg, asError = TRUE)  : easy handle already used in multi handleIn a related question on Stackoverflow one answer is to use Rcurl directly but this does not seem to work with twitteR package. Anybody got an idea on this?;[education, open-source];95;
5292;1;2015-03-06T10:28:04.483;Generic strategy for object detection;I have a huge collection of objects from which only a tiny fraction are in a class of interest. The collection is initially unlabelled, but labels can be added using an expensive operation (for example, by human).Currently I use the simple generic machine learning strategy:Use hand-crafted rules to select a smaller subset of objects (thus leaving out a fraction of interesting ones).Label part of the smaller subset, and use these for training and choosing a classification algorithm and its parameters.Classify the remaining objects in the smaller set (and also perhaps in the big set).This has two drawbacks:The labeller still needs to see a huge number of uninteresting objects, and therefore is able to label only a very small fraction of interesting ones. The objects not in the smaller set are completely ignored in the learning phase, resulting in a loss of some information (the classification algorithm might not work well on this complement).It seems that it would be better to use online learning: i.e., select the objects to show to the labeller based on the previous labels. But then it becomes no longer obvious that the result of classification algorithm retains the nice theoretical properties (i.e., statistical consistency).Is there a general framework for active object detection which works either theoretically or practically (or both)? I could not get the complete picture from the Wikipedia article active learning. ;[education, open-source];50;
5294;1;2015-03-09T16:53:10.930;How are neural nets related to Fourier transforms?;This is an interview question How are neural nets related to Fourier transforms?I could find papers that talk about methods to process the Discrete Fourier Transform (DFT) by a single-layer neural network with a linear transfer function. Is there some other correlation that I'm missing?;[education, open-source];128;
5295;2;2015-03-09T18:21:32.663;;Python has some very good tools for working with big data:numpyNumpy's memmory-mapped arrays let you access a file saved on disk as though it were an array.  Only the parts of the array you are actively working with need to be loaded into memory.  It can be used pretty much the same as an ordinary array.h5py and pytablesThese two libraries provide access to HDF5 files.  These files allow access to just part of the data. Further, thanks to the underlying libraries used to access the data, many mathematical operations and other manipulations of the data can be done without loading it into a python data structure.  Massive, highly structured files are possible, much bigger than 5 TB.  It also allows seamless, lossless compression.databasesThere are various types of databases that allow you to store big data sets and load just the parts you need.  Many databases allow you to do manipulations without loading the data into a python data structure at all.pandasThis allows higher-level access to various types of data, including HDF5 data, csv files, databases, even websites.  For big data, it provides wrappers around HDF5 file access that makes it easier to do analysis on big data sets.mpi4pyThis is a tool for running your python code in a distributed way across multiple processors or even multiple computers.  This allows you to work on parts of your data simultaneously.daskIt provides a version of the normal numpy array that supports many of the normal numpy operations in a multi-core manner that can work on data too large to fit into memory. blazeA tool specifically designed for big data.  It is basically a wrapper around the above libraries, providing consistent interfaces to a variety of different methods of storing large amounts of data (such as HDF5 or databases) and tools to make it easy to manipulate, do mathematical operations on, and analyze data that is too big to fit into memory.;;;
5298;2;2015-03-09T21:54:19.297;;"They are not related in any meaningful sense. Sure, you can use them both to extract features, or do any number of things, but the same can be said about a many techniques. I would have asked ""what kind of neural network?"" to see if the interviewer had something specific in mind.";;;
5299;2;2015-03-09T21:56:00.827;;The similarity is regression.NNs can be used for regression and the fourier transform is in its heart just a curve fit of multiple sin and cos functions to some data.;;;
5300;2;2015-03-10T09:29:06.893;;"Also: ""Business Intelligence developer""";;;
5301;1;2015-03-10T10:20:22.510;Algorithm for multiple extended string matching;I need to implement an algorithm for multiple extended string matching in text. Algorithms to match regular expression would be perhaps too slow.Extended means the presence of wildcards (any number of characters instead of a star), for example:abc*def //matches abcdef, abcpppppdef etc.Multiple means that the search is going on simultaneously for multiple string patterns (not a separate search for each pattern), for example:abc*defabcwhateversome*stringQUESTION:What is the fast algorithm that can do multiple extended string matching?Preferably, optimized for SIMD instructions and multicore implementation. Open source implementation (C/C++/Python) would be great as well. I'm interested in 10 Gbps+ performance on a single core of a modern CPU.Thank you;[education, open-source];97;
5302;1;2015-03-10T18:03:32.227;Transposing Every nth row to column in a large dataset;"I am attempting to work with a very large data-set (~1.5mil lines) for the first time in SAS and I am having some difficulty. The data-set I have is formatted as a ""long"" .txt file as follows:'cat1/: Topic1_Variable1''cat2/: Topic1_Variable2''cat3/: Topic1_Variable3''cat4/: Topic1_Variable4''cat1/: Topic2_Variable1''cat2/: Topic2_Variable2''cat3/: Topic2_Variable3''cat4/: Topic2_Variable4''cat1/: Topic3_Variable1''cat2/: Topic3_Variable2''cat3/: Topic3_Variable3''cat4/: Topic3_Variable4'...Fro analysis and sharing with others, I really would like to see it formatted as follows:cat1              cat2              cat3              cat4Topic1_Variable1  Topic1_Variable2  Topic1_Variable3  Topic1_Variable4Topic2_Variable1  Topic2_Variable2  Topic2_Variable3  Topic2_Variable4Topic3_Variable1  Topic3_Variable2  Topic3_Variable3  Topic3_Variable4I think that this may be easier in R, but I honestly am drawing a complete blank in SAS. I've even played with MS Access to try to get it to look the way I want but the program crashes every time (due to the size?). At any rate, I have looked into some of the statements in PROC TRANSPOSE and PROC SQL but it seems that most functions within those procedures are utilized to combine duplicate 'Topics'. In the data I have been provided, each ""group"" represents an individual response to a question with several thousand individuals repeated, I want to retain the independence of each occurrence and not perform a UNION as defined in PROC SQL. At this point, I feel like I am over-thinking this but I just can't get around the mental block and actually do what I am working toward. Any help or guidance is much appreciated. I'm open to trying all suggestions or ideas, I think I have access to most statistical computing programs. ";[education, open-source];55;
5303;1;2015-03-10T19:51:59.277;Couple PCA plot and clusters to labels;"I am trying my first 'project' concerning machine learning and I am a bit stuck.However, I am not sure if it's even possible but here goes my question.What I want to achieve is clustering user groups based on the amount of visits a user does on a certain website.So I started out with this feature matrix:USER    abc.be  abc.be/a    abc.be/b    xyz.be  xyz.be/a123      0        0           0            0      1456      1        0           1            0      0789      2        3           1            0      0321      1        0           1            0      1654      1        1           1            1      1987      0        1           0            3      0So I got in this example 5 features (my 5 different websites).So then I used PCA to come to 2 dimensions, so I could plot it and see how it went.My feature matrix (in my example) is 5 columns * 6 rows.My PCA matrix is 2 columns * 6 rows.I came to this plot (please note that this plot uses different data then the example but the idea is the same)The green points are my PCA pointsThe red circles are my K-Means centroids.But the part I am struggling with is this: so I got my clusters (red circles) but how can I use this to say:""Looks like most users go to either site A or site B)?So how can I couple my clusters to a feature label from my feature matrix?Or how does one approach this?Any help is appreciated :)";[education, open-source];97;
5304;1;2015-03-11T04:18:21.610;How to make an effective sampling from a database of text documents?;"Problem: I want to know methods to perform an effective sampling from a database.The size of the database is about 250K text documents and in this case each text document is related to some majors (Electrical Engineering, Medicine and so on). So far I have seen some simple techniques like Simple random sample and Stratified sampling; however, I don't think it's a good idea to apply them for the following reasons:In the case of simple random sample, for instance, there are a few documents in the database that talk about majors like Naval Engineering or Arts. Therefore I think they are less likely to be sampled with this method but I would like to have some samples of every major as possible.In the case of stratified sampling, most of the documents talk about more than one major so I cannot divide the database in subgroups because they will not be mutually exclusive, at least for the case in which each major is a subgroup.Finally, I cannot use the whole database due to expensive computational cost processing. So I would really appreciate any suggestions on other sampling methods. Thanks for any help in advance.";[education, open-source];76;
5305;1;2015-03-11T05:25:47.053;How do I get Twitter Dataset for Visualization;I am a newbie. I would like to do visualization on twitter data : top trends based on country (over map) and time variations (for each months in 1 year or for each year) . Can someone tell me where can I get the twitter data set and any advice on how to start proceeding would be really help full.   Thanks;[education, open-source];187;
5306;2;2015-03-11T06:52:39.440;;"(Fresh and complete) Twitter data is not easy to get nowadays, as Twitter is actively monetizing it (through GNIP). There is still a large Twitter graph collected by Kwak et al. for their ""What is Twitter, a social network or a news media?"" WWW'10 publicly available, but as to tweets, most nice datasets have disappeared from the Internet due to complaints from Twitter. There are still tiny datasets of tweets available online (e.g.), but they are small and stale. Thus, I see two ""legal"" ways to go about getting tweets now: 1) crawl Twitter using the new version of Twitter API (the obvious limitation of this approach is the limit on the number of requests you can send; this approach is unlikely to give you a representative picture of what is trending if you collect raw tweets, just because you won't have enough tweets); 2) buy data from Twitter (GNIP), which will cost an awful lot of money. Alternatively, you can 3) study Twitter API for other means of getting ""summarized"" information from it, not tweets directly. For example, they may provide a method to retrieve, say, 10 most popular hash tags at the moment, maybe, even filtered by geo-location -- you cannot do much data science with the result, but at least you can use it in your visualization application.";;;
5308;2;2015-03-11T11:44:01.873;;"Off the top of my head, I would consider, at least, two approaches, as follows.Graph sampling. If you can (and it makes sense to) model your population (database of text documents) as a graph, then consider graph sampling. Check this relevant answer of mine on the topic on Cross Validated site (contains many references - I would start with this nice overview paper).Topic modeling. Another alternative is topic modeling (also referred to as topic models). See this introductory paper as well as this more detailed paper on the subject. Also, take a look at this interesting paper on topic modeling of streaming documents (or other situations, such as large databases, when unsupervised solutions make more sense). Finally, speaking about software for topic modeling, while many options do exist, I suggest investigating MALLET, which is an interesting Java-based software package for ""statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text"".";;;
5309;1;2015-03-12T11:08:43.067;How can the performance of a neural network vary considerably without changing any parameters?;I am training a neural network with 1 sigmoid hidden layer and a linear output layer. The network simply approximates a cosine function. The weights are initiliazed according to Nguyen-Widrow initialization and the biases are initialized to 1. I am using MATLAB as a platform.Running the network a number of times without changing any parameters, I am getting results (mean squared error) which range from 0.5 to 0.5*10^-6. I cannot understand how the results can even vary that much, I'd imagine there would at least be a narrower and more consistent window of errors.What could be causing such a large variance?;[education, open-source];60;
5310;2;2015-03-12T13:18:03.737;;In general, there is no guarantee that ANNs such as a multi-layer Perceptron network will converge to the global minimum squared error (MSE) solution. The final state of the network can be heavily dependent on how the network weights are initialized. Since most initialization schemes (including Nguyen-Widrow) use random numbers to generate the initial weights, it is quite possible that some initial states will result in convergence on local minima, whereas others will converge on the MSE solution.;;;
5311;1;2015-03-13T02:32:55.357;Post Hoc Analysis in Mixed linear models;"I tested whether different version-styles of a loading screen (hourglass vs. progress bar) in different progression patterns (linear, accelerate, decelerate, irregular, binary)affect time estimations within subjects. By analyzing the data with a binomial linear mixed effects model I have found significant results for the interaction effect of ""versionStyle x DisplayDuration x progressionPattern"" I would like to run a post hoc analysis to test across which condition and Display duration the time estimation was significantly affected by the version Style""Is there some command that I can use in R?This is the code I used for my analysis:(data1 <- glmer(Long ~ DisplayDur * Pattern * Proggression+ (1 + DisplayDur + Pattern+ Progression| Subjects),                dat=anadat,family=""binomial"",control=glmerControl(optimizer=""bobyqa"")))";[education, open-source];17;
5312;2;2015-03-13T12:16:11.643;;There's a tweet collection (comprising of 16 million tweets) that you can get from NIST. This collection is used for the ad hoc search task on micro-blogs. TREC 2011 Microblog collection;;;
5313;1;2015-03-13T14:10:11.143;Forecasting sales and creating model;In a assignment we are given macro economic indicators like GDP, Consumer price index, Producer Price index and Industrial production index. Also we are given Crude oil, Sugar prices and FM-CG Sales. We are required to forecast future quarter sales and give a model. As I'm new to this subject, I don't know where to start with it, or what to read. Can anyone provide me with some examples of what to do, or any PDFs which might be helpful. ;[education, open-source];36;
5314;2;2015-03-13T15:24:57.223;;"If you are new to forecasting, I would recommend starting with a very simple model for your sales, e.g., Exponential Smoothing, which will use only the sales data and no external information, but will capture trend and seasonality. Here is an extremely good free online textbook.Once you are comfortable with your simple model, you can look at more complex ones that do include external causal variables. For instance, you could run ARIMAX models (the X stands for eXternal or eXplanatory variables). For this, look at chapter 8 in the online textbook and check the auto.arima() function in the forecast package for R, where you can specify external variables using the xreg parameter.Alternatively, you could look at other methods like Neural Networks, or possibly even Vector Autoregression. To learn more, you could look into standard econometrics textbooks.Whatever you do, keep two things in mind:If you include explanatory variables in your forecast, you will need to forecast those variables themselves. In an assignment-type situation, you may be able to work with actual future macroeconomic variables, but of course in a ""real"" forecasting situation, you don't know next month's sugar prices yet. Forecasting these to feed the sugar price forecast into your sales forecast model adds an additional bit of uncertainty.You may just find out that your fancy model with lots of additional explanatory variables does not do a much better job at forecasting than your original simple model. This would not at all be surprising. Simple methods very often outperform complex ones in forecasting.";;;
5315;2;2015-03-13T15:41:35.577;;You're going to need to do simple random sampling, but maintain counts of the labels you've seen thus far. When you reach quota for a label, you'll need to cull all documents with that label from the pool, and sample from the remaining documents without that label. I think this will be a fair way of getting a stratified sample when your labels are overlapping.;;;
5316;1;2015-03-13T16:41:29.280;General approach to extract key text from sentence (nlp);Given a sentence like:Complimentary gym access for two for the length of stay ($12 value per person per day)What general approach can I take to identify the word gym or gym access?;[education, open-source];208;2
5318;1;2015-03-13T22:43:42.760;Probability of similarity of two clusters;For example, there are 4 objects. Each object has some elements. There are 3 clusters. Each element of each object belongs to some cluster. Clustering was done before, and we know only how many elements of each object belong to some cluster.How we can calculate the porbability of similarity of two objects?         Cluster1  Cluster2  Cluster3Object1    500       300        200Object2    200       200        100Object3    250       2500       1250       Object4    190       210        300;[education, open-source];87;
5319;2;2015-03-14T13:02:21.530;;I asked a similar question: http://stats.stackexchange.com/q/140406/70282What I ended up doing is having a while loop repeatably create NNs of a small size looking for the best root mean square error.Essentially:For hidden layer size of 1 to 10 doFor trials of 1 to 10:Create NN and test for rmse with test data.Remember NN and best rmse for particular hidden layer size.Done for.Done for.Review results choosing the smallest hidden layer size which had a small rmse.By separating your data into training and tedtinh data, you don't pick an over fit NN.;;;
5320;1;2015-03-14T14:48:12.233;Need help with python code as part of a data analysis project;I am new to python programming. As part of a data analysis project, I am trying to get a scatter plot of Salary vs Wins for each year of 4 consecutive years (so I am trying to get 4 different scatter plots, one for each year). I am using the following code:teamName = 'OAK'years = np.arange(2000,2004)for yr in years:    df = joined[joined['yearID']==yr]    plt.scatter(df['salary']/1e6,df['W'])    plt.title('Wins vs Salaries in year' + str(yr))    plt.xlabel('Total Salary (in millions)')    plt.ylabel('Wins')    plt.xlim(0,180)    plt.ylim(30,130)    plt.grid()    plt.show()However, I am only getting one plot corresponding to 2003. Can anyone point out the mistake ?Thanks;[education, open-source];61;
5322;1;2015-03-14T19:18:11.747;Cost function for support vector regression;"The optimisation problem for support vector regression is (see http://alex.smola.org/papers/2003/SmoSch03b.pdf):minimise:\begin{align*}C\sum_{i=0}^{l}(\xi_{i} +\xi^{*}_{i})+ \frac{1}{2}\lVert w \rVert^{2}\end{align*}subject to the constraints:\begin{align*}& y_{i} - <w,x_{i}> - b \leq \epsilon + \xi_{i} \\& <w,x_{i}> + b - y_{i} \leq \epsilon + \xi^{*}_{i} \\& \xi_{i}, \xi^{*}_{i} \geq 0\end{align*}I do not understand where the $\lVert w \rVert^{2}$ comes from. I understand how the $\lVert w \rVert^{2}$ is derived in the case of support vector classification (by maximizing the margin), but not in the case of regression.The paper says that the ""goal is to find a function $f(x)$ that has at most $\epsilon$ deviation from the actually obtained targets $y_{i}$ for all the training data, and at the same time is as flat as possible. In other words, we do not care about errors as long as they are less than $\epsilon$, but will not accept any deviation larger than this.""But I am not sure what ""flat"" means.Does someone know?";[education, open-source];79;
5323;1;2015-03-15T11:47:55.003;Implementation of Association Rules in Javascript;I have implemented an interactive visualization with d3.js, javascript to explore the frequency and various combinations of co-occurring item sets. I want to complement the interactive exploration with some automated options.Does someone know an efficient javascript implementation of the association rules mining ? My typical scenario will have just up to 30 different items.There are some good web site with implementations of frequent item set mining (improvements from the initial apriori algorithm): http://www.borgelt.net/apriori.html Any help is greatly appreciated.;[education, open-source];62;
5324;2;2015-03-15T18:37:55.840;;I don't know how efficient they are, but I did find some implementations:https://github.com/dmarges/apriorihttps://github.com/seratch/apriori.js;;;
5325;2;2015-03-15T18:53:08.857;;"Flat means parallel to the x axis; having a small slope. The smaller w is, the closer f(x) is to b; recall that $f(x) \equiv \left< w, x \right> + b$. One way to think about this is as a form of regularization; the flatter the function, the simpler or more parsimonious it is. This applies to both classification and regression.";;;
5326;1;2015-03-15T18:54:51.473;How to read file from user in Shiny and assign it to a variable in global.r?;I want to read a csv file as input from user in Shiny and assign it to a variable in global.r file.The code I have in ui.R isfileInput('file1', 'Choose CSV File',                     accept=c('text/csv', 'text/comma-separated-values,text/plain', .csv'))The code I have in main Panel of server.R istextOutput('contents')The code I have currently in server.R isoutput$contents <- renderText({          if(is.null(input$file1))return()      inFile <- input$file1      data<-read.csv(inFile$datapath)  print(summary(data)) })I want to assign this input$file1 to a variable called 'data' in global.r. Please let me know if this is possible. Thanks;[education, open-source];445;
5327;2;2015-03-15T19:42:16.513;;All of your plots are appearing on top of each other.  You need to invoke plt.subplot(xxx) before you create each plot.  For info on how the xxx command works, go to the MATLAB documentation.You might end up with multiple figures - see this page for info about that.;;;
5328;1;2015-03-16T11:20:08.417;use classification to improve clustering;I am from information security field, and have some introductory level understanding of machine learning field. My problem is to identify behavioural patterns from network traffic. If I use supervised learning ( classification ) the results are very promising but I encounter of a problem of missing out a pattern with slight change in behaviour. And If I use clustering then i need to do a much work on manual tuning of centroids position to get better clusters. I am thinking to ensemble classification and clustering and use classification output to tune cluster centroids. Is this is a good idea to address the problem?;[education, open-source];42;
5329;1;2015-03-16T11:33:30.593;Measuring similarity for sets with same cardinality;The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.I had 100 sets all of same cardinality. By mistake I calculated the similarity measures as the ratio of intersection with total elements in a set (i.e 100).This gives different similarity values than the original Jaccard formula.I was wondering if the original formula has considered the union of two sets to handle cases where there might be sets with different cardinalities.I think though numerically my values are different, they repersent the same idea.If anybody could verify/disverify what I am trying to do ?;[education, open-source];64;
5330;1;2015-03-16T19:53:31.303;subsetting R data frame;"I am subsetting some data frames and am sure there is a better way.Essentially, I have two data frames.  The first is the actual data.  The second has some meta data and importantly a flag on whether or not the row is in the subset I am interested in.  All I would like to do is pull out the subset and write a file.For instance, here is my subset data frame:     head(temp[,c(1,4:8)])     ensembl_gene_id   FGF Secreted  SigP Receptors    TF1 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE2 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE3 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE4 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE5 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSE6 ENSMUSG00000000001 FALSE    FALSE FALSE      TRUE FALSEHere is my actual data:Expt1_C1_EXONS_dxd_res_wDesc[1:5,1:5]                                   groupID featureID exonBaseMean dispersion         statENSMUSG00000000001:E001 ENSMUSG00000000001      E001    624.80240 0.04271781  1.255734504ENSMUSG00000000001:E002 ENSMUSG00000000001      E002     30.92281 0.02036015  0.514038911ENSMUSG00000000001:E003 ENSMUSG00000000001      E003     41.61413 0.01546023 10.105615831ENSMUSG00000000001:E004 ENSMUSG00000000001      E004    137.47209 0.03975305  0.281105120ENSMUSG00000000001:E005 ENSMUSG00000000001      E005     85.97116 0.05753662  0.005482149`What I was doing is:write.table(Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,],            ""Expt1_C1_EXONS_dxd_res_wDesc_FGF.tab"",col.names=T,row.names=F,quote=F)This is taking 20+ min per subset just to subset and write the data.  The data frame is 370,000 rows by 27 variables, so large but not huge.  I have about 30 of these to do.  Is there a more efficient way?  Note, the groupID does NOT equal the first column in my subset data frame.  In some instances the groupID contains a concatenated set of ensembl ids.  So I have preprocessed to get the temp data frame to have what I want in the same row order.Thanks,Bob";[education, open-source];110;
5331;1;2015-03-17T02:44:16.263;Better approach for handwriting recognition?;I am trying to write an ANN in python for handwriting recognition by mouse movements. ( like identify characters we draw in paint app n convert it to text) The question might seem that I haven't done my homework, but I am *not * looking for image datasets for handwritten characters.( at least that's what I've figured out so far)Is there any database for such (array or co-ordinates corresponding to each character) inputs? If not, how can I create this database from existing ones like MNIST?Currently, I have written a script to generate my own database because I could not find any. But it seems a very tedious task to generate a good database. I am a beginner in ML. Could somebody point me in right direction upon how else can we do it?;[education, open-source];34;
5332;1;2015-03-17T04:25:34.463;does pruning a decision tree always make it more general?;If I prune a decision tree, does that make the resulting decision tree always more general than the original decision tree?Are there examples where this is not the case?;[education, open-source];31;
5333;2;2015-03-17T07:21:54.110;;If you filter something out by choosing one branch over another branch in the tree, the observations you did not choose are forever lost.But to directly answer your question - no, it does not always make it more general. If you construct a tree where all the decision are exactly the same, then pruning does not make it more general.;;;
5334;2;2015-03-17T07:30:40.047;;Only reason for such a long subset time is that you're running out of memory and the OS starts to swap. AFAIK base data.frame will create a copy of the data in memory before writing it to file.What you can do is use either dplyr or data.table packages as they manage some of these steps without copying the data.;;;
5336;1;2015-03-17T12:24:24.303;Time series change rate calculations for displaying trend line chart;"I'm struggling to find a solution to produce a line chart which shows a trend from a Time Series metric. I try to make my line chart look similar to the following:Basically, I want to make use of relative change rates, but I struggle to find a calculation which makes a visualization as seen above possible. The graph should always be relative to a specific time window, meaning that I can query my data dynamically with a start and end timestamp.My (incomplete) sample data set is the following:timestamp,value,change_rate,change_rate_absolute_sum,change_rate_delta,change_rate_sum1426587900000,778;0.00129,0.00129;0.00129,0.001291426588200000,778;0,0.00129;-0.00129,01426588500000,1189;0.52828,0.52957;0.52828,0.528281426588800000,1195;0.00505,0.53462;-0.52323,0.005051426589100000,1195;0,0.53462;-0.00505,01426589400000,1196;0.00084,0.53546;0.00084,0.000841426589700000,1286;0.07525,0.61071;0.07441,0.075251426590000000,1290;0.00311,0.61382;-0.07214,0.003111426590300000,1294;0.0031,0.61692;-0.00001,0.00311426590600000,1296;0.00155,0.61847;-0.00155,0.001551426590900000,1356;0.0463,0.66477;0.04475,0.04631426591200000,1358;0.00147,0.66624;-0.04483,0.001471426591500000,1358;0,0.66624;-0.00147,01426591800000,1360;0.00147,0.66771;0.00147,0.001471426592100000,1408;0.03529,0.703;0.03382,0.035291426592400000,1390;-0.01278,0.69022;-0.04807,-0.012781426592700000,1391;0.00072,0.69094;0.0135,0.000721426593000000,1410;0.01366,0.7046;0.01294,0.013661426593300000,1414;0.00284,0.70744;-0.01082,0.002841426593600000,1410;-0.00283,0.70461;-0.00567,-0.002831426593900000,1414;0.00284,0.70745;0.00567,0.002841426594200000,1420;0.00424,0.71169;0.0014,0.004241426594500000,1417;-0.00211,0.70958;-0.00635,-0.00211Any ideas warmly appreciated. Thanks a lot in advance!";[education, open-source];60;
5337;2;2015-03-17T14:44:22.403;;Yes, the Jaccard similarity score is normalized by the union to deal with sets of different cardinality. Without this normalization (if you used just the intersection), very small sets would always have very low scores. When the cardinalities of all your sets are the same, the union of any two sets will be a straightforward function of the intersection (this is easy to visualize - as the two sets intersect more and more, their unions get smaller and smaller). The formula is:union = 2 * cardinality - intersectionSo the Jaccard score in your case would be:intersection / (200 - intersection)If you plot this, you'll see it's monotonically the same as what you did. ;;;
5338;2;2015-03-17T14:48:29.080;;"While it is not very clear to me what specific relationships between your data you want to display, I can give some general advice. I think that this time series visualization calls for so-called diverging color scheme (as opposed to sequential, categorical and other ones). ColorBrewer is nice online tool for selecting an appropriate color scheme and other parameters (note that ColorBrewer's color schemes have built-in support in major data analysis and visualization software, such as R, Python, d3.js, plot.ly and others).If you would use R environment, it would be quite easy to produce the time series trend line chart that you want by using ggplot2 package and its scale_color_gradient2() function:... + scale_color_gradient2(midpoint=midValue, low=""blue"", mid=""white"", high=""orange"" )A quick glance at your profile suggested me that you would prefer a JavaScript solution. In that case, I would advise to use d3.js library or one of multiple other JavaScript visualization libraries or packages. For more details, check this relevant answer of mine.";;;
5341;1;2015-03-17T19:34:26.720;PCA: How to Extract and Weight Variables;Background: I'm working with log returns for about 400 tech stocks. I want to use PCA to reduce these into principal components (Internet companies, software developers, circuit board manufacturers, etc), which then are to be used as sector-related indices. Parallel analysis is my method of determining how many factors to extract in the first place. Rotation is done via Oblimin with Kaiser Normalization in SPSS.Questions: How do I determine which variables need to be trimmed? Those with low communality? Or should I look at the pattern matrix and start cutting variables which happen to load on multiple factors? After that, can I use the figures in the pattern matrix to weight each stock's contribution to each component's variance? For example, if the rotation yields .71 for Stock A and .10 for Stock B, I could multiply the return for each stock in each period, T, by its respective weight. ;[education, open-source];60;
5342;2;2015-03-17T20:13:00.767;;If you label metric 3 as $x_3 = \{1,0\}$, where $1$ means it is an anomaly, this becomes a  logistic regression problem where $\mathbb{P}(X_3 = 1) = logit(\beta_0 + \beta_1 x_1 + \beta_2 x_2)$.;;;
5343;2;2015-03-18T01:34:33.133;;"Do you change the patterns often? If not, then you can use Aho-Corasick method, whose idea is, first, to build a finite automaton based on your patterns and, then, to make a single pass over the text with this automaton to find matches (if the automaton visits a ""matching"" state, then there is a match). The complexity of the automata building should be linear in the length of the patterns (# of patters * max pattern length), and the matching phase should be linear in the size of the text you search in.";;;
5344;1;2015-03-18T08:10:08.943;procedure for gradient descent;"I'm going to describe the procedure for gradient descent using the language of English:1) For all of the files (e.g. documents) in our training corpus.--> commence, now we are considering our first document2) For all the features in the feature vector associated with that training example (document)3) Take the dot product of the vector of weight values * and the vector of features specific to the document under consideration* vector of weight values is applicable to all features, though all features will not be present in all documents, indeed only some features will be present in each individual document4) Take the value resulting from step 3 and apply this to our loss function. I'm using the Hinge Loss function, i.e. the formulaIs that equation saying that the hinge loss objective function should be the sum of the misclassification error for every file in our training set?So is that equivalent to the java code:Math.max(0, 1 - y * value_from_step_three)With y = {-1, 1}^ THIS IS MY FIRST POINT OF CONFUSIONI guess that is inaccurate but I'm not sure, according to my mental model it should be right. 5) For every weight, update it as:new weight valuej = old weight valuej - learning rate * value from step 4 * value of featurej.I'm a bit confused about what value of featurej should be, since I guess this would correspond to a vector, that has values for every weight under consideration, i.e.Example:Document 1 = [""I"", ""am"", ""awesome""]Document 2 = [""I"", ""am"", ""great"", ""great""]Dictionary is:[""I"", ""am"", ""awesome"", ""great""]So the documents as a vector would look like:Document 1 = [1, 1, 1, 0]Document 2 = [1, 1, 0, 2]There will be weights:[Weight""I"", Weight""am"", Weight""awesome"", Weight""great""]";[education, open-source];38;
5345;1;2015-03-18T11:39:40.353;IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio);I use RStudio for R programming. I remember about solid IDE-s from other technology stacks, like Visual Studio or Eclipse.I have two questions:What other IDE-s than RStudio are used (please consider providing some brief description on them).Does any of them have noticeable advantages over RStudio?I mostly mean debug/build/deploy features, besides coding itself (so text editors are probably not a solution).;[education, open-source];2574;2
5347;2;2015-03-18T12:47:38.797;;Without looking into it too deeply (i don't have access to R on this computer), it seems...temp is a data frame with x rows, Expt1_C1_EXONS_dxd_res_wDesc is a data frame with y rows. temp$FGF is therefore a vector of length x. Since temp is a subset, x < yWhen you write Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,] you might be getting errors (NA's) due to the mismatch in length. It may also be because you are not specifying which rows you want, the vector temp$FGF contains TRUE and FALSE. It doesn't contain row indices. temp$FGF==TRUE returns (or at least selects based on) indices.(As i say, i don't have R in front of me to check.)Also, as far as i remember, you need to specify whether you want TRUE or FALSE entries to be included. R does not automatically assume you want TRUE.(Again, i don't have R in front of me to check.);;;
5348;1;2015-03-18T14:01:37.680;Extracting the attribute values of centroids when doing MOA clustering;I am using MOA and I would like to obtain the values of centroids. Do you know how I can do it? Should I do it from the visualization interface?Thanks,Laia;[education, open-source];14;
5349;1;2015-03-18T17:25:32.460;Terminology: SOMs, batch learning, online learning, and stochastic gradient descent;I'm not sure which word to use to differentiate a self-organizing map (SOM) training procedure in which updates for the entire data set are aggregated before they are applied to the network from a training procedure in which the network is updated with each data point individually.In the case of other algorithms I'd say stochastic gradient descent, but I'm not sure gradient descent is correct for SOM learning.  To my knowledge, SOM learning does not follow any energy function exactly, so that should mean it doesn't do gradient descent exactly, right?Another term would be online learning, but `online' sort of implies I train my SOM in the real world with data points streaming in, eg. from a set of sensors.  It may also imply that I use every data point only once, which is not what I want to say.;[education, open-source];47;
5350;2;2015-03-18T22:58:40.487;;My best guess: After three weeks he’d harvested 6 million questions and answers from 20,000 women all over the country. I assume that we could collect both what the women answered, what answers they would accept and the importance they attribute to the question. This is the training set. He picked out the 500 questions that were most popular with both clusters.This is the feature selection.The loss function to be minimized is could be something like the negative sum of (logarithms of) the match percentages $m(x,s,w, y_i, t_i, v_i)$ between his answers $x$ (a vector of length 500), the answers he accepts $s$, the importances $w$ he attributes to each questions, the $i$'th woman's answers $y_i$, the answers $t_i$ she would accept and the importances she assigns to them $v_i$.  He’d already decided he would fill out his answers honestlymeaning that $x$ and $s$ are fixed, only $w$ is adjusted to minimize the goal function. If the match percentage were something simple like 'number of questions which both answered the same way times your importance assigned to the question', a simple 'give most importance to the question you answered the same way as women in your preferred cluster and zero importance to other questions' should maximize this. However, the match percentage function is more complicated. One would start with equal importance assigned to all questions. Then calculate the contribution of each question to the overall loss function: questions $j$ for which his answers do not match many of the women's answers will get a reduced weight in the next round and the weight of well matching questions will be increased (I would have to think about how the weight update would look in detail though).Note that the question relevances in the vector $w$ can only take on a discrete set of values (0, 1, 10, 50, 250) making it a discrete optimization problem and thus potentially NP-hard. In practice, I would allow the weights $w$ take on any real value between 0 and 250 and then round to the nearest allowed value in the end.  (Disclaimer: I have never used OkCupid);;;
5351;2;2015-03-19T00:21:43.773;;Unfortunately, IntelliJ IDEA currently doesn't support R - neither via the Ultimate edition, nor via plug-ins: https://www.jetbrains.com/idea/features/editions_comparison_matrix.html. Having said that, if you have desire, time and good Java knowledge, you can develop an R plug-in yourself.In my opinion, a better option would be Eclipse, which offers R support via StatET IDE: http://www.walware.de/goto/statet. However, I find Eclipse IDE too heavyweight. Therefore, my preferred option is RStudio IDE - I don't know why one would prefer other options. I especially like RStudio's ability of online access to the full development environment via RStudio Server.;;;
5352;2;2015-03-19T07:34:00.850;;VisualStudio added syntax highlighting for R a few days ago: https://www.visualstudio.com/news/2015-mar-10-vsoThe current RStudio preview is pretty cool as well - you can switch to a dark theme, code completion is working well, you can filter in the viewer, etc.;;;
5353;2;2015-03-19T07:38:57.917;;I would recommend regexp pattern matching. I know that usual implementations are slow but you have to study Thompson's construction algorithm for nondeterministic automaton. See the wikipedia dedicated article. However here the wikipedia fails to present this treasure properly. I would strongly recommend to study carefuly this blog article: Regular expressions can be simple and fast. For implementations you have pointers in the given article (for example awk and grep uses this implementation). ;;;
5354;2;2015-03-19T09:51:34.480;;I did not use shiny but I tried in my gui code which I wrote using gwidgets to make a ariable global I used data<<-read.csv(inFile$datapath)you can try this.;;;
5355;2;2015-03-19T16:35:56.013;;What about ESS, the R (and other stats languages) package for the Emacs editor?It's not formally an IDE, though it has many, if not more of the features of RStudio, just in a different UI (code completion, inline help, object-aware autocomplete, debugging etc.).;;;
5356;1;2015-03-19T19:38:12.417;(Cross Posting) Categorizing short sentences into a taxonomy;"I feel like I might be in the wrong exchange, but I do not see a NLP exchange, so I figured this was the best bet. My current task is to take short sentences and try to categorize them based on some taxonomy. I'll try to be as detailed as possible in this question, but if you need any more details, I'll do my best to provide them.Now there are two main issues with this:Dealing with a single sentence, versus having a whole article or document, makes categorizing difficult (as far as I know)I do not know what taxonomy I should be trying to use (i.e. a small set of topics, or something larger). I imagine this will be determined based on how accurate I can get this model to workTo give some examples of what I am thinking about:""Barack Obama is a Socialist""  --> This should be categorized as ""Politics"", or ""US Politics""""Global Warming is a Hoax"" --> This should be categorized as ""global warming"", or ""global warming debate""""Charlie Seen's latest rant about Obama is stupid"" --> This should be categorized as ""Entertainment"", ""Hollywood"", maybe ""Politics"".Research I have done shows that categorizing with short sentences (and not a large article) makes things difficult, and a common way of dealing with it is to utilize wikipedia / dbpedia. What I would like is this:Research papers which have tried to categorize sentences, tweets, sm posts, etc into a set of topicsCommon methods for dealing with small amounts of textLinks to software, apis etc that may help with categorizing (most APIs I have seen work better with documents)For the record, I know this is a quite a bit vague. I'll try to provide any information needed to help figure out what strategies would work well. Any insights, links and thoughts would be highly appreciated.";[education, open-source];30;
5357;1;2015-03-20T14:56:23.420;Data Science in C (or C++);"I'm an R language programmer. I'm also in the group of people who are considered Data Scientists but who come from academic disciplines other than CS.This works out well in my role as a Data Scientist, however by starting my career in R and only having basic knowledge of other scripting/web languages, I've felt somewhat inadequate in 2 key areas:Lack of a solid knowledge of programming theoryLack of a competitive level of skill in faster and more widely used languages like C, C++ and Java, which could be utilizes to increase the speed of the pipeline and Big Data computations as well as to create DS/data products which can be more readily developed into fast back-end scripts or standalone applications The solution is simple of course -- go learn about programming, which is what I've been doing by enrolling in some classes (currently C programming). However, now that I'm starting to address problems #1 and #2 above, I'm left asking myself ""Just how viable are languages like C and C++ for Data Science?"".For instance, I can move data around very quickly and interact with users just fine, but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations? So. can C do the job -- what tools are available for advanced statistics, ML, AI, and other areas of Data Science? Or must I loose most of the efficiency gained by programming in C by calling on R scripts or other languages?The best resource I've found thusfar in C is a library called Shark, which gives C/C++ the ability to use Support Vector Machines, linear regression (not non-linear and other advanced regression like multinomial probit, etc) and a shortlist of other (great but) statistical functions.";[education, open-source];968;4
5358;1;2015-03-20T15:29:12.540;What approaches are there to not display a search result that a user has no permission to see?;"QuestionWhat approaches are there to not display a search result that a user is not supposed to see?ExampleSuppose we have the following situation.High-level viewIn an enterprise, there are different repositories like websites, databases, filesystem folder etc. And there is a search engine that crawls all those repositories and creates an index. The user can then use the search engine UI to perform searches.Let's say the search engine was Apache Lucene and the encompassing indexing and retrieval system Apache Solr.User permissions are managed via Microsoft Active Directory.The content in the different repositories are managed with all kinds of applications like MS Word, custom-made applications, database management tools etc.Search-index and user permissionsThe index contains the documents A1, A2, B1 and B2.User A has only permission to see documents of type A*.Performing a searchNow, user A performs a search for A* AND B*.Displaying the resultsA system administrator with permission to see everything should see all four documents in the search results: A1, A2, B1 and B2.User A however, who only has permissions to see documents of type A*, should/will only see two documents in the search results: A1 and A2. So user A shouldn't even know that documents B1 and B2 exist.Elaboration on the questionWhat ways are there to implement that requirement that users only see documents they are allowed to see?I suppose it would be a bad idea to add the information to the index whether a user may see a document. I think that because of this use case: suppose all of a sudden user A may only see document A1 and not A2 anymore; those permissions are now set for example on the filesystem. Now it takes a while until the permissions in the index are updated and during that time user A would still be able to see document A2 in the search results even if he can't click on the result and access it anymore.Also, the above approach probably would break down when there are hundreds of millions of documents and thousands of employees.Where and how would the security aspect be implemented in a content indexing and retrieval system in the above scenario?";[education, open-source];28;
5359;2;2015-03-20T17:08:05.330;;I have added the following code to global.R filedata <- reactiveValues()I used assign function in server.R to assign values to data in global.routput$contents <- renderText({          if(is.null(input$file1))return()      inFile <- input$file1      data2<-read.csv(inFile$datapath)  assign('data',data2,envir=.GlobalEnv)  print(summary(data)) });;;
5360;2;2015-03-20T17:41:43.970;;Since you aren't sure which software you would like to use, I can explain this concept only in general, by example. The clearest example is a content management system like Drupal or Wordpress (not necessarily a data science tool but the idea is the same).In a content management system, there are different types of content which can be accessed. In your case, there would be two types - document type A and document type B. There can be an unlimited number of documents, each assigned one or more types. The software has a permission system that gives access to each document type. This is built with roles. In your example, there would be a system administrator role, and a general user role.Each role has permissions associated with it. You would configure system administrator to be allowed to view document types A and B, but you would configure general users to only be able to see document type A.Each user registers an account in the software system. A site administrator assigns each account one or more roles. So a user with the system administrator role would be able to see all documents of type A and B. However, if a user only has the general user role, they would only be able to see documents of type A. Also noteworthy is the fact that each user can have multiple roles and will get all permissions associated with every role they have.USER -has-> ROLE(s)ROLE -has-> PERMISSION(s)CONTENT TYPE -requires-> PERMISSION(s)Every time the content is requested, the software system ensures that the user making the request has the permission to access the content.When a search is run, the software system has to validate every potential result individually before displaying the search results to the user. The user must have the permission to see each piece of content that is be delivered back from the search. ;;;
5361;2;2015-03-20T21:24:40.410;;R is one of the key tool for data scientist, what ever you do don't stop using it. Now talking about C, C++ or even Java.  They are good popular languages. Wether you need them or will need them depend on the type of job or projects you have.  From personal experience, there are so many tools out there for data scientist that you will always feel like you constantly need to be learning.  You can add Python or Matlab to things to learn if you want and keep adding.  The best way to learn is to take on a work project using other tools that you are not comfortable with.  If I were you, I would learn Python before C.  It is more used in the community than C.  But learning C is not a waste of your time. ;;;
5362;2;2015-03-21T01:33:11.087;;The vim-r-plugin is surprisingly good. You can send lines and paragraphs of code from vim into a tmux session running R in a similar manner to R-Studio. It has these commands if you want to check out what functionality it adds to vim. Of course I use all my other normal vim plugins - auto-complete, folding, etc.;;;
5365;2;2015-03-21T13:04:43.810;;"I would be keen to understand why you would need another language (apart form Python) if your goal is "" but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations"".For that kind of thing, C is a waste of time. It's a good tool to have but in the ~20 years since Java came out, I've rarely coded C.If you prefer the more functional-programming side of R, learn Scala before you get into too many procedural bad habits coding with C.Lastly learn to use Hadley Wickham's libraries - they'll save you a lot of time doing data manipulation.";;;
5366;2;2015-03-21T13:23:35.003;;If you're a R user, there is a lot of good practical information at http://www.rdatamining.com. Look at their text mining examples.Also, take a look at the tm package.This is also a good aggregation site- http://www.tapor.ca/;;;
5367;1;2015-03-21T17:05:07.510;Best way to format data for supervised machine learning ranking predictions;"I'm fairly new to machine learning, but I'm doing my best to learn as much as possible.I am curious about how predicting athlete performance (runners in particular) in a race of a specific starting lineup. For instance, if RunnerA, RunnerB, RunnerC, and RunnerD are all racing a 400 meter race, I want to best predict whether RunnerA will beat RunnerB based on past race result information (which I have at my disposal). However, I have many cases where RunnerA has never raced against RunnerB; yet I do have data showing RunnerA has beat RunnerC in the past, and RunnerC has beat RunnerB in the past. This logic extends deeper as well. So, it would seem that RunnerA should beat RunnerB, given this information. My real concern is when it gets more complicated than this as I add more features (multiple runners, different distances, etc), and so I'm turing to ML algorithms to help my predictions.However, I am having difficulty figuring out how to include this in my row data that I can train (after all, correctly formatting data is 99% of proper machine learning), and I am hoping that someone here might have thought along the same lines in the past and might be able to shed some light. Example:I am currently trying to include RunnerX-RunnerY past race data by counting all the races that RunnerX and RunnerY have run together and normalizing them on a scale from -1 to 1; -1 indicating RunnerX lost all past races against RunnerY; and +1 indicating that RunnerX has  won all past races against RunnerY; and +1 indicating. And 0 indicating an equal number of wins and losses (or no past races against each other).For instance, if RunnerA is racing RunnerB, and RunnerA has beat RunnerB in the past, then I want the algorithm to know that (denoted by a +1 on the RunnerB column of row RunnerA); same for vice versa. Taking it another step further, If RunnerA is racing RunnerC (but the two have never raced each other in the past), and RunnerA has beat RunnerD in a past race, and RunnerD has beat RunnerC in a past race, then I want the algorithm to learn that RunnerA should beat RunnerC. I say beat here, but I mean an ""average beat"" for any RunnerX-RunnerY combinations when data for more than 1 past race is available.I have set my data up as:name     track   surface  distance  age    RunnerA   RunnerB   RunnerC   RunnerDRunnerA  Home    2        400       11     0         1         0         1RunnerC  Away    2        400       12     0         0         0         -1RunnerD  Home    2        400       10     0         0         1         0which shows that RunnerA has beat RunnerB and RunnerD in the past. RunnerC has lost to RunnerD. And RunnerD has beat RunnerC.The problem:The problem is that I don't really think this is a correct display of the information for an ML algorithm.From what I understand, ML data should be row independent. And this data isn't because row 1 (RunnerA) has beat RunnerD, yet the data indicating RunnerD has beat RunnerC is in row 3.Does anyone have any ideas how I might be able to incorporate this past win-percentage-for-runner-pair-combination data??? I'm totally stuck here. I've read a lot about some algorithms that estimate the win loss by simply totaling win statistics, but those don't say anything about the actual probability of a particular runner to beat another particular runner.Any pointers would be super helpful.Thanks!!!";[education, open-source];68;0
5368;2;2015-03-21T20:09:53.157;;"In my opinion, ideally, to be a more well-rounded professional, it would be nice to know at least one programming language for the most popular programming paradigms (procedural, object-oriented, functional). Certainly, I consider R and Python as the two most popular programming languages and environments for data science and, therefore, primary data science tools.Julia is impressive in certain aspects, but it tries to catch up with those two and establish itself as a major data science tool. However, I don't see this happening any time soon, simply due to R/Python's popularity, very large communities as well as enormous ecosystems of existing and newly developed packages/libraries, covering an very wide range of domains / fields of study.Having said that, many packages and libraries, focused on data science, ML and AI areas, are implemented and/or provide APIs in languages other than R or Python (for the proof, see this curated list and this curated list, both of which are excellent and give a solid perspective about the variety in the field). This is especially true for performance-oriented or specialized software. For that software, I've seen projects with implementation and/or APIs mostly in Java, C and C++ (Java is especially popular in the big data segment of data science - due to its closeness to Hadoop and its ecosystem - and in the NLP segment), but other options are available, albeit to a much more limited, domain-based, extent. Neither of these languages is a waste of time, however you have to prioritize mastering any or all of them with your current work situation, projects and interests. So, to answer your question about viability of C/C++ (and Java), I would say that they are all viable, however not as primary data science tools, but as secondary ones.Answering your questions on 1) C as a potential data science tool and 2) its efficiency, I would say that: 1) while it's possible to use C for data science, I would recommend against doing it, because you'd have a very hard time finding corresponding libraries or, even more so, trying to implement corresponding algorithms by yourself; 2) you shouldn't worry about efficiency, as many performance-critical segments of code are implemented in low-level languages like C, plus, there are options to interface popular data science languages with, say, C (for example, Rcpp package for integration R with C/C++: http://dirk.eddelbuettel.com/code/rcpp.html). This is in addition to simpler, but often rather effective, approaches to performance, such as consistent use of vectorization in R as well as using various parallel programming frameworks, packages and libraries. For R ecosystem examples, see CRAN Task View ""High-Performance and Parallel Computing with R"".Speaking about data science, I think that it makes quite a lot of sense to mention the importance of reproducible research approach as well as the availability of various tools, supporting this concept (for more details, please see my relevant answer). I hope that my answer is helpful.";;;
5369;2;2015-03-21T20:58:48.757;;"You need to analyze sentence structure and extract corresponding syntactic categories of interest (in this case, I think it would be noun phrase, which is a phrasal category). For details, see corresponding Wikipedia article and ""Analyzing Sentence Structure"" chapter of NLTK book.In regard to available software tools for implementing the above-mentioned approach and beyond, I would suggest to consider either NLTK (if you prefer Python), or StanfordNLP software (if you prefer Java). For many other NLP frameworks, libraries and programming various languages support, see corresponding (NLP) sections in this excellent curated list.";;;
5370;2;2015-03-21T21:58:34.687;; Or must I loose most of the efficiency gained by programming in C by calling on R scripts or other languages?Do the opposite: learn C/C++ to write R extensions. Use C/C++ only for the performance critical sections of your new algorithms, use R to build your analysis, import data, make plots etc.If you want to go beyond R, I'd recommend learning python. There are many libraries available such as scikit-learn for machine learning algorithms or PyBrain for building Neural Networks etc. (and use pylab/matplotlib for plotting and iPython notebooks to develop your analyses). Again, C/C++ is useful to implement time critical algorithms as python extensions.;;;
5371;1;2015-03-22T18:53:39.820;R and Python, memory differences yielding performance differences;I think there are numerous posts regarding which one to use: R or Python. However, I'm curious about how their architecture differences yield differences in speed performance, not which one to use.This blog post performs a small test between R and python to show that the (optimized) python code was 2x faster than R code.* And I've read in this post that R tends to put everything in memory, which is why computations on large datasets is generally slow.But what makes python's low level memory management so much different than R, which helps it yield these benchmarks?*Though python was 2x faster in this test than R, I'm not saying that python is generally 2x faster than R.;[education, open-source];206;
5373;1;2015-03-23T14:00:02.720;sk-learn - ValueError: array is too big.;I have a large dataset with characters and 90000 intances and I have the error ValueError: array is too big when I have the following code before the plot_kmeans_digits.py code:data2=list(csv.DictReader(open('C:\diabeticdata.csv', 'rU')))vec = DictVectorizer()data = vec.fit_transform(data2).toarray()Do you know how I can solve this error?Thanks in advance. ;[education, open-source];86;
5374;1;2015-03-23T15:48:18.683;Feature scaling;I am struggling with a conceptual problem related to feature scaling.Let's assume I am building a classifier (e.g., a NN) and let's assume I rely on future scaling for the input features of my model.In this context I will normalise the training set using its mean and its std and I would do the same with the testing set using the testing mean and std.Let us also assume I succeed in building my classifier and I move to production where I try to classify new inputs.  However for such new inputs the mean and std are unknown! How can I scale them appropriately before processing with my model? May be I could use the mean and std from training+testing.....I really don't know which is the correct practice here....any hint?Thank you for your help!;[education, open-source];52;
5375;1;2015-03-23T17:36:03.680;Which one is easier to debug between theano and caffe?;I am investigating whether to use theano or caffe for convnets. I would like to know which one provides a better debug environment. In caffe, it seems you don't write any code just a config in protobuf - how do you debug the convnet then?;[education, open-source];90;
5376;2;2015-03-23T21:48:30.313;;You should apply the normalization only on your training dataset. Your test set should be kept completely separate and should be used only when your final model has been chosen. If you use include the testing set in the normalization, it can be seen as using the testing set in the training procedure. This is called data snooping.   You should pre-process training dataset and use the obtained mean and std  when processing the testing set afterwards. Note that the testing dataset transformations will likely be imperfect (it will not have zero mean or unity standard deviation) but this testing dataset can safely be used since it has not affected any step of the learning process.;;;
5379;1;2015-03-24T06:01:50.617;Tools to preprocess a big data for dashboards?;I have a complex dataset with more than 16M rows coming from pharmaceutical industry. Regarding the data, it is saved in a sql server with multiple (more than 400) relational tables. Data got several levels of hierachies like province, city, postal code, person, and antigens measures, etc. I would like to create many dashboards in order to observe the changes & trends happening. I can use Pentaho, R (shiny) or Tableau for this purpose. But the problem is data is so huge, and it take so long to process it with dashboard softwares. I have a choice of making cube and connect it to dashboard. My question here is whether there are  any other solutions that I can use instead of making a cube? I don't want to go through the hassle of making & maintaining a cube. I would like to use a software where I specify relationships between tables, so the aggregation/amalgamation happens smoothly and output processed tables that can connect  to dashboards. I hear Alteryx is one software that can do it for you (I haven't tried it myself, and it is an expensive one!). I understand this task needs two or more softwares/tools. Please share your input & experience. Please mention what tools do you use, size of your data, and how fast/efficient is the entire system, and other necessary details. ;[education, open-source];118;1
5380;1;2015-03-24T08:12:53.140;In a SVD with user/video bias, why is the UV contribute so small?;I'm testing a SVD-based collaborative filter on my data set, in which the label, $r_{ij}$, is a real value from 0 to 1.Like the many papers suggested, to have a better performance, instead of using $ \hat{R} = U \cdot V^T $ directly, I use $\hat{R} = \mu + B_u + B_v + U \cdot V^T $, where $\mu$ is the average rating, $B_u$ is the bias of user, and $B_v$ is the bias of item.Thus, this model corresponds to a loss function:$\min_{B_u, B_v, U, V}  = ||I\circ(R-\mu-Bu-B_v-U\cdot V^T)||_F^2 + \lambda (||B_u||_F^2 + ||B_v||_F^2 + ||U||_F^2 + ||V||_F^2)$where I is the masking matrix in which $I_{ij} = 1$ if $R_{ij}$ is known, and $||\cdot||_F$ is the frobenius norm.Then, I solve this by gradient descent, it seems to work fine, and the test RMSE is 0.25. However, when I investigate the contribution of each part in predict function $\hat{R} = \mu + B_u + B_v + U \cdot V^T $, I notice that, $\mu$ is about 0.5, $b_u$ and $b_i$ are about $\pm0.3$, but the part of $ U \cdot V^T $ is quite small, normally about $\pm 0.01$.Why does this part contribute so small? Since this part is the actual part where the collaborative filter works, I expect it to contribute more in prediction.;[education, open-source];56;1
5381;2;2015-03-24T11:16:07.133;;This site lists historical market capitalizations and enterprise values for S&P 100 and NASDAQ-100 companies for the past 10 years. You can export the data sets to Excel.http://marketcapitalizations.com/historical-data/historical-data-categories/valuations/You can also try to contact them for data for a longer period of time.;;;
5382;2;2015-03-24T15:07:07.623;;"Good to hear from the author. As he said, it appears to be possible through the Google Adwords Keyword Planner. Specifically, the ""Search for new keyword and ad group ideas"" option.I am unable to get Google to actually generate his list, but I imagine it is just a matter of picking the correct seed words and settings. I get the same order-of-magnitude results when I directly search his terms (""sexless marriage"", ""unhappy marriage"", etc).Another option for generating his list would be to search against a dictionary of negative terms ("" marriage"") and then do some sorting and filtering. You could probably automate this through Google's API.";;;
5383;1;2015-03-24T15:25:02.270;Recommendation - item based vs user based;I have one clarification - First the definitions- User-based: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.Item-based: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed offline.Now the question - in item based CF the similarity between items are tracked via user behavior - since user behavior is changing will it not impact the similarity between items?;[education, open-source];69;
5384;2;2015-03-24T15:35:16.227;;This problem looks like a lot with the problem of ranking college football teams. I have never worked on this ranking problem, but I believe you can borrow some tools used there to build your model.Here goes a couple of references:Colley Matrix Rankings - This was one of computer rankings used by the BCS. It is also the only one that shared their methodology.An example of the Colley Matrix Rankings - An easy example to follow.The Perron-Frobenius Theorem and the Ranking of Football Teams - A well known reference that presents some ranking methods. This paper also shows how to assess the probability of winning a game based on the rankings of the teams.;;;
5385;2;2015-03-24T20:47:36.317;;As a data scientist the other languages (C++/Java) come in handy when you need incorporate machine learning into an existing production engine.Waffles is both a well-maintained C++ class library and command-line analysis package.  It's got supervised and unsupervised learning, tons of data manipulation tools, sparse data tools, and other things such as audio processing.  Since it's also a class library, you can extend it as you need.  Even if you are not the one developing the C++ engine (chances are you won't be), this will allow you to prototype, test, and hand something over to the developers.    Most importantly, I believe my knowledge of C++ & Java really help me understand how Python and R work.  Any language is only used properly when you understand a little about what is going on underneath.  By learning the differences between languages you can learn to exploit the strengths of your main language.  ;;;
5386;1;2015-03-25T09:51:57.847;Identity covariance matrix, decorrelated data?;Why would you want to decorrelated data?As I am reading about PCA and whitening on image data for DNN, I wonder what is the purpose of achieving the identity covariance matrix in your data is?Is it to remove interaction between variables, thus allow simpler models to express interactions without having to compute x1*x2?;[education, open-source];47;
5387;2;2015-03-25T10:16:41.007;;We have dashboards that show information about some processes, which have billions of rows in the database. It's not queried directly though, but instead from pre-aggregated data.We have automated scripts running in database that populate the aggregated data tables specifically for dashboards. In most extreme cases the raw data is so large and comes in at such a high pace that there's a two-tier aggregation in place. First tier will de-normalize the data and 2nd tier will do the actual sum/count type of aggregation.So you don't need two tools since you can do aggregation purely in database.edit (answering question from comment): Our dashboards are in Tableau. Data is all in PostgreSQL databases. Automation is done with unix cronjob executing a database function. The function in turn queries which tasks it has to run and runs them. There is a few moving pieces to the whole system, but the architecture isn't too complicated. Write down your ideas and go talk to some architects / engineers - they will know the best way to approach this.;;;
5388;2;2015-03-25T12:52:43.580;;"Shallow Natural Language Processing technique can be used to extract concepts from sentence.-------------------------------------------Shallow NLP technique steps:1) Convert the sentence to lowercase2) Remove stopwords (these are common words found in a language. Words like for, very, and, of, are, etc, are common stop words)3) Extract n-gram i.e., a contiguous sequence of n items from a given sequence of text (simply increasing n, model can be used to store more context)4) Assign a syntactic label (noun, verb etc.)5) Knowledge extraction from text through semantic/syntactic analysis approach i.e., try to retain words that hold higher weight in a sentence like Noun/Verb-------------------------------------------Lets examine the results of applying the above steps to your given sentence Complimentary gym access for two for the length of stay ($12 value per person per day).1-gram Results: gym, access, length, stay, value, person, daySummary of step 1 through 4 of shallow NLP:1-gram          PoS_Tag   Stopword (Yes/No)?    PoS Tag Description-------------------------------------------------------------------    Complimentary   NNP                             Proper noun, singulargym             NN                              Noun, singular or massaccess          NN                              Noun, singular or massfor             IN         Yes                  Preposition or subordinating conjunctiontwo             CD                              Cardinal numberfor             IN         Yes                  Preposition or subordinating conjunctionthe             DT         Yes                  Determinerlength          NN                              Noun, singular or massof              IN         Yes                  Preposition or subordinating conjunctionstay            NN                              Noun, singular or mass($12            CD                              Cardinal numbervalue           NN                              Noun, singular or massper             IN                              Preposition or subordinating conjunctionperson          NN                              Noun, singular or massper             IN                              Preposition or subordinating conjunctionday)            NN                              Noun, singular or massStep 4: Retaining only the Noun/Verbs we end up with gym, access, length, stay, value, person, dayLets increase n to store more context and remove stopwords.2-gram Results: complimentary gym, gym access, length stay, stay valueSummary of step 1 through 4 of shallow NLP:2-gram              Pos Tag---------------------------access two          NN CDcomplimentary gym   NNP NNgym access          NN NNlength stay         NN NNper day             IN NNper person          IN NNperson per          NN INstay value          NN NNtwo length          CD NNvalue per           NN INStep 5: Retaining only the Noun/Verb combination we end up with complimentary gym, gym access, length stay, stay value3-gram Results: complimentary gym access, length stay value, person per daySummary of step 1 through 4 of shallow NLP:3-gram                      Pos Tag-------------------------------------access two length           NN CD NNcomplimentary gym access    NNP NN NNgym access two              NN NN CDlength stay value           NN NN NNper person per              IN NN INperson per day              NN IN NNstay value per              NN NN INtwo length stay             CD NN NNvalue per person            NN IN NNStep 5: Retaining only the Noun/Verb combination we end up with complimentary gym access, length stay value, person per dayThings to remember:Refer the Penn tree bank to understand PoS tag description https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.htmlDepending on your data and the business context you can decide the n value to extract n-grams from sentenceAdding domain specific stop words would increase the quality of concept/theme extractionDeep NLP technique will give better results i.e., rather than n-gram, detect relationships within the sentences and represent/express as complex construction to retain the context. For additional info, please refer http://stats.stackexchange.com/a/133680/66708Tools:You can consider using OpenNLP / StanfordNLP for Part of Speech tagging. Most of the programming language have supporting library for OpenNLP/StanfordNLP. You can choose the language based on your comfort. Below is the sample R code I used for PoS tagging.Sample R code:Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7') # for 32-bit versionlibrary(rJava)require(""openNLP"")require(""NLP"")s <- paste(""Complimentary gym access for two for the length of stay $12 value per person per day"")tagPOS <-  function(x, ...) {  s <- as.String(x)    word_token_annotator <- Maxent_Word_Token_Annotator()    a2 <- Annotation(1L, ""sentence"", 1L, nchar(s))    a2 <- annotate(s, word_token_annotator, a2)    a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)    a3w <- a3[a3$type == ""word""]        POStags <- unlist(lapply(a3w$features, `[[`, ""POS""))    POStagged <- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")    list(POStagged = POStagged, POStags = POStags)  }  tagged_str <-  tagPOS(s)  tagged_str#$POStagged    #[1] ""Complimentary/NNP gym/NN access/NN for/IN two/CD for/IN the/DT length/NN of/IN stay/NN $/$ 12/CD value/NN per/IN     person/NN per/IN day/NN""    #    #$POStags#[1] ""NNP"" ""NN""  ""NN""  ""IN""  ""CD""  ""IN""  ""DT""  ""NN""  ""IN""  ""NN""  ""$""   ""CD"" #[13] ""NN""  ""IN""  ""NN""  ""IN""  ""NN"" Additional readings on Shallow & Deep NLP:Shallow and Deep NLP Processing for ontology learning: a Quick Overview Click HereIntegrating Shallow and Deep NLP for Information Extraction Click Here";;;
5389;2;2015-03-25T19:45:31.320;;"Lasagne (docs) is very nice, as it uses theano (→ you can use the GPU) and makes it simpler to use. The author of lasagne won the Kaggle Galaxy challenge, as far as I know. It is nice with nolearn. Here is an MNIST example network:#!/usr/bin/env pythonimport lasagnefrom lasagne import layersfrom lasagne.updates import nesterov_momentumfrom nolearn.lasagne import NeuralNetimport sysimport osimport gzipimport pickleimport numpyPY2 = sys.version_info[0] == 2if PY2:    from urllib import urlretrieve    def pickle_load(f, encoding):        return pickle.load(f)else:    from urllib.request import urlretrieve    def pickle_load(f, encoding):        return pickle.load(f, encoding=encoding)DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'DATA_FILENAME = 'mnist.pkl.gz'def _load_data(url=DATA_URL, filename=DATA_FILENAME):    """"""Load data from `url` and store the result in `filename`.""""""    if not os.path.exists(filename):        print(""Downloading MNIST dataset"")        urlretrieve(url, filename)    with gzip.open(filename, 'rb') as f:        return pickle_load(f, encoding='latin-1')def load_data():    """"""Get data with labels, split into training, validation and test set.""""""    data = _load_data()    X_train, y_train = data[0]    X_valid, y_valid = data[1]    X_test, y_test = data[2]    y_train = numpy.asarray(y_train, dtype=numpy.int32)    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)    y_test = numpy.asarray(y_test, dtype=numpy.int32)    return dict(        X_train=X_train,        y_train=y_train,        X_valid=X_valid,        y_valid=y_valid,        X_test=X_test,        y_test=y_test,        num_examples_train=X_train.shape[0],        num_examples_valid=X_valid.shape[0],        num_examples_test=X_test.shape[0],        input_dim=X_train.shape[1],        output_dim=10,    )def nn_example(data):    net1 = NeuralNet(        layers=[('input', layers.InputLayer),                ('hidden', layers.DenseLayer),                ('output', layers.DenseLayer),                ],        # layer parameters:        input_shape=(None, 28*28),        hidden_num_units=100,  # number of units in 'hidden' layer        output_nonlinearity=lasagne.nonlinearities.softmax,        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9        # optimization method:        update=nesterov_momentum,        update_learning_rate=0.01,        update_momentum=0.9,        max_epochs=10,        verbose=1,        )    # Train the network    net1.fit(data['X_train'], data['y_train'])    # Try the network on new data    print(""Feature vector (100-110): %s"" % data['X_test'][0][100:110])    print(""Label: %s"" % str(data['y_test'][0]))    print(""Predicted: %s"" % str(net1.predict([data['X_test'][0]])))def main():    data = load_data()    print(""Got %i testing datasets."" % len(data['X_train']))    nn_example(data)if __name__ == '__main__':    main()Caffe is a C++ library, but has Python bindings. You can do most stuff by configuration files (prototxt). It has a lot of options and can also make use of the GPU.";;;
5391;2;2015-03-25T21:33:45.957;;Having only taken a few courses on this stuff, I'm going to offer my understanding on this.The main reason seems to be simplifying, as you assume. It manifests in different areas though. One is mathematical - if your co-variance matrix is identity matrix then the math becomes much easier. The one directly following from this is computational - inverting a matrix is very costly, but identity matrix is its own inverse, so that's perfect.Another part of the simplicity is definitely the ease of interpretation. Independent variables are easy to explain to a non-technical person, but as soon as you get into covariances, that goes out the window.;;;
5393;1;2015-03-26T00:58:14.003;Mahout SSVD, how should I set k and p;I have been looking at Stochastic Single Value Decomposition in Mahout for implementing a distributed LSA algorithm. However, I am having trouble finding the best way to set k and p such that k+p < min(m,n). Is there an optimal way to set k and p? I know that p should not exceed 10% of k and that k is the rank (typically from 20 to 200 according to the documents). Can I relate it to the number of dependent vectors?;[education, open-source];26;
5394;2;2015-03-26T03:15:05.417;;I think that depends on the pattern of your data, its skewness and sparsity. Because it would press these two bias term, which is designed to detect the difference with global mean. For example, you can plot mean and std of the user's bias: ($R_u$ - global_mean) then make a histogram of std. Another way, just make up a rating matrix $R'$ according to the $\mu + B_u + B_v$ you've got. And run the solver again and see what happens. These parts play its own role, even your change on global_mean would make a difference to the final RMSE, I think the role of that latent factors are born to capture the REST non-linear pattern in user & item's interaction.So, as for your question: Why does this part contributes so small?Again, you can simply using $R = UV^T$, in this time, I think it contributes quite a lot, right? The contribution of each part is hard to say.;;;
5395;2;2015-03-26T03:20:39.833;;Sure, as @LaruiK said. You've notice the factor which would change the similarity measure of the 'co-view' data, a simple way it to integrate a time decay function into your model.;;;
5396;2;2015-03-26T03:39:33.163;;Now you face the problem of scalability.Have you ever heard of random projection? Here is my suggestion:let's say $S$ is your item-item relation matrix, its dimension is too large to decompose, now you can multiply an smaller matrix $U$(randomly generated) and get $R = SU$. And $R$ is small also.Then just do SVD of $R$, if $U$ is invertible, and the difference (l2-norm or any norm you like) between $R$ and $S$ is small, it might helps.As for the second issue: NaN valueIf it's dense, try to complete the matrix(using mean?).;;;
5397;2;2015-03-26T12:44:44.903;;"As I understand it, these are the properties that you're seeking in a sample dataset:Text dataIt should be informal, i.e. have typos, slang, and basically something not professionally edited Something other than Twitter (I don't blame you, Twitter is a useful yet way overused example datasource in text mining)Here are some recommendations:Emails from the SpamAssassin corpus -- note that both ""ham"" (non-spam) and spam datasets are availablemicroblogPCU data set from UCI, which is data scraped from the microblogs of Sina Weibo users -- note, the raw text data is a mix of Chinese and English (you could perform machine translation of the Chinese, filter to only English, or use it as-is)Amazon Commerce reviews dataset from UCIWithin the bag-o-words dataset, try using the Enron emailsThe Twenty Newsgroups datasetThis nice collection of SMS spamYou can always scrape (extract) your own text data from the Internet; I'm not sure which language or statistical package you're using, but XPath-based packages are available in R (rvest, scrapeR, etc) and Python to accomplish this";;;
5399;2;2015-03-26T18:12:19.117;;Sorry for the short answer, but if you use shiny with rmarkdown, it can be done as shown in this question:http://stackoverflow.com/questions/29253481/data-specific-selectinput-choices-in-rmd-shiny/29255723#29255723Create a reactive function that reads the file in!;;;
5400;2;2015-03-26T20:00:57.663;;It depends on what data you are using to calculate similarity between your items.If you are using data from a user interaction with an item - like viewing a web page or buying an item - then each time there is a user interaction with an item, the similarity between it and other items will change. If you are doing this calculation offline, then your model won't take that change into account until the next time the data set is updated and your recommendations are recalculated.However, if you are using item meta-data to calculate similarity, then user behavior won't make any difference. For example, if you took the number of shared tags between two items, or the number of shared inbound links, etc. I have always used user browsing data to build content recommendation engines, but it is certainly possible to calculate similarity in many different ways. Have fun!;;;
5401;1;2015-03-27T07:20:28.740;Which book is the best for introduction to analysis social network using python3;"I am a beginner studying social network analysis.I installed python 3 just 2 weeks ago.There are a lot of books for python and social network analysis.I couldn't choose one of them.I found one book named ""Mining the Social Web (Analyzing Data from Facebook, Twitter, Linkedln, and Other Social Media Sites)"" written by Matthhew A. Russell.This book looks very interesting and fits in my purpose, but it is based on python 2. Is there any good books with python 3? I usually use Twitter, Facebook, or Blog data.In addition, could you recommend any good book for nodeXL and UCINET? ";[education, open-source];87;
5402;2;2015-03-27T08:00:09.137;;"Russell's book is fine. You might also like Social Network Analysis for Startups. All the examples are in python. You can do all your analysis in that using packages like networkx. NodeXL is for the Excel crowd. Definitely not the ideal tool for the job; I would shy away from it.The obvious book for NodeXL is Analyzing Social Media Networks with NodeXL, which is written by the authors of NodeXL.";;;
5403;2;2015-03-27T08:02:29.943;;"What you are asking for is usually called basket analysis.I think you should get maximum value from the data you have by using both of them: user & items.What you said about ""item-based"" approach means something like recommendation based on item tags or categories. It isn't a recommendation system in the full sense, because it uses your categorization/tagging. In other words, you will never place diapers and beer into a single category, but it is still legendary buying pattern :). Item-based data is used typically in basket analysis algorithms, such as frequent pattern mining. In couple words: you search for most frequent item sets (items bought together or coherently) and make suggestions based on them.User-based data shouldn't be ignored either. Clustering approach works here: you can find some groups of customers which buying attitude to item sets (found above) is different from the average.You can find more at statistics stack exchage site. Questions like this.And more.";;;
5404;1;2015-03-27T08:41:13.680;How does SQL Server Analysis Services compare to R?;This may be too broad of a question with heavy opinions, but I really am finding it hard to seek information about running various algorithms using SQL Server Analysis Service Data Mining projects versus using R. This is mainly because all the data science guys I work with don't have any idea about SSAS because no one seems to use it. :)The Database GuyBefore I start, let me clarify. I am a database guy and not a data scientist. I work with people who are data scientist who mainly use R. I assist these guys with creating large data sets where they can analyze and crunch data.My objective here is to leverage a tool that came with SQL Server that no one is really leveraging because no one seems to have a clue about how it works in comparison to other methods and tools such as R, SAS, SSPS and so forth in my camp.SSASI have never really used SQL Server Analysis Services (SSAS) outside of creating OLAP cubes. Those who know SSAS, you can also perform data mining tasks on cubes or directly on the data in SQL Server. SSAS Data Mining comes with a range of algorithm types:Classification algorithms predict one or more discrete variables,based on the other attributes in the dataset.Regression algorithms predict one or more continuous variables, suchas profit or loss, based on other attributes in the dataset.Segmentation algorithms divide data into groups, or clusters, ofitems that have similar properties.Association algorithms find correlations between different attributesin a dataset. The most common application of this kind of algorithmis for creating association rules, which can be used in a marketbasket analysis.Sequence analysis algorithms summarize frequent sequences or episodesin data, such as a Web path flow.Predicting Discrete ColumnsWith these different algorithm options, I can start making general predictions from the data such as finding out simply who is going to buy a bike based on a predictable column, Bike Buyers, against an input column, Age. The histogram shows that the age of a person helps distinguish whether that person will purchase a bicycle.Predicting Continuous ColumnsWhen the Microsoft Decision Trees algorithm builds a tree based on a continuous predictable column, each node contains a regression formula. A split occurs at a point of non-linearity in the regression formula. For example, consider the following diagram.ComparisonWith some of that said, it seems I can run a range of algorithms on the data and also have various functions available to me in SSAS to run against the data. It also seems I can develop my own algorithms in Visual Studio and deploy them to SSAS (if I'm not mistaken).So, what am I missing here in regards to languages and tools from R? Is it just that they have more flexibility to deploy and edit complex algorithms versus SSAS etc?;[education, open-source];467;
5406;1;2015-03-27T11:22:16.703;need explanation about the 2 condition before applying logestic regression;I want to apply LR to a classification problem, the otto competition in kaggle. And I see a post talks about 2 conditions needed to be checked before applying LR.whether the regressors are normally distributed within the classes.whether the variance-covariance matrices are equal for all classes.I have the data say X with 1 dimension represents label, and p dimension represent feature.Here is my understanding and confusion.On condition 1, I can plot each regressor to see whether it is normal. But is there any easy way?On condition 2, I can compute the variance-covariance matrices, but why they should be equal? Is this caused by the assumption that we made that $X=(x_1,x_2,...,x_p)$ is draw from a multi-variate Gaussian(or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix?;[education, open-source];31;
5407;2;2015-03-27T11:22:23.797;;"In my opinion, it seems that SSAS makes more sense for someone who:has significantly invested in Microsoft's technology stack and platform;prefer point-and-click interface (GUI) to command line;focus on data warehousing (OLAP cubes, etc.);has limited needs in terms of statistical methods and algorithms variety;has limited needs in cross-language integration;doesn't care much about openness, cross-platform integration and vendor lock-in.You can find useful this blog post by Sami Badawi. However, note that the post is not recent, so some information might be outdated. Plus, the post contains an initial review, which might be not very accurate or comprehensive. If you're thinking about data science, while considering staying within Microsoft ecosystem, I suggest you to take a look at Microsoft's own machine learning platform Azure ML. This blog post presents a brief comparison of (early) Azure ML and SSAS.";;;
5408;1;2015-03-27T12:36:11.577;Clustering not producing even clusters;I'm using k-means clustering to processes running on machines. Dataset sample : machine name, processm1,javam2,tomcatm1,wordm3,excelBuild a matrix of associated counts :    java,tomcat,word,excelm1,1,0,1,0m2,0,1,0,0m3,0,0,0,1I then run k-means against this dataset (have tried Euclidean and Manhattan distance functions) The dataset is extremely sparse which I think is causing the generated clusters to not make much sense as many machines get grouped into the same cluster(as they are very similar) How to achieve clusters where each cluster contains approx equal number of points ? Or perhaps this is not possible due to the sparseness of the data and instead I should try to cluster on a different attributes of dataset ?;[education, open-source];60;
5409;1;2015-03-27T15:31:49.090;Huge discrepancies in Logistic Regression and SVM using HOG features to identify an Object;"I am doing some research on Logistic regression and SVM using different parameters using HOG features. I am facing a bit of problem while understanding each classifier with combination of different parameters and different HOG features.My findings and confusions are given below, For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(1,1)Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')Output:     Total dataset=216, Correct prediction=209,Wrong Prediction=7    , For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(1,1)Classifier: SVC(C=1000.0,  gamma=0.1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5Classifier: SVC(C=100.0,  gamma=0.1, kernel='rbf')Output:     Total dataset=216, Correct prediction=211,Wrong Prediction=5Classifier: SVC(C=1.0,  gamma=0.1, kernel='rbf')Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6            -------------------------------------------------------------------------            For Hog:    orientations=9, pixelsPerCell=(9,9), cellsPerBlock=(3,3)Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=213,Wrong Prediction=3For Hog:    orientations=9, pixelsPerCell=(6,6), cellsPerBlock=(3,3)Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=210,Wrong Prediction=6Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4For Hog:    orientations=18, pixelsPerCell=(9,9), cellsPerBlock=(3,3)Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=3For Hog:    orientations=18, pixelsPerCell=(6,6), cellsPerBlock=(3,3)Classifier: SVC(C=1000.0,  gamma=1, kernel='rbf'),Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=100.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4Classifier: SVC(C=1.0,  gamma=1, kernel='rbf')Output:     Total dataset=216, Correct prediction=212,Wrong Prediction=4For the above results we can conclude that ""C"" of SVM and orientation of HOG doesn't matter much for the combinations, so we do not have the issue of highbias or variance and bin size. What matters is the value of gamma and combination of HOG with ""cellsPerBlock"".From my analysis, I found1. Using ""rbf"" SVM with HOG ""cellsPerBlock=(1,1)"" doesn't give proper    outcome.2. The ""rbf"" SVM with HOG ""cellsPerBlock=(1,1)"" works best with gamma=0.13. The ""rbf"" SVM with HOG ""cellsPerBlock=(3,3)"" and different combination of ""pixelspercells"" works best with gamma=1I've also tried pixelspercell = (3,3) but all the different combinations of ""rbf"" kernels fails to classify properly."""" The problem I face is understanding this discrepancy""""My Confusion even eggravates when I use Logistic regression for the Combinations of HOG surprisingly Regularized Logistic Regression works better with cellsPerBlock=(1,1)Below are my findings with 2nd order Regularized Logistic Regression: I used 0.9 as my threshold level, i.e below 0.9 would be classifies as 0 and above 0.9 would be classified as 1Q:   Why did I put 0.9 as threshold- Sol: Because the test data that had the class(labels)=1, 99% of them output probability >0.9     However the test data that had the class(labels)=0, many instances outputprobability >0.5   So inorder to filter out many test data instances that gave probability >0.5 A threshold of 0.9 was used.1.  orientations=9,    pixelsPerCell=(6,6)cellsPerBlock=(3,3)Output:     Total dataset=216, Correct prediction=187, Wrong Prediction=292.  orientations=9,    pixelsPerCell=(9,9)    cellsPerBlock=(3,3)    Output:     Total dataset=216, Correct prediction=202, Wrong Prediction=143.  orientations=9,    pixelsPerCell=(9,9)    cellsPerBlock=(1,1)    Output:     Total dataset=216, Correct prediction=211, Wrong Prediction=54.  orientations=9,    pixelsPerCell=(6,6)    cellsPerBlock=(1,1)    Output:     Total dataset=216, Correct prediction=206, Wrong Prediction=105.  orientations=9,    pixelsPerCell=(3,3)    cellsPerBlock=(1,1)    Output:     Total dataset=216, Correct prediction=203, Wrong Prediction=13";[education, open-source];20;
5410;2;2015-03-27T15:41:42.213;;A couple of ways to improve your design:Consider a different normalization: The sigmoid function will attenuate large moves.  It is likely precisely these large non-linear moves that attracted you to using neural networks in the first place.  Why remove them?  A simple whitening of the data may be betterAs pointed out by Nima, your model can only predict what is knowable from the data you give it.  If you are only fitting on data using historical prices, it will only give you what is predictable from them.  Things such as news events / earnings surprises / holidays / option market flows will not be fed into your model.  Consider adding these series as well.Lots more historical data.  Neural networks often require very large sample sizes because they are trying to estimate a very large parameter space.  Although, more data doesn't always imply more information, this will still likely help.Experiment with different network architectures.  The number of layers / size of the layers / different gradient decent algorithms / different activation functions / dropout etc..;;;
5411;2;2015-03-27T15:45:50.857;;I have had success using Mogwai Python Library  which is more actively maintained than Bulbs at this point.Though I agree that this belongs on stack overflow.  ;;;
5412;1;2015-03-28T03:18:48.543;parsing data by NLP using just offsets of words;"E.g 1: ""The speed of the car is 35 km/h and speed limit is 50 km/h"" E.g 2: ""The car is crossing the current speed limit by driving at 90 km/h"".E.g.3: ""Driving at 50 km/h your car will hit the speed limit.""  I very new in neural networks and in NLP too. Good in perl and in writing parsers for web pages. NLP is asking me my code to learn english and word meanings. I wish to try differently.I want to train my first neural network classification algo using offsets to words. I can train that to parse speed: in e.g1 the 35 is true and 50 is false. That in e.g2 90 is true. and e.g3 50 is false.Suppose I have a collection of lot of such sentences, so I can use my offset based training for my algorithm. There will be very confusing sentences too.The way my algo would work is to use offets of all words which are near the text which is to be classified. In sentence one it would output 11 parameters, is = -1, km/h=+1, car=-2, and=+2,the=-3,speed=+3,of=-4,limit=+4,speed=-5,is=+5,The=-6,km/h=+6. Same thing would be done for all sentences. the algo will train with these offsets and the result. There are too many words and they all will become parameters.Can you please suggest how durable is my proposed learning algorithm to parse out actual car driven speed from sentences? ";[education, open-source];44;
5413;1;2015-03-28T22:35:01.270;How to create and format an image dataset from scratch for machine learning?;I've only worked with ML with .csv formats. I've worked with image formats too but only premade imagesets (MNIST,etc). If I were to create an imageset from scratch, how are the class labels typically formated? Would I have to manually title the image of a jpeg? Best,Jeremy;[education, open-source];50;
5414;2;2015-03-29T11:33:26.277;;Looking at existing challenges around and their data format (for example http://www.kaggle.com/c/datasciencebowl/data) I would say put the images in a folder per class. You can use the file names for the index.;;;
5416;1;2015-03-30T04:07:36.377;Difference between Spark MLlib kmeans & Spark MLlib StreamingKmean implementation? Is it possible to implement Streaming Naive Bayes easily?;I am new to Spark and MLlib. I was looking at the Kmeans (http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) and StreamingKmeans implementation in Spark MLlib (http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means). What I am confused about is, is StreamingKmeans derived from Kmeans implementation in MLlib? Is it possible to write similar code for Naive Bayes (http://spark.apache.org/docs/latest/mllib-naive-bayes.html) if I want to convert Naive Bayes to Streaming. If so, how can i achieve this? Any pointers and help will be really appreciated.;[education, open-source];73;1
5417;1;2015-03-30T04:35:58.667;Choosing between Storm+Trident-ML, Storm+SAMOA or Spark Streaming+MLlib;I want to implement Streaming Naive Bayes in a distributed system. What are the best approach to choose framework. Should I choose:Storm alone and implement streaming naive bayes on my own in storm topology.Storm + TridentMLStorm + SAMOASpark Streaming + MLlibWhat is the best framework set to choose and start working on. Any suggestion will be of great help.;[education, open-source];332;4
5418;2;2015-03-30T04:46:43.637;;It means the average predictions of the user across all items, and the average of each item across all users accurately predicts the ratings -- you have an easy data set.;;;
5419;2;2015-03-30T14:10:48.077;;I can't comment due to reputation, but you really need to tell us what version of SQL Server you are running, maybe some more information about how the data is structured and how you're pulling the data into these dashboard. Maybe even how long it's taking and what resources you have available that actually know what they are doing.OLTPThat said, it sounds like you have a OLTP database with lots of tables. As I don't know the relationships of these tables or how you are pulling the data from these tables, I can only assume you are pulling data from many of these tables. If optimizing the tables is not helping due to how many joins and records you are pulling, then:SQL Server Analysis ServicesIt sounds like you need to create a multidimensional database that can be used for reporting. SQL Server Analysis Services helps you do that by allowing you to define OLAP Cubes in many different structures from MOLAP to ROLAP.Multidimensional Database (Data Warehousing)Another way is creating a new database that is going to be the foundation for your multidimensional data. Therefore, you would need to create a complex ETL System within SQL Server that converts those 400 tables into facts and dimensions automatically on a daily basis and pushes it into your new database. This is a similar process to what SSAS is going to do for you when you define cubes in SQL Server.Preaggregate Tables or ViewsIf you can't do that yourself, then another way is just building new tables in your database that are just preaggregates of the 400 tables that will be used for reporting. Basically established how you are reading the data for your dashboards and find ways to preaggregate that data into fewer tables before you actually pull it into a report.Automation & ToolsThis (as well building multidimensional data) is accomplished simply by creating stored procedures or SSIS packages and automating the process every day. Then Tableau, SSRS or whatever queries the new tables rather than the previous 400 tables that may be slowing the process.Hire Someone, You Already Have The ToolsThe last and final way is finding a tool that does the ETL for you. There are plenty of ETL vendors out there that can possibly address this problem. But keep in mind, you likely have all the tools you need to do this. You just need to hire the talent to do it either temporarily on contract or full-time.If I had no knowledge of what I was doing in SQL, I would contract a ETL Developer, SQL Developer or BI Developer to help me. Because why buy another tool box when you already have a good tool box available to you?;;;
5420;1;2015-03-30T15:31:17.133;Http Server Using Torch7;I'm starting to learn Torch7 to get into the machine learning/ deep learning field and I'm finding it fascinating (and very complicated haha). My main concern, however, is if I can turn this learning into an application - mainly can I turn my Torch7 Lua scripts into a server that an app can use to perform machine learning calculations? And if it's possible, how?Thank you;[education, open-source];38;
5421;1;2015-03-30T16:11:11.883;twitter-tweet filter for analysis;"i was working with a matlab based twitter class called ""twitty.m""(downloadable from the URL: http://in.mathworks.com/matlabcentral/fileexchange/34837-twitty).Now , they had instructed me to set my twitter credentials , which i had done and stored in a structure in matlab.Now , when i call the functions associated with it , i receive an error , which is as below:     tw.search('matlab')   Error using twitty/callTwitterAPI (line 1974)      Java exception occurred:     javax.net.ssl.SSLHandshakeException:     sun.security.validator.ValidatorException: PKIX  path   buildinfailed:     sun.security.provider.certpath.SunCertPathBuilderException: unable to       find valid certification path to requested target    at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Unknown       Source)      at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(Unknown Source)           at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Unknown                                                                      Source)           at com.sun.net.ssl.internal.ssl.Handshaker.fatalSE(Unknown Source)at com.sun.net.ssl.internal.ssl.ClientHandshaker.serverCertificate(Unknown    Source)   at com.sun.net.ssl.internal.ssl.ClientHandshaker.processMessage(Unknown    Source)   at com.sun.net.ssl.internal.ssl.Handshaker.processLoop(Unknown  Source)   at com.sun.net.ssl.internal.ssl.Handshaker.process_record(Unknown        Source)   at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(Unknown  Source)now , i'm not able to comprehend the information given. Also , when it's verifying the credentials , this error creeps in :  tw = twitty(credentials);    Error using twitty (line 298) The supplied credentials are not valid.Now , can anyone explain the error shown";[education, open-source];44;
5424;1;2015-03-31T16:26:21.110;Resampling the Validation Sample;I have a large data set where training is quite (time) costly but whose result is rather volatile. I split the original data into a training / validation / test sample and I am considering getting a notion for the volatility by re-sampling the validation sample:Bootstrap sample the validation sample For each bootstrap sample,score the data with the model created from the training data Plot / average the performance measures from each bootstrap sample.Of course, bootstrapping the training data, building a new model on each sample and scoring the out of sample data with each is the original bootstrap idea, but requires retraining a model many times. Has anyone used the procedure I mention and/ or find serious issue with it?  ;[education, open-source];32;1
5427;1;2015-04-01T15:23:17.997;How to generate synthetic dataset using machine learning model learnt with original dataset?;Generally, the machine learning model is built on datasets. I'd like to know if there is any way to generate synthetic dataset using such trained machine learning model preserving original dataset characteristics ?  [original data --> build machine learning model --> use ml model to generate synthetic data....!!!]Is it possible ? Please point me to related resource if possible.;[education, open-source];175;
5428;1;2015-04-01T20:56:22.337;Canonical VS Graph Isomorphism;I'm having a having a hard time understanding the difference between an isomorphism in graphs and canonical graphs. I have read through the Wikipedia articles, but it still isn't clicking.Can somebody explain the difference, perhaps with an example?;[education, open-source];128;
5429;2;2015-04-01T22:37:39.300;;"The general approach is to do traditional statistical analysis on your data set to define a multidimensional random process that will generate data with the same statistical characteristics.  The virtue of this approach is that your synthetic data is independent of your ML model, but statistically ""close"" to your data. (see below for discussion of your alternative)In essence, you are estimating the multivariate probability distribution associated with the process.  Once you have estimated the distribution, you can generate synthetic data through the Monte Carlo method or similar repeated sampling methods.  If your data resembles some parametric distribution (e.g. lognormal) then this approach is straightforward and reliable.  The tricky part is to estimate the dependence between variables. See: http://www.encyclopediaofmath.org/index.php/Multi-dimensional_statistical_analysisIf your data is irregular, then non-parametric methods are easier and probably more robust.  Multivariate kernal density estimation is a method that is accessible and appealing to people with ML background. For a general introduction and links to specific methods, see: https://en.wikipedia.org/wiki/Nonparametric_statistics .To validate that this process worked for you, you go through the machine learning process again with the synthesized data, and you should end up with a model that is fairly close to your original.  Likewise, if you put the synthesized data into your ML model, you should get outputs that have similar distribution as your original outputs.In contrast, you are proposing this: [original data --> build machine learning model --> use ml model to generate synthetic data....!!!]This accomplishes something different that the method I just described.  This would solve the inverse problem: ""what inputs could generate any given set of model outputs"". Unless your ML model is over-fitted to your original data, this synthesized data will not look like your original data in every respect, or even most.Consider a linear regression model. The same linear regression model can have identical fit to data that have very different characteristics. A famous demonstration of this is through Anscombe's quartet.Thought I don't have references, I believe this problem can also arise in logistic regression, generalized linear models, SVM, and K-means clustering.There are some ML model types (e.g. decision tree) where it's possible to inverse them to generate synthetic data, though it takes some work.  See: Generating Synthetic Data to Match Data Mining Patterns.";;;
5430;2;2015-04-02T01:33:37.173;;If I were you, I would pick anyone of the frameworks I am comfortable with and implement the use-case. Spark-Streaming + MLlib should work and would be my choice since its user base is on the rise and it is one of the most popular project under the Apache Umbrella with good enterprise business plan. Both Cloudera and Hortonworks provide enterprise level support. Now, in theory Spark-Streaming lacks behind Storm in stream processing, but the framework is cool in a way that it provides you the option to do streaming, common map and reduce, graph processing and SQL under the same framework. So once you have the pipeline to convert your data to RDD you are good for most of the common jobs related to Data Analysis. It's written from scratch in Scala which is a very powerful language and provides huge scalability in a distributed setup when handling concurrency. Hope this helps, feel free to reach out to me with any questions you have. ;;;
5431;2;2015-04-02T10:50:12.953;;Problem is resolved. I created the folders in terminal.6b.  hdfs dfs -mkdir /userhdfs dfs -mkdir /user/hduserPut this file to HDFS:hadoop fs -put file1.txt /user/hduser/file1.txthadoop fs -put file.txt file.txt;;;
5432;2;2015-04-02T14:08:05.547;;I am using this one, so far so good.Online terminals: http://www.tutorialspoint.com/codingground.htm Also, R-Fiddle is an option.http://www.r-fiddle.org/#/;;;
5433;1;2015-04-02T14:17:54.813;Word normalization and dataset structure for multi-contextual text analysis;I'd like to experiment basic text analysis: word frequency, distances, etc. But, my long term goal is to produce concept map of texts and corpus. So I am more interested in concepts than in words. Which method of word standardization (tokenizing, stemming, ...) would be more appropriate for that purpose? What are the possible approach to manage many overlapping word-sets (for multi-contextual analysis).;[education, open-source];31;
5435;1;2015-04-02T20:17:19.077;How does the algorithm with the complexity of O(N**3) work?;I have watched an explanation of Big O Notations from this eloquent YouTube video: https://www.youtube.com/watch?v=V6mKVRU1evUHowever, he does not mention algorithms with two asterisk signs. How does the algorithm with the complexity of O(N**3) work?;[education, open-source];39;
5436;1;2015-04-02T23:00:20.687;how to modify sparse survey dataset with empty data points?;"I am working on a data set where the categorical variables have lots of empty spaces (not ""NA"" but """"). For example, one variable has 14587 empty spaces out of 14644 observations. There are many such variables where most of the observations are empty.In fact it is a survey dataset where the participant just chose to ignore a particular question.I have never handled similar dataset. I am looking for advise as to how best to handle such datasets before any modeling is done. Deleting the rows or the variables with lots of empty spaces doesn't seem feasible. Thanks a lot.";[education, open-source];35;
5437;2;2015-04-02T23:38:56.127;;"I would consider approaching this situation from the following two perspectives:Missing data analysis. Despite formally the values in question are empty and not NA, I think that effectively incomplete data can (and should) be considered as missing. If that is the case, you need to automatically recode those values and then apply standard missing data handling approaches, such as multiple imputation. If you use R, you can use packages Amelia (if the data is multivariate normal), mice (supports non-normal data) or some others. For a nice overview of approaches, methods and software for multiple imputation of data with missing values, see the 2007 excellent article by Nicholas Horton and Ken Kleinman ""Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models"".Sparse data analysis, such as sparse regression. I'm not too sure how well this approach would work for variables with high levels of sparsity, but you can find a lot of corresponding information in my relevant answer.";;;
5438;1;2015-04-02T23:52:02.953;What is the term for when a model acts on the thing being modeled and thus changes the concept?;I'm trying to see if there is a conventional term for this concept to help me in my literature research and writing.  When a machine learning model causes an action to be taken in the real world that affects future instances, what is that called?  I'm thinking about something like a recommender system that recommends one given product and doesn't recommend another given product.  Then, you've increased the likelihood that someone is going to buy the first product and decreased the likelihood that someone is going to buy the second product.  So then those sales numbers will eventually become training instances, creating a sort of feedback loop.Is there a term for this?;[education, open-source];131;
5440;2;2015-04-03T00:51:29.800;;Though it is not specifically a term, focused on machine learning, but I would refer to such behavior of a statistical model, using a general term side effect (while adding some clarifying adjectives, such as expected or unexpected, desired or undesired, and similar). Modeling outcome or transitive feedback loop outcome might be some of the alternative terms.;;;
5441;2;2015-04-03T01:44:18.450;;"There are three terms from social science that apply to your situation:Reflexivity - refers to circular relationships between cause and effect.  In particular, you could use the definition of the term adopted by George Soros to refer to reverse causal loop between share prices (i.e. present value of fundamentals) and business fundamentals.  In a way, the share price is a ""model"" of the fundamental business processes.  Usually, people assume that causality is one-way, from fundamentals to share price.Performativity - As used by Donald MacKenzie (e.g. here), many economic models are not ""cameras"" -- taking pictures of economic reality -- but in fact are ""engines"" -- an integral part of the construction of economic reality. He has a book of that title: An Engine, Not a Camera.Self-fulfilling Prophecy - a prediction that directly or indirectly causes itself to become true, by the very terms of the prophecy itself, due to positive feedback between belief and behavior. This is the broadest term, and least specific to the situation you describe.Of the three terms, I suggest that MacKenzie's ""performativity"" is the best fit to your situation.  He claims, among other things, that the validity of the economic models (e.g. Black-Scholes option pricing) has been improved by its very use by market participants, and therefore how it reflects in options pricing and trading patterns.";;;
5442;1;2015-04-03T03:07:50.493;Learning resources for data science to win political campaigns?;Does anyone know, where I can learn about applying data science to win a political campaign? I know the Obama campaign had 12 data scientists in 2008 and 165 data scientists in 2012. In 2012, they ran over 65,000 simulations every night, for 14 months. They correctly predicted every state within 0.5% and Florida within 0.05%. How did they do this? And where can I find the data they used?;[education, open-source];156;
5443;1;2015-04-03T20:28:36.710;Do data scientists use Excel?;"I would consider myself a journeyman data scientist.  Like most (I think), I made my first charts and did my first aggregations in high school and college, using Excel.  As I went through college, grad school and ~7 years of work experience, I quickly picked up what I consider to be more advanced tools, like SQL, R, Python, Hadoop, LaTeX, etc.We are interviewing for a data scientist position and one candidate advertises himself as a ""senior data scientist"" (a very buzzy term these days) with 15+ years experience.  When asked what his preferred toolset was, he responded that it was Excel.I took this as evidence that he was not as experienced as his resume would claim, but wasn't sure.  After all, just because it's not my preferred tool, doesn't mean it's not other people's. Do experienced data scientists use Excel?  Can you assume a lack of experience from someone who does primarily use Excel?";[education, open-source];725;4
5444;2;2015-04-03T22:37:22.080;; Do experienced data scientists use Excel?I've seen some experienced data scientists, who use Excel - either due to their preference, or due to their workplace's business and IT environment specifics (for example, many financial institutions use Excel as their major tool, at least, for modeling). However, I think that most experienced data scientists recognize the need to use tools, which are optimal for particular tasks, and adhere to this approach. Can you assume a lack of experience from someone who does primarily  use Excel?No, you cannot. This is the corollary from my above-mentioned thoughts. Data science does not automatically imply big data - there is plenty of data science work that Excel can handle quite well. Having said that, if a data scientist (even experienced one) does not have knowledge (at least, basic) of modern data science tools, including big data-focused ones, it is somewhat disturbing. This is because experimentation is deeply ingrained into the nature of data science due to exploratory data analysis being a essential and, even, a crucial part of it. Therefore, a person, who does not have an urge to explore other tools within their domain, could rank lower among candidates in the overall fit for a data science position (of course, this is quite fuzzy, as some people are very quick in learning new material, plus, people might have not had an opportunity to satisfy their interest in other tools due to various personal or workplace reasons).Therefore, in conclusion, I think that the best answer an experienced data scientist might have to a question in regard to their preferred tool is the following: My preferred tool is the optimal one, that is the one that best fits the task at hand.;;;
5445;1;2015-04-03T23:19:27.283;Logistic Regression Cost Function Error;"with regards to the Logistic Regression cost function of:And hypothesis:Is there a way to tell the +/- of the error for how ""confident"" the hypothesis is?E.g. if the +/- of the error was 0.1, I would know that if my hypothesis predicted 0.4 it could be 0.1 greater (0.5) or 0.1 less (0.3)This is for binary classification";[education, open-source];34;
5446;2;2015-04-04T06:57:47.793;;This is an interesting and relevant question. I think that from data science perspective, it should not be, in principle, any different from any other similar data science tasks, such as prediction, forecasting or other analyses. Similarly to any data science work, the quality of applying data science to politics very much depends on understanding not only data science approaches, methods and tools, but, first and foremost, the domain being analyzed, that is politics domain.Rapidly rising popularity of data science and machine learning (ML), in general, certainly has a significant impact on particular verticals and politics is not an exception. This impact can be seen not only in increased research interest in applying data science and ML to political science (for example, see this presentation, this paper, this overview paper and this whole virtual/open issue in a prominent Oxford journal), but in practical applications. Moreover, a new term - political informatics or poliInformatics or poli-informatics - has been coined to name an interdisciplinary field, which stated goal is to study and use data science, big data and ML in the government and politics domains. As I've said earlier, the interest in applying data science to politics goes beyond research and often results in politics-focused startups, such as PoliticIt or Para Bellum Labs. Following the unfortunate, but established trend in startup ecosystem, many of those ventures fail. For example, read the story of one of such startups.I am pretty sure that you will be able to find neither proprietary algorithms that political startups or election data science teams used and use, nor the their data sets. However, I am rather positive that you can get some understanding about typical data sets as well as data collection and analysis methods via the resources that I have referenced above. Hope this helps.;;;
5447;2;2015-04-04T07:32:19.127;;Logistic regression is just a generalized linear model, so there is a linear regression lurking in here whose errors are supposed to be normally distributed, and from which you can get confidence intervals in the usual way. That is you have a linear predictor (the -theta' * x part), transformed by the inverse of a link function (the logistic function). You can transform prediction +/- confidence interval using this inverse link to get your +/- in probability terms. It won't be symmetric.Nice writeup of how to do this in R: http://stackoverflow.com/a/14424417/64174;;;
5448;2;2015-04-04T08:30:27.967;;Excel allows only very small data and doesn't have anything that is sufficiently useful and flexible for machine learning or even just plotting. All I would do in Excel, is stare at a subset of the data for a first glance over the values to make sure I don't miss anything visible by eye.So, if his favourite tool is Excel, this might suggest he rarely deals with machine learning, statistics, larger data sizes or any advanced plotting. Someone like this I wouldn't call a Data Scientist. Of course titles don't matter and it depends a lot on your requirements.In any case, don't make a judgement by statements of experience or CV. I've seen CVs and known the people behind it.Don't assume. Test him! You should be good enough to set up a test. It has been shown that interviews alone are close to useless to determine skills (they only show personality). Set up a very simple supervised learning test and let him use any tool he wants.And if you want to screen people at an interview first, then ask him about very basic but important insights about statistics or machine learning. Something that every single of your current employees knows.;;;
5449;2;2015-04-04T12:53:13.217;;"Let me first clarify that I am starting my journey into data science from a programmer and database developer standpoint. I am not a 10-year data science expert nor a statistical god. However, I do work data scientist and large datasets for a company that works with rather large clients worldwide.From my experience, data scientist use whatever tools they need to get the job done. Excel, R, SAS, Python and more are all tools in a toolbox for good data scientist. The best can use a wide variety of tools to analyze and crunch data.Therefore, if you find yourself comparing R to Python, then you're likely doing it all wrong in the data science world. Good data scientist use both when it makes sense to use one over the other. This also applies to Excel.I think that it's rather hard to find anyone that is going to have experience in so many different tools and languages while been great at everything. I also think it's going to be hard to find data scientist specifically that can not only program complex algorithms but also know how to use them from a statistical standpoint too. Most of the data scientist I've worked with come in about 2 flavors. Those that can program and those that can't. I rarely work with data scientist that can pull data in Python, manipulate it with something like Pandas, fit a model to the data in R and then present it to management at the end of the week.I mean, I know they exist. I've read many data science blogs from guys developing web scrappers, pushing it into Hadoop, pulling it back out in Python, programming complex things and running it through R to boot. They exist. They're out there. I just haven't ran into too many that can do all of that. Maybe it's just my area though?So, does that mean only specializing in one thing bad? No. Plenty of my friends specialize in just one main language and kill it. I know plenty of data guys who only know R and kill it. I also know plenty of people who just use Excel to analyze data because that's the only thing most non-data scientist can open and use (especially in B2B companies). The question you really need to answer is if this one thing is the ONE thing you need for this position? And most importantly, can they learn new things?P.SData Science is not just restricted to ""BIG DATA"" or NoSQL.";;;
5450;2;2015-04-04T20:37:55.330;;Most non-technical people often use Excel as a database replacement. I think that's wrong but tolerable. However, someone who is supposedly experienced in data analysis simply can not use Excel as his main tool (excluding the obvious task of looking at the data for the first time). That's because Excel was never intended for that kind of analysis and as a consequence of this, it is incredibly easy to make mistakes in Excel (that's not to say that it is not incredibly easy to make another type of mistakes when using other tools, but Excel aggravates the situation even more.)To summarize what Excel doesn't have and is a must for any analysis:Reproducibility. A data analysis needs to be reproducible.Version control. Good for collaboration and also good for reproducibility. Instead of using xls, use csv (still very complex and has lots of edge cases, but csv parsers are fairly good nowadays.) Testing. If you don't have tests, your code is broken. If your code is broken, your analysis is worse than useless. Maintainability.Accuracy. Numerical accuracy, accurate date parsing, among others are really lacking in Excel.More resources:You shouldn’t use a spreadsheet for important work (I mean it)Microsoft's Excel Might Be The Most Dangerous Software On The PlanetDestroy Your Data Using Excel With This One Weird Trick!Excel spreadsheets are hard to get right;;;
5452;2;2015-04-04T23:43:50.157;;I think most people are answering without having a good knowledge of excel. Excel (since 2010) has an in memory columnar [multi table] database , called power pivot (which allows input from csv/databases etc), allowing it to store millions of rows (it doesn't have to be loaded on a spreadsheet).It also has an ETL tool called power query allowing you to read the data from a variety of sources (including hadoop). And it has a visualisation tool (power view & power map). A lot of Data Science is doing aggregation and top-n analysis at which power pivot excels.  Add to this the interactive nature of these tools - any user can easily drag and drop a dimension on which to break up the results adn I hope you can see the benefits. So yes you can't do machine learning, but I would question how much machine learning is done by data scientists day to day: eg when I want to analyse the prediction errors made in machine learning program I find it easiest to slice and dice the errors with excel. ;;;
5457;2;2015-04-05T07:57:05.907;;I think in the literature the overarching term often used is  active learning  Active Learning ...in particular Multi-Armed Contextual Bandit;;;
5458;1;2015-04-05T10:00:43.167;Classification of skills based on job ads;"I have around 1,000 job ads in the filed of IT (in excel file). I want to find the skills which are mentioned in each of ads. and then find the similar jobs based on skills.My method: I created 12 categories Such as programming skills,  testing skills,  communication skills, network skills, ... . Each advertisement may belong to 3-4 categories. In this case, some said multi-variate classification or Multi label classification is useful. But I don't know how to do this kind of classification in RapidMiner.1- Does anyone know how to do multi-variate classification or Multi label classification in RapidMiner? or is there another way?2- Do you recommend ""classification"" in order to analysis required job skills? or another technique? 3- Is there any better way to classify the skills which are stated in job ads?I'm new in the field of text mining. Please let me know if you have any idea. Thanks";[education, open-source];56;
5459;1;2015-04-05T14:37:45.787;"SAS. How to write ""OR""";"How to write ""OR"" in this example?DATA a1;set a;if var1=1 OR 2;run;P.S. va1 is the categorial (with categories: 1, 2, 3)";[education, open-source];30;
5460;2;2015-04-05T17:42:10.457;;i'm assuming this will be migrated to stack overflow, but instead of trying to do if var1=1 or 2 wouldn't it be better to use if var1 in (1, 2)?...and somebody with enough reputation should probably create an sas tag (and an spss tag while you're at it) unless data scientists only use open source languages like r and python now...;;;
5461;2;2015-04-06T01:07:35.447;;Erin Hartmann who just finished up a PhD in Political Science at UC-Berkley has a paper that uses Obama for America data from 2012.  She is using the paper on the job-market for an American Politics sub-field position  Might want to check out her work, though I'm not sure it will answer your question.  She has a public site here: http://www.erinhartman.com;;;
5462;1;2015-04-06T04:18:57.317;Correct tagging but wrong parse tree;If I parse following sentence: He is playing cricket in ground with grass.with stanford parser, the result is: (ROOT  (S    (NP (PRP He))    (VP (VBZ is)      (VP (VBG playing)        (NP          (NP (NN cricket))          (PP (IN in)            (NP (NN ground))))        (PP (IN with)          (NP (NN grass)))))    (. .)))Is there any parser who can correct the result on the basis of probability as the probability of appearing grass with ground is higher than cricket?Or parser which do chunking before parsing like this: He is playing cricket |in ground| |with grass|.and calculate probability with all combinations before generating the parse tree.He is playing cricket |in ground| |with grass|.He is playing cricket |with grass| |in ground|.;[education, open-source];15;
5464;1;2015-04-06T17:36:22.860;Is it ethical to claim experience with big data when the data isn't a part of the new advertising/social media/retail fad?;Obviously most employers, when hiring a data scientist, would prefer experience with big data and/or data science.  But what can one safely assume they will acknowledge as experience?Let's say someone often launches software on a computing cluster that typically generates an amount of data.  I'm not sure what the best measure of this data is for data science.  I'll call it one or two thousand rows, 200k or 300k points per row...certainly under 500k.  Then for each point, let's call it 25 or 30 values.  This amounts to 30 or 40 gig of data.  300 or 400 times of this and you can call it a study - maybe one or two studies per year.  I'm under the impression that this is much smaller than a data scientist at Google or Facebook would be used to, but it's certainly too big for my home computing systems.If someone's been working with this for years (some people at this company have been doing it since before data science was coined/before social media existed), is it fair for them to claim big data experience?  According to this answer, it's not the amount of data but what needs to be done with the data that matters - is that a universally accepted opinion?For what it's worth, working with this data entails manipulation/cleaning of the data with some proprietary languages, shell scripts, and a lot of Python.  A little bit of R but that's a more recent thing.  It involves tons of data visualization, drawing conclusions, and presenting to management/convincing decision makers.  Some of it involves trend determination, extrapolation, and comparisons made between data sets that aren't related in a straightforward way, so it sounds data science-ish to me.  But I will be the first to admit that I have a limited understanding of what data science currently is....bonus points if you can tell me whether or not this is an Easter egg or is the actual answer total for this site at the moment:Edit:I'll try to clarify.  What do data science employers acknowledge as experience with big data/data science?  Does the size of the data above qualify experience with it?  Or is it universally accepted among those in the field that it's not at all the size of the data, but what you need to do with the data?;[education, open-source];101;
5466;2;2015-04-06T19:20:18.873;;I've been using Anaconda Python 3.4 and Pandas to search 10M row database to match 20K of login credentials. Takes about a minute. The pandas internals make great use of memory. That said, truly big data requires a processing architecture matched to the problem. Pandas is just the glue (logic) in this equation, and other tools can do this as well. R, Scala, Haskell, SAS, etc. can replicate some of the logic - perhaps just enough to answer questions faster. But python makes a good (best?) general-purpose tool. You can run R code in python, as well as most other languages. Although interpretive, there are high performance techniques and tools such as pypy that can make python run almost as fast as benchmark tools with only slightly more effort. And python has many libraries that do just about everything - see above list.If you are asking if you should learn and use python, my answer is yes Articles indicate that python is used more than R among people who use both. But few data science problems are solved by a single tool. It may become your go-to tool, but its only that - a tool. And just as no sane person builds a house with just a hammer, no sane Data Scientist uses just one tool.;;;
5469;2;2015-04-06T22:07:18.737;;looooopsI can imagine it will run slow :)avoid when possible very long for-loops in R. If so, keep the iterations as simple as possible, be careful using slow search functions and do not subset or edit large data structures.restructure your dataI was talking to a guy, at a party, doing similar analysis. He said they arranged the data in a matrix of nrow sentences(200.000) and ncol(7000) words. Any element count how many times a given word occoured in a given sentence. If you need to do more analysis on the data set later, any operations can be done really fast. The result of your sentiment analysis would be the inner product of any row to a scoring vector(+1 +2 -1, score for word). The matrix would take some ~2Gb space if you use the bigmemory package and choose shorts(max 255 counts) instead of integers. bigmemory goes smoothly with multi-core(foreach, doMC, etc.) and elements can be modified individually much faster than for R matrices. The matrix can be saved to shared memort or HDD. bigmemory is using a C++ data structure which makes it possible to do some really fast operations with Rcpp. Steap laerning curve though.Sparse matrix would also be a good idea.bigmemory package do not support windowsMaybe get some inspiration here:http://stackoverflow.com/questions/10233087/sentiment-analysis-using-r;;;
5470;1;2015-04-07T07:18:19.127;Display more than two variables on the bar chart in Tableau;I am trying to put more than several variables on the bar chart in Tableau.Display two of them works fine: use Dual Axes chart type + select Syncronize axes option.Var1 and Var2:Var2 and Var3:I want to add Var3 somehow to the first chart.As a bars, or may be circles. But I want them to be on the same scale as the first two variables, and all start from zero.After dragging Var3 to the first chart I got the stacked bars...;[education, open-source];70;
5471;1;2015-04-07T14:05:28.010;Building a static local website using Rmarkdown: step by step procedure;I am trying to understand the procedure of building a static local website using R and Rmarkdown. I am aware of a Rmarkdown website where the procedure is outlined, but unfortunately I do not understand the steps. Does anybody here have some experience in building a static local website and would be so kind as to describe the procedure in more detail?   ;[education, open-source];85;1
5473;2;2015-04-07T14:46:19.027;;Data can be big in various ways. It can be large N (observations) and small P (variables per observation), like access logs for example. It can be large P and small N, like in biostatistics where you have tens of thousands of gene expressions from tens or hundreds of people. Or it can be both N and P, like in Facebook data. Data can also be made big, by normalizing every single detail of it, meaning that you'll be able to make a massive relational database schema that actually represents simple data.All kinds of big data needs different approaches and companies mostly care about their own version of big data. That's why the answer isn't black and white.To answer the question you posed in title then I believe it's ethical to claim experience with big data as long as you can put it into context of the business or domain where you encountered it and can describe some situations where a different set of tools or skills was needed than is used for smaller toy examples.;;;
5474;2;2015-04-08T00:00:33.730;;"this might take a few iterations, but perhaps it can be moved to chat if it gets too long.  are you wondering about a specific application or just the general concept?  as far as i know, the canonical form of something is the unique/simplest arrangement/representation of that something.  as for isomorphism (one to one relationship, a function gets you from one set to the other), you can oversimplify it and think about factors of numbers.for a moment consider an analogy - instead of graphs, using sets of numbers.  this is randomly off the top of my head so hopefully there aren't too many holes in it.  we have$A = 6, 10, 14$and$B = 18, 30, 42$let's call $A$ and $B$ isomorphic because to get between any value in $A$ to the corresponding value in $B$, you multiply or divide by 3.  now let's introduce another isomorphic set:$C = 3, 5, 7$it looks like $A$, $B$, and $C$ are all isomorphic because there is a one to one relationship between any of their initial values.  the same can be said for any of their middle values, or any of their ending values.  while it's great that they're all isomorphic, we'll consider something additional about $C$.  i'm going to call $C$ canonical because if you remove the greatest common factor from $A$, or you remove the greatest common factor from $B$, you get $C$.  there is no common factor we can remove to reduce $C$ itself.extending this, you can see that a canonical representation of a graph is an isomorphic version of that graph, but not necessarily the other way around.  all isomorphic versions of said graph will reduce to the same canonical form.EDIT:that being said, the definition of canonical is more complicated for graphs.  first, for graph isomorphism, this page about a code named bliss presents some small graphs as an example.  they define $G_{1}$ asand $G_{2}$ asand offer two transformations for $G_{1}$ that will make $G_{1}$ and $G_{2}$ isomorphic.  first, they consider $\{\langle 1\to 2, 2\to 4, 3\to 1, 4\to 3\rangle\}$, and then $\{\langle 1\to 2, 2\to 4, 3\to 3, 4\to 1\rangle\}$ - both of these transformations swap vertices.  the transformations result in the following isomorphic graphs:each pair of connected vertices has a corresponding pair in $G_{2}$ ($2\to 1,3,4; 1\to 4; 3\to 4$).now, to discuss canonical graphs, we must identify a cost function with which we can measure what is the 'most canonical' form.  this paper mentions metrics such as area, minimum angle, and total number of bends.  for simplicity, i'll try to minimize bends and favor angles that are closer to $90^{\circ}$.if you use a coordinate transformation, you can see that this simple graphand this graphare isomorphic.  the same edges connect the same pairs of vertices, and if you move vertices from one around, you can make the other.  this graphis also isomorphic with the first two, but since it has the optimal amount of bent edges (zero) and optimal angles (four right angles), it is also the canonical representation for these graphs.for further reading, you might consider this paper, which discusses several algorithms used to find canonical representations for web mining, or this paper, which discusses canonical representations of binary decision diagrams, word-level decision diagrams, binary moment diagrams, taylor expansion diagrams, and finite field decision diagrams.  tl;dr:just have a look at the minion with bananas on slide 10 of this presentation.";;;
5477;2;2015-04-08T03:47:26.333;;In most things, related to R, there are many approaches to solve a problem, sometimes too many, I would say. The task of building a static website, using RMarkdown, is not an exception.One of the best, albeit somewhat brief, sets of workflows on the topic include the following one by Daniel Wollschlaeger, which includes this workflow, based on R, nanoc and Jekyll, as well as this workflow, based on R and WordPress. Another good workflow is this one by Jason Bryer, which is focused on R(Markdown), Jekyll and GitHub Pages.Not everyone likes GitHub Pages, Jekyll, Octopress and Ruby, so some people came up with alternative solutions. For example, this workflow by Edward Borasky is based on R and, for a static website generator, on Python-based Nicola (instead of Ruby-based Jekyll or nanoc). Speaking about static website generators, there are tons of them, in various programming languages, so, if you want to experiment, check this amazing website, listing almost all of them. Almost, because some are missing - for example, Samantha and Ghost, listed here.Some other interesting workflows include this one by Joshua Lande, which is based on Jekyll and GitHub Pages, but includes some nice examples of customization for integrating a website with Disqus, Google Analytics and Twitter as well as getting custom URL for the site and more.Those who want a pure R-based static site solution, now have some options, including rsmith (https://github.com/hadley/rsmith), a static site generator by Hadley Wickham, and Poirot (https://github.com/ramnathv/poirot), a static site generator by Ramnath Vaidyanathan.Finally, I would like to mention an interesting project (from an open science perspective) that I recently ran across - an open source software by Mark Madsen for a lab notebook static site, which is based on GitHub Pages and Jekyll, but also supports pandoc, R, RMarkdown and knitr.;;;
5479;1;2015-04-08T07:17:40.713;LFR as synthesized data for dynamic community detection methods;"In recent methods for community detection in dynamic networks, LFR benchmark is used as dynamic dataset generator, but I thought it is for static community based data generation. For example in paper ""Overlapping Communities in Dynamic Networks: Their Detection and Mobile Applications"" [http://www.cise.ufl.edu/~mythai/files/mobicom_CS.pdf]LFR is used. But I don't know how this dataset is generated. Here [http://pages.towson.edu/npnguyen/Databases/AFOCS.zip] is the code of this paper and ground truth included is only for one snapshot.Thanks";[education, open-source];27;
5480;2;2015-04-08T07:20:29.317;;In his book Data Smart, John Foreman solves common data science problems (clustering, naive bayes, ensemble methods,...) using Excel. Indeed it's always good to have some knowledge of Python or R but I guess Excel can still get most of the job done !;;;
5481;1;2015-04-08T16:51:01.880;Data store for testing data products?;"Is there a recommended approach for storing processed data for testing new data products?  Basically, I'd like to have a system where a data scientist or an analyst could think of a new data product to present to users, do the data processing to create it, and then put it in a data store that our application can then access easily.What I'm not sure about is what kind of data store would be good for this type of ""testing"" use case.  Since it would need to be flexible enough to handle different types of data products, like aggregates, windowed data, etc.  And ideally it wouldn't require a huge instrumentation process to try out new things.";[education, open-source];52;
5483;2;2015-04-08T19:21:29.893;;Try looking at Git Large File Storage (LFS). It is new, but might be the thing worth looking at.As I see, a discussion on Hacker News mentions a few other ways to deal with large files:git-annex (and e.g. using it with Amazon S3)Mercurual Largefiles extension;;;
5485;1;2015-04-09T08:24:04.600;Correlations - Get values in the way we want;"I have :a matrix X with N linesa vector YI've computed the Euclidean distance with Y for each line of X.What I get is a vector of distances.What I want is a vector of scores between 0 and 1, 1 meaning ""very"" high correlation, 0 meaning ""no"" correlation.Here what I did :I divided the vector of distances by the max distance inside it.I get vector D. 1 - D is the final result with values between 0 and 1.The problem is that I get many values (75%) too close to 1.Do you think what I did is correct ?How would you get a better result ?(Between 0 and 1 but not everything too close to 1)For now, I tried to take the square of the result. (To stay between 0 and 1 but to minimize the values)Here a picture of the distance values I want to turn in a score";[education, open-source];81;
5488;2;2015-04-09T12:21:08.427;;As Andre Holzner has said, extending R with C/C++ extension is a very good way to take advantage of the best of both sides. Also you can try the inverse , working with C++ and ocasionally calling function of R with the RInside package  o R. Here you can find howhttp://cran.r-project.org/web/packages/RInside/index.htmlhttp://dirk.eddelbuettel.com/code/rinside.htmlOnce you're working in C++ you have many libraries , many of them built up for specific problems, other more generalhttp://www.shogun-toolbox.org/page/features/http://image.diku.dk/shark/sphinx_pages/build/html/index.htmlhttp://mlpack.org/;;;
5489;1;2015-04-09T14:27:36.663;Extracting model equation and other data from 'glm' function in R;"I've made a logistic regression to combine two independent variables in R, using pROC package and I obtain this: summary(fit)Call: glm(formula = Case ~ X + Y, family = ""binomial"", data = data)Deviance Residuals:   Min       1Q     Median     3Q      Max  -1.5751  -0.8277  -0.6095   1.0701   2.3080  Coefficients:             Estimate  Std. Error z value Pr(>|z|)    (Intercept) -0.153731   0.538511  -0.285 0.775281    X           -0.048843   0.012856  -3.799 0.000145 ***Y            0.028364   0.009077   3.125 0.001780 ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)Null deviance: 287.44  on 241  degrees of freedomResidual deviance: 260.34  on 239  degrees of freedomAIC: 266.34Number of Fisher Scoring iterations: 4>     fitCall:  glm(formula = Case ~ X + Y, family = ""binomial"", data = data)Coefficients:  (Intercept)       X            Y     -0.15373     -0.04884      0.02836  Degrees of Freedom: 241 Total (i.e. Null);  239 ResidualNull Deviance:      287.4 Residual Deviance:  260.3        AIC: 266.3Now I need to extract some information from this data and I'm not sure about how to do it. First, I need the model equation: suppose that fit is a combined predictor called CP; could it be CP=-0.15-0.05X+0.03Y?Then, the resulting combined predictor from the regression should present a median value, so that I can compare median from the two groups Case and Controls which I used to make the regression (in other words, my X and Y variables are N-dimensional with N = N1+N2, where N1 = Number of Controls, for which Case=0, and N2 = Number of Cases, for which Case=1).";[education, open-source];180;
5490;2;2015-04-09T15:27:53.033;;"In order to extract some data from the fitted glm model object, you need to figure out where that data resides (use documentation and str() for that). Some data might be available from the summary.glm object, while more detailed data is available from the glm object itself. For extracting model parameters, you can use coef() function or direct access to the structure.UPDATE:From Princeton's* introduction to R course's website, GLM section - see for details & examples: The functions that can be used to extract results from the fit  include- 'residuals' or 'resid', for the deviance residuals- 'fitted' or 'fitted.values', for the fitted values (estimated probabilities)- 'predict', for the linear predictor (estimated logits)- 'coef' or 'coefficients', for the coefficients, and- 'deviance', for the deviance.  Some of these functions have optional arguments; for example, you can  extract five different types of residuals, called ""deviance"",  ""pearson"", ""response"" (response - fitted value), ""working"" (the  working dependent variable in the IRLS algorithm - linear predictor),  and ""partial"" (a matrix of working residuals formed by omitting each  term in the model). You specify the one you want using the type  argument, for example residuals(lrfit,type=""pearson"").*) More accurately, this website is by Germán Rodríguez from Princeton University.";;;
5491;2;2015-04-10T08:37:18.263;;I haven't used them but there was a similar discussion in a finance groupdata repository softwaresuggestions scidb, zfs, http://www.urbackup.org/;;;
5492;1;2015-04-10T12:12:55.500;Predictive Model for Customer Payment Pattern;I am working for a logistics firm and there are approx. 750+ customers who avail our services. I am in the process of building and generating some insight for the business based on the payments made by these customers for the last 1 year. Some make payment on time whereas some are late & some are extremely late. Could you please advice which modeling technique or statistical approach would be best in this case. I can think of creating clusters based on the payment history and highlights & placing all defaulters in one cluster where our company can focus. PLease suggest. ;[education, open-source];57;1
5493;1;2015-04-10T12:42:13.517;Post processing with Random Forest;"I understand that by filtering out the instances with labels that Random Forest trees are uncertain upon with their decisions, and model these with another classifier could give a better overall result. My question is, how can I ""combine"" two (or more) classifiers' classification on a single unlabeled dataset?";[education, open-source];83;2
5496;2;2015-04-10T17:31:30.327;;Here are some things that come to mind: 1) ARMA/ARIMA models: These may show if there is any seasonal trends in their payment history.2) Logistic Regression: Will allow you model the conditional probability of a missed payments based off of number of previous missed payments. From here you need to determine what the threshold for risk is since that's a little more subjective.;;;
5501;1;2015-04-11T15:14:18.350;Finding AUC using scoreKDD for KDD Cup 2012 track2 data;"I'm trying to apply classification algorithms to KDD Cup 2012 track2 data http://www.kddcup2012.org/c/kddcup2012-track2The training data is 10 GB. As I'm working on my local Ubuntu 14.04 system with 4GB RAM, loading it is not possible. So I'm using VowpalWabbit(it being out-of -core). I've obtained the predictions for linear regression using this data. To find AUC, they have provided scorekdd.py : http://www.kddcup2012.org/c/kddcup2012-track2/dataBut execution of scorekdd.py on this complete data gives message ""process killed"", maybe because python tries to load this data into primary memory.scorekdd.py expects the training data file as first input and predictions as second input file. So as first input file to scorekdd.py I gave only the first 2 columns(scorekdd uses only those 2). This input file and the prediction file are each around 1GB.Execution of these in scorekdd took place for about 8 hours and then laptop screen went dark.I think still some process is taking place since the laptop is heated up. The process has been taking place for about 13 hours for now. Can I conclude that it is not possible to run scorekdd on this data on my local machine ?I had tried using perf http://osmot.cs.cornell.edu/kddcup/software.html for finding AUC but when I used it on a subset of the data it gives ""Aborting.  Exceeded 500000 items.""Are there better ways to find AUC ? Or does anyone know that KDDCup expects us to use just this scorekdd.py ?";[education, open-source];25;
5502;1;2015-04-11T17:45:26.600;Possibility of working on KDDCup data in local system;I'm trying to apply classification algorithms to KDD Cup 2012 track2 data using Rhttp://www.kddcup2012.org/c/kddcup2012-track2It seems not possible to work with this 10GB training data on my local system with 4GB RAM.Can anyone work on this data using this kind of a local system ? Or is using a cluster the norm ?It would be great if anyone could provide me with any guidance on how to get started with working on a cluster and the normally used type of cluster for such tasks;[education, open-source];71;
5503;1;2015-04-11T21:35:14.517;Neural Network Golf: smallest network for a certain level of performance;I am interested in any data, publications, etc about what is the smallest neural network that can achieve a certain level of classification performance.  By small I mean few parameters, not few arithmetic operations (=fast).  I am interested primarily in convolutional neural networks for vision applications, using something simple like CIFAR-10 without augmentation as the benchmark.  Top-performing networks on CIFAR in recent years have had anywhere between 100 million and 0.7 million parameters (!!), so clearly small size is not (always) a bad thing.  Small networks are also in general faster to train and overfit less.  Moreover, recent work on Knowledge Distillation, FitNets, etc show ways of making smaller networks from large networks while preserving most of the performance.Another question is, what is the best performance achievable with a network no larger than a fixed size?Examples of especially small networks that get good performance (100k parameters with 10% on CIFAR, anyone?) or systematic studies of the size vs performance tradeoff would be appreciated.;[education, open-source];74;1
5504;1;2015-04-12T02:30:05.370;Data mining- Clustering techniques;"I have a project for comparison between clustering techniques using the data set of SSA for birth names from 1910-2013 years for the different states.I have finished applying my clustering techniques on my data set and the output of the clusters were the clusters of the states for each year.Now I can know from my results; which states are close to each other with the birth names and which were not. By looking I would like to have interesting results to make my project report interesting, Which states are similar in birth names are not enough to make the read be excited about my project. My questions:any ideas of what can be learned from my project?How can I show the comparison between the clustering techniques? anyway other than seeing how Homogeneous the clusters are?";[education, open-source];102;
5505;2;2015-04-12T05:23:13.703;;I think that you have, at least, the following major options for your data analysis scenario:Use big data-enabling R packages on your local system. You can find most of them via the corresponding CRAN Task View that I reference in this answer (see point #3).Use the same packages on a public cloud infrastructure, such as Amazon Web Services (AWS) EC2. If your analysis is non-critical and tolerant to potential restarts, consider using AWS Spot Instances, as their pricing allows for significant financial savings.Use the above mention public cloud option with R standard platform, but on more powerful instances (for example, on AWS you can opt for memory-optimized EC2 instances or general purpose on-demand instances with more memory).In some cases, it is possible to tune a local system (or a cloud on-demand instance) to enable R to work with big(ger) data sets. For some help in this regard, see my relevant answer.For both above-mentioned cloud (AWS) options, you can find more convenient to use R-focused pre-built VM images. See my relevant answer for details. You may also find useful this excellent comprehensive list of big data frameworks.;;;
5506;2;2015-04-12T15:36:25.557;;It's common to perform clustering on certain features, and then use characteristics such as mean differences between other features (not used in clustering) to show how clusters differ.  It's not always meaningful to say that clusters differ merely by lower distances between cluster centers and objects assigned to them (as in k-means).As an example, you might acquire other features (variables) for the states and then average each feature among objects assigned to each cluster.  Then show bar charts of averages for those variables among all the clusters to assist in explaining how the clusters differ.  You might also test for significantly different means of the supplemental features across clusters. ;;;
5507;2;2015-04-12T16:01:40.447;;Several kernel functions can serve as similarity functions (=scores). See a list, for example, here. You can try several of them and see which suits you the best.You need something that drops fast at low distances. You can try$$ score = 1/(1+distance)^2$$and adjust coefficient in front of distance so that the score fits between 0 and 1 About your picture: what are axis labels? and what are x-ticks?;;;
5508;2;2015-04-12T16:03:49.887;;"Regarding the approach, SVM with an RBF kernel does a good job, but SVMs can be slowed down by large object sizes, unless you are employing CV with e.g. one tenth of the data randomly assigned to each fold.  However, did you ask yourself why you are employing SVMs in the first place?  Have you tried multivariate linear regression, $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}$, where each record of $\mathbf{Y}$ is coded $y_{ij}=+1$ if the $i$th object is in class $j$, and $y_{ij}=-1$ otherwise?  If the classification accuracy is appreciably high using linear regression, then your data are linearly separable, and more complex methods such as SVMs and ANNs aren't needed.  Step 2 would be to show that k-nearest neighbor, naive Bayes, linear (Fisher) discriminant analysis, polytomous logistic regression, etc., break down and fail. For terminology, you might couch the issue of having more class weights in the context of ""lower proportions of objects in certain classes,"" or ""near-zero class size.""  Skew tends to be used for describing the distribution of a feature's values, as in skewness, fat tails, etc.     How many features do you have?  Did you try unsupervised clustering (class discovery) on the 100,000 objects before trying supervised classification (class prediction) with SVM?  Maybe the 100,000 objects can be grouped into fewer classes than 50, for which the new class membership could be used as the target class during classification analysis.  This may alleviate the problem of having near-zero class size.";;;
5509;2;2015-04-12T16:08:13.467;;Probably kNN and naive Bayes classifier.  kNN is very fast, but NBC can break down a lot.   Linear regression is also one-step solution that does not involve gradient-based learning, so that might help.  LDA is your next choice. ;;;
5510;2;2015-04-12T16:12:39.063;;Wouldn't recommend use of complex methods first.  Use faster simple approaches initially (kNN, NBC, etc.), then progress through linear regression, logistic regression, LDA, CART(RF), KREG, and then to least squares SVM, gradient ascent SVM, ANNs, and then metaheurustics (greedy heuristic hill climbing with GAs, swarm intelligence, ant colony optimization, etc.);;;
5511;2;2015-04-12T16:24:28.790;;There are three approaches you could take: APP, OAA, AAO.  APP is discrimination between objects based on all possible pairs of classes.  OAA is use of one-against all (remaining classes), and AAO is all at once such as use of an F-test for multiple classes simultaneously (or Hotelling's test for MVN).  APP and AAO are actually multiple binary classification runs but with more than two of your original classes.  Each of these approaches yields different results for various classifiers employed.Random sampling is a good technique.   You might also try to cluster all of the objects into centers using k-means, and then use the centers as new objects.  Either way, linear and non-linear dimension reduction methods might help get away from the large sample size.     ;;;
5512;2;2015-04-12T21:54:11.407;;"Here's a solution using the caret package for R. A Random Forest is first trained on the data. All observations for which the probability (from the voting) is less than 99% are then passed to model 2, linear discriminant analysis. Only the probabilities from unseen resampling observations are used, since the Random Forest will otherwise fit the training data perfectly. That is what caret is needed for.The accuracy is a little higher for the uncertain cases, but this is probably overfitting since I have tried several different models and the data set is small.I'd like to know if this really improves the performance in your application for the out of sample test data. Are there any papers that recommend this approach? This approach seems to resemble boosting. I tried it on some of my data but could not improve the out of sample performance I got from model 1 (a Random Forest).data(iris)library(caret)myTrainControl <- trainControl(method = ""cv"", number = 10,                               savePredictions = T,                               classProbs = TRUE)set.seed(4213) # To get the same resamples every timeM1 <- train(y = iris$Species,            x = iris[, !(names(iris) == ""Species"")],            method = ""rf"",            trControl = myTrainControl)M1pred <- M1$pred[M1$pred$mtry == 2, ]confusionMatrix(M1pred$pred, M1pred$obs)# Accuracy 96%# Inspect the predicted class probabilities:probs <- cbind(M1pred$setosa,                   M1pred$versicolor,               M1pred$virginica)colnames(probs) <- c(""setosa"", ""versicolor"", ""virginica"")maxCol <- max.col(probs)probs <- cbind(probs, maxCol)maxProbs <- apply(probs, 1, function(x) x[x[""maxCol""]])summary(maxProbs)# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.# 0.5120  0.9805  0.9980  0.9645  1.0000  1.0000# Let's define anything below 99% as uncertain:uncertain <- which(maxProbs < 0.99)length(uncertain) # 45# Find rows in iris data that belong to 'uncertain' cases:uncertainIndex <- M1pred$rowIndex[uncertain]    confusionMatrix(M1pred$pred[uncertain],                reference = M1pred$obs[uncertain])# M1 Performance in uncertain cases:#              Reference# Prediction   setosa versicolor virginica# setosa          5          0         0# versicolor      0         17         3# virginica       0          3        17## Overall Statistics# Accuracy : 0.8667# Train new model on uncertain data only:irisUncertain <- iris[uncertainIndex, ]set.seed(4213) # To get the same resamples every timeM2 <- train(y = irisUncertain$Species,            x = irisUncertain[, !(names(irisUncertain) == ""Species"")],            method = ""lda"",            trControl = myTrainControl)M2pred <- M2$predconfusionMatrix(M2pred$pred, reference = M2pred$obs)#              Reference# Prediction   setosa versicolor virginica# setosa          4          0         0# versicolor      1         16         0# virginica       0          4        20# # Overall Statistics# Accuracy : 0.8889 # (Small discrepancy: Why does caret report an accuracy of 88,5% for M2?)# For new data predictions can be made as follows:# (just as an example from the original data again)# Some 'uncertain' cases are 'certain' now using the full M2 modelnewdat <- irisUncertain[30:34, -5]M1maxProbs <- apply(predict(M1, newdat, type = ""prob""), 1, max)ifelse(M1maxProbs < 0.99,       paste(""M2:"", predict(M2, newdat)),       paste(""M1:"", predict(M1, newdat)))# 85               94              107              126 # ""M2: versicolor"" ""M1: versicolor""  ""M2: virginica""  ""M1: virginica"" # 71 # ""M2: virginica""  ";;;
5513;2;2015-04-13T00:22:32.793;;You might try Azure Table Storage.  Since you can't lock yourself down to a specific schema (since one data product might be aggregates whereas another might be time series or something else), Azure Table storage would give you the flexibility of storing data from multiple sources, each having their own format.This would also lend itself to making a system highly scalable, as you could use Azure Service Bus in conjunction with Azure Table Storage.You might check out this tutorial at Pluralsight, Applied Windows Azure, as it shows a number of examples, one using Table Storage and Service Bus, another using Hadoop, and I suspect that some of these might match the extensibility you are looking for.;;;
5514;1;2015-04-13T08:45:07.773;SAS PROC means (two variants together);"PROC means data=d mean; var a;class b; var a;run; I want to perform the ""PROC means"" for continuous ""var a"":1) in general and2) by classes.But it performed by the classes only.How to make procedure for ""var a"" here in general too?P.S. SAS WARNING: Analysis variable ""a"" was defined in a previous statement, duplicate definition will be ignored.";[education, open-source];19;
5515;2;2015-04-13T08:59:00.397;;"Just run PROC means data=d mean; var a; run;for overall data";;;
5516;1;2015-04-13T10:43:43.617;SAS PROC anova (how do better?);"PROC anova data=a;class b; model var1=b; run;And what to do if I have a few continuous variable: var1, var2, var3.";[education, open-source];9;
5517;2;2015-04-13T10:52:02.360;;"PROC means data=d mean; var a; run;PROC means data=d mean; class b; var a; run;I knew that can be write so separately!But I thought, it is can be write somehow together!";;;
5519;1;2015-04-13T16:23:14.553;about predict the class for a new datapoint;I have a new data point and want to classify it into the existing classes.I can calculate pairwise distance for the new point to all existing points(in the existing classes). I know using KNN would be a straightforward to classify this point. Is there a way I could randomly sampling existing classes and then correlated the new point to a potential classes without calculating all pairwise distances? ;[education, open-source];52;
5521;2;2015-04-13T23:54:08.857;;Since you have your clusters set up already you should be able to calculate cluster centroids and then determine the distance between a each cluster centroid and the new data point. You could follow the same process with random samples of each cluster. ;;;
5524;1;2015-04-14T07:17:08.857;Understanding of minbucket function in CART model using R;"Assume the training data is fruit, which I am going to use for prediction in a CART model in R:fruit = data.frame(color=c(""red"", ""red"", ""red"", ""yellow"", ""yellow"",                           ""orange"", ""green"", ""pink"", ""red"", ""red""),                   isApple=c(TRUE, TRUE, FALSE, FALSE, FALSE,                             FALSE, FALSE, FALSE, TRUE, FALSE))mod = rpart(isApple ~ color, data=fruit, method=""class"", minbucket=1)prp(mod)Could anyone explain what is exactly the role of minbucket in plotting CART tree for this example if we are going to use minbucket = 2, 3, 4, 5?Say fruit is my data frame, I'm finding whether the outcome is apple or not? I have 5 red apples (4 TRUE, 1 FALSE), one FALSE value is tomato here, so what ever is red need not be an apple. But if I give minbucket=5 or 4 here, there is no split at all. Only for minbucket 1 to 3 there is a split beyond 3 there is no split. But I have more than 3 observation in my leaf node.";[education, open-source];46;
5525;2;2015-04-14T08:13:07.223;;Geop Solutions write blogs related to bigdata and CRM on weekly basis. You can read their blog at https://geopsolutions.wordpress.com/;;;
5526;2;2015-04-14T15:50:49.573;;Reading the documentation:minbucket: the minimum number of observations in any terminal ‘<leaf>’          node.  If only one of ‘minbucket’ or ‘minsplit’ is specified,          the code either sets ‘minsplit’ to ‘minbucket*3’ or          ‘minbucket’ to ‘minsplit/3’, as appropriate.So if you set minbucket to 4, then minsplit is 12, and you've only got 10 observations so it refuses to split. Because:minsplit: the minimum number of observations that must exist in a node          in order for a split to be attempted.Its all there in the documentation.;;;
5527;2;2015-04-14T17:17:57.647;;I think you need to take a step back and figure out what you're trying to do at a higher level.How were the existing classes built? If they were built by clustering unlabeled data, then with this new data point you're continuing with the clustering process.If the existing classes are labeled data, then k-NN is one possible classification method, and there are plenty more (decision trees, naive bayes, neural networks, etc.). If you're doing clustering, then there are several ways of assigning a point to a cluster, among measuring the distances from the point to cluster centroids is one. There's also single-linkage (distance is min of distances from point to points of cluster), complete-linkage (max of distances). These different methods will give clusters with different shapes and there's no universally best approach. You could test them with points that are already in clusters... but then if you're certain of what classes they re in, then you have a classification problem.So... if it's classification, then you can use k-NN, that's similar to the idea of assigning a point to a cluster according to distance. But it's not defined as finding the nearest cluster, it's defined as finding the classes of the k nearest points, then applying a vote or something. 1-NN is basically like single-linkage clustering. kNN does require finding the most similar (training) data points to your new data point. Sampling is definitely sub-optimal, but it may be good enough if you classes are well separated. If the cost of calculating distances is high, then one way of reducing the cost of calculation is the idea of skyline clustering: use a cheap distance metric to determine a subset of points that are likely to be among the k nearest neighbours, then compute these neighbours using the more expensive distance metric.Finally, if you will be classifying many points and not updating your model, it may be worth training a model (e.g. a decision tree) on the existing classes.;;;
5528;1;2015-04-14T20:44:56.423;Complete link clustering;I'm conjecturing that with Complete-linkage clustering two elements from the same cluster will always be closer to each other some other element from another cluster.In more formal terms:Let $C$ be a clustering. $\not\exists z \in C_j$ s.t. $\bigtriangleup(x, z) < \bigtriangleup(x, y)$ where $x,y \in C_i$, $C_i \neq C_j$ and $C_i, C_j \in C$.I haven't been able to prove the conjecture yet, thus I'm wondering whether I'm right or wrong. If this is indeed the case, I would much appreciate a sketch a proof. I'm pretty sure I can work my way from there.On a side note (not that I think it makes a difference), I'll be applying the clustering algorithm on a one-dimensinal dataset.Your input is much appreciated.;[education, open-source];34;
5529;1;2015-04-15T11:47:44.013;Pandas time series optimization problem: add year;"I have a pandas DataFrame containing a time series column. The years are shifted in the past, so that I have to add a constant number of years to every element of that column.The best way I found is to iterate through all the records and usex.replace(year=x.year + years)  # x = current element, years = years to addIt is cythonized as below, but still very slow (proofing)cdef list _addYearsToTimestamps(list elts, int years):    cdef cpdatetime x    cdef int i    for (i, x) in enumerate(elts):        try:            elts[i] = x.replace(year=x.year + years)        except Exception as e:            logError(None, ""Cannot replace year of %s - leaving value as this: %s"" % (str(x), repr(e)))    return eltsdef fixYear(data):    data.loc[:, 'timestamp'] = _addYearsToTimestamps(list(data.loc[:, 'timestamp']), REAL_YEAR-(list(data[-1:]['timestamp'])[0].year))    return dataI'm pretty sure that there is a way to change the year without iterating, by using Pandas's Timestamp features. Unfortunately, I don't find how. Could someone elaborate?";[education, open-source];167;1
5530;1;2015-04-15T13:10:08.470;Abstract data type?;I don't know, if I can ask this here, but I'm interested to know what kind of abstract data type (ADT) does Twitter use to model relations between profiles and why. I'm just starting to learn ADTs, so I would like to learn how they work in the real world as well as their applications.;[education, open-source];41;1
5531;2;2015-04-15T13:37:52.470;;"Make a pandas Timedelta object then add with the += operator:x = pandas.Timedelta(days=365)mydataframe.timestampcolumn += xSo the key is to store your time series as timestamps.  To do that, use the pandas to_datetime function:mydataframe['timestampcolumn'] = pandas.to_datetime(x['epoch'], unit='s')assuming you have your timestamps as epoch seconds in the dataframe x.  That's not a requirement of course; see the to_datetime documentation for converting other formats.";;;
5533;2;2015-04-15T18:04:28.930;;"Any platform, focused on social networking (not necessarily Twitter), at its core uses the most appropriate and natural abstract data type (ADT) for such domain - a graph data structure.If you use Python, you can check nice NetworkX package, used for ""the creation, manipulation, and study of the structure, dynamics, and functions of complex networks"". Of course, there are many other software tools for various programming languages for building, using and analyzing network structures. You might also find useful the relevant book ""Social Network Analysis for Startups: Finding connections on the social web"", which provides a nice introduction into the social network analysis (SNA) and uses the above-mentioned NetworkX software for SNA examples. P.S. I have no affiliation whatsoever with NetworkX open source project or the book's authors.";;;
5534;1;2015-04-15T23:53:13.957;How to scrape imdb webpage?;I am trying to learn web scraping using Python by myself as part of an effort to learn data analysis. I am trying to scrape imdb webpage whose url is the following: http://www.imdb.com/search/title?sort=num_votes,desc&start=1&title_type=feature&year=1950,2012 I am using BeautifulSoup module. Following is the code I am using:r = requests.get(url) # where url is the above url    bs = BeautifulSoup(r.text)for movie in bs.findAll('td','title'):    title = movie.find('a').contents[0]    genres = movie.find('span','genre').findAll('a')    genres = [g.contents[0] for g in genres]    runtime = movie.find('span','runtime').contents[0]    year = movie.find('span','year_type').contents[0]    print title, genres,runtime, rating, yearI am getting the following outputs:The Shawshank Redemption [u'Crime', u'Drama'] 142 mins. (1994)Using this code, I could scrape title, genre, runtime,and year but I couldn't scrape the imdb movie id,nor the rating. After inspecting the elements (in chrome browser), I am not being able to find a pattern which will let me use similar code as above.Can anybody help me write the piece of code that will let me scrape the movie id and ratings ? ;[education, open-source];402;
5535;2;2015-04-16T07:38:38.360;;"Adapted from Pete's answer, here's an implementation of the solution, and the demonstration.#!/usr/bin/env python3import randomimport pandasimport timeimport datetimedef getRandomDates(n):    tsMin = time.mktime(time.strptime(""1980-01-01 00:00:00"", ""%Y-%m-%d %H:%M:%S""))    tsMax = time.mktime(time.strptime(""2005-12-31 23:59:59"", ""%Y-%m-%d %H:%M:%S""))    return pandas.Series([datetime.datetime.fromtimestamp(tsMin + random.random() * (tsMax - tsMin)) for x in range(0, n)])def setMaxYear(tss, target):    maxYearBefore = tss.max().to_datetime().year    # timedelta cannot be given in years, so we compute the number of days to add in the next line    deltaDays = (datetime.date(target, 1, 1) - datetime.date(maxYearBefore, 1, 1)).days    return tss + pandas.Timedelta(days=deltaDays)data = pandas.DataFrame({'t1': getRandomDates(1000)})data['t2'] = setMaxYear(data['t1'], 2015)data['delta'] = data['t2'] - data['t1']print(data)print(""delta min: %s"" % str(min(data['delta'])))print(""delta max: %s"" % str(max(data['delta'])))";;;
5536;2;2015-04-16T10:07:48.187;;"You can get everything from div with class=""rating rating-list""All you need to do is retrive attribute id: [id=""tt1345836|imdb|8.5|8.5|advsearch""]When you have this content, you split this string by '|', and you get:1. parameter: movie id3. parameter: movie score";;;
5537;1;2015-04-16T10:12:56.220;Finding predictions using biglm without finding errors;I'm using the biglm R package for linear regression. click/impression is the output required. But the test data does not contain click and impression. The predict function of biglm gives the error Error in eval(expr, envir, enclos) : object 'click' not foundI assume this is because predict tries to compute the standard errors also.Is there a method to just obtain the predictions ? I tried assigning values to se.fit and type attributes, but I get the same error. ;[education, open-source];38;
5538;2;2015-04-16T14:34:23.550;;"biglm calls model.frame which ""all the variables in the formula are included in the data frame"" see documentation for model.frame. This is the issue that comes up when predict is called on the biglm class. It looks for those values in the predict function. To get around this you can just create a variable and encode it with 0. See below...data(trees)ff<-log(Volume)~log(Girth)+log(Height)chunk1<-trees[1:10,]chunk2<-trees[11:20,]chunk3<-trees[21:31,]a <- biglm(ff,chunk1)summary(a)#produces same errorchunk2 <- select(chunk2, -Volume)predict(a, chunk2)#Fixedchunk2$Volume <- 0predict(a, chunk2)";;;
5539;2;2015-04-16T15:57:01.270;;I have been able to figure out a solution. I thought of posting just in case if it is of any help to anyone or if somebody wants to suggest something different. All suggestions are welcome. Thanks.bs = BeautifulSoup(r.text)for movie in bs.findAll('td','title'):    title = movie.find('a').contents[0]    genres = movie.find('span','genre').findAll('a')    genres = [g.contents[0] for g in genres]    runtime = movie.find('span','runtime').contents[0]    rating = movie.find('span','value').contents[0]    year = movie.find('span','year_type').contents[0]    imdbID = movie.find('span','rating-cancel').a['href'].split('/')[2]    print title, genres,runtime, rating, year, imdvidThe output looks like this:The Shawshank Redemption [u'Crime', u'Drama'] 142 mins. 9.3 (1994) tt0111161;;;
5541;2;2015-04-16T19:02:45.040;;"The Wikipedia entry for Receiver operating characteristic references this paper for the Gini=2AUROC-1 result: Hand, David J.; and Till, Robert J. (2001); A simple generalization of the area under the ROC curve for multiple class classification problems, Machine Learning, 45, 171–186.  But I'm afraid I don't have easy access to it to see how close it comes to what you want.";;;
5542;2;2015-04-16T19:24:55.447;;As explained in the comments by @Emre, there is a python script on github which you can use to set up a cluster of whatever size you like: Spark Cluster on Google Compute Engine | Ido Green.  It allows you to specify the instance type to use for the workers (slaves) and separately what to use for the master.Note that it uses gcutil, which is deprecated in favor of gcloud compute;;;
5543;1;2015-04-16T21:11:14.103;R lm(log(y)~x,data) models and predict, need to remember the exp. R2 differences;"I noticed a difference between R and Excel when it comes to trend lines.  Basically I am automating a simple process that someone does.  He/she takes web traffic and finds the best trend line by looking at the R squared to predict what the web traffic will be a few months from now to make sure we will have the servers to handle it.In R, one can do: lm(log(y)~x,data) but you must ""remember"" to do exp(predict(...)).  Is there a better predict function that remembers the left hand operations?Also the R-squared reported for log(y)~x is the r squared for x vs that y's log.  The R-squared against the original y was better.  Is there a nifty function that calculates that?As you can see from this output, what summary reports as the rsquared is not ""correct"" while my rsq data accurately shows the best rmse. (See source code below.)[1] ""rmse is 305221.416535481 rsq is 0.749525713697497 summary$r.squared 0.705953722113025""[1] ""rmse is 304961.752311906 rsq is 0.749815047530537 summary$r.squared 0.706163612870978""[1] ""rmse is 318083.254498832 rsq is 0.723564406971294 summary$r.squared 0.723564406971402""[1] ""rmse is 317352.614485029 rsq is 0.724832898371528 summary$r.squared 0.724832898371531""I want to learn the best way to do this in R...rmse=function(x,y){  return( sqrt(sum((x-y)^2)/length(x)));}rsq=function(actual,pred){      return( cor(actual,pred,method=c(""pearson""))^2);}findBestFormula=function(data,yvar,xvar){  #This figures out the best Excel-like trend function based on lowest rmse  data$y=data[,yvar];  data$x=data[,xvar];    models=list(lm(log(y)~log(x),data),              lm(log(y)~x,data),             lm(y~log(x),data),              lm(y~x,data));  preds=list(exp(predict(models[[1]],data)),    exp(predict(models[[2]],data)),    (predict(models[[3]],data)),    (predict(models[[4]],data)));  rs=c();  for(i in (1:4)){    #say(names(models[[i]]))    rs[i]=rmse(data$y,preds[[i]]);    print(paste(""rmse is"",rs[i],""rsq is"",rsq(data$y,preds[[i]]),                ""summary$r.squared"",summary(models[[i]])$r.squared));  }  best=min(rs)  #say(best);  for(i in (1:4)){    if(best==rs[i]){      return(list(model=models[[i]],pred=preds[[i]],modelNum=i,rmse=best));      }  }}summary(findBestFormula(weekDayDf,'daily','d')$model)Any tips on niftier code to write the above?";[education, open-source];59;
5544;1;2015-04-16T23:09:45.397;Difference between OLS(statsmodel) and Scikit Linear Regression;"I have a question about two different methods from different libraries which seems doing same job. I am trying to make linear regression model.Here is the code which I using statsmodel library with OLS :X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.3, random_state=1)x_train = sm.add_constant(X_train)model = sm.OLS(y_train, x_train)results = model.fit()print ""GFT + Wiki / GT  R-squared"", results.rsquaredThis print out GFT + Wiki / GT  R-squared 0.981434611923and the second one is scikit learn library Linear model method:model = LinearRegression()model.fit(X_train, y_train)predictions = model.predict(X_test)print 'GFT + Wiki / GT R-squared: %.4f' % model.score(X_test, y_test)This print out GFT + Wiki / GT R-squared: 0.8543So my question is the both method prints our R^2 result but one is print out 0.98 and the other one is 0.85. From my understanding, OLS works with training dataset. So my questions, Is there a way that work with test data set with OLS ?Is the traning data set score gives us any meaning(In OLS we didn't use test data set)? From my past knowledge we have to work with test data.What is the difference between OLS and scikit linear regression. Which one we use for calculating the score of the model ? Thanks for any help.";[education, open-source];64;
5545;1;2015-04-17T13:48:39.730;Big Data Analytics with Microsoft Azure;I am studying Microsoft Azure Machine Learning Studio with HDInsights for big data analytics. Having been worked with R and Python in the field, ML Studio seems a bit rudimentary at first glance (similar to Rapidminer, but quite less elaborated). The only real advantage seems to be the flawless (?) cooperation with Azure's HDInsights Hadoop version (and other Microsoft cloud services), which is comfortable.Is it worth the effort to learn? ML Studio processes could be extended by R scripts, as far as I see (but how far?), maybe this is a good way to combine the two.Has anybody deeper experience with Azure for machine learning? Perhaps any better alternative?;[education, open-source];51;
5547;1;2015-04-17T18:56:36.563;Theano/Lasagne/Nolearn Neural Network Image Input;"I am working on image classification tasks and decided to use Lasagne + Nolearn for neural networks prototype.All standard examples like MNIST numbers classification run well, but problems appear when I try to work with my own images.I want to use 3-channel images, not grayscale.And there is the code where I'm trying to get arrays from images: img = Image.open(item) img = ImageOps.fit(img, (256, 256), Image.ANTIALIAS) img = np.asarray(img, dtype = 'float64') / 255. img = img.transpose(2,0,1).reshape(3, 256, 256)    X.append(img)Here is the code of NN and its fitting:X, y = simple_load(""new"")X = np.array(X)y = np.array(y)net1 = NeuralNet(    layers=[  # three layers: one hidden layer        ('input', layers.InputLayer),        ('hidden', layers.DenseLayer),        ('output', layers.DenseLayer),        ],    # layer parameters:    input_shape=(None, 65536),  # 96x96 input pixels per batch    hidden_num_units=100,  # number of units in hidden layer    output_nonlinearity=None,  # output layer uses identity function    output_num_units=len(y),  # 30 target values    # optimization method:    update=nesterov_momentum,    update_learning_rate=0.01,    update_momentum=0.9,    regression=True,  # flag to indicate we're dealing with regression problem       max_epochs=400,  # we want to train this many epochs        verbose=1,        )  net1.fit(X, y)I recieve exceptions like this one:Traceback (most recent call last):  File ""las_mnist.py"", line 39, in <module>    net1.fit(X[i], y[i])  File ""/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py"", line 266, in fit    self.train_loop(X, y)  File ""/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py"", line 273, in train_loop    X, y, self.eval_size)  File ""/usr/local/lib/python2.7/dist-packages/nolearn/lasagne.py"", line 377, in train_test_split    kf = KFold(y.shape[0], round(1. / eval_size))IndexError: tuple index out of rangeSo, in which format do you ""feed"" your networks with image data?Thanks for answers or any tips!";[education, open-source];454;
5548;1;2015-04-17T19:03:34.930;Algorithms to find average distance between data maxima;I'm having trouble figuring out a way to analyze some simple data. When graphed, the data I have make a somewhat sinusoidal curve. What I want to do is to find the x-values of the maximum peaks of the sinusoidal curve. I then want to subtract each of these x-values from the last peak found and average these differences to obtain an average distance between peaks. Is there an easy way to do this with Excel, Mathematica, or MatLab (the programs I have available to me?).Thanks in advance!!!;[education, open-source];34;
5549;1;2015-04-17T19:14:49.843;What are some good sources to learn fraud/anomaly detection in normal/time-series data?;I would like to know more on fraud/anomaly detection. I am looking for good source or survey article/book etc out there which will give me some preliminary idea of the area. Any suggestion is greatly appreciated.Thanks;[education, open-source];83;
5550;2;2015-04-18T08:23:52.907;;Usually this is done as outliers analysis (fraud is an outlier vs normal usage). For this aspect, you can find more info in the data mining: concepts and techniques book, even if general purpose book.I am convinced that learning this kind of basis is needed to understanding the domain specific methods.;;;
5551;1;2015-04-18T16:40:30.900;What algorithmic approach for selecting similar, relevancy based documents;"I have an application that tracks people making mentions of various topics. We've used a Bayes algorithm to do some simple classification (users give a thumbs up/thumbs down) to pick the people that they believe are the best fit for their project. Our intention was to use this data to help us sort and order the influencers based on ""fit"" to the customer's needs.  However, we've got some trained data, and now all we can do is say are they similar to the ""thumbs up"" group, or the ""thumbs down"" group.  What algorithm should we have used for this instead? Basically, the ideal is to have a score.. and the biggest, smallest based on the trained data is the one that gets shown first. Thoughts? Even better if its in Ruby. ";[education, open-source];29;
5552;2;2015-04-18T19:45:50.017;;This sounds like an interesting project. I recently worked on an almost case study. In order to get only 3 most accurate predictions, I think you may wanna sort those correctly predicted 5 matches by probability of event (win, draw or loose) and then select the first three matches.I hope your models are able to give you probabilities of events. I hope this helps :-);;;
5553;2;2015-04-19T02:46:50.280;;I teach a Business Analytics course that includes SQL and Excel. I teach in a business school so my students aren't the most technically capable, which is why I didn't use something like R, Pandas, or Weka. That being said, Excel is a powerful enough tool to use for some data analysis. It gets most of this power from its ability to act as a front end to SQL Server Analysis Services (a component in SQL Server for data analysis) using the Data Mining Add-In.SSAS lets you construct decision trees, perform linear and logistic regressions, and even make bayesian or neural networks. I've found that using Excel as a front-end is a less threatening approach to doing these kinds of analyses since they've all used Excel before. The way to use SSAS without Excel is through a specialized version of Visual Studio and that isn't the most user friendly tool out there. When you combine it with a few other Excel tools like Power Query and Power Pivot, you're able to do some fairly sophisticated analysis of data.Full Disclosure, I'm probably not going to use it again when I teach the new version of the course next year (we're splitting it into two courses so one can focus more heavily on data analysis). But that's just because the university was able to get enough licenses for Alteryx which is even easier to use and more powerful but is $4-85k/user/year if you can't get it free somehow. Say what you will about Excel, but it beats that price point.;;;
5554;2;2015-04-19T12:17:00.210;;I also asked it in lasagne-users forum and Oliver Duerr helped me a lot with code sample:https://groups.google.com/forum/#!topic/lasagne-users/8ZA7hr2wKfM;;;
5555;1;2015-04-19T19:31:12.250;conversion rate optimization;I am working on a conversion rate optimization assignment.The way I thought about it is to use machine learning to select the landing / conversion page that offers the highest probability of conversion given the features.But the general question that I have is, besides improving conversion given the prospect profile, how much improvement can you get by focusing on other, non-machine learning areas, such as creating a better conversion funnel or designing better conversion pages in the first place?Thanks a lot;[education, open-source];20;
5557;1;2015-04-20T11:44:39.870;percentage of confidance on desion trees results;I am looking towards a solution where classification algorithms produce output with some confidence value. but I am confused whether classification algorithms are able to produce results with percentage of confidence? Thanks;[education, open-source];23;
5558;1;2015-04-20T15:18:38.840;"Finding frequencies in a noisy, ""uneven"" dataset";"I'm working on a problem where frequency analysis applies (decomposition of a signal into frequencies, that is), but it's noisy and the samples are unevenly spaced.Specifically: given a list of items purchased at a bar/restaurant, try to estimate the number of guests on the check based on distinct ""frequencies"" of purchase. The logic is that if there are N guests on a check, then it's reasonable to see N frequencies of drinks being purchased, one person buying every 10 minutes, another every 15, etc. (Plenty of other properties of the check should be included, but here I'm focusing specifically on estimating distinct frequencies).So more formally: given a noisy, unevenly spaced time series, find the smallest number of frequencies which reproduce the signal while minimizing the error (... for some sensible definition of how to minimize both the error and the number of frequencies simultaneously).This is more a machine learning problem than signal processing. I realize it's also an open question, but can anyone point me in the right direction? Is there a particular method or algorithm that applies here?";[education, open-source];39;
5559;2;2015-04-20T15:23:01.027;;The first technique that comes to mind is cubic spline interpolation, which is good at approximating sinusoidal functions without suffering from Runge's phenomenon. Here's a visual of cubic splines applied to a similar problem: Matlab has an implementation documented here.Once you have a representation for the underlying function you should be able to find the local maxima easily and take the differences you are looking for.;;;
5560;2;2015-04-20T16:09:13.293;;Some classification algorithms can indeed return a probability distribution over the considered classes (see Wikipedia on probabilistic classification).In the topic of your question you're asking about Decision Trees. Well, these have their limitations in terms of providing probability estimates (see this paper on probability estimates from decision trees).In case you would like to play around with this, it's very easy to start with scikit-learn:import numpy as npfrom sklearn.tree import DecisionTreeClassifierdtc = DecisionTreeClassifier()# training samplesX = np.array([[1, 1, 0],              [1, 1, 0],              [0, 0, 1],              [0, 0, 1]])# target values for training samplesy = np.array([0, 0, 0, 1])dtc.fit(X, y)print 'Class probabilities for training samples:'print dtc.predict_proba(X)print 'Probabilities for previously unseen samples:'for sample in ((1, 0, 1), (0, 0, 1), (1, 1, 1), (0, 0, 0)):     print 'Sample {}. Result: {}'.format(sample, dtc.predict_proba(sample))This code returns following results:Class probabilities for training samples:[[ 1.   0. ] [ 1.   0. ] [ 0.5  0.5] [ 0.5  0.5]]Probabilities for previously unseen samples:Sample (1, 0, 1). Result: [[ 0.5  0.5]]Sample (0, 0, 1). Result: [[ 0.5  0.5]]Sample (1, 1, 1). Result: [[ 0.5  0.5]]Sample (0, 0, 0). Result: [[ 1.  0.]]At this scale the results are easily interpretable:a sample of features (1, 1, 0) is classified as 0 with 100% probabilitya sample of features (0, 0, 1) is classified 50/50 as 0 or 1. etc.This brings us to an important question of how accurate is your model?;;;
5561;1;2015-04-20T17:04:42.813;Can someone explain the following error in my python code?;I am analyzing a dataset in Python for strictly learning purpose.In the code below that I wrote, I am getting some errors which I cannot get rid off. Here is the code first:plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)plt.xlabel('Year')plt.ylabel('Rating')remove_border()I am getting the following errors:1. TypeError: 'str' object is not callable2. NameError: name 'remove_border' is not definedAlso, the label='Decade Average' is not showing up in the plot.What confuses me most is the fact that in a separate  code snippet for plots (see below), I didn't get the 1st error above, although remove_border was still a problem.plt.hist(movieDF.rating, bins = 5, color = 'blue', alpha = 0.3)plt.xlabel('Rating')Any explanations of all or some of the errors would be greatly appreciated. ThanksFollowing the comments, I am posting the data and the traceback below:decade_mean is given below.year1970    8.9250001980    8.6500001990    8.6157892000    8.3789472010    8.233333Name: rating, dtype: float64traceback:TypeError                                 Traceback (most recent call last)<ipython-input-361-a6efc7e46c45> in <module>()      1 plt.plot(decade_mean.index, decade_mean.values, 'o-',color='r',lw=3,label = 'Decade Average')      2 plt.scatter(movieDF.year, movieDF.rating, color='k', alpha = 0.3, lw=2)----> 3 plt.xlabel('Year')      4 plt.ylabel('Rating')      5 remove_border()TypeError: 'str' object is not callableI have solved remove_border problem. It was a stupid mistake I made. But I couldn't figure out the problem with the 'str'.;[education, open-source];90;
5562;2;2015-04-20T17:25:20.867;;Just to point you in one possible direction: you could treat this problem as one of probabilistic mixture modeling.Imagine that each person's drink ordering is governed by a probability distribution. That distribution may be characterized by the time since their last drink order.  As time passes, the probability that they will order another drink increases until eventually they order another drink and the time resets.One possible model for the time between drink orders for a single person is the exponential distribution.  If you consider many people ordering, the resultant times would likely be a mixture of exponentials. The problem then comes down to fitting a mixture of exponentials to the table's drink data.  You would likely have to supply some additional prior data as well to get a meaningful model (otherwise it's difficult to tell the difference between 5 people ordering 2 drinks a piece or 1 person ordering 10 drinks).;;;
5563;2;2015-04-20T20:26:15.337;;" So my question is the both method prints our R^2 result but one is print out 0.98 and the other one is 0.85.There is a lot of information missing here, but it looks like the first example could be finding the correlation between the predicted and actual values of data from your training set and the second example is finding the correlation between the predicted and actual values of your testing set. This would also explain why the correlation in the second result is lower. The performance of a model on novel (testing) data should nearly always be poorer than on the training dataset that was used to fit the model. Is there a way that work with test data set with OLS ?The statsmodels documentation describes a predict() method that can be used to make predictions on inputs from a design matrix.  You should be able to use this method on your test data once you have fit the model. Is the traning data set score gives us any meaning(In OLS we didn't use test data set)? From my past knowledge we have to work with test data.Evaluation on training data can often be used as a quick check for ""best case"" generalization performance.  However, this is often only used as a diagnostic to ensure your model has fit properly and should not be taken to be a true indicator of generalization.";;;
5564;2;2015-04-21T01:47:50.237;;"Seems to me that remove_border() is not being recognized as a function. Seeing how you haven't listed a package it belongs to, did you implement this yourself? Python is guessing that remove_border is a string and you're trying to ""call it"" - treat it like a function. The 2nd error is saying the same thing, the function remove_border isn't defined anywhere that Python can find it.";;;
5565;1;2015-04-21T05:38:55.830;Using EM (Expectation Maximization) algorithm for Training Logistic Regression;Is it possible to learn the weights for a logistic regression classifier using EM (Expectation Maximization)algorithm? Is there any instance reference?Thanks;[education, open-source];58;1
5566;1;2015-04-21T05:55:07.347;How can I use Data Science to profoundly contribute to Humanity?;Undergraduate researcher here. I worked at many traditional scientific research labs, ranging from cancer biology, to radiation medicine, and to supercapacitors. I'm thinking of switching to Statistics and Computer Science from Computational Biology to join the exciting field of Data Science. What ways can I contribute to Humanity without a technical background in basic science? I don't know how I will feel about making better clickbaits. I've thought about continuing in scientific research but I will lack the science background. I've thought about working for the United Nations. What other ways can I contribute? Inspire me!;[education, open-source];165;1
5567;1;2015-04-21T07:00:39.540;Prove Reccurrent Neural Network can exhibit oscillatory behavior;I understand how recurrent neural networks work, however I'm trying to build a deep intuitive understanding of their behavior which is difficult for me because they exhibit such complex behaviors.The most difficult thing for me to understand is how a Reccurrent Neural Network can exhibit an oscillatory behavior along with the notion of exploding weights. For one, I'm guessing that oscillatory behaviors are only possible for certain activation functions and configurations. Here are my following questions:1) Can sigmoid activation functions exhibit oscillatory behavior? I've convinced myself that they don't since they have a positive range between 0 and 1 which doesn't allow for negative derivatives, but maybe I'm wrong. Is there a formal proof out there for whether it can or cannot?2) Which activation functions can exhibit oscillatory behaviors and are there proofs out there for them? I believe tanh might have this behavior but I'm not sure.3) What are exploding weight derivatives and how do they occur? There is polar opposite of exploding weight's which seems to be where the weights do not learn and stay stagnate. What causes these issues? I imagine this is dependent upon the activation function as well.Lastly, I began reading this article:http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCsQFjAB&url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FChristian_OReilly%2Fpublication%2F52004955_Permanent_oscillations_in_a_3-node_recurrent_neural_network_model%2Flinks%2F0c96052464be171d68000000.pdf&ei=X_E1Vf7ULNGsogSo9YHIBQ&usg=AFQjCNE0fex0s7hY2w_upMNkmaxIseUKww&bvm=bv.91386359,d.cGUbut got confused around page 10 because the author omit's explaining opaque assumptions such as why we are defining the matrix measure as is and so forth(it's a good paper nonetheless). Is there a more transparent paper or lecture out there that can shed light on this subject, or maybe simple explanation for what the author is trying to get at?;[education, open-source];25;
5571;2;2015-04-21T09:30:38.253;;Seems that remove border is not defined. You have to define the function before used. I do not know where the string error comes, is not clear to me. If you post the full traceback it will be clearer. Finally your label is not show because you have to call the method plt.legend();;;
5572;1;2015-04-21T09:47:05.917;Finding parameters with extreme values (classification with scikit-learn);I am currently working with the forest cover type prediction from Kaggle, using classification models with scikit-learn. My main purpose is learning about the different models, so I don't pretend to discuss about which one is better.When working with logistic regression, I wonder if I need the 'penalty' parameter (where I can choose L1 or L2 regularization). Based on what I found, these regularization terms are useful to avoid over-fitting, specially when the parameter values are extreme (by extreme I understand the range of some parameter values are very large compared to other parameters, Correct me if I am wrong. In this case, wouldn't it be enough to apply log-scale or normalization to these values?).The main questions are: as the number of parameters is large, are there visualization techniques and tools in scikit-learn which can help me to find parameters with extreme values? is there any statistical function/tool which returns how extreme the values of parameters are?;[education, open-source];40;
5573;2;2015-04-21T12:28:13.413;;"If by ""parameters"" you mean features (called ""Data Fields"" at Kaggle), then, yes, you can log-scale those. To visualize them you can just use histograms.To do it for all features in python, for example, you can put your data in pandas DataFrame (let us call it ""data"") and then use data.hist()This has nothing to do with the regularization in any model.If by ""parameters"" you mean the coefficients obtained after fitting the logistic regression, then one uses regularization. This has, however, is not directly related to log-transform. How you list/visualize your coefficients depends on the programming tool you use for logistic regression (or other model)";;;
5574;1;2015-04-21T13:18:17.537;Use of Nash-Equilibrium in big data environments;Has there been a successful implementation of Nash-Equilibrium in big data problems like suggesting a best buy in a stock market, in traffic monitoring systems or crowd control systems.All the above mentioned scenarios have competitive environments and one needs to get the best possible solution in them, which should suit well for Nash-Equilibrium cases.;[education, open-source];100;
5576;2;2015-04-21T19:31:39.673;;Check out term frequency-inverse document frequency (TF-IDF), if you're parsing large chunks of text for mentioned topics. The metric measures importance of topics bounded by how often they appear across a corpus of documents, which helps to weed out topics that may be very common, but are very common to all discussions, and therefore not very useful.It's a simple calculation, but there is a ruby gem that calculates it directly: tf-idf-similarity.Hope this helps.;;;
5577;2;2015-04-21T20:36:48.103;;There are almost too many possible uses for data science in the nonprofit sector to count. You might want to look at the question from the other way around--find a cause that interests you and see how data science would apply. For example:Disaster relief efforts need better modeling and forecasting in everything from anticipating possible disasters to optimizing relief efforts. How can we identify the population most at risk from natural or human-created disasters, what are the most effective ways to mitigate the risks, and during a disaster, what are the most effective ways to deliver relief?In education, what methods work best? In a country or region with low female literacy, what would be the economic impact of increasing female literacy by a certain percentage? For poverty relief, based on a particular area's demographics and economy, what is the most affordable way to reduce poverty? How could a funding organization recognize sooner whether an ongoing project is likely to succeed or fail?Or if you're interested as much or more in the human aspects, you could look at ways for communities or countries to build reliable data banks, getting all of the government, nonprofit, educational, and other organizations to share and use data effectively. I hope this helps!;;;
5578;2;2015-04-21T22:30:04.510;;"I'll offer a slightly different perspective. While you can help many needy humans -- e.g. individual refugees, poor people, sick people, war-torn people -- to help humanity it's necessary to address root causes: i.e. why are there so many refugees, persistent poverty, persistent (preventable) illness, and chronic violence. The root causes of so many of these are in the domain of the social sciences.So far, data science has only contributed to root cause analysis when it operates within a solid research program, integrating with other types of research and findings. (This research may or may not take place in an academic environment, or maybe in a mixture of environments.) It also places premium value on domain knowledge on the part of the data scientists involved.A related theme is to empower ""change agents"" (a.k.a. social entrepreneurs) with data science and related services.  Nearly all of them are under-resourced and often face severe or menacing opposition.  To do this, you will need to make partnerships with some of them to understand their needs and their world.";;;
5579;2;2015-04-21T22:56:21.440;;"Since I have already answered a similar question on Data Science StackExchange site, plus some related ones, I will mention all of them here and let you decide, if you find them helpful:on data science project ideas, including those with social impact;on working as a data scientist for a non-profit company;on using data science in political campaigns.";;;
5580;1;2015-04-21T23:06:28.997;How do we mine associations from sequences?;My data mining problem is a next web page prediction using the existing web data. For that I have a set of frequent sequences which are obtained using cspade algorithm in R. Now I am not sure how to mine set of associations so that I can predict the next page? can someone help?Frequent sequencies look like this: items     support    1 [{1}]  0.1640261    2 [{10}] 0.05112657    3 [{11}] 0.05818949    4 [{12}] 0.11333700    5 [{13}] 0.07773954    6 [{14}] 0.12036354    7 [{15}] 0.02950037    8 [{17}] 0.01111922    9 [{2}]  0.17708912   10 [{3}]  0.12320245   11 [{4}]  0.12297109   12 [{5}]  0.02524403   13 [{6}]  0.21933426   14 [{7}]  0.08134223   15 [{8}]  0.09659857   16 [{9}]  0.09111978   17 [{6},      {9}]  0.01086563   18 [{9},        {9}]  0.04410508   19 [{9},        {9},        {9}]  0.02321639   20 [{9},        {9},        {9},         {9}]  0.01316606   21 [{8},         {8}]  0.06783368   22 [{8},         {8},         {8}]  0.05253996   23 [{8},         {8},         {8},         {8}]  0.04431926 ;[education, open-source];27;
5581;1;2015-04-22T03:44:02.667;what can be done with the following data?;I have scraped the rating that a customer gave in the following categories namely overall rating score, value for money, seat comfort,staff service, catering and entertainment from an airline forum. I would like to know what quality information I can deduce from such ratings about an airline. I have rating of nearly 300 customers who traveled in an airline.  ;[education, open-source];87;
5582;1;2015-04-22T06:05:29.730;Basic ML based Prediction model;I had this basic query on ML and would like to get basic ideas on modelling prediction models using ML and Python.Say I have a training data of 1000 items asItem_name, Attrib_1, Attrib_2, Attrib_3,....Attrib_N, CostAnd my aim is to create a model to predict cost for a new item given the attributes.So where should I start and what are the different ways to prediction and solve this problem ? Also how to evaluate different methods ?;[education, open-source];79;
5585;1;2015-04-22T12:32:10.243;Optimizing Weka for large data sets;First of all, I hope I'm in the right StackExchange here. If not, apologies!I'm currently working with huge amounts of feature-value vectors. There are millions of these vectors (up to 20 million possibly). They contain some linguistic/syntactic features and their values are all strings.Because most classifiers do not handle string data as values, I convert them to binary frequency values, so an attribute looks like this:@attribute 'feature#value' numericAnd per row, the value is either 1 or it is absent (so note it's a sparse ARFF file). The thing is, with 250K rows, there are over 500K attributes and so, most algorithms have a hard time with this.There are a lot of algorithms. I'm really curious as to what you would consider a suitable one (preferably unsupervised, but anything works), and if you even have some ideas how I could improve performance. I can train on small subsets of data, but the results only get better when using large amounts of data (at least 7 million events). For now, I've been using NaiveBayes variations (Multinomial and also DMNBText) and those are really the only ones that are able to chew up data with acceptable speed. Thanks a lot. If you need more information, please let me know.Cheers.;[education, open-source];72;
5586;2;2015-04-22T13:35:06.473;;I think you're looking at it the wrong way. First, you need to have a question and then you may find out whether the data could potentially hold the answer to that question. Right now, you're asking us to what questions this data may hold the answer. And that is a very difficult question!But to provide you with at least some ideas:Find out what people deem important (the correlation between overall rating and the other ratings in (linear) regression analyses)Find out the relation between certain characteristics of people (age/gender/nationality, if available) and their overall rating (or whatever rating available)Basically, the relation between certain variables and their outcomes. But make sure you have a clear question before you start looking for answers.;;;
5587;2;2015-04-22T15:45:51.943;;This individual works with 'Big Data' and primarily uses Excel? Seriously?!?! Excel only handles up to 1, 048, 576 rows of data in a a single spreadsheet. For data sets beyond that it needs a plugin. Also pivot tables in Excel have severe restrictions on the analysis that can be performed using them. What types of data analytics tasks would need to be performed in the job for which you are recruiting?I suggest you conduct interviews that include tests of the sort of tasks that will need to be done in the job under consideration. Without violating confidentiality, privacy or data protection, the programming or data analysis task set as part of the interview should include a (pseudonymized) subset of a dataset relevant to the post being interviewed for. Otherwise you might end up recruiting someone that is articulate in a conversation-based interview but is not actually competent in carrying out the actual job.;;;
5588;2;2015-04-22T16:48:21.760;;"The type of ML problem you are trying to solve is a regression problem.  Essentially, given your attributes, predict a continuous variable.There are multiple ways to solve your problem, but (assuming attributes are numerical) they all boil down to ""plot your data points, then draw a line through them"".  The different methods calculate the placement of this predictor based on various different measures of error; the most basic probably being least squares, which aims to minimize the sum of squared errors.  Metacademy is a great website for learning ML related stuff online, because they provide roadmaps that tell you what you need to learn before learning ______, as well as how to learn those prerequisites.Here is a link to the metacademy page for linear regression, which is where you're going to want to start.  I'd recommend watching the coursera videos from Andrew Ng.  If you're just getting started with ML, his course is a wonderful introduction.  However, a common complaint for the coursera class is it's too shallow, and is to be considered as an overview of machine learning.  If you consider yourself a little more mathematically savvy, I hear the Stanford course (also taught by him) may be a more in depth resource for you.You should implement your own basic linear regression at first, just to get a deeper understanding of how it works.  Afterwords, you can just use a python ML library called scikit learn to perform the regression (link here).  I strongly recommend you take this approach for all ML algorithms; strive to understand at least how the algorithms work at a basic level.  It is far too easy for someone just starting to learn about ML to see all the implemented algorithms in scikit learn go on to use the algorithms as black boxes without understanding what the algorithm is actually doing.Lastly, in order to evaluate the different methods, you should split your dataset into two different parts; a training set with which to train your models on, then a testing set with which to test how accurate the predictions are.  The idea here being that your data is representative of the ""wild"" data that you'll see, so by purposely hiding some data, you can effectively simulate how the algorithm will perform in ""real world"" conditions.  Generally, people go with something along the lines of 70% of your data being used to train, and 30% of it being used to test.  Later on, as you get into complicated models with multiple parameters, it will help to split your data into 3 parts: training, used to train your model, validation, used to adjust model parameters, and finally, a test set, used to measure how accurate the model is.Good luck!";;;
5589;1;2015-04-22T18:00:27.083;Downloading a large dataset on the web directly into AWS S3;Does anyone know if it's possible to import a large dataset into Amazon S3 from a url?  Basically, I want to avoid downloading a huge file and then reuploading it to S3 through the web portal.  I just want to supply the download url to S3 and wait for them to download it to their filesystem.  It seems like an easy thing to do, but I just can't find the documentation on it.Thanks,Will;[education, open-source];64;
5590;1;2015-04-22T21:26:38.700;What is the best way to scale a numerical dataset;I have a dataset with differents attributes which don't have the same range on their values which is a problem when we need to compute distance beetween objects. After some research i found that i can do the regularisation job with this formula : (value-min)/(max-min)  where min and max are respectively the minimum and maximum value in the domain of val attribute.The question is that one, does it exist other ways ?Thank you for your help.;[education, open-source];47;
5591;2;2015-04-23T07:34:34.667;;There is pretty much mess in terminology in your question :).Data Regularization is used for model selection, it is not about data processing. Here it is described in more friendly manner.What you mean is Feature Scaling. It can be done in several ways including Rescaling, the method you described. You may also use Standardization (normalization) and Scaling to unit length.These answers may be helpful:Normalization vs ScalingNormalization vs Standardization;;;
5592;2;2015-04-23T08:10:27.503;;Complete-linkage clustering tends to find compact clusters with equal diameters. It is define the inter-group distances as the largest distance of objects between two clusters. Beside, it is may produce clusters that has the same number of objects and the distance between objects is contiguous. So, I can say that your conjecture is correct. I cannot wait for any experts to explain this in detail.;;;
5593;2;2015-04-23T09:12:02.230;;I may have missed something in the definition in Wikipedia, but wouldn't the set {1,2,4,5,7,8} with $\Delta(x,y) = |x-y|$ be a counterexample? After the 4th step there are two clusters, {{1,2}, {4,5}}, {7,8} and x=5, y=1, z=7 seems to violate your claim.;;;
5594;1;2015-04-23T12:50:52.173;non-linear optimization for a linear classifier? (scikit-learn);Using scikit-learn, why would you use bfgs optimization which is non-linear for a linear classifier as logistic regression? I am confused. Does the optimization method finds the optimum of the chosen score function? if so, which one? I can't choose it when defining the estimator. does the linearity or non-linearity of the score function depend on the model (whether it is linear or non-linear)?;[education, open-source];42;
5595;2;2015-04-23T13:02:21.580;;Refer Aws documentation : http://aws.amazon.com/code there are libraries available for most of the programing languages.So you can create a bucket and configure in your code to fetch data from url and write to this bucket in s3 for eg in python :from boto.s3.key import Keyk = Key(bucket)k.key = 'foobar'k.set_contents_from_string(url_data)Ref : https://boto.readthedocs.org/en/latest/s3_tut.html;;;
5596;2;2015-04-23T15:10:58.970;;The new Amazon Machine Learning Service may work for you. It works directly with Redshift and might be a good way to start. http://aws.amazon.com/machine-learning/If you're looking to process using EMR, then you can use Redshift's UNLOAD command to land data on S3. Spark on EMR can then access it directly without you having to pull it into HDFS.Spark on EMR: https://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923;;;
5600;1;2015-04-23T21:59:23.603;Using clustering and Lasso with cv;I used clustering on my dataset. Now when I'm trying to use a LASSO with cv to predict a response, one of the variables it takes into consideration is which cluster a new point is classified into.(I included the cluster variable as a predictor to see if being in a particular group affects the response) Since the information on all variables is already captured by the cluster variable,using it again in the Lasso model with some other variables,does it become redundant/biased?;[education, open-source];37;
5601;1;2015-04-24T07:50:31.950;How to classify whether text answer is relevant to an initial text question;"I have a text classification problem in which i need to classify an answer to a message as either relevant or not. In the first phase of my calculations, I have already used a SVM to determine if the original message was relevant or not, deciding whether a message contains a hint or question if somebody's twitter account has been hacked.example:""Hey @foobar, have you been hacked?""   <-- relevant""My bank account has just been hacked"" <-- not relevantHowever, when I want to classify whether the answer is relevant, I would want to have both the original message and the answer as input, right? An answer is relevant in my case if it, in any way, responds to the original message. Is this approach possible using a SVM or any other machine learning tool? I'm using python with the scikit-learn library.example:""Hey @foobar, have you been hacked?""""@barfoo it seems so, thx for suggesting"" <-- relevant""Hey @foobar, have you been hacked?""""Lose 20 pounds quickly! http://blabla.com"" <-- not relevantI'm not very experienced in this field, so any input would be very appreciated.";[education, open-source];85;1
5602;1;2015-04-24T14:50:51.290;Training Dataset required for Classifier;I am currently trying to develop a classifier in python using Naive Bayes technique. I need a dataset so that I can train it. My classifier would classify a new document given to it into one of the four categories : Science and technology, Sports, politics, Entertainment. Can anybody please help me find a dataset for this. I've been stuck on this problem for quite some time now. Any help would be greatly appreciated.;[education, open-source];90;
5603;2;2015-04-24T15:15:06.430;;"Both message and answer are your input, so your feature vector should contain information about both.Here's a simple structure of a possible solution using scikit-learn:import numpy as npfrom sklearn.svm import SVCfrom sklearn.feature_extraction import DictVectorizerdataset = ((""Hey @foobar, have you been hacked?"",            ""@barfoo it seems so, thx for suggesting"",            True), # True for relevant, False for not relevant           (""Hey @foobar, have you been hacked?"",            ""Lose 20 pounds quickly! http://blabla.com"",            False))def extractMessageFeatures(message):    # here comes your real feature extraction algorithm    return { 'message_predicted_spam': False,             'message_contains_valid_username': True }def extractAnswerFeatures(answer):    # here comes your real feature extraction algorithm    return { 'answer_predicted_spam': False,             'answer_contains_valid_username': True }def extractFeatures(data):    features = []    for instance in data:        instanceFeatures = extractMessageFeatures(data[0])        instanceFeatures.update(extractAnswerFeatures(data[1]))        features.append(instanceFeatures)    return featuresdef trainClassifier(data):    features = extractFeatures(data)    vec = DictVectorizer()    featureVector = vec.fit_transform(features)    print vec.get_feature_names()    print featureVector.toarray()    svc = SVC()    svc.fit(featureVector, np.array([i[2] for i in data]))    return svcclf = trainClassifier(dataset)# now, you can clf.predict(...)Now, the hardest part is to decide which features to extract from both messages and answers. It's up to you.One of the simplest solutions would be to use n-gram features.Other approach would be to use some spam detection to decide whether answer is spam or not and treat this information as a feature.You can also use Twitter-specific information (for example, whether users are mentioning each other in their tweets, using the same hashtags etc.).You can combine these features in whatever fashion you like, of course.Except of creating feature extraction functionality you need a labeled dataset of messages, answers and relevant/non-relevant labels.But once you have both (feature extraction functionality and a proper dataset), you're good to go with a task which clearly matches standard machine learning approaches.";;;
5605;2;2015-04-24T21:17:21.963;;"You have at least three options:Use some of many available datasets (for example: BBC documents); if you need more, simply go to Google Scholar or any similar service and search for classification news politics sports. In the articles you may find many references to available datasets;Crawl any news service, and use clustering techniques to group articles into clusters (this often separates the articles along their domain, such as politics, sports etc.), and label the articles by their affiliation to a cluster;Crawl politics-specific, sports-specific news services and use them as labeled datasets.";;;
5606;1;2015-04-25T01:48:19.270;Question on decision tree in the book Programming Collective Intelligence;"I'm currently studying Chapter 7 of the Book Programming Collective intelligence. I find the output of the function mdclassify() p.157 confusing. The function deals with missing data. The explanation provided is : ""In the basic decision tree, everything has an implied weight of 1, meaning that the observations count fully for the probability that an item fits into a certain category. If you are following multiple branches instead, you can give each branch a weight equal to the fraction of all the other rows that are on that side."" From what I understand, a instance is then split between branches.Hence, I simply don't understand how we can obtain : {'None': 0.125, 'Premium': 2.25, 'Basic': 0.125} as 0.125+0.125+2.25 does not sum to one nor even an integer. How was the new observation split?The code is here : https://github.com/arthur-e/Programming-Collective-Intelligence/blob/master/chapter7/treepredict.pyUsing the original dataset, I obtain the tree shown here: https://mattscodecave.com/system/uploads/assets/000/000/021/original/tree3.jpg?1398786714Can anyone please explain me precisely what the numbers precisely mean and how they were exactly obtained ?Many thanks for your help !PS : The 1st example of the book is wrong as described on their errata page but just explaining the second example (mentioned above) would be nice.";[education, open-source];66;1
5607;2;2015-04-25T05:50:05.257;;I would go for dimensionality reduction. You can start with SVD (should be available in Weka). If SVD is too slow / too memory consuming, then there are still some options:CUR-decomposition: a variant of singular-value decomposition that keeps the matrices of the decomposition sparse if the original matrix is sparse (see: this chapter of Mining Massive Datasets book)Random projections: projecting the data onto a random lower-dimensional subspace (see: the Random projection in dimensionality reduction: Applications to image and text data paper)Coresets: Given a matrix A, a coreset C is defined as a weighted subset of rows of A such that the sum of squared distances from any given k-dimensional subspace to the rows of A is approximately the same as the sum of squared weighted distances to the rows in C see the Dimensionality Reduction of Massive Sparse Datasets Using Coresets paper)That's the tip of an iceberg. More approaches are there in the wild. The problem is that I doubt any of these solutions come with Weka (please, correct me if I am wrong on this). I would search for a usable Java implementation of any of these algorithms and try to port it to work with Weka's arff files.;;;
5609;1;2015-04-25T17:35:30.843;Algorithm to find common sequence;"Assume that ""1,2,3"" are the ids of users, active means that person visited the stackoverflow in last one month (0=passive, 1=active), and there are positive and negative votes.id  question       votes                 active 1     1        -1, +1, -1, -1, -1         0 1     2        -1, +1, -1, -1, +1         0 2     1        +1, +1, -1, -1             0 3     1        +1, +1, +1, -1, +1         1 3     2        +1, +1, -1, +1, +1, +1     1 3     3        -1, +1                     1I want to know what makes the users stop using stackoverflow. Think that, I have already calculate the how many times did they get negative votes, total vote, average vote for each question...I wonder what kind of information could I get from these sequences. I want to find something like this: these users who are passive have two negative votes sequentially. For example, one positive vote after two negative votes in the second question of user 1, doesn't prevent the user churn. User 3 doesn't have any 2 negative votes sequentially in any of his questions. Hence he is still active.I'm looking for something like PrefixSpan Algorithm but order is important for me. I mean, I can't write the sequences like <(-1 +1 -1 -1 -1) (-1 +1 -1 -1 +1 )> or <(-1) (+1) (-1) (-1) (-1) (-1) (+1) (-1) (-1) (+1 )>. Because the first one loses the order, and the second one jumbled the questions together. Is there any algorithm to find these sequences which is common for churners?";[education, open-source];91;
5610;1;2015-04-25T18:29:33.000;What are Hybrid Classifiers used in Sentiment Analysis?;What are Hybrid classifiers used for sentiment analysis? How are they built? Please suggest good tutorial/book/link for reference. Also how are they different from other classifiers like SVM and Naive Bayes? ;[education, open-source];48;
5611;1;2015-04-25T19:43:00.023;Tidying Time Intervals for Plotting a Histogram in R;I'm doing some cluster analysis on the MLTobs from the LifeTables package and have come across a tricky problem plotting frequency of the Year variable in the mlt.mx.info dataframe. Year contains the period that the life table was measured over, in intervals. Here's a table of the data:    1751-1754 1755-1759 1760-1764 1765-1769 1770-1774 1775-1779 1780-1784 1785-1789 1790-1794         1         1         1         1         1         1         1         1         1 1795-1799 1800-1804 1805-1809 1810-1814 1815-1819 1816-1819 1820-1824 1825-1829 1830-1834         1         1         1         1         1         2         3         3         3 1835-1839 1838-1839 1840-1844 1841-1844 1845-1849 1846-1849 1850-1854 1855-1859 1860-1864         4         1         5         3         8         1        10        11        11 1865-1869 1870-1874 1872-1874 1875-1879 1876-1879 1878-1879 1880-1884 1885-1889 1890-1894        11        11         1        12         2         1        15        15        15 1895-1899 1900-1904 1905-1909 1908-1909 1910-1914 1915-1919 1920-1924 1921-1924 1922-1924        15        15        15         1        16        16        16         2         1 1925-1929 1930-1934 1933-1934 1935-1939 1937-1939 1940-1944 1945-1949 1947-1949 1948-1949        19        19         1        20         1        22        22         3         1 1950-1954 1955-1959 1956-1959 1958-1959 1960-1964 1965-1969 1970-1974 1975-1979 1980-1984        30        30         2         1        40        40        41        41        41 1983-1984 1985-1989 1990-1994 1991-1994 1992-1994 1995-1999 2000-2003 2000-2004 2005-2006         1        42        42         1         1        44         3        41        22 2005-2007        14 As you can see, some of the intervals sit within other intervals. Thankfully none of them overlap. I want to simplify the intervals so intervals such as 1992-1994 and 1991-1994 all go into 1990-1994.An idea might be to get the modulo of each interval and sort them into their new intervals that way but I'm unsure how to do this with the interval data type. If anyone has any ideas I'd really appreciate the help. Ultimately I want to create a histogram or barplot to illustrate the nicely.;[education, open-source];35;
5612;2;2015-04-25T19:48:05.637;;The UCI Machine Learning repo is my go to place to find data sets to work on across a really broad range of topics. The data sets are already cleaned and tagged with the task they were originally purposed for. If you google the data set you'll often find papers citing them or forums with questions that you may have yourself. There's a number of text based classification tasked data sets in there.;;;
5614;2;2015-04-25T23:46:19.727;;"This was a x-post. There is a great solution over here, where I originally asked the question. If I understand your problem, you'll want something like this:bottom <- seq(1750, 2010, 5)library(dplyr)new_df <- mlt.mx.info %>%  arrange(Year) %>%  mutate(year2 = as.numeric(substr(Year, 6, 9))) %>%  mutate(new_year = paste0(bottom[findInterval(year2, bottom)], ""-"",(bottom[findInterval(year2, bottom) + 1] - 1)))View(new_df) So what this does, it creates bins, and outputs a new column  (new_year) that is the bottom of the bin. So everything from 1750-1754  will correspond to a new value of 1750-1754 (in string form; the  original is an integer type, not sure how to fix that). Does this do  what you want? Double check the results, but it looks right to me.  @goodtimeslim";;;
5615;1;2015-04-26T01:38:52.180;Gradient Descent Step for word2vec negative sampling;For word2vec with negative sampling, the cost function for a single word is the following according to word2vec:$$E = - log(\sigma(v_{w_{O}}^{'}.u_{w_{I}})) - \sum_{k=1}^K log(\sigma(-v_{w_{k}}^{'}.u_{w_{I}}))$$$v_{w_{O}}^{'}$ = hidden->output word vector of the output word$u_{w_{I}}$ = input->hidden word vector of the output word$v_{w_{k}}^{'}$ = hidden->output word vector of the negative sampled word$\sigma$ is the sigmoid functionAnd taking the derivative with respect to $v_{w_{O}}^{'}.u_{w_{j}}$ is:$\frac{\partial E}{\partial v_{w_{j}}^{'}.u_{w_{I}}} = \sigma(v_{w_{j}}^{'}.u_{w_{I}}) * (\sigma(v_{w_{j}}^{'}.u_{w_{I}}) - 1)$ $ if w_j = w_O $$\frac{\partial E}{\partial v_{w_{j}}^{'}.u_{w_{I}}} = \sigma(v_{w_{j}}^{'}.u_{w_{I}}) * \sigma(-v_{w_{j}}^{'}.u_{w_{I}})$ $ if w_j = w_k \ for \ k = 1...K$Then we can use chain rule to get $ \frac{\partial E}{\partial v_{w_{j}}^{'}} = \frac{\partial E}{\partial v_{w_{j}}^{'}.u_{w_{I}}} * \frac{\partial v_{w_{j}}^{'}.u_{w_{I}}}{\partial v_{w_{j}}^{'}} $Is my reasoning and derivative correct? I am still new to ML so any help would be great! ;[education, open-source];423;
5616;2;2015-04-26T06:04:19.777;;"There are four features:referer,location,FAQ,pages.In your case, you're trying to classify an instance where FAQ and pages are unknown: mdclassify(['google','France',None,None], tree).Since the first known attribute is google, in your decision tree you're only interested in the edge that comes out of google node on the right-hand side.There are five instances: three labeled Premium, one labeled Basic and one labeled None.Instances with labels Basic and None split on the FAQ attribute. There's two of them, so the weight for both of them is 0.5.Now, we split on the pages attribute. There are 3 instances with pages value larger than 20, and two with pages value no larger than 20.Here's the trick: we already know that the weights for two of these were altered from 1 to 0.5 each. So, now we have three instances weighted 1 each, and 2 instances weighted 0.5 each. So the total value is 4.Now, we can count the weights for pages attribute:pages_larger_than_20 = 3/4pages_not_larger_than_20 = 1/4 # the 1 is: 0.5 + 0.5All weights are ascribed. Now we can multiply the weights by the ""frequencies"" of instances (remembering that for Basic and None the ""frequency"" is now 0.5):Premium: 3 * 3/4 = 2.25 # because there are three Premium instances, each weighting 0.75;Basic: 0.5 * 1/4 = 0.125 # because Basic is now 0.5, and the split onpages_not_larger_than_20 is 1/4None: 0.5 * 1/4 = 0.125 # analogouslyThat's at least where the numbers come from. I share your doubts about the maximum value of this metric, and whether it should sum to 1, but now that you know where these numbers come from you can think how to normalize them.";;;
5618;1;2015-04-26T11:09:49.133;Correlation between time to event data and continuous data;I want to measure the correlation between the survival time which is a time to event data and the patient's activity count which is measured on continuous scale. What type of correlation coefficient is available to measure the strength of these two variables?;[education, open-source];45;
5619;1;2015-04-27T06:43:09.423;Multiple classes DBN network with theano;I am planning to do scene recognition with point cloud data sets. These are generally (x,y,z) pairs points and a set of points represents an object. I have all the points labelled according to their class. I have previously implemented a network that takes a whole point cloud and a single label for it for training. How can i modify my network to use it with multiple labels i.e. a point cloud example which has 2 or more labels for it.self.logLayer = LogisticRegression(        input=self.sigmoid_layers[-1].output,        n_in=hidden_layers_sizes[-1],        n_out=n_outs)previously n_outs = 10 and a cloud would belong to only 1 of the 10 labels. Now i still have 10 labels but a cloud will belong to multiple classes.;[education, open-source];22;
5620;2;2015-04-27T07:24:13.680;;Use library(reshape) for most of the pivot & filter operations. Should resolve your problem.;;;
5621;2;2015-04-27T07:26:25.893;;Use Event as Row Label, Time as Column label and Activity as Value in a Pivot.If you're doing this in R, use library(reshape). Using melt and cast functions, you can get a solution to your problem i guess. Let me know if you need any further clarifications.;;;
5622;2;2015-04-27T07:29:06.110;;Use sigmoidal functions to get the best correlational value. Use Octave/MATLAB for processing your matrix using the function.;;;
5623;2;2015-04-27T07:33:24.683;;http://www.kdnuggets.com/datasets/index.htmlThis should get you maximum datasets for your classification exercise.;;;
5624;1;2015-04-27T09:09:55.170;Which classifier is efficient in dealing with test query which belongs to no trained class?;Suppose classifier trained with 5 class, and input query content does not belong to any of the trained class data.Naive bayes provides and random class as a result here. Which classifier deals best in such scenario?;[education, open-source];36;
5626;2;2015-04-27T11:58:50.443;;Another good normalization is zScore normalization. Was already implemented in python in the module scipy.stats;;;
5627;2;2015-04-27T13:42:48.057;;"You have trained a model to recognize or discriminate between several particular classes. So when having a new test sample (that according to your knowledge) does not belong to any of this class, the model will fit it to the most similar class. This is, off course, a very general way of putting it. The behavior and things to consider wether you are using a probabilistic graphical model (like Naive Bayes), linear or non linear classifiers, etc, will vary. However, the principle is the same: the model has learnt the relation among the features of the training data you have used to match particular classes.More specifically, if you are using a probabilistic approach you can  use the probability of belonging to a class and define a threshold. So if a probability higher than certain value (let's say 65%) is not given, then your confidence on the result will be low, and you might say ""I cannot say wether it belongs to any of this 5 classes with certainty"".Other non probabilistic methods have interesting approaches for scoring a new example in terms of probabilities. Check this link for SVM (http://stats.stackexchange.com/questions/55072/svm-confidence-according-to-distance-from-hyperline), which is actually the approach used in Python scikit-learn: http://scikit-learn.org/stable/modules/svm.html#scores-and-probabilitiesIf you use Random Forests you could use the voting of each tree to define a rate of confidence in a similar way (http://stats.stackexchange.com/questions/94845/how-to-estimate-confidence-level-for-svm-or-random-forest)A similar thing can be done with K-Nearest-Neighbors by analyzing the distance of the K or J (where J < K) nearest neighbors and just trust the result if they are close to your test sample in some ratio (very generally said, this can be done in many ways).Now, methods based on deep learning are trying to learn features in an unsupervised way, so your problem could solved in a more interesting way. However, it would not be strict classification anymore, nor you would have the amount of data and servers required to even try it (but just saying :) )";;;
5628;2;2015-04-27T17:46:32.680;;Not a typical problem formulation indeed. I suggest two approaches:Use a simple bayesian model where the states or nodes are each of your features. Connect the nodes directly when they haven a direct time dependency (for example vote1 -> vote2 -> vote3, but not vote3 -> vote1) and calculate the probability of a final (output) node to be something (for example user type 1, user type 2, etc). You could also use a hidden markov model that naturally models transitions between states. In your case it would output just 1 state (the prediction). I am not very fan of this model for your problem, but it could be nice to try. You will need a lot of data though.You could use Fuzzy Inductive Reasoning (FIR). It is definitely not a simple algorithm, but works quite well for classification having time series or time-related features. In FIR you want to find an optimal mask of a given depth. If you have, say, 5 features per sample, and for instance you want to know which features to select you will learn an optimal mask for your data that address this (and creates fuzzy rules, etc...). Also if you want to relate two or more examples you will define a depth higher than 1 for the mask. This will allow you to find a mask that for instance uses features 1,2 and 4 of sample t, features 2,3 ,4 and 5 of sample t+1 and features 1 and 4 of sample t+2. Moving t from 1 to N number of samples. So, interesting combinations of features from samples that have a time-relation (or some sort of sequence) are created.;;;
5629;1;2015-04-27T18:29:45.513;How do I access data on my EBS Volume from R-Studio Server on Ubuntu EC2 Instance;I have setup R-Studio Server on an Ubuntu EC2 instance for the first time and successfully started r-studio server in my browser. I also have putty ssh client. How do I set path in r-studio server to my mounted EBS volume and why do I not see the contents of my EBS volume in the r-studio files area (bottom right side? ) . Also, I had a file in an s3 bucket. I passed this command to bring it from s3 to my ebs volume: s3cmd get s3://data-analysis/input-data/filename.csv . I assume this command downloads the file from s3 into the ebs volume. But I can't find it in RStudio Server! I have scoured the internet looking for help on this but not able to solve my problem.;[education, open-source];36;
5630;2;2015-04-27T20:50:57.117;;"One thing you could do is use an associations rule approach, in the sense of finding the most frequent patterns associated with being passive, or over-associated with being passive:set a frequency thresholdfigure out what kind of patterns you're looking for, for example frequent sequences. It's good if you can partially order them, similarly to set inclusion in the a priori algorithm. For example, look for sequences, and order them by ""is subsequence of"".Starting with a reasonably small list of minimal patterns that you're looking for, go through your data counting the occurrences of these patterns.  Keep the most frequent patterns (above your threshold), this is your step-1 set of frequent patterns.Generate candidates for step-i+1 patterns by adding something to your step-i patterns.Count how many times the candidates occur. Go to step 4 until you have no more frequent patterns for step i.You then have a bunch of rule candidates, and you want to use association rules metrics to find the patterns that best predict being passive. ";;;
5631;2;2015-04-27T21:46:21.373;;Spark is intended to be pointed at large distributed data sets, so as you suggest, the most typical use cases will involve connecting to some sort of could system.  In fact, if the data set you aim to analyze can fit on your local system, you'll usually find that you can analyze it just as simply using pure python.  If you're trying to leverage a series of local VMs, you're going to run out of memory pretty quickly and jobs will either fail or grind to a halt.With that said, a local instance of spark is very useful for the purpose of development.  One way that I've found that works is if I have a directory in HDFS with many files, I'll pull over a single file, develop locally, then port my spark script to my cloud system for execution.  If you're using AWS, this is really helpful for avoiding big fees while you're developing.;;;
5632;1;2015-04-28T06:48:25.867;Predicting earthquakes using disturbances in DTH TV transmission;It is said that before an earthquake happens, a viewer experiences disturbances in DTH TV transmission in the form of distorted images on the screen which automatically correct after a few seconds.Is it possible to identify patterns of such disturbances by continuously monitoring TV images so that earthquakes can potentially be predicted at least few minutes in advance and many lives could be saved?;[education, open-source];47;
5634;2;2015-04-28T08:44:09.613;;"I think that if there are disturbances caused by something preceding earthquake, they might be used.The problem is that you need to find which cause produces that effect and measure it. For example you can team up with geologists, geophysicists or someone similar and try to build as complete hypothesis as possible and then design experiment to gather data. If your experiment would bring some non noise data then you can start to thinking about machine learning algorithms which will work in this situation.But you might also try to do it on ""brute force"" way by actually recording DTH TV images in seismically unstable regions and try to correlate those videos with seismic data. Then after some manual examination and categorization you could define (or not if this cause and effect hypothesis is wrong) some possible glitches that are observed and try to develop software that tries to detect them (OpenCV might be useful for example).IMHO in either way, some domain knowledge related to earthquakes would be more useful on beginning stage, and experience related to machine learning would be more useful on latest stages of such ambitious project.";;;
5635;1;2015-04-28T12:58:33.920;large database operation. check for relatedness between entities;"I have one small list of entities, such as:RussiaVladimirMoscowThen I have a massive database of JSON indices. For each entry there are multiple alpha-numeric identifiers.So for instance, for Russia there might only be one. But for Vladimir maybe there will be 100. They're stored in JSON but I read them into my java program like this:        // GET JSON DATA        File f = new File(""/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json"");        String jsonTxt = null;        if (f.exists())        {            InputStream is = new FileInputStream(""/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/user666.json"");            jsonTxt = IOUtils.toString(is);        }        //reformat        jsonTxt = ( jsonTxt.substring(1, jsonTxt.length()-1) ).replace(""\\"","""");        Gson json = new Gson();        Map<String, HashSet<String>> mast_Q_storage_map = new HashMap<String, HashSet<String>>();        mast_Q_storage_map = (Map<String, HashSet<String>>) json.fromJson(jsonTxt, mast_Q_storage_map.getClass());I want to get all of the values associated with the entities from the small list. So I need to search the big list and retrieve their values. Then I want to try to determine if there is a relationship between any of the entities in the sentence, as in I want to check if Russia   X VladimirRussia   X MoscowMoscow   X VladimirMoscow   X RussiaVladimir X RussiaVladimir X Moscowwill result in a relationship, I've been trying to do it like this, but I'm running into big problems:        // Read in all the sentences, that are in files, in this folder        final File folder = new File(""/home/matthias/Workbench/SUTD/nytimes_corpus/wdtk-parent/wdtk-examples/JSON_Output/"");        for (final File fileEntry : folder.listFiles())         {            BufferedReader br = new BufferedReader(new FileReader(fileEntry));            try             {                //Store the filename                //System.out.println(fileEntry.getName());                StringBuilder sb = new StringBuilder();                String line = br.readLine();                while (line != null)                 {                       sb.append(line);                    sb.append(System.lineSeparator());                    line = br.readLine();                }                String everything = sb.toString();                //System.out.println(everything);                Document doc = Jsoup.parse(everything);                Elements contents = doc.getElementsByTag(""sentence"");                for (Element content : contents)                 {                    //store the sentence number                    String number = content.select(""sentence"").text();                    number = number.substring(0, number.indexOf("" ""));                     System.out.println(number);                    //get all the entities in this sentence                    Elements pers = content.select(""PERSON"");                    Elements locs = content.select(""LOCATION"");                    Elements orgs = content.select(""ORGANIZATION"");                    //collect all the elements to a list, all the elements of one sentence                    List<String> list = new ArrayList<String>();                    for (Element per : pers)                     {                        list.add(per.text().trim());                    }                    for (Element loc : locs)                     {                        list.add(loc.text().trim());                    }                    for (Element org : orgs)                     {                        list.add(org.text().trim());                    }                    System.out.println();                    System.out.println();                    System.out.println();                    System.out.println(""This is list of sentence elements:"");                    for (String s : list)                        System.out.println(s);                    System.out.println();                    System.out.println();                    System.out.println();                    List<String> Q_value_list = new ArrayList<String>();                    // for the list of Q values to keys                    for (Entry<String, HashSet<String>> e : mast_Q_storage_map.entrySet())                     {                        for (String s : list)                        {                            if (e.getKey().contains(s))                             {                                //List<String> Q_value_list = new ArrayList<String>(e.getValue());                                System.out.println(e.getKey() + "" :: "" + Q_value_list.toString());                            }                        }                    }                                //czeher                                for (String home:Q_value_list)                                 {                                  for (String away:Q_value_list)                                   {                                    String URL_czech = ""http://milenio.dcc.uchile.cl/sparql?default-graph-uri=&query=PREFIX+%3A+%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0D%0ASELECT+*+WHERE+%7B%0D%0A+++%3A""                                                        + home + ""+%3FsimpleProperty+%3A""                                                        + away + ""%0D%0A%7D%0D%0A&format=text%2Fhtml&timeout=0&debug=on"";                                    URL wikidata_page = new URL(URL_czech);                                    HttpURLConnection wiki_connection = (HttpURLConnection)wikidata_page.openConnection();                                    InputStream wikiInputStream = null;                                        try                                         {                                            // try to connect and use the input stream                                            wiki_connection.connect();                                            wikiInputStream = wiki_connection.getInputStream();                                        }                                         catch(IOException error)                                         {                                            // failed, try using the error stream                                            wikiInputStream = wiki_connection.getErrorStream();                                        }                                    // parse the input stream using Jsoup                                    Document docx = Jsoup.parse(wikiInputStream, null, wikidata_page.getProtocol()+""://""+wikidata_page.getHost()+""/"");                                    Elements link_text = docx.select(""table.sparql > tbody > tr:nth-child(2) > td > a"");                                    //link_text.text();                                    for (Element l : link_text)                                     {                                        String output = l.text();                                        System.out.println( output );                                    }                                  }                    }                }            }            finally             {                br.close();            }        }";[education, open-source];15;
5636;2;2015-04-28T15:29:24.093;;Cluster analysis is not supposed to produce paritions of equal size. It is meant to discover structure in the data.If the majority of objects is highly similar, then this majority is supposed to be in the majority cluster.Consider all your data is identical. Any clustering algorithm producing more than one cluster has failed, in my opinion...So you may be using the wrong class of algorithms for your problem.;;;
5637;2;2015-04-28T19:06:07.887;;Looks good to me. This derivative is also presented in the paper (equations 56-58).The paper you're linking to is the most advanced attempt - at least to best of my knowledge - to explain how word2vec works, but there is also a lot of other resources on the topic (just search for word2vec on arxiv.org). If you're interested in word2vec, you may find GloVe interesting too (see: Linking GloVe with word2vec).;;;
5638;2;2015-04-28T19:34:42.863;;In sentiment analysis you may want to combine a number of classifiers. Let's say: a separate classifier for emoticons, another one for emotionally loaded terms, another one for some special linguistic patterns and - let's say - yet another one to detect and filter out spam messages. It's all up to you.You can use either SVM, Naive Bayes, or anything else that best suits your problem. You may use majority voting, weights (for example based on cross validation results), or any other more advanced technique to decide which class is the most appropriate one.Also, googling for hybrid sentiment returns tons of papers containing answers to the questions that you have stated. Please, don't ask us to rewrite this papers here.;;;
5639;1;2015-04-29T07:27:33.967;Fraud detection use text mining;I would like to find different patterns recognition algorithm to detect different type of fraud. I have 1 million unstructured text documents about the clients' information with metadata about the client name, viewers, location in the cloud. Here are the patterns I was thinking of and the related fraud that i would like to detect: Numeric Patterns - fictitious invoice numbers, fictitiously-generated transaction amounts.  Time Patterns - transactions occurring too regularly, activity at unusual times or dates.  Name Patterns - similar and alerted name and addresses.  Geographic Patterns - Proximity relationships between apparently unrelated entities. What technique can I use ? any keywords? thx.;[education, open-source];187;2
5641;2;2015-04-29T12:07:56.710;;Since you obviously posses an AWS account I'd recommend the following:  Create an EC2 instance (any size)  Use wget(or curl) to fetch the file(s) to that EC2 instance. For example: wget http://example.com/my_large_file.csv.  Install s3cmd Use s3cmd to upload the file to S3. For example: s3cmd cp my_large_file.csv s3://my.bucket/my_large_file.csvSince connections made between various AWS services leverage AWS's internal network, uploading from an EC2 instance to S3 is pretty fast. Much faster than uploading it from your own computer. This way allows you to avoid downloading the file to your computer and saving potentially significant time uploading it through the web interface.;;;
5642;2;2015-04-29T13:53:09.180;;The UK Government provide an excellent source of non-personal data collected throughout government departments: http://data.gov.uk;;;
5643;1;2015-04-29T14:51:59.177;Automomatic typographical error correction;"Does anyone know where I might be able to find a list of the most common typing errors and their corrections? This is separate from more complicated considerations concerning general spelling checking (which can have very many candidates in relation to the correct spelling of the word in question); rather I am looking for a similar list as used by Microsoft Word (for instance correcting  tehwith  theor becaisewith  becauseNot only can the manner in which these sort of errors are fixed be hard-coded, their frequent occurrence in text provides significant dividends in textual mining (provided that such a list of errors and corrections can be obtained, of course).";[education, open-source];36;
5644;1;2015-04-29T16:43:34.397;How is frequent itemsets compared with item-based collaborative filtering in recommender systems?;What is the difference between data mining approaches: frequent itemsets and item-based collaborative filtering in the area of recommender systems?;[education, open-source];30;
5645;2;2015-04-29T17:05:22.277;;I see this question got up-voted so someone else is looking for the answer. Here it is: I was logged in as the default user on Ubuntu instance when i did the $s3cmd get s3://data-analysis/input-data/filename.csvThe data got saved from the s3 bucket to the home directory of the user named ubuntu. this can be verified with : $ df -h /home/ubuntu/ Since I had followed randy zwitch's tutorial on installing R on aws - i had created a user named rstudio and that was the user name I was using to log-in to rstudio server. Hence I had to move the file from the default 'ubuntu' user's home directory to the user 'rstudio'. this can be done with: $ sudo mv /home/ubuntu/filename.csv /home/rstudioHappy to answer follow up questions. ;;;
5646;2;2015-04-29T17:07:14.157;;I will reference two sources:Wikipedia has its own lists: Lists of common misspellingsAspell is certainly a place to visit. You can find a list of misspellings and corrections here.Still, it is a good idea to think of using a spell checker, such as Aspell, since it gives you the ability to deal with no-so-common misspellings (of course, at the cost of false positives). A good starting point is Peter Norvig's essay on creating a spelling corrector.;;;
5647;2;2015-04-29T18:44:29.593;;"I think by ""frequent itemsets"" you mean the association rules that you derive from counting frequent itemsets. They tell us:people who do (like, buy, etc.) X tend to do (...) Y.For this you only need ""positive"" information, i.e. people who liked/bought/did.Say this is about movies, you only know who liked the movie, among the others you don't know whether they disliked it or didn't see it.item-based CF, and other CF techniques are typically used when you have both positive and negative feedback, such as ratings 0 to 5. You then use this information to predict unknown ratings.For item-based CF, you're trying to predict a person's rating on item Y, and you look how this person has rated other similar items, where ""X is similar to Y"" means that other people who have rated both X and Y have given them very similar ratings. The information you get is then:Your have previously liked similar items to Y, therefore we predict you'll like Y.User-based CF is perhaps closer to association rules, and you get:People similar to you have liked Y, therefore we predict you will like Y.";;;
5648;1;2015-04-29T20:39:45.520;Where is the error in the following code?;"I am trying to analyze the movie database using python, downloaded from imdb. While trying to generate some plots, I am running into errors which confuses me.I am trying to generate a matrix of small figures which can show me any hidden pattern etc. Here is the code:fig, axes = plt.subplots(nrows=4, ncols=6, figsize=(12, 8),                          tight_layout=True)bins = np.arange(1950,2012,3)for ax, genre in zip(axes.ravel(), movieGenre):    ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')    ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')    ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)    ax.xaxis.set_ticks(np.arange(1950, 2013, 30))    ax.set_yticks([])    ax.set_xlabel('Year')The first hist isn't working, but the second one is working when I am commenting out the first one. Here is the traceback: KeyError                                  Traceback (most recent call last)<ipython-input-158-c2e7c2737372> in <module>()      4 bins = np.arange(1950,2012,3)      5 for ax, genre in zip(axes.ravel(), movieGenre):----> 6     ax.hist(movieDF[movieDF['%s'%genre]==1].year, bins=bins, histtype='stepfilled', normed=True, color='r', alpha=.3, ec='None')      7     ax.hist(movieDF.year, bins=bins, histtype='stepfilled', ec='None', normed=True, zorder=0, color='grey')      8     ax.annotate(genre, xy=(1955, 3e-2), fontsize=14)/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/axes.pyc in hist(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)   8247         # Massage 'x' for processing.   8248         # NOTE: Be sure any changes here is also done below to 'weights'-> 8249         if isinstance(x, np.ndarray) or not iterable(x[0]):   8250             # TODO: support masked arrays;   8251             x = np.asarray(x)/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/series.pyc in __getitem__(self, key)    477     def __getitem__(self, key):    478         try:--> 479             result = self.index.get_value(self, key)    480     481             if not np.isscalar(result):/Users/dt/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/index.pyc in get_value(self, series, key)   1169    1170         try:-> 1171             return self._engine.get_value(s, k)   1172         except KeyError as e1:   1173             if len(self) > 0 and self.inferred_type == 'integer':KeyError: 0Here is the first few columns of the data:     imdbID     title                      rating    vote      runtime  year    genre0   tt0111161   The Shawshank Redemption    9.3      1,439,277  142    1994 [Crime, Drama]1   tt0468569   The Dark Knight             9.0      1,410,124  152    2008 [Action, Crime, Drama]2   tt1375666   Inception                   8.8      1,209,159  148    2010 [Action, Mystery, Sci-Fi, Thriller]3   tt0137523   Fight Club                  8.9      1,123,462  139    1999 [Drama]4   tt0110912   Pulp Fiction                8.9      1,117,193  154    1994 [Crime, Drama]movieGenre is basically collecting all the different genres from 'genre' column with duplicates removed: movieGenre = set(movieDF.genre.sum()) . I then added a single column to movieDF data frame for each genre such that if a particular movie belong to that genre, then that cell is True otherwise it is False.  So for example, for the movie Inception, the Action column is marked True but Crime column is marked False and so forth.Thanks.";[education, open-source];93;
5649;2;2015-04-30T06:19:06.690;;There is nothing wrong with using Pearson's correlation coefficient (frequently called THE correlation coefficient) with these variables. I would also have a look at Spearman's rank correlation coefficient. It is less sensitive to extreme values. http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficientHowever, both coefficients have the drawback that they only measure linear relationships. Hence it is worthwile to have a look at the scatterplot of the variables. Adding a smoother to the graph can be worthwile too, especially  if the plot is clutterd and you have trouble to spot a relationship with the naked eye.;;;
5650;1;2015-04-30T08:01:39.763;Pound notation in Summation;I was going through a paper comparing glove and word2vec. I came across the pound notation shown below. What does it mean when used like this?The link for paper is here;[education, open-source];32;
5651;2;2015-04-30T08:18:14.520;;That just means count. #(w) is the number of times w occurs in the corpus.;;;
5652;1;2015-04-30T10:06:11.107;Gradient descent parameter estimation Package for R;I am looking for a package that does gradient descent parameter estimation in R, maybe with some bootstrapping to get confidence intervals. I wonder if people call it something different here as I get almost nothing on my searches, and the one article I found was from someone who rolled their own. It is not that hard to implement, but I would prefer to use something standard.;[education, open-source];40;
5653;1;2015-04-30T11:50:44.760;International Journal;I would like to post a paper in International Conference on Soft Computing. I want to know whether the journal is a reputed journal. ;[education, open-source];27;
5654;1;2015-04-30T12:26:47.637;What types of features are used in a large-scale click-through rate prediction problem?;Something that I often see in papers (example) about large-scale learning is that click-through rate (CTR) problems can have up to a billion of features for each example. In this Google paper the authors mention: The features used in our system are drawn from a variety of sources,  including the query, the text of the ad creative, and various  ad-related metadata.I can imagine a few thousands of features coming from this type of source, I guess through some form of feature hashing.My question is: how does one get to a billion features? How do companies translate user behavior into features in order to reach that scale of features?;[education, open-source];105;3
5655;2;2015-04-30T13:38:55.750;;"First of all, since you only seem to want to know about the Python error, your question is probably better suited for Stack Overflow.Secondly, this is not fun to answer with the information you've provided.  This is certainly not a minimal working example (MWE) - I can't copy your code as is, and run it myself.  I'm left to guess many things:Are you importing matplotlib?Did you rename matplotlib.pyplot as plt?Are you importing numpy and did you rename it np?Does your traceback literally end in KeyError is 0 or is it KeyError: 0 or KeyError: False?  Can you show the full traceback?  The traceback for a MWE is typically not long.In the positional argument (movieDF[movieDF['%s'%genre]==1].year) on your problematic line, you ask for a key (movieDF['%s'%genre]==1) that looks like it should evaluate to a Boolean.  Is this never 0?  I'm forced to grasp at straws because you haven't provided a sample dataset; I shouldn't have to download from IMDb myself.Edit:Just to troubleshoot, can you place x = movieDF[movieDF['%s'%genre]==1].year on a line by itself and tell me what x[0] is?  What type would that x be?  Is it a dictionary, where the .hist() method is expecting a list or other array?  This documentation makes it sound like the method will not accept a dictionary...what type is movieDF.year?";;;
5656;1;2015-04-30T13:42:05.343;K-means: What are some good ways to choose an efficient set of initial centroids?;When a random initialization of centroids is used, different runs of K-means produce different total SSEs. And it is crucial in the performance of the algorithm. What are some effective approaches toward solving this problem? Recent approaches are appreciated.;[education, open-source];117;1
5657;2;2015-04-30T18:26:46.633;;I may be misunderstanding your question, but usually k-means chooses your centroids randomly for you depending on the number of clusters you set (i.e. k). Choosing the number for k tends to be a subjective exercise. A good place to start is an Elbow/Scree plot which can be found here:http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method;;;
5658;2;2015-04-30T19:17:04.320;;You can check it here. At least that one is in the list:http://www.scimagojr.com/journalrank.php?category=1712&area=0&year=2013&country=US&order=sjr&min=0&min_type=cd&page=0;;;
5659;2;2015-04-30T19:23:00.970;;Instead of scraping,  you might try to get the data directly here: http://www.imdb.com/interfaces.  It looks like they have data available via ftp for movies, actors, etc.;;;
5660;2;2015-05-01T08:47:31.613;;"Ok, after a lot of looking I found the ""optim"" routine which is in ""stats"", one of the packages that is always loaded. It has quite a few methods including conjugate gradients and BGGS and a few others and worked well on the first few examples I tried. It doesn't seem to get a lot of attention strangely. I guess optimization people tend to use Matlab.I knew there had to be something.";;;
5661;2;2015-05-01T12:54:39.857;;The usual approach to this problem is to re-run your K-means algorithm several times, with different random initializations of the centroids, and to keep the best solution. You can do that by evaluating the results on your training data or by means of cross validation. There are many other ways to initialize the centroids, but none of them is going to perform the best for every single problem. You could evaluate these approaches together with random initialization for your particular problem. ;;;
5662;2;2015-05-01T14:06:08.007;;An approach that yields more consistent results is K-means++.  This approach acknowledges that there is probably a better choice of initial centroid locations than simple random assignment. Specifically, K-means tends to perform better when centroids are seeded in such a way that doesn't clump them together in space. In short, the method is as follows:Choose one of your data points at random as an initial centroid.Calculate $D(x)$, the distance between your initial centroid and all other data points, $x$.Choose your next centroid from the remaining datapoints with probability proportional to $D(x)^2$Repeat until all centroids have been assigned.Note: $D(x)$ should be updated as more centroids are added.  It should be set to be the distance between a data point and the nearest centroid.You may also be interested to read this paper that proposes the method and describes its overall expected performance.;;;
5663;1;2015-05-01T15:19:17.943;Reference about social network data-mining;I am not in the data science field, but I would like to examine in depth this field and, particularly, I would like to start from the analysis of the social networks data.I am trying to find some good references, both paper, websites and books, in order to start learning about the topic. Browsing on the internet, one can find a lot of sites, forum, papers about the topic, but I'm not able to discriminate among good and bad readings.I am an R, Matlab, SAS user and I know a little bit of python language.Could you suggest any references from which I could start studying and deepen the industry?;[education, open-source];143;3
5664;2;2015-05-01T15:29:48.200;;Without more detail, it's difficult to give detailed advice, but at first glance, this seems like a textbook case for machine learning. See if you can get a similar data set with known fraudulent activity and, depending on the data, pick a machine learning approach. ;;;
5665;2;2015-05-01T16:52:15.717;;"As a bit of general feedback, I think you would do well to improve your output format.  The problem with the format as it stands is there is not a transparent way to programmatically get the data. Consider instead trying:print ""\t"".join([title, genres,runtime, rating, year])The nice thing about a tab delimited file is that if you end up scaling up, it can easily be read into something like impala (or at smaller scales, simple mySql tables).  Additionally, you can then programatically read in the data in python using: line.split(""\t"")The second bit of advice, is I would suggest getting more information than you think you need on your initial scrape.  Disk space is cheaper than processing time, so rerunning the scraper every time you expand your analytic will not be fun.";;;
5666;1;2015-05-01T19:31:34.570;Analyzing survey data for predictions;I've got survey data that resembles:|-------------| Q1a | Q1b | Q1c | Q2a | Q2b | Q2c | Classification| Respondent  | 1   | 0   | 0   | 1   | 0   | 0   | Red| Respondent  | 0   | 0   | 1   | 1   | 0   | 0   | Green| Respondent  | 0   | 1   | 0   | 0   | 0   | 1   | YellowI am trying to predict the classification for new respondents. Currently I'm using a Naive Bayes, and getting pretty bad accuracy (~20%). I don't have much training data, and the training data is hand scraped from non-standard sources (internal company procedures are a mess here).I'm looking for other ways to predict the classification.I'm thinking about assigning weights to each question, and magically predicting the result based on those, somehow. Although I don't really know where to start learning about how to do that, and whether it's appropriate for this data. I have very little background in this :(Any ideas or tips on predicting the classification column with no training data?;[education, open-source];48;
5667;1;2015-05-01T20:32:51.900;Spark, optimally splitting a single RDD into two;I have a large dataset that I need to split into groups according to specific parameters.  I want the job to process as efficiently as possible.  I can envision two ways of doing soOption 1 - Create map from original RDD and filterdef customMapper(record):    if passesSomeTest(record):        return (1,record)    else:        return (0,record)mappedRdd = rddIn.map(lambda x: customMapper(x))rdd0 = mappedRdd.filter(lambda x: x[0]==0).cache()rdd1 = mappedRdd.filter(lambda x: x[1]==1).cache()Option 2 - Filter original RDD directlydef customFilter(record):    return passesSomeTest(record)rdd0 = rddIn.filter(lambda x: customFilter(x)==False).cache()rdd1 = rddIn.filter(customFilter).cache()The fist method has to itterate over all the records of the original data set 3 times, where the second only has to do so twice, under normal circumstances, however, spark does some behind the scenes graph building, so I could imagine that they are effectively done in the same way.  My questions are:a.) Is one method more efficient than the other, or does the spark graph building make them equivalentb.) Is it possible to do this split in a single pass;[education, open-source];357;
5668;2;2015-05-01T20:57:34.133;;Can you give a bit more information on the size of the data you're training on (and if it's really 6 parameters you're basing the predictions off of)?  If it's really 6 questions with binary answers (1, 0 as you suggest), then there are 2^6 (i.e. 64) unique answer combinations, and to determine a probability for them you'll want a multiple entries per combination.  Standard error scales like 1/sqrt(n) so for 10% accuracy you'll need roughly 6,400 inputs which given your description, sounds like more data than you may have.  You may want to invest time into automating data collection.If on the other hand, you have a reasonably large data set and are hoping for some alternative models, both boosted decision trees and random forest models sound like good candidates for this problem.  ;;;
5669;2;2015-05-01T21:42:55.350;;"My favorite place to find information about social network analysis is from SNAP, the Stanford Network Analysis Project. Led by Jure Leskovec, this team of students and professors has built software tools, gathered data sets, and published papers on social network analysis.http://snap.stanford.edu/The collection of research papers there is outstanding. They also have a Python tool you could try. http://snap.stanford.edu/snappy/index.htmlThe focus is on graph analysis, because social networks fit this model well. If you are new to graph analysis, I suggest you take a undergraduate level discrete mathematics course, or check out my favorite book on the topic ""Graph Theory with Algorithms and its Applications"" by Santanu Ray.For a hands-on approach to social network analysis, check out ""Mining the Social Web"" by Matthew A Russell. It has examples which cover how to collect and analyze data from the major social networks like Twitter, Facebook, and LinkedIn.It was Jure Leskovec who initially excited me about this field. He has many great talks on YouTube, for example: https://www.youtube.com/watch?v=LmQ_3nijMCs";;;
5670;2;2015-05-02T01:16:38.717;;Neural Networks are not a great introductory model, simply because of the complexity that you describe.  If you're trying to get your feet wet, boosted decision trees tend to perform well by comparison, and are a bit more intuitive.  If you want a description on this method, and are already familiar with Coursera, The University of Washington has an introductory course on data science which explains it quite well.;;;
5671;2;2015-05-02T01:26:46.707;;A lot of the features you mentioned are categorical, and with so many levels of each, the dimensions of your problem will expand.  Rather than initially focusing on Lasso and Ridge regression, why don't you first look for clusters among the samples (records) to learn about the data set?  Under you current approach, you're throwing everything into a model and expecting a high AUC(?).  You may find several clusters where frequencies of the category levels predominate in one or more potential clusters.  If you don't know what the cluster structure is of the samples (records), try k-means clustering based on centroids of feature values to see if there are unique clusters.  Once you get a handle on the cluster structure, then address regression issues.  Your regression models may be breaking down, in part, because of large inhomogeneities in your data, along with the previously suggested issues.  Machine learning is all about performing unsupervised class discovery followed by class prediction (your output binary variable).   At this point, it's not clear that you studied the data to learn about its cluster structure, and rather threw it into a supervised model expecting to attain high AUC values.   ;;;
5672;2;2015-05-02T02:55:09.197;;I'm going to pursue the following series of online courses on the Coursera: Become a Social Scientist: Methods and Statistics by University of Amsterdam. The good news - it is free, or you can get a nice-looking certificate for $49 or so. The bad news - the nearest enrollment is Aug 31st 2015. You will have opportunity to get a lot of information in condensed way during a short time frame and you will be enforced to actually apply the knowledge in exercises, quizzes and project assignments. You will also have opportunity to discuss lessons/projects on the forum with many other students and lecturers. [update] I apologize, I just remembered there is another course Introduction to Statistics for the Social Sciences by University of Zurich - just started April 28th, 2015. If you want to pursue it - do not forget about deadlines for quizzes and homeworks. Good luck!;;;
5674;2;2015-05-03T00:30:18.570;;"Learning curves or bias-variance decomposition are the gold standard for detecting high variance, aka: overfitting.  Separate your data (in your case the ""back data"") into 60% training data and 40% testing data.  Fit the model on the training data as you usually would and see how well it is working with the test data.  Finally, when you think you have the model that you want, split each of the training and test sets into 10-100 subsets and retrain and test with incrementally larger sets.  Apply your favorite performance metric and plot the results of performance vs. the number of cases used for testing and training.  The curves will never come together if the model is overfit (high variance).  The curves will come together but the performance will be lower than desired if the model is underfit (high bias) and the lines will come together at an acceptable performance for a well performing model that is not overfit.Here is an example of overfitting and underfitting with root mean square error as the performance metric: Here is a pretty good link on the process and here is another one.  Hope this helps!";;;
5675;2;2015-05-03T00:58:54.410;;I would suggest playing around with this in either R or Python using Scikit Learn.  The text will take a bit of playing with, but is pretty straightforward to normalize and turn into a feature set.  I suggest applying TFIDF to normalize each text document against the entire corpus.  You will then have lots of text features that you can join with your metadata features to do novelty detection.  Scikit-Learn and it's excellent User Guide are a great resource for getting started in this arena as are Mahout (Java based) and the book Mahout In Action.  There are also countless packages in R to accomplish this.One thing to note is that you seem to suggest specific ways that you want to detect fraud.  This type of hunch based machine learning is known as applying heuristics and tends to perform worse than pure machine learning methods.  You should just focus on using novelty detection algorithms or possibly anomaly detection algorithms and let the statistics find the fraud rather than trying to apply your own intuition.;;;
5676;2;2015-05-03T04:22:43.507;;I have worked on building a fraud detection solution using text mining, so I understand the scenario that has led to this question. I'll talk about the approach/techniques that is to be followed to build the fraud detection solution.I'll divide it into 4 sectionsBuild line of business fraud dictionaryFraud detection & scoringToolsAdditional notesBuilding Fraud Dictionary:As a first step you'll have to identify fraud concepts with help from subject matter expert for the line of business in concern. Fraud concept is a person, characteristic, entity, or event that represents a suspicious scenario and similar concepts can be grouped together to form a concept category. Each concept is further represented by words, phrases, entities etc. The presence of these words/phrases in the document implies occurrence of fraud concept in that transaction. The end result of this exercise will be a fraud dictionary that is a repository of concepts and suspicious key words.For example considering the data set mentioned in the question, the numeric patterns become the concept category, and fictitious invoice numbers and fictitiously-generated transaction amounts are 2 different concepts. There will be a keywords/phrases that is associated to this concept which should be captured onto the fraud dictionary.You can use NLP technique to build the dictionary. Steps:1) Convert the sentence to lowercase2) Remove stopwords, these are common words found in a language. Words like for, very, and, of, are, etc, are common stop words3) Extract n-gram i.e., a contiguous sequence of n items from a given sequence of text. simply increasing n, model can be used to store more context4) Assign a syntactic label (noun, verb etc.)5) Knowledge extraction from text through semantic/syntactic analysis approach i.e., try to retain words that hold higher weight in a sentence like Noun/VerbFraud detection & scoring:Phrases/Keywords to be semantically matched with Phrases/Keywords from Fraud Dictionary and Fraud concepts in transactions can be identified. At this point, care needs to be taken to maintain the context (negation, positive sense) in which a keyword/phrase has been used to avoid false positives.For each identified occurrence of Fraud concept in the transaction, a weight of 1 can be assigned. This is done for calculating a suspicious score for the transaction. Higher the score high suspecious.Note that using a combination of larger data set with high textual content, and extensive fraud dictionary would result in higher number potentially identified.Tools:You can consider using OpenNLP / StanfordNLP for Part of Speech tagging. Most of the programming language have supporting library for OpenNLP/StanfordNLP. You can choose the language based on your comfort. Additional notes:You can refer here to get an idea of extracting concepts from sentencesFruther read my blog on Text mining 101 to learn more about TM process overviewCalculate term weight (TF-IDF)Similarity distance measure (Cosine)Overview of key text mining techniquesHope this helps.;;;
5677;1;2015-05-03T07:11:50.737;How should I create a single score with two values as input?;I have two series of values, a and b as inputs and I want to create a score, c, which reflects both of them equally.  The distribution of a and b are belowIn both cases, the x-axis is just an index.How should I go about creating an equation c = f(a,b) such that a and b are (on average) represented equally in c?Edit: c = (a+b)/2 or c = ab will not work because c will be too heavily weighted by a or b.I need a function, f, where c = f(a,b) and c' = f(a + stdev(a),b) = f(a, b + stdev(b));[education, open-source];72;
5678;2;2015-05-03T09:54:34.857;;"Different domains might require different approaches, the news domain being one of the more elusive ones. News items are extremely short-lived (one, maybe two days): the very items the user viewed two days ago are just not interesting anymore. Also, CF works well if a large amount of items have ""enough"" events (that is, lots of users have viewed/rated them), and depending on the size of your data, most news items just don't collect enough views during their lifetime for classical CF methods to be effective. For this reason, news recommender systems tend to use some combination of content-based filtering (the tags and categories you mention), popularity-based recommendation (this often discarded approach might work well for news), and recency scoring. There is some literature around it, try googling for ""news recommender system"".";;;
5679;1;2015-05-03T14:12:09.000;Understanding text conversion into SVM input;In Support Vector Machines, when used for sentiment analysis, text gets converted into a set of data points. How does this happen, usually?;[education, open-source];102;
5680;2;2015-05-03T18:35:31.813;;using R syntax: c <- a/2+b/2or you can create a function:c <- function(a,b) { return(a/2+b/2) };;;
5681;2;2015-05-03T21:36:27.670;;The problem is that you're converting a large sparse dataset into a dense array via toarray(). Don't do that :) Why do you need it in dense array format? Try posting more of your code to see if we can figure out better ways to solve the problem. With any large dataset, you want to stay sparse as long as possible and only work with smaller sets of dense data (if dense is needed at all).;;;
5682;2;2015-05-04T00:22:47.427;;Text can be converted to data via the use of concept clusters (after stemming and stopping), or to count (frequencies) via use of n-grams.  N-grams are basically tabulations of the 1-gram count (frequency) of alphabet characters (a though z) in each document, and counts of 2-grams (aa to zz), 3-grams (aaa through zzz), up to about 5-grams (aaaaa through zzzzz).  Beyond 5-grams, the data will be sparse and less informative.  Thus, a dataset can be constructed for which rows represent documents, and columns represent n-grams. The data values themselves are the total number of occurrences of each gram found in each document.  FYI - n-grams have proven to be the best technique for identifying different languages based on characters.    Regarding SVMs, focus on the SVM literature.           ;;;
5683;2;2015-05-04T04:29:06.830;;If you're looking for something where A and B are equally represented, consider trying something like Z score normalization (or standard score):c = (a-u_a)/sigma_a + (b-u_b)/sigma_bThat score equally represents the two, but would be on a smaller scale.  It really shouldn't matter since the numbers are arbitrary, however, if you need to scale it up, you could do something like:c2 = (sigma_a+sigma_b)*(c) + u_a + u_b;;;
5684;1;2015-05-04T13:26:04.767;How Mllib in Spark select variables in logistic regression;I have a question about MLlib in Spark.(with Scala)I'm trying to understand how LogisticRegressionWithLBFGS and LogisticRegressionWithSGD work. I usually use SAS or R to do logistic regressions but I now have to do it on Spark to be able to analyze Big Data.How is the variable selection done? Is there any try of different variable combinations in LogisticRegressionWithLBFGS or LogisticRegressionWithSGD? Something like a test of significance of variable one by one? Or a correlation calculation with the variable of interest? Is there any calculation of BIC, AIC to choose the best model?Because the model only returns weights and intercept...How can I understand those Spark functions and compare to what I'm used to with SAS or R ?;[education, open-source];195;
5685;2;2015-05-04T14:59:36.430;;First, the spark programming guide for LogisticRegressionWithSGD recommends using L-BFGS instead, so perhaps focus on the one.  As for variable selection, the model description on the MLLib page for regressions has a nice explanation of how models are constructed and selected, but it does not address variable selection.  This leads me to believe that it considers all variables, and simply chooses the model with the best fit.;;;
5686;2;2015-05-04T19:55:24.797;;That really is a nice question, although once you're Facebook or Google etc., you have the opposite problem: how to reduce the number of features from many billions, to let's say, a billion or so.There really are billions of features out there.Imagine, that in your feature vector you have billions of possible phrases that the user could type in into search engine. Or, that you have billions of web sites a user could visit. Or millions of locations from which a user could log in to the system. Or billions of mail accounts a user could send mails to or receive mails from.Or, to swich a bit to social networking site-like problem. Imagine that in your feature vector you have billions of users which a particular user could either know or be in some degree of separation from. You can add billions of links that user could post in his SNS feed, or millions of pages a user could 'like' (or do whatever the SNS allows him to do).Similar problems may be found in many domains from voice and image recognition, to various branches of biology, chemistry etc. I like your question, because it's a good starting point to dive into the problems of dealing with the abundance of features. Good luck in exploring this area!UPDATE due to your comment:Using features other than binary is just one step further in imagining things. You could somehow cluster the searches, and count frequencies of searches for a particular cluster.In a SNS setting you could build a vector of relations between users defined as degree of separation instead of a mere binary feature of being or not being friends.Imagine logs that global corporations are holding on millions of their users. There's a whole lot of stuff that can be measured in a more detailed way than binary.Things become even more complicated once we're considering an online setting. In such a case you do not have time for complicated computations and you're often left with binary features since they are cheaper.And no, I am not saying, that the problem becomes tractable once it's reduced to a magical number of billion features. I am only saying that a billion of features is something you may end up after a lot of effort in reducing the number of dimensions.;;;
5687;1;2015-05-05T07:12:46.797;How to approach automated text writing?;"What are the tools, practices and algorithms used in automated text writing?For example, lets assume that I have access to wikipedia/wikinews and similar websites API and I would like to produce article about ""Data Science with Python"".I believe that this task should be divided into two segments. First would be text mining and second would be text building. I'm more or less aware how text mining is performed and there are lots of materials about it in Internet. However, amount of materials related to automated text building seems to be lower. There are plenty of articles which says that some companies are using it, but there is lack of details. Are there any common ideas about such text building?";[education, open-source];50;
5688;2;2015-05-05T07:47:45.473;;"You should probably do some reading in the field of ""Natural Language Generation"", since this seems to relate most directly to your question. But the way you have described the process -- ""text mining...text building"" -- leads me to wonder if you are aiming for something much more ambitious.  It seems as though you aim to automate the process of 1) reading natural language texts, 2) understanding the meaning, and then 3) generate new texts based on that semantic knowledge.  I'm not aware of any general-purpose end-to-end systems that can do that, not even specialized systems by the likes of Palantir.  What you are aiming for would probably pass the Turing Test for fully capable Artificial Intelligence.";;;
5689;1;2015-05-05T08:14:29.573;Naive Bayes vs. SVM performance;I've used scikit-learn in Python to compare results of naive Bayes and SVM. I've found that naive Bayes is quicker than SVM. Could anyone shed some light on reasons for such finding?;[education, open-source];66;
5690;1;2015-05-05T10:10:19.463;Classify the coordinate data;I'd like to classify the data on coordinate.Here are 2 example data.data1 = [(1,1), (2,2), (3, 3), (4, 2), (5, 3), (6, 0)]data2 = [(1,1), (2,2), (3, 10), (4, 9), (5, 10), (6, 0)]The bold part have the same wave in above data.The length are all different in my data set.Is there any way that I can find the similar wave in many data like this?Thank you.;[education, open-source];41;
5691;2;2015-05-05T10:32:10.873;;Calcualte distance?Distance = data2 - data1 = [(1,0), (2,0), (3, 7), (4, 7), (5, 7), (6, 0)]And what will you treat as the same wave?Any three (or more) nearby points that have the same distance... ;;;
5693;1;2015-05-05T14:37:25.377;NoSQL engine/service recommendation for geolocation data;"First of all, I am new in this field we call big data, so my questions may be naive. In order to build an application, which deals with geolocation data, which could be : latitude and longitude coordinates and Geography SQL Server column types.I need to have the following elements made easy:Scalability : be prepared to receive huge amount of data, adding servers to the system have to be easyProximity requests : in example, how much points are in a circle (at meter scale). Data must be accessible rapidly after being written. I've been looking around for existing solutions, which are ""Hadoop friendly"" (Hortonworks, Cloudera) and available DBMS, like Cassandra. I have found some interesting information, but I still think it's hard to decide, which one to choose. It also need drivers for NodeJS & .NET (Hadoop with Cassandra seem to be OK with that). I've also looked around the MongoDB ecosystem, but, again, I feel that it is hard to know where to look at. By (little) experience with Mongoose, MongoDB can be disqualified by the third point because data writes are slow. But my model could certainly be improved.Do any of you have some recent experiences, manipulating massive amount of geolocation data? I would appreciate sharing them here as well as any quality and recent literature on the subject.";[education, open-source];111;
5694;1;2015-05-05T17:48:35.433;Dimensionality and Manifold;A commonly heard sentence in unsupervised Machine learning is High dimensional inputs typically live on or near a low dimensional  manifoldWhat is a dimension? What is a manifold? What is the difference? Can you give an example to describe both?Manifold from Google/Wikipedia: In mathematics, a manifold is a topological space that resembles  Euclidean space near each point. More precisely, each point of an  n-dimensional manifold has a neighbourhood that is homeomorphic to the  Euclidean space of dimension n.Dimesion from Google/Wikipedia: In physics and mathematics, the dimension of a mathematical space (or  object) is informally defined as the minimum number of coordinates  needed to specify any point within it.What does the Google/Wikipedia even mean in layman terms? It sounds like some bizarre definition like most machine learning definition?They are both spaces, so what's the difference between a Euclidean space (i.e. Manifold) and a dimension space (i.e. feature-based)? ;[education, open-source];152;
5695;1;2015-05-06T10:26:56.647;Neural networks with non-negative weights;Could you tell me, are there any techniques for building neural networks with non-negative weights?;[education, open-source];51;
5696;2;2015-05-06T11:48:06.327;;The differences in speed between Naive Bayes and SVM simply boils down to the formulation and the assumptions of each model, and has little to do with the particular library or implementation. Not only is naive bayes a simple probabilistic classifier, it also makes an additional assumption of independence between its features, so that parameter estimates can be calculated independently and thus possibly very quickly.In comparison, with SVM, every single record in the data base will require a computation of the distance function to determine the optimal decision boundary. We can observe that for SVM, we have a complexity of $O(nfeatures \times nobservations^2)$ or more depending on the choice of SVM and kernel. On the other hand, for naive bayes we would have a complexity of $O(nfeatures \times nobservations)$. Hence in general, we can say that SVM will take longer than Naive Bayes. ;;;
5697;2;2015-05-06T12:25:24.843;;"One possible method to approach neural networks with non-negative weights is using Feedforward neural network. We can construct the neural network using optimization techniques with the constraint of non-negative weights, as opposed to the normal back propagation method.A modified Matlab example taken for here is as follows:load iris_dataset% Number of neuronsn = 4;% Number of attributes and number of classifications[n_attr, ~]  = size(irisInputs);[n_class, ~] = size(irisTargets);% Initialize neural networknet = feedforwardnet(n);% Configure the neural network for this datasetnet = configure(net, irisInputs, irisTargets); %view(net);fun = @(w) mse_test(w, net, irisInputs, irisTargets);% Add 'Display' option to display result of iterationsps_opts = psoptimset ( 'CompletePoll', 'off', 'Display', 'iter', 'MaxIter', 100); %, 'TimeLimit', 120 );% There is n_attr attributes in dataset, and there are n neurons so there % are total of n_attr*n input weights (uniform weight)initial_il_weights = ones(1, n_attr*n)/(n_attr*n);% There are n bias values, one for each neuron (random)initial_il_bias    = rand(1, n);% There is n_class output, so there are total of n_class*n output weights % (uniform weight)initial_ol_weights = ones(1, n_class*n)/(n_class*n);% There are n_class bias values, one for each output neuron (random)initial_ol_bias    = rand(1, n_class);% starting valuesstarting_values = [initial_il_weights, initial_il_bias, ...                   initial_ol_weights, initial_ol_bias];% alter the patternsearch function with the appropriate constraints, in your case we would change it so that the lower bounds of the weights are zero [x, fval, flag, output] = patternsearch(fun, starting_values, [], [],[],[], zeros(size(starting_values)), 1e10, ps_opts); where the mse_test.m function is as follows:function mse_calc = mse_test(x, net, inputs, targets)net = setwb(net, x');y = net(inputs);[row col] = size(y);mse_calc = sum(sum((y - targets).^2))/(row * col);end";;;
5698;2;2015-05-06T12:56:09.333;;" What is a dimension?To put it simply, if you have a tabular data set with m rows and n columns, then the dimensionality of your data is n What is a manifold? The simplest example is our planet Earth. For us it looks flat, but it reality it's a sphere. So it's sort of a 2d manifold embedded in the 3d space.  What is the difference? To answer this question, consider another example of a manifold: This is so-called ""swiss roll"". The data points are in 3d, but they all lie on 2d manifold, so the dimensionality of the manifold is 2, while the dimensionality of the input space is 3.There are many techniques to ""unwrap"" these manifolds. One of them is called Locally Linear Embedding, and this is how it would do that:Here's a scikit-learn snippet for doing that:from sklearn.manifold import LocallyLinearEmbeddinglle = LocallyLinearEmbedding(n_neighbors=k, n_components=2)X_lle = lle.fit_transform(data)plt.scatter(X_lle[:, 0], X_lle[:, 1], c=color)plt.show()";;;
5699;2;2015-05-06T13:33:46.497;;You can easily take an existing Multilayer Perceptron implementation and modify the backpropagation algorithm to prevent weights from becoming negative. Of course, you would also need to make sure you initialize weights with only non-negative values.But you need to be aware that you are limiting the decision surfaces that the neural network can learn. So you should consider whether a neural network with only non-negative weights can provide an acceptable solution for your given data set.;;;
5700;2;2015-05-06T18:27:39.693;;"The dimensionality of a dataset is the number of variables used to represent it. For example, if we were interested in describing people in terms of their height and weight, our ""people"" dataset would have 2 dimensions. If instead we had a dataset of images, and each image is a million pixels, then the dimensionality of the dataset would be a million. In fact, in many modern machine learning applications, the dimensionality of a dataset could be massive. When dimensionality is very large (larger than the number of the samples in the dataset), we could run into some serious problems. Consider a simple classification algorithm that seeks to find a set of weights w such that when dotted with a sample x, gives a negative number for one class and a positive number for another. w will have a length equal to the dimensionality of the data, so it will have more parameters than there are samples in the entire dataset. This means that a learner will be able to overfit the data, and consequently won't generalize well to other samples unseen during training. A manifold is an object of dimensionality d that is embedded in some higher dimensional space. Imagine a set of points on a sheet of paper. If we crinkle up the paper, the points are now in 3 dimensions. Many manifold learning algorithms seek to ""uncrinkle"" the sheet of paper to put the data back into 2 dimensions. Even if we aren't concerned with overfitting our model, a non-linear manifold learner can produce a space that makes classification and regression problems easier.";;;
5701;1;2015-05-06T21:52:55.810;Lightweight data provenance tool;One of the problems I often encounter is that of poor data provenance.When I do research I continuously make modifications to my code and rerun experiments. Each time I'm faced with a number of questions, such as: do I save the old results somewhere, just in case? Should I include the parameter settings in the output filenames or perhaps save them in a different file? How do I know which version of the script was used to produce the results?I've recently stumbled upon Sumatra, a pretty lightweight Python package that can capture Code, Data, Environment (CDE) information that can be used to track data provenance. I like the fact that it can be used both from the command line and from within my Python scripts and requiring no GUI. The downside is that the project seems inactive and perhaps there's something better out there.My question is: what is a good lightweight data provenance solution for my research? I'm coding small projects mostly in Python in the terminal on a remote server over SSH, so a command line solution would be perfect for me.EDIT: I have stuck with Sumatra. When I posted this question I didn't look into the web interface yet, but that turns out to be a unique selling point. It displays a very detailed overview of the experiments, capturing not only the state of the data and code, but also the Python environment (package dependencies and versions!) and platform information (architecture and kernel version!).EDIT: I've updated the subject of my question to emphasize that I'm mostly concerned about  provenance.;[education, open-source];54;
5702;2;2015-05-06T23:01:53.847;;Yes, you should save result files before you make major mods to the code.  Disk space is cheap, so you're unlikely to run into issues unless your results sets are prolific.  I would suggest storing old results sets with folder names that include a time stamp of when they were generated.As far as time shots of your code, using github (or some other code repository tool) is as easy as can be, will save version information, allows for collaboration, and is an all around great way to backup and version your code. The combination of these two things, you'll effectively have an easy way of mapping a result set to a specific version of the code.;;;
5703;1;2015-05-07T00:16:08.680;Does the ontology in the Semantic Web is dead?;"Is the Semantic Web dead? Are ontologies dead?I am developing a work plan for my thesis about ""A knowledge base through a set ontology for interest groups around wetlands"". I have been researching and developing ontologies for it but I am still unclear about many things. What is the modeling language for ontologies?Which methodology for ontologies is better? OTK or METHONTOLOGY?Is there any program that does  as does Cratilo is a software for analyzing of textual corpora and for extraction of specific terms of the domain of study (it is developed by professors Jorge Antonio Mejia, Francisco Javier Alvarez and John Albeiro Sánchez, Institute of Philosophy the University of Antioquia). It enables lexical analysis of texts, identifying the words that appear their frequency and location in the text. Through a process of recognition, Cratylus identifies all the words in the text and builds a database becomes the draft analysis of the work. Are there other similar tools?Can the terms found by Cratilo be used to create a knowledge base?What are the existing open semantic frameworks that can be used for such things? Is there software that automatically creates RDF, OWL, and XML? How does Tails work? Jena? Sesame? ";[education, open-source];69;1
5704;2;2015-05-07T03:52:08.877;;One way of doing dimensional reduction is to do feature hashing.  This was known about in the 1960's.  So for example if your data is a sparse set of points in 3 dimensions (x,y,z) you create a (good) hash function h(x,y,z).  You can use that of course for a hash table or a Bloom filter lookup. This is a good form of data compression.  I don't know why the AI community doesn't use it.  It is much more to the point than a neural net. ;;;
5705;2;2015-05-07T04:05:13.373;;For sure you need to learn some maths.  However you should also make an effort to gain some broader engineering and science skills.  There are far too many people going into computer science and all they know is a few programming languages and math. The end result is a very boring person with little in the way of creativity to do anything new. Take a year out when you are 18 or 19 to travel the world.   ;;;
5706;1;2015-05-07T04:11:56.600;"What is the ""dying ReLU"" problem in neural networks?";"Referring to the Stanford course notes on [Convolutional Neural Networks for Visual Recognition][1], a paragraph says: ""Unfortunately, ReLU units can be fragile during training and can  ""die"". For example, a large gradient flowing through a ReLU neuron  could cause the weights to update in such a way that the neuron will  never activate on any datapoint again. If this happens, then the  gradient flowing through the unit will forever be zero from that point  on. That is, the ReLU units can irreversibly die during training since  they can get knocked off the data manifold. For example, you may find  that as much as 40% of your network can be ""dead"" (i.e. neurons that  never activate across the entire training dataset) if the learning  rate is set too high. With a proper setting of the learning rate this  is less frequently an issue.""What does dying of neurons here mean? Could you please provide an intuitive explanation in simpler terms.";[education, open-source];121;1
5707;1;2015-05-07T06:22:51.470;Time Complexity notation in Big Data platforms;I am redesigning some of the classical algorithms for Hadoop/MapReduce framework. I was wondering if there any established approach for denoting Big(O) kind of expressions to measure time complexity?For example, hypothetically, a simple average calculation of n (=1 billion) numbers is O(n) + C operation using simple for loop, or O(log) I am assuming division to be a constant time operation for the sake for simplicity. If i break this massively parallelizable algorithm for MapReduce, by dividing data over k nodes, my time complexity would simply become O(n/k) + C + C'. Here, C' can be assumed as the job planning time overhead. Note that there was no shuffling involved, and reducer's job was nearly trivial.I am interested in a more complete analysis of algorithm with iterative loops over data and involve heavy shuffling and reducer operations. I want to incorporate, if possible, the I/O operations and network transfers of data.;[education, open-source];24;1
5708;1;2015-05-07T08:52:40.293;Which classification algorithms to try for classifying text data into 300 categories;I have 40000 rows of text data of health care domain. Data has one column for text (2-5 sentences) and one column for its category.I want to classify that into 300 categories. Some categories are independent while some are somewhat related. Distribution of data among categories is not uniform either i.e some of the categories(around 40 of them) have less data about 2-3 rows.I am attaching log probablity of each class/categories. (OR distribution of classes) here.;[education, open-source];138;2
5709;2;2015-05-07T09:09:39.333;;"I think Social Media Mining: An Introduction by Zafarani et. al. is an excellent starting point. You can find more about it here. Also a free PDF version is available.It first goes through the essentials in graph theory and data mining. It covers some more advanced topics in graph mining, social network analysis, recommendation systems, etc.Besides, I have seen some online courses in coursera (example). I am not sure about their quality though.Finally, note that social network analysis is data mining for the social media data like Facebook. It is not social science at all; it is computer science. While you may end up borrowing some ideas from them, what you will end up doing is far from what social science guys are doing. So, going through social science courses and books is likely not a good idea at this point.P.S. The book is the text book for the social media mining course offered in my university.";;;
5710;1;2015-05-07T13:23:47.440;Why does logistic regression in Spark and R return different models for the same data?;I've compared the logistic regression models on R (glm) and on Spark (LogisticRegressionWithLBFGS) on a dataset of 390 obs. of 14 variables.The results are completely different in the intercept and the weights.How to explain this?Here is the results of Spark (LogisticRegressionWithLBFGS) :model.intercept  :  1.119830027739959model.weights : GEST    0.30798496002530473 DILATE  0.28121771009716895 EFFACE  0.01780105068588628 CONSIS -0.22782058111362183 CONTR  -0.8094592237248102 MEMBRAN-1.788173534959893 AGE    -0.05285751197750732 STRAT  -1.6650305527536942 GRAVID  0.38324952943210994 PARIT  -0.9463956993328745 DIAB   0.18151162744507293 TRANSF -0.7413500749909346 GEMEL  1.5953124037323745Here is the result of R :             Estimate Std. Error z value Pr(>|z|)   (Intercept)  3.0682091  3.3944407   0.904 0.366052    GEST         0.0086545  0.1494487   0.058 0.953821    DILATE       0.4898586  0.2049361   2.390 0.016835 *  EFFACE       0.0131834  0.0059331   2.222 0.026283 *  CONSIS       0.1598426  0.2332670   0.685 0.493196    CONTR        0.0008504  0.5788959   0.001 0.998828    MEMBRAN     -1.5497870  0.4215416  -3.676 0.000236 ***   AGE         -0.0420145  0.0326184  -1.288 0.197725    STRAT       -0.3781365  0.5860476  -0.645 0.518777    GRAVID       0.1866430  0.1522925   1.226 0.220366    PARIT       -0.6493312  0.2357530  -2.754 0.005882 **  DIAB         0.0335458  0.2163165   0.155 0.876760    TRANSF      -0.6239330  0.3396592  -1.837 0.066219 .  GEMEL        2.2767331  1.0995245   2.071 0.038391 *  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1;[education, open-source];318;1
5712;2;2015-05-07T15:34:57.470;;A quick glance at the docs for LogisticRegressionWithLBFGS indicates that it uses feature scaling and L2-Regularization by default. I suspect that R's glm is returning a maximum likelihood estimate of the model while Spark's LogisticRegressionWithLBFGS is returning a regularized model estimate. Note how the estimated model weights of the Spark model are all smaller in magnitude than those in the R model.I'm not sure whether or not glm in R is implementing feature scaling, but this would also contribute to different model values.;;;
5713;2;2015-05-07T17:31:42.180;;There are too many too general problems in your post.We're definitely in an AI summer era right now (as opposed to AI winter), and the research on Semantic Web receives less attention.Still, there are many projects related to building ontologies. Google has Knowledge Graph and Knowledge Vault. Both of these are using Freebase (among other sources).There are dozens of links I can give you to answer some of your questions, but the best thing you can do is to browse W3C Semantic Web pages.Take a look at RDF, SPARQL, OWL, Virtuoso, Protege - these are de facto standards.In terms of extracting ontologies from textual corpora - there are various tools out there. Neither of them is perfect, so you really have to do some research and find something that suits your needs. For example, there's the OntoLearn Reloaded (this paper is relatively new, so you can check out the bibliography to seek out for other approaches).;;;
5714;1;2015-05-07T19:54:35.190;HiveQL stats raw vs totalSize;"When running ANALYZE TABLE Table1 PARTITION(units) COMPUTE STATISTICS;I recieve the following:stats: [numFiles=25, numRows=56351355, totalSize=291119418, rawDataSize=6649459890]What is the difference between totalSize and rawDataSize?Obviously there are more partitions I assume I just need to add one of these stats for each partition but what is rawDataSize and totalSize?";[education, open-source];21;
5715;1;2015-05-07T20:08:05.440;Performance profiling and tuning in Apache Spark;Beyond the immediate suspects defined in the spark documentation, what are some ways to profile, tune and boost performance of an Apache Spark application? ;[education, open-source];109;1
5716;2;2015-05-07T20:36:54.033;;(While I think that your question is usually not regarded as a good question for this site, I think that it should be preserved because almost everyone new to the field is similarly confused at first and it's hard to find straight, sober, and balanced information about it.)Is the Semantic Web dead? Are ontologies dead?Many people moved away from the Semantic Web. On the other hand, there are still many using it. There's always been a lot of confusion about where its value lies. There are use cases that genuinely benefit from the semantic web technologies but they are quite few and far between.Linked Open DataThat's partially why the subfield of Linked Open Data was created. You can see Linked Data as a more pragmatic stripped down version of the Semantic Web (or as a necessary enabler of the grander semantic web vision).Linked Data, while more pragmatic, still uses ontologies. It is just not so uptight about using OWL and designing your ontologies first and in a very formal way.A knowledge base such as Freebase (now being replaced by Wikidata) doesn't even build on Semantic Web technologies (it's always been available also in the form of RDF dumps though). On the other hand, even Freebase builds on concepts similar to those in the Semantic Web and it does have a form of ontologies.Software to generate ontologiesThere is no software that would create high-quality ontologies automatically for you. At least not in the sense of OWL/first-order logic ontologies. On the other hand, many kinds of software, including Cratilo mentioned by you, can help you to build a lower step in the semantic spectrum such as a glossary or maybe even a folksonomy which can later be transformed into more of a taxonomy or ontology by other algorithms. There might be even software to create fullblown OWL ontologies but I think that's still rather an active area of research.Ontologies and methodologiesAs always, it depends on what exactly your needs and goals are. In fields such as biomedicine and life sciences in general, they create complex formal ontologies because they have data with highly varied structure and ontologies help them keep things organized and usable. In contrast, an eshop might be better off sticking with normal relational database modelling and adding only global identifiers (URIs) in the spirit of Linked Data if they want to build a proper knowledge graph later. Even if you need ontologies you might want to skip the methodologies first. Especially if you are really new to the field and have little idea what your needs are.Jena, Sesame, ...Jena, Sesame, Virtuoso, etc. are triplestores - they are used to store and query RDF. Most ontologies can be represented in RDF. Even those written in OWL. RDF has the semantic part, RDF/S, which allows you to formulate some basic ontologies. That might well be enough for start. Even RDF/S can get confusing and convoluted when you start to think about blank nodes, named graphs (are they fixed? are they dynamic?), etc.While I think that your question is usually not regarded as a good question for this site, I think that it should be preserved because almost everyone new to the field is similarly confused at first and it's hard to find straight, sober, and balanced information about it.;;;
5717;1;2015-05-07T21:41:25.300;Where is the cost parameter C in the RBF kernel in SVM?;RBF kernel using SVM depends on two parameters C and gamma. If the equation of the kernel RBF as the following:$K(X,X')= \exp(\gamma||X-X'||^2)$In the equation I can see where can I use gamma, but I can't find the C parameter.So, can enybody tell me please?Thanks in advance,;[education, open-source];65;
5718;1;2015-05-07T21:53:32.540;Add Custom Labels to NLTK Information Extractor;I am working on an information extractor specifically purposed with parsing relationships between entities such as movies, directors, and actors. NLTK appears to provide the necessary tools to construct such a system. However, it is not clear how one would go about adding custom labels (e.g. actor, director, movie title).Similarly, Chapter 7 of the NLTK Book discusses information extraction using a named entity recognizer, but it glosses over labeling details.So, I have two questions:How would I add custom labels?If I have bare lists of relevant named entities (e.g. movies, actors, etc.), how can I include them as features? It appears that I would need to use IOB format, but I am unsure about how to do this when I only have lists of named entities.;[education, open-source];63;
5720;2;2015-05-08T00:32:02.250;;The C parameter in SVMs doesn't have to do anything with the kernel function. C is the penalty associated to the instances which are either misclassified or violates the maximal margin. As you may know already, SVM returns the maximum margin for the linearly separable datasets (in the kernel space).It might be the case that the dataset is not linearly separable. In this case the corresponding SVM quadratic program is unsolvable. To make it solvable, we redefine the quadratic program to return the maximum margin with respect to some error cost C for the points violating the maximum margin (misclassified samples also violate the maximum margin). Below you can see a simple example I found on the Internet. This uses linear kernel but the story is the same for other non-linear kernels as well.The effect C has on the classification result is shown in the picture below:As you can see, if you set the error penalty to be very high, you will end up with less misclassifications and fewer support vectors (the highlighted points are SVs). However, if you set it too high when you are using non-linear kernels (specially Gaussian) you might overfit.If you are interested in the theory behind it, here is what we are trying to optimize:If we write its dual, we'll get:See how convenient it is! No C in the kernel function!In other words, you are mapping the samples to a higher dimensional space using the kernel function, then solving it using some penalty term C.;;;
5722;2;2015-05-08T03:41:54.230;;"This question isn't terribly clear.  Data analysis and strategic modeling (game theory) are different tasks.  Nash equilibrium is a way of understanding the incentives they have by assuming a set of players with assumed utility function and making deductive inferences about what they ought to do to maximize those utility functions given their interaction.  Data analysis is an inductive process.  There are a number of ways game theory and data analysis might interact, here are the easy top two: Someone might use data to infer players' utility functions (I'm sure this exists in econometrics-land somewhere; also, political scientists have a technique called ""ideal point estimation,"" to infer political preferences from voting behavior---which you can easily google to learn more);Someone might use game theory to generate behavioral predictions which are testable by data.  Thinking about the specific kinds of cases you mention, the obvious application would be in the stock market one.  Suppose you have a ML model that can reliably predict the market behavior of other people at time T from a given feature set.  Then the consumer of the ML model might have an optimal purchase at T-1, and finding that optimal purchase is going to be strategic.  But combining the two approaches might just break the ML. This is really interesting to think about... musing out loud...Consider the simple case of a two-player market in one stock. Player 1 wants to buy at T-1 if player 2 will be buying at T (because the price will go up); player 1 wants to sell at T-1 if player 2 will sell at T (because the price will go down).  The naive approach for player 1 is ""use my ML model to predict what player 2 will do, then do it first at T-1."" But, of course, P1's behavior at T-1 is itself observable by P2, and changes P2's behavior (the price has gone up); moreover, by definition P1's behavior at T-1 can't be a feature of the ML model used to predict P2's behavior at T, because it's behavior that is chosen on the basis of the ML prediction.  All sorts of fun puzzles begin here, but none of them look real good...";;;
5723;1;2015-05-08T03:47:00.857;In Weka, how to draw learning curve evaluated on both test and training set?;This is just for finding overfitting gap.After initial research, I can only find method to draw learning curve using evaluation of test set.  However, I could not evaluate on training set and over the two learning curves.;[education, open-source];39;
5724;2;2015-05-08T04:27:17.680;;When class-conditional distributions are gaussian with equal covariance matrices, the optimal decision boundary is a hyperplane.  This is the core concept behind Linear Discriminant Analysis (LDA).For any data point $x$, the probability that $x$ comes from class $\omega_1$ is: $P(x|\omega_1) \sim N(\mu_1,\Sigma) = (2\pi)^{-1}|\Sigma|^{-1/2}\exp\left\{ (-1/2) (x-\mu_1)'\Sigma^{-1}(x-\mu_1)\right\}$Similarly, $P(x|\omega_2) \sim N(\mu_2,\Sigma)$.Let's denote the prior probability of class 1 and 2 as $P(\omega_1)$ and $P(\omega_2)$, respectively.Equal Misclassification Cost:If the cost of misclassification is equal, we want to assign new data points such that the probability of misclassification is minimized. This decision rule assigns points to class 1 when $x$ satisfies:$P(x|\omega_1)P(\omega_1) > P(\omega_2)(P(x|\omega_2)$We can establish a similar rule for assigning points to class 2. o find the decision boundary, we need to find the values of $x$ that satisfy$P(x|\omega_1)P(\omega_1) = P(x|\omega_2)P(\omega_2)$The values of $x$ that satisfy this equality here lie on a line (or hyperplane for higher dimensions).Unequal Misclassification Cost:For the second question, you will need an additional term.  Let's call $C(\omega_1)$ the cost of making an error when the data point was actually from class 1 and $C(\omega_2)$ the cost of making an error when the data point was from class 2.To minimize the cost of an error, your new decision rule would be to assign $x$ to class 1 when $x$ satisfies:$P(x|\omega_1)P(\omega_1)C(\omega_1) > P(x|\omega_2)P(\omega_2)C(\omega_2)$As before, to find the decision boundary, you need to solve for $x$ when:$P(x|\omega_1)P(\omega_1)C(\omega_1) = P(x|\omega_2)P(\omega_2)C(\omega_2)$This decision boundary will still be a line / hyperplane, however it may have a different offset or orientation from the solution in the case of equal misclassification cost.;;;
5725;1;2015-05-08T05:27:14.263;Datasource for regression model prediction : Machine Learning;I am trying to work using Amazon machine learning, but the data set that I have is small. The model I want to build is for regression based predictions and the domain I am aiming for the data set to belong is financial, say product price prediction, price and demand prediction based on macro/micro economic factor.I am looking for a data set that contains factors that lead to variations in value of a product or commodity. For example, I would like to predict the value of 1 unit of polyester yarn after 1 year.  The factors which influence the yarn price are say - prices of crude oil, GDP of country,figures of IIP, inflation etc. So I would like a data set that contains the quotes of these factors on which the final price depends.I find difficulty assembling this data myself because I don't know all the factors that contribute to a certain predictive price. Does anyone know of a dataset I can start with that sounds like it might contain these factors?;[education, open-source];47;
5726;2;2015-05-08T06:57:00.863;;I have a very limited knowledge of game theory, but hope to learn more. However, I think that potential applications of Nash equilibrium in the context of big data environments, implies the need of analyzing a large number of features (representing various strategic pathways or traits) as well as large number of cases (representing significant number of actors). Considering these points, I would think that complexity and, consequently, performance requirements for Nash equilibrium in big data applications grow exponentially. For some examples from the Internet load-balancing domain, see paper by Even-Dar, Kesselman and Mansour (n.d.).The above-mentioned points touch only the volume aspect of 4V big data model (an update of Gartner's original 3V model). If you add to that other aspects (variety, velocity and veracity), the situation seems to become even more complex. Perhaps, people with econometrics background and experience will have some of the most comprehensive opinions on this interesting question. A lot of such people are active on Cross Validated, so I will let them know about this question - hopefully, some of them will be interested to share their view by answering this question.ReferencesEven-Dar, E., Kesselman, A., & Mansour, Y. (n.d.). Convergence time to Nash equilibria. Retrieved from http://www.tau.ac.il/~mansour/course_games/nash-load.pdf;;;
5729;2;2015-05-08T17:32:38.147;;In general, a decent starting point for problems like these is Naive Bayes (NB) classification using a simple bag of words model. Here are some slides describing NB as applied to natural language processing. There's nothing especially fancy about this approach, but it's pretty easy to implement and will give you a starting point to expand from.Once you've found some initial results assuming independence among your features and your output labels, you'll probably have a better sense of where the model is weak.  From that point forward you can apply some feature engineering (maybe TF-IDF) as well as some post processing to deal with samples that get assigned to related categories.;;;
5730;1;2015-05-09T05:44:50.633;Finding user similarities within informal data sets;I'm new to all this and am putting together a learning project. I've decided on finding similarities between users in a data set such as http://en.wikipedia.org/wiki/Enron_Corpus. After doing a bit of research, I also came across Dataset for Named Entity Recognition on Informal Text. So I'm not short of data or a goal, I need to understand high-level techniques to get there.One valuable comment noted that this question appears too broad. What I was hoping to find with this question was the breadth of techniques I should focus research on, not answers that are immediately implementable. Please consider vague answers as entirely appropriate!!Expanding on the goal, I am hoping to discover which authors might have affinity toward each other, or conversely do not care much for each other. So I will definitely need to start with Named Entity Recognition and build a means to organize the documents against those entities. Beyond that, I am not so sure.What high level concepts should I be looking at? Thanks!;[education, open-source];65;
5731;2;2015-05-09T08:01:05.720;;There are multiple datasets available in the internet but you need to pay for most of them.One of the best datasets for starting (and it is free) is the Quantquote Free dataset. You can download it from here. This is the description (borrowed from their website): This collection of daily resolution data goes back to 1998 for all symbols currently active in the S&P500. It is updated quarterly, the last update was 07/31/2013.;;;
5732;1;2015-05-09T10:52:56.290;Creating Strings corresponding to Location Co-ordinates;What are some Python libraries which can convert a (X,Y) tuple to strings? (1.23,4.56) yields strings “1_4”, “12_45”, “123_456”.;[education, open-source];16;
5733;2;2015-05-09T11:21:34.607;;Your question has nothing to do with NLP or text-mining (as you claim in the attached tags), or data science in general. It's a pure programming question best suitable for StackOverflow.Moreover, you don't really need any libraries to do, what you want to do. A simple function will do. NOTE: I am using map and reduce functions on purpose to include at least a little bit of data science-related stuff (WINK):coord = (1.23, 4.56)def get_coord(coord):   coord_str = map(lambda x: str(x).replace('.', ''), coord)   for level in range(1, len(coord_str[0])+1):      yield reduce(lambda x, y: (x[:level]+'_'+y[:level]), coord_str)print list(get_coord(coord))Running this code will result in printing:['1_4', '12_45', '123_456']You're not discussing any corner cases in your question, so I assume there are none.;;;
5734;2;2015-05-09T14:25:59.330;;"A ""dead"" ReLU always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights.In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane outside of all possible input data.Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ""Leaky"" ReLUs with a small positive gradient for negative inputs (y=0.01x when x < 0 say) are one attempt to address this issue and give a chance to recover.The sigmoid and tanh neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.";;;
5735;1;2015-05-09T21:57:21.073;Library/package/tool for geographical data visualizations;I am looking for a good free existing tool which visualizes geographical data (let's say in the form of coordinates) by plotting them on a map. It can be a library (see this question on StackOverflow, which suggests a Python library called basemap, which is interesting but not dynamic enough, namely it does not allow for interactivity) or a complete toolkit. Existing things I found are oriented towards realizing web pages, which are not my ultimate goal (see Exhibit or Modest Maps). I'd like something to feed with data which spits out an interactive map where you can click on places and it displays the related data. ;[education, open-source];43;
5736;2;2015-05-10T01:42:25.457;;Assuming you want your visualizations to be available on the internet, there are many options for this:Google Maps (and Google Fusion Tables)Mapbox (and the related LeafletJS)CartoDBD3It really comes down to exactly what you want to do and how much control you want to have over the map.  Additionally you should consider what sort of interactivity you need.  Google Maps is probably the most user friendly for very basic map functions but is somewhat limited in what you can do stylistically.  Mapbox and CartoDB are both user friendly and offer good options for styling and displaying different varieties of data.  However, they also both tend to require subscription fees for anything other than small maps. Also, the last time I checked, CartoDB explicitly handles animation and time-series data where Mapbox does not.D3 will probably give you the most control over display, animation, and interactivity but also has a long learning curve. Even if you don't want the map to be available on the internet, this is still a very good tool for making interactive visualizations that run in the browser.If you don't care as much about the visualizations being online, you can get a lot of work done in open GIS software like QGIS or GrassGIS, though I don't know if user interactivity is really an option there.As I said though, it really comes down to the specifics of exactly what you're trying to do and how comfortable you are with various aspects of mapping and coding.;;;
5737;1;2015-05-10T09:58:56.670;What kind of research can be done with an email data set?;"I found a data set called Enron Email Dataset. It is possibly the only substantial collection of ""real"" email that is public. I found some prior analysis of this work:A paper describing the Enron data was presented at the 2004 CEAS conference.Some experiments associated with this data are described on Ron Bekkerman's home pageParakweet has released an open source set of Enron sentence data, labeled for speech acts.Work at the University of Pennsylvania includes a query dataset for email search as well as a tool for generating spelling errors based on the Enron corpus. I'm looking for some interesting current trend topics to work with.please give me some suggestions.";[education, open-source];138;
5738;2;2015-05-10T10:03:20.360;;Once you have your own lists of named entities, and you're only interested in extracting the relations, I believe there are simpler solutions (although I never tried relation extraction in NLTK, so I might be wrong):ReVerb - a tool written in Java. Once it produces the results, you can simply keep the rows, where your labels are present as objects of the relation.OpenIE - the successor of ReVerb (also written in Java). The authors claim better perfomance, and the output might be more informative.IEPY - a relation extraction tool in Python. You should be able to provide your own labels/named entities using gazetees.MITIE - this library has bindings in Python, and it offers relation extraction functionality.;;;
5741;1;2015-05-10T11:49:07.233;When to Perform non-linear dimension reduction;When do we feel need to go through non-linear transformation like kernel PCA ? Please share an example;[education, open-source];49;
5742;2;2015-05-10T16:14:00.560;;Kaggle has a short summary of applications:http://www.kaggle.com/wiki/DataScienceUseCasesRevolution Analytics published many general case studies, datasheets, and white papers:http://www.revolutionanalytics.com/resources/case-studies?type[0]=case_study&keys=For applications in sciences and engineering, you can consult Nutonian case studies:http://www.nutonian.com/industries/Analyx told potential clients about applications in commerce:http://www.analyx.com/en/company/references/cases.aspxThe Financial Times published a collection of stories about business applications of big data:http://im.ft-static.com/content/images/e91a32d0-2bac-11e3-bfe2-00144feab7de.pdf.McKinsey outlined applications back in 2011:http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovationOther consulting firms made similar reports.Gartner created Hype Cycle for Big data:Not to mention the case studies and white papers by other companies that want to promote their products.;;;
5743;2;2015-05-10T18:49:29.037;;"I haven't seen this mentioned, but it's important to keep in mind that you may see a decrease in salary. I say this without knowing how much you make, but moving from (I assume) an experienced IT professional to an entry level data scientist level may not earn you as much.Here's a link to the a portion of the 2015 Burtch Works study on Data Science salaries:http://www.burtchworks.com/files/2015/05/DS-2015_Changes-in-Base-Salaries.pdfAs you can see, the median salary for level 1 individual contributors is 90k (across the nation). The full report has the breakdown based on region but again, assuming you're an experienced IT professional, you're probably making more than that.Anecdotal story with n=1: One of my classmates in my DS masters program was an experienced Java developer with a house, family, etc. Although he was very interested in data analytics (paid for the program out of pocket) his potential salary doing data analytics wouldn't be able to support the lifestyle he currently had as a Java developer. As a result he essentially ""wasted"" his degree and went back to development. I would really hate to see that happen to more people.";;;
5744;2;2015-05-10T21:12:49.860;;"There are good/fast ways to model graphs in RDBMS, and dumb/slow ways. Some use clever indexing and Stored Procs, trading CPU load and tuned temp tables on RAM disks for faster graph retrieval speed.Some use precomputed graph paths (this may be less feasible in social network scenario, but in a tree with majority of nodes being leaf nodes, it's a pretty good tradeoff space-for-timeSome simply compute in a loop, using un-tuned in-indexed temp table. From the #s thrown in the article, that smells like what they did (30 second- performance on fairly smallish data-set)For example, I have my own tree computation.It is encapsulated in a highly-tuned stored procWhile it's running in an enterprise-sized-hardware Sybase ASE15 dataserver, that server is shared with a couple terabytes of data from all other enterprise apps, some much more data hungry than mine; and isn't dedicated solely to executing my queries.I did not have access to the main speedup tool, a temp table on a RAM disk.A representative set of data I was retrieving that seems to somewhat match theirs was getting a 150,000 node subtree out of 2.5M node full forest dataset (unlimited depth of tree, which varies between 5 and 15, but smaller average arity of a given node than the 50 friends listed in the experiment) I tuned it to the point that this query ~30-45 seconds. It most certainly does NOT exhibit the exponential slowdown that the figures in the question seem to indicate on their RDBMS performance, which is extra double strange given there is no exponential growth in the result set (which to me reeks of un-tuned index on a temp table from personal experience).So, this comparison is most likely incorrect and based on poor RDBMS side design, although as the previous answer noted, it is impossible to ascertain without them open sourcing 100% of their code and table definitions.";;;
5745;2;2015-05-11T04:21:20.767;;The following are some research that can done on e-mail dataset:linguistic analysis to abbreviate an email messageCategorize e-mail as spam/ham using machine learning techniques.identifying concepts expressed in a collection of email messages, andorganizing them into an ontology or taxonomy for browsing;;;
5746;1;2015-05-11T06:53:52.973;Fast k-means like algorithm for 10^10 points?;"I am looking to do k-means clustering on a set of 10-dimensional points.  The catch: there are 10^10 points.  I am looking for just the center and size of the largest clusters (let's say 10 to 100 clusters); I don't care about what cluster each point ends up in.  Using k-means specifically is not important; I am just looking for a similar effect, any approximate k-means or related algorithm would be great (minibatch-SGD means, ...).  Since GMM is in a sense the same problem as k-means, doing GMM on the same size data is also interesting.At this scale, subsampling the data probably doesn't change the result significantly: the odds of finding the same top 10 clusters using a 1/10000th sample of the data are very good.  But even then, that is a 10^6 point problem which is on/beyond the edge of tractable.  ";[education, open-source];184;
5747;1;2015-05-11T11:14:21.043;"Different ""end of study"" times for different cohorts - Cox PH model in survival analysis";"I have a dataset with 4 cohorts of about the same size (~700 people each). I'm trying to apply a Cox PH model using the time needed to pass a very difficult exam as my ""time"" variable. The cohorts differ because they are different classes (class of 2009, class of 2010, 2011 and 2012). These are clearly also the time they enter in the study.All times are censored after 2013. Is there any way of accounting for the fact that the ""time of study"" is different for each cohort? I was thinking of stratifying for cohorts, but the coefficients would clearly be negative and would decrease faster and faster. This however would be due not to a real decrease in the hazard, but to the fact that the latter cohorts are monitored for less time. ";[education, open-source];17;
5749;2;2015-05-11T12:49:42.757;;"This is only possible with KnowledgeFlow. In WekaManual.pdf (which is included in Weka package) for version 3.7.12 there is an example in Chapter 7.4.2 ""Plotting multiple ROC curves"" with picture and step-by-step instructions. For other Weka versions it is the same, just find the appropriate chapter.To give an impression on how it goes, I extracted the picture from the manual. It will draw two curves for two classifiers. For your question it is very similar. You use one classifier and then connect trainingSet to one ClassifierPerformanceEvaluator and testSet to another.";;;
5750;1;2015-05-11T13:07:53.560;Determine what data, does certain column have in common;I have an Excel sheet with n columns, these columns contain info about the students. For admissions we have the score of test scores in multiple subject areas, scores from an interview, and scores of a written test and comprehension test. There is a column which contains student's academic level (High, M.High, Middle, M.Low, Low). I want to compare the last column with the others variables and see whether there are common features that passing students have in common.Is there software for this? If this can be done with excel, how can I do it? Does SPSS provide this kind of analysis?;[education, open-source];45;
5751;1;2015-05-11T15:07:47.143;How do you compare term counts between two different periods, with different underlying corpus sizes, without bias?;"I'll set the question up with an example. You are analysing news coverage text data from 2014, and find that a term appears less often in the third quarter of 2014 than the final quarter (let's imagine it's the term ""Christmas""). Unfortunately, there are also far less news articles in the third quarter than in the second (due to the lack of news in the summer). So how do we accurately compare the counts in each quarter? We assume that there will be a greater number of occurrences in the fourth quarter, but how much does the magnitude of this difference depend on the change in size of the underlying text?Heap's law shows the relationship between text size and number of unique terms. It's non-linearity implies that the rate of new, unique words introduced by the text decreases as you increase the size of the text, and the proportion of the text taken up by each existing word therefore increases. This applies given documents taken from the same 'distribution' of text, in other words the underlying zipfian distribution of word ranks is identical (see wiki). In my example above this is obviously not the case, since the underlying topics, and resultant term distribution, will change between summer and winter, especially with regards to the term ""Christmas"". But take the same term count but over the whole of 2013 and 2014; you would reasonably expect the general underlying term distribution to be the same in each period, so Heap's law applies, but what if the volume of text has changed? Simply normalising by the size of the text, or the number of documents, does not, as far as I can tell, account for the relative change in expected value of the term count.I have a hunch that this might be a simple application of Heap's or Zipf's laws, but I can't see how to apply them to this particular question. Appreciate any insight.";[education, open-source];113;1
5752;2;2015-05-11T15:30:10.893;;"Okay, I eventually found what I was looking for in the Data Mining Community. There seem to be two candidates, CRISP-DM which comes from SPSS originally but is ""Cross-Industry"", and SEMMA which comes from SAS. They are both pretty much what I was looking for.CRISP-DM http://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_MiningAnd SEMMA http://en.wikipedia.org/wiki/SEMMA";;;
5753;2;2015-05-11T16:14:02.463;;Wonderful dataset with many opportunities to brush up on text analysis skills!My first thought would be to try some Topic Modelling on the dataset. If you are using Python there is a library I've used called gensim which has some fairly thorough tutorials to get you started. A friend of mine did something similar with the Enron dataset, using parallelized preprocessing and distributed latent Dirichlet allocation to infer topics over the email corpus. ;;;
5754;2;2015-05-11T17:01:05.930;;As a side comment note that using K-means for 10D data might end up in nowhere according to the curse of dimensionality. Of course it varies a bit according to the nature of the data but once I tried to determine the threshold in which K-Means starts behaving strange regarding the dimensionalty, I got something like 7D. After 7 dimensions it started to miss correct clusters (my data was manually generated according to 4 well-separated Gaussian distributions and I used MATLAB kmeans function for my little experiment).;;;
5755;2;2015-05-11T18:14:43.463;;A good place to start would be to look at the Spearman's Rank Correlation between your dependent variable (academic level) and your individual independent variables (other columns). This should give you a basic indication of whether any of your columns are correlated with academic level.  This should be straightforward to implement in both Excel and SPSS.If you wanted to go a step further you could set the problem up as one of Multinomial Logistic Regression. This would allow you to build a model to directly attempt to predict academic level from your other variables. I'm certain there's probably some way to do this in Excel, but SPSS can definitely handle it.;;;
5756;2;2015-05-11T20:06:17.680;;So how can I couple my clusters to a feature label from my feature matrix?Principal Components are not intuitive features. What seems common here is to cluster users based on PCs and investigate clusters based on original features afterward i.e. extract different clusters and plot data based on different subsets of features and use different colors for different clusters. It might give you some intution.These two paths can be seen in many PCA results where the information varies with e.g time. For instance in your case a beginner user visits less than an old one and the number of new users are most probably higher than old ones so data will be more dense around origin and far it gets from the origin less dense it becomes. Such a phenomenon affects your PCs as well so you'll see some paths along different lines in PC space.Hope it helps :);;;
5757;2;2015-05-11T20:22:42.187;;I personally use NetworkX and igraph where the first one is for programming in Python and the second one has interfaces for Python, R and C.If you are not into programming much, Gephi is what you are looking for!;;;
5758;2;2015-05-11T20:37:43.223;;So what you need is Modularity score. Speaking as a Graph Clustering guy (my master thesis topic, PhD research and my main research direction during last 2.5 years) I recommend you to go through what physicists did in Complex Networks field under the name of Community Detection. If you search Prof. Mark Newman who first proposed Modularity score you'll find a bunch of interesting papers in this field. Infomap algorithm by Martin Rosvall, Louvain algorithm by Vincent Blondel and CNM algorithm by Aaron Clauset are some of the most known algorithms. The most commonly used algorithm for graph clustering nowadays is the one by Vincent Blondel which has implementations for both NetworkX and igraph (if you are a python guy!). This algorithm is originally for weighted graphs and probably answers your question.Hope it helps, Good luck!;;;
5759;2;2015-05-11T20:57:50.870;;What you are looking for can be found in KONECT (the website is down as I'm writing this but it should be fixed soon!). It's almost the most comprehensive data collection for network analysis. But the question is which one is more standard to use?Well, there is no clear answer except of Zachary's Karate Club!If you do a literature review in Community Detection algorithms you'll see that almost all shining papers use different networks. My suggestion is going through what Andrea Lancichinetti and Santo Fortunato did for benchmarking graphs. They proposed some benchmark graph generation algorithms e.g. this one. Hope it helps :) ;;;
5760;2;2015-05-11T21:06:39.957;;Using Community detection you can build a recommendation system. The most commonly used algorithm in this field is Blondel Algorithm which u have probably seen in SNAP. Blondel is almost the fastest Community Detection algorithm among widely accepted ones and its result is pretty acceptable (at least according to modularity score). As a side comment, you may also want to have a look at NetworkX and igraph libraries for more graph algorithms and visualizations.Hope it helps, good luck.;;;
5761;1;2015-05-11T21:40:07.770;computational time and precision trade off;"I know that there are a number of predictive models (generized linear ones, trees, neural network, support vector machines, knn, Naive Bayes, ...) that have been proposed to perform various analytical tasks. Now I am striving to find appropriate references about their performance when the data becames ""Big"". In other words, how is their performance when the data becames really big. Does the training time increase more than linear? Is there any comparative benchmark between computational time and precision when the data becames high (for the various predictive models).";[education, open-source];37;
5762;2;2015-05-11T22:48:36.383;;You can simply convert a MxN matrix (your image) into a MN dimensional vector. For instance if you have 10 images each of which 10x10 pixels, you can convert them to 10 vectors each of them containing 100 element (you can do it row-wise or column-wise, does not matter). Then putting all these vectors on each other constructs a nxd matrix where n is the number of images and d is the number of pixels in each image. So far we just prepared data for analysis. Now let's go to the question:I'm looking for a supervised learning algorithm that can take 2d datafor input and output.What does that mean? First- The dimension of input-output is not a property of a machine learning algorithm. Second- By 2d, you mean a matrix? I'm afraid your data is not really 2d! If you mean a matrix, a MxN matrix is a data of M samples in N dimensional space. also those vectors you may produce from your images are in MN dimensional space.Third- A supervised algorithm takes some teaching samples (called training) to learn a task. What are those teachers here? If you have a training data which contains some blurred and corresponding original images then you can try to learn the place of white pixels by exploring blurred area. My suggestion:If a smoothing filter has been applied to your image, the blurred pixels most probably look like a gaussian. So first extract the gaussians.As you already have the place of sharp whites correspond to those gaussians you can use the mean, location of the mean and also variance as features (inputs) and the place of sharp white pixels as the target (output).The classifier could be a simple neural network (a MLP should work here) or a simple SVM.The answer above can not be evaluated without more information about your data and a more specific description of the question (e.g. What are your images exactly? if you can post one of them here I can probably help more);;;
5763;2;2015-05-12T09:30:09.273;;It seems to be working well on my configuration:Ubuntu Vivid and R:> sessionInfo()R version 3.1.2 (2014-10-31)Platform: x86_64-pc-linux-gnu (64-bit)locale: [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8    [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  [9] LC_ADDRESS=C               LC_TELEPHONE=C            [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       attached base packages:[1] stats     graphics  grDevices utils     datasets  methods   base     other attached packages:[1] twitteR_1.1.8loaded via a namespace (and not attached): [1] bit_1.1-12     bit64_0.9-4    bitops_1.0-6   DBI_0.3.1      httr_0.6.1     [6] magrittr_1.5   RCurl_1.95-4.6 rjson_0.2.15   stringi_0.4-1  stringr_1.0.0 [11] tools_3.1.2  Maybe you should update packages versions?;;;
5764;2;2015-05-12T10:10:58.977;;You're learning, are you? Try to find something easy and interesting to start. Why don't you start off something easy like building a Bayesian model to predict which email will get deleted. You should glance over those deleted emails, are they spams? are they just garbage?Here, you have a simply supervised model where the data-set already labels the emails for you (deleted or not). Think of something easy like words, titles, length of the email etc, see if you can build a model that predicts email deletion.;;;
5765;2;2015-05-12T10:59:50.547;;The InterQuartileRangeFilter from weka library uses an IQR formula to designate some values as outliers/extreme values. Any value outside this range $[Q_1 - k(Q_3-Q_1), Q_3 + k(Q_3-Q_1)]$ is considered some sort of an outlier, where $k$ is some constant, and $ IQR = Q_3 - Q_1$. By default weka uses $k=3$ to define something as outlier, and $k=3*2$ to define something as extreme value (extreme outlier). The formula guarantees that at least 50% value are considered non-outliers. Having a single variable (univariate sample of values), it's practically impossible to reproduce your result.Note however that this filter can be applied to a data frame. When applied like this, it will consider as an outlier any instance of the data frame which has at least one value of the instance considered as outlier for that variable. Now, supposing that you have a data frame with 2 variables, totally uncorrelated (independent). Considering again that only 10% of the values from each variable are considered outliers, due to independence, one can expect that $(1-0.9)^2$ values will not be outliers. If you have $p$ variables like that in your data frame, you might expect to have only $(1-0.9)^k$ normal values, and is not very hard to arrive in that situation.There are two things which you will have to consider. One is to increase the factors for outliers if in general too many values are considered outliers (ideally you would like to take a look at each variable graphically and if possible to get some idea about the distribution beneath). The second one is to check if you have many values which are totally independent. The second hint does not solve your problem but might give you a reason why it happens. ;;;
5766;1;2015-05-12T14:28:14.390;new and fresh ideas with stack exchange dataset;I'm a new bee for data analysis . i need to work on a research project in big data analysis. first of all i search for a dataset and i found interest in stack exchange  data dump. however i browse for researches i found a lot .And whatever i thought about a idea based on this dataset , its already done by someone else . please help me out with a new and useful idea for my research .;[education, open-source];32;
5767;2;2015-05-12T15:17:37.060;;It might help to have a bit more information on what you want to do, but have a look at Kaggle - it's a regular big data science competition, all of the data sets are huge and available for download.There's also some smaller datasets made available by Tableau.Hope that give you some inspiration!;;;
5769;2;2015-05-12T15:51:41.727;;Not much to add to the provided comments. Only thing is maybe this infographic comparing R vs Python for data science purposes http://blog.datacamp.com/r-or-python-for-data-analysis/;;;
5770;2;2015-05-12T17:23:52.323;;For question 1, I feel there are a great deal of things you can look into with this.  The basic conclusion is which states are culturally similar to one another (physical proximity would lead to such similarity).  Having a 'cultural similarity score' from year to year, you could determine the connectedness of states over time, see if major events effect how connected the country is..... there is a lot of potential here, and reenforces the notion that you need to know what question you're trying to answer before you get too far into an analysis.As for two, this is a bit trickier.  The visual I see is each state getting a node at it's center, and like states having color coded connectors.  The visual I'm seeing is similar to the graph they use for the scikit learn site for affinity propagation:http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html;;;
5771;1;2015-05-12T18:34:44.010;Ranking Bias in Learning to Rank;Users tend to click on results ranked highly by search engines much more often than those ranked lower. How do you train a search engine using click data / search logs without this bias? I.e. you don't want to teach the search engine that the results that are currently ranked highly should necessarily continue to be ranked highly just because they were frequently clicked.;[education, open-source];15;
5772;2;2015-05-12T20:08:04.033;;Have you seen this paper?Optimizing Search Engines using Clickthrough DataI stumbled upon this the other day, and I'm still reading through it, but the author attempts to deal with the problem you describe. You may also find Improving Web Search Ranking by Incorporating User Behavior Information useful.;;;
5773;2;2015-05-12T20:08:09.497;;The question is too general to have a specific recipe as an answer. Depends on the data and the relation between features but to the best of my knowledge:When the relation between your features are non-linear i.e. when a non-linear interpretation of correlation between features makes more sense than the linear interpretation.A detailed explanation of what I mean can be found in this nice answer. Please note that the $X^TX$ in the 4th line of the first paragraph should be $XX^T$, because in data mining, we conventionally use $N\times D$ format for data matrices. So in this example, we are looking for the gram matrix of the rows and not columns.Hope it helps :);;;
5775;1;2015-05-12T22:33:38.013;Multi-label text classification with minimum confidence threshold;"Building out a system that tries to apply zero or more predefined labels to text.For each label, we've:built out a reasonably good vocabulary of high-value words/featuresdeveloped a corpus containing thousands of labeled entriestrained a NaiveBayesClassifier for each topic that does a good job of classifying valid vs noisy contentThe problem seems to be that the individual classifier is great at differentiating between valid & noisy content WITHIN a topic:""the green energy bill will revolutionize..."" (green = ""green energy"")""the green bay packers went on to lose their..."" (green != ""green energy"") ...but when classifying content that shouldn't match ANY topic it has a very high rate of false positives. There's no ""everything else"" label!tl;dr it's good at subtle, in-topic differentiation, but terrible at broad topic labelingAre there any algorithms that help you classify into N categories, but allow for ""everything else"" which might not fit into ANY of the categories?";[education, open-source];85;
5776;1;2015-05-13T01:27:32.653;Best or recommended R package for logit and probit regression;Could somebody please recommend a good R package for doing logit and probit regression? I have tried to find an answer by searching on Google but all the links I find go into lengthy explanations about what logit regression is, which I already know, but nobody seems to recommend an R package.Thanks in advance.Jerome Smith;[education, open-source];105;1
5777;1;2015-05-13T02:24:54.520;Why does the listed order of features specified in the data set matter to the random forest classifier;>>> from sklearn.ensemble import RandomForestClassifier>>> clf = RandomForestClassifier(n_estimators=10, random_state=1)>>> Y=[0,1]>>> X = [[3,2,1,0], [7,6,5,4]]>>> clf = clf.fit(X, Y)>>> print clf.feature_importances_[ 0.2  0.1  0.1  0. ]>>> X = [[0, 1, 2,3], [4,5,6,7]]>>> clf = clf.fit(X, Y)>>> print clf.feature_importances_[ 0.2  0.1  0.1  0. ]>>> X = [[3,2,1,0], [7,6,5,4]]>>> clf = clf.fit(X, Y)>>> print clf.feature_importances_[ 0.2  0.1  0.1  0. ]>>> X = [[3,1,2,0], [7,5,6,4]]>>> clf = clf.fit(X, Y)>>> print clf.feature_importances_[ 0.2  0.1  0.1  0. ]Assume the features have names.When I shuffle/change the listed order of features specified in the training data set, the importance for each feature changes.That means the resulted random forest classifier also changes.Note that I have rule out the effect of randomness, by fixing the random seed.Why does the listed order of features specified in the data set matter to the random forest classifier, given that the random seed is fixed? Thanks.;[education, open-source];37;
5778;2;2015-05-13T02:47:46.463;;Unless you have some very specific or exotic requirements, in order to perform logistic (logit and probit) regression analysis in R, you can use standard (built-in and loaded by default) stats package. In particular, you can use glm() function, as shown in the following nice tutorials from UCLA: logit in R tutorial and probit in R tutorial.If you are interested in multinomial logistic regression, this UCLA tutorial might be helpful (you can use glm() or packages, such as glmnet or mlogit). For the above-mentioned very specific or exotic requirements, many other R packages are available, for example logistf (http://cran.r-project.org/web/packages/logistf) or elrm (http://cran.r-project.org/web/packages/elrm).I also recommend another nice tutorial on GLMs from Princeton University (by Germán Rodríguez), which discusses some modeling aspects, not addressed in the UCLA materials, in particular updating models and model selection.;;;
5780;2;2015-05-13T07:59:15.927;;"k-means is based on averages.It models clusters using means, and thus the improvement by adding more data is marginal. The error of the average estimation reduces with 1/sqrt(n); so adding more data pays off less and less...Strategies for such large data always revolve around sampling:If you want sublinear runtime, you have to do sampling!In fact, Mini-Batch-Kmeans etc. do exactly this: repeatedly sample from the data set.However, sampling (in particular unbiased sampling) isn't exactly free either... usually, you will have to read your data linearly to sample, because you don't get random access to individual records.I'd go with MacQueen's algorithm. It's online; by default it does a single pass over your data (although it is popular to iterate this). It's not easy to distribute, but I guess you can afford to linearly read your data say 10 times from a SSD?";;;
5782;2;2015-05-13T12:51:59.373;;"Since your fixed the seed, with default option bootstrap=True each tree is build on a fixed subsample of your data. With the default option max_features='auto' each tree uses 2 features. Since your fixed the seed, it is always the same, let us say, ""1st and 4th"" features for the first tree. And they are different in each of your shuffles. So, the tree is different. The same applies to each tree.By the way, with 4 features in total, only 6 (4*3/2=6) various trees are possible. With n_estimators=10 some of these trees are necessarily repeated.And in each shuffle those are different trees.";;;
5783;1;2015-05-13T15:06:49.200;Preparation for Career in Data Analysis Without College;I dropped out of college but am interested in a career in data analysis. Now I am self-studying approximately 10 hours per day.  Browsing through job postings on Linkedin has allowed me to compose a rough curriculum.  It would be of great help to me if you would either add a subject I have omitted or eliminate a subject that is not necessary for success in the market place.  Curriculum (in 3-subject groupings):Group 1Single-variable calculusIntro to pythonSQLGroup 2Multi-variable calculus/linear algebraDiscrete mathData structures and algorithmsGroup 3Calculus-based statistics and probabilityHadoop stackDifferential equationsGroup 4Statistical learning/predictive modellingPython data analysis techniques/Statistical programming in RFundamentals of machine learningAll the while I plan to practice using any data sets I can find online.  Will this be sufficient to land a job in data analysis?  Of course I plan to learn far more than just this, but is this foundation solid enough to land an entry level data engineering/science position?;[education, open-source];111;
5784;2;2015-05-13T15:33:22.677;;"At least based on what I and other data analysts/scientists do in my company, your technical topics list seems sufficient. But I would also add:Visualization (ggplot2 in R, matplotlib in Python, d3.js for really cool stuff)Design of experimentsCommunication skills are also quite important.For more inspiration, here's a good ""curriculum"" represented as a metro map: http://nirvacana.com/thoughts/becoming-a-data-scientist/Let me also recommend building up a portfolio of data science projects. This could consist of your analyses of data sets you find online (e.g. on UCI), Kaggle competitions, or class projects (e.g. via Udacity or Coursera). That way, you can give direct proof of your technical skills, your communication in the form of reports or graphics, and your ability to extract insight.";;;
5785;2;2015-05-13T16:20:31.883;;"Looks like the common approaches to multi-class classification actually solve this challenge.Building individual Naive Bayes Classifiers with only the training data for a single label is insufficient - we must also the include data from other labels as ""everything else"".See Text-Classification-Problem, what is the right approach?";;;
5788;2;2015-05-13T18:40:46.997;;If you are looking to calculate the relative importance of a word in this scenario, you might consider doing an inverse document frequency score on a quarterly basis: tf-idfOnce you have inverse frequency scores for all the words in the corpus from quarter to quarter, you can normalize by the range of all scores, and then do a quarter to quarter comparison.If you're trying to do a prediction, historical data from previous years would be very useful.  You can find trends over the course of a year by doing a regression on historical data and normalize to the predicted document output for that year.;;;
5789;1;2015-05-13T21:01:03.070;LinkedIn web scraping;"I recently discovered a new R package for connecting to the LinkedIn API. Unfortunately the LinkedIn API seems pretty limited to begin with; for example, you can only get basic data on companies, and this is detached from data on individuals. I'd like to get data on all employees of a given company, which you can do manually on the site but is not possible through the API.import.io would be perfect if it recognised the LinkedIn pagination (see end of page).Does anyone know any web scraping tools or techniques applicable to the current format of the LinkedIn site, or ways of bending the API to carry out more flexible analysis? Preferably in R or web based, but certainly open to other approaches.";[education, open-source];743;1
5790;2;2015-05-13T23:52:57.347;;Beautiful Soup is specifically designed for web crawling and scraping, but is written for python and not R:http://www.crummy.com/software/BeautifulSoup/bs4/doc/;;;
5791;1;2015-05-14T01:57:48.797;What kind of machine learning algorithm can I use?;"I have a data set of tweets regarding vaccines.  They have been collected from an API because they have keywords like ""flu, measles, MMR, vaccine"" etc.I need to find tweets specifically about measles and the outbreak that occurred in California this past February.  It isn't enough to search the data set for words like ""California"" and ""Measles"" because tweets like ""MMR vaccination rates in Palo Alto on the rise"" are about measles and California, but wont be captured by a naive search.Are there any unsupervised algorithms that could help me out?";[education, open-source];70;
5792;2;2015-05-14T05:33:12.030;;"The approach suggested by j.a.gartner is good if you want to analyze the quarter-to-quarter change in frequency for a given term, considering only that term in isolation.  What it doesn't do is evaluate the relative frequency compared to all other terms in the corpus.  For this you could compare frequency rank for all terms quarter-to-quarter. This is the analysis used in Zipf's Law.  Another advantage of this analysis is that you can test whether the generating process is or is not stationary (e.g. governed by same distribution quarter to quarter).The advantage of comparing frequency rank is that it doesn't depend on the relative size of the corpus for each quarter (as long as they are all ""large"").  In simple language, the frequency rank of the word ""Christmas"" in the 4th quarter will probably be the same, regardless of whether the (US English news) corpus is 10,000 articles or 1,000,000 articles.";;;
5793;1;2015-05-14T06:46:54.377;What is the difference between Latent and Explicit Semantic Analysis;"I'm not quite sure what ""latent"" refers to in this context. In Computing Semantic Relatedness usingWikipedia-based Explicit Semantic Analysis they say ''Our semantic analysis is explicit in the sense that we manipulate  manifest concepts grounded in human cognition, rather than  'latent concepts' used by Latent Semantic Analysis''.What does that mean in simple terms?I had a look at the related Wikipedia articles, ( Latent (wikipedia.org/wiki/Latent_semantic_analysis), Explicit wikipedia.org/wiki/Explicit_semantic_analysis) ), and I wasn't able to make heads or tails of it.Perhaps someone with a better appreciate of the nuance involved here might be able to provide me with a clear indication of the similarities and differences, the pros and cons between these two methods for accessing document/text fragment relatedness to a particular concept.  ";[education, open-source];48;
5794;2;2015-05-14T13:41:48.987;;"The difference is that with ESA, the concepts are already known and labeled (hence, ""manifest concepts""), whereas in LSA the concepts are latent (they are undefined and need to be discovered).Note the following statement from the ESA Wikipedia page:The name ""explicit semantic analysis"" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.[3][1]In short, ESA uses prior knowledge of relationships between words and concepts (as well as labels for those concepts), whereas LSA does not.References cited by Wikipedia for the quote above:[1] Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich (2011). ""Concept-Based Information Retrieval using Explicit Semantic Analysis"" (PDF). ACM Transactions on Information Systems 29 (2). Retrieved January 3, 2015.[3] Gabrilovich, Evgeniy; Markovitch, Shaul (2007). Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis (PDF). Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI). pp. 1606–1611.";;;
5795;2;2015-05-14T17:30:10.477;;Divide the data into windows and find features for those windows like autocorrelation coefficients, wavelets, etc. and use those features for learning.For example, if you have temperature and pressure data, break it down to individual parameters and calculate features like number of local minima in that window and others, and use these features for your model. ;;;
5796;2;2015-05-15T00:32:05.850;;"A couple ideas:If you have a large data set of tweets, you could try using latent semantic indexing to find out which terms are semantically related based on their usage and co-occurrence. Then after transformation, you can apply some document similarity metric, e.g. cosine similarity, to find tweets most relevant to your queries, e.g. ""California measles"".Use knowledge or lexical databases like DBPedia and WordNet to calculate semantic relatedness between your query and the text of your tweets, or to identify tweets with related concepts. ";;;
5797;1;2015-05-15T08:02:41.523;Data cleaning: Relationships between columns;I have a training data set distributed in two files.File 1: This contains actual classification for each X1. X1 is unique in this file. X1 has one-to-one relationship with X2, i.e. X2 is also unique. Y is binary.| X1 | X2 | Y  | | 1  | 4  | 0  | | 3  | 5  | 1  | ...| 8  | 9  | 1  | File 2: This contains the real 'observations' of the experiment. X1 can appear multiple times. | X1 | X3 | X4 | | 3  | 4  | 5  | | 3  | 1  | 2  | ...| 1  | 4  | 8  | Here I can combine the two tables to have a structure like below and use them as observations:| X1 | X2 | X3 | X4 | Y || 3  | 5  | 4  | 5  | 1 || 3  | 5  | 1  | 2  | 1 |...| 1  | 4  | 4  | 8  | 0 |For test data I have similar structure, just the Y column is missing in File 1.I have multiple concerns here:X1 and X2 has one-to-one dependency in the data, i.e. X1 = f(X2) and X2 = f(X1)Y = f'(X1) or f'(X2)Frequency distribution of X1,X2 and Y changes dramatically in the new joined data set.Questions:Does this kind of transformation of data leads to any insights?Does regression and ensemble learning techniques are capable of capturing these internal relationships?;[education, open-source];42;
5798;2;2015-05-15T08:42:14.577;;"I see several issues in your data.First of all, if there is a one-to-one relationship between X1 and X2, you should remove one of the two columns, because they are redundant. Redundant data may have a negative impact on your classification performance.  Secondly, the fields X3 and X4 also seem to be totally redundant, since the value of the class label Y only depends on X1/X2. So unless the columns X3 and X4 may be interesting on their own, I don't see the point of including them into the data. Having dealt with these issues, and in order to obtain Y from X1/X2, there are two possibilities. If file1 contains the value of Y for any possible value of X1 in your domain, you don't need any machine learning technique. You have a perfect mapping. Otherwise, you will need to apply machine learning to find a function that ""fills the gaps"". Depending on the nature of the Y variable, you will need to use a regression (if Y is a real number) or classification (if Y is a discrete variable). ";;;
5799;1;2015-05-15T08:48:02.190;Why are HMMs called linear-chain?;"I found in many sources that Hidden Markov Models are linear-chain networks(e.g. in Predicting Structured Data book by MIT). However, as I understand it, HMMs can have any edges in its graph. Even simple example of HMM in wikipedia has non-linear graph:  .So, the question is: what is the formal definition linear-chain structure and in which case forward-backward and Viterbi algorithms can give precise results.I have also taken into consideration this picture, taken from CRF tutorial, which says, that linear-chain CRF is ""generative-discriminative pair"" to HMM.";[education, open-source];47;0
5800;2;2015-05-15T10:00:06.307;;A good reference can be found there http://scikit-learn.org/stable/modules/ ;;;
5801;1;2015-05-15T13:51:50.087;Various algorithms performance in a problem and what can be deduced about data and problem?;"HI I am currently trying to apply various algorithms to a classification problem to assess which could be better and then try to fine tune the bests of the first approach. I am a beginner so I use Weka for now. I have basic ML concept understanding but am not in the details of algorithms yet.I observed that on my problem, RBF networks performed vastly worse than IBK and other K methods.From what I read about RBF networks, ""it implements a normalized Gaussian radial basisbasis function network. It uses the k-means clustering algorithm to provide the basis functions and learns either a logistic regression (discrete class problems) or linear regression (numeric class problems) on top of that. Symmetric multivariate Gaussians are fit to the data from each cluster. If the class is nominal it uses the given number of clusters per class.It standardizes all numeric attributes to zero mean and unit variance.""So basically, it also use k means to classify at first. But for some reason, I get the worst results with it using my metrics (ROC), while K methods are among the bests. Can I deduce from that fact something important about my data, like the fact that it has not a gaussian distribution, or is not fitted for logistic regression, or whatever I can't figure out?I also observed that random forests get similar results to K methods, and that adding a filter to reduce dimensionality improved these RF, random projection being better than PCA?Can this last point means that there is much randomness in my data so random dimension reduction is better than ""ruled"" dimension reduction like PCA? What can I deduce from the fact that RF perform equally to K methods?I feel there is some signification here, but I am not skilled enough to understand what, and I would be very glad for any insights. Thanks by advance.";[education, open-source];22;
5802;1;2015-05-15T14:14:20.280;Learning time of arrival (ETA) from historical location data of vehicle;I have location data of taxis moving around the city sourced from: Microsoft ResearchOverall it has around 17million data points.I have converted the data to JSON and filled up mongo. A sample looks like this:{'lat': 39.84349, 'timestamp': '2008-02-08 17:38:10', 'lon': 116.33986, 'ID': 1131}{'lat': 39.84441, 'timestamp': '2008-02-08 17:38:15', 'lon': 116.33995, 'ID': 1131}{'lat': 39.8453, 'timestamp': '2008-02-08 17:38:20', 'lon': 116.34004, 'ID': 1131}{'lat': 39.84615, 'timestamp': '2008-02-08 17:38:25', 'lon': 116.34012, 'ID': 1131}{'lat': 39.84705, 'timestamp': '2008-02-08 17:38:30', 'lon': 116.34022, 'ID': 1131}{'lat': 39.84891, 'timestamp': '2008-02-08 17:38:40', 'lon': 116.34039, 'ID': 1131}{'lat': 39.85083, 'timestamp': '2008-02-08 17:38:50', 'lon': 116.3406, 'ID': 1131}It consists of a taxiID - ID field, timestamp of its latitude and longitude combination.My question is: I want to use this data to calculate estimated time of arrival(ETA)  So far, I am doing it a crude way by querying mongoDB with aggregation. It is totally inefficient.I am looking at some sort of learning algorithm where the historical data can be used to train it. In the end, given two points, the algorithm should traverse the possible route by referring historical data and give an estimate of time.Calculating time estimate is not a problem at all if I get the array of JSON documents between the points. But, getting those right arrays is.Any pointers in this direction will be very helpful. ;[education, open-source];64;1
5803;1;2015-05-15T22:18:30.280;Attributing causality to single quasi-independent variable;Apologies if this isn't the correct place to ask - I'm not sure if this fits best with Stats or Data Science.I'm using analytics to help marketers identify attributes of their users correspond to successful conversions (such as someone buying a product, signing up for a newsletter, or subscribing to a service). Attributes could be things like which site they came from (referrer), their location, time/day of week, device type, browser, etc.What I'd like to say (although I'm not certain it's possible) is to isolate differences in conversion rate to an individual attribute, something like, '11% of your users from Facebook converted whereas only 3% of non-Facebook users converted', which would mean that the attribute 'referrer' and the level of the attribute 'Facebook' are responsible for driving conversions.Given that I may have 100s of quasi-independent variables, is it even possible to isolate the effect to one variable and one level of that variable? As opposed to a combination of them that is more likely to be driving the difference? If so, what technique or conceptual paradigm do I use to identify which variable-level is responsible for the greatest lift in my dependent variable, conversion rate?;[education, open-source];36;
5804;1;2015-05-16T00:15:37.807;Complex Chunking with NLTK;"I am trying to figure out how to use NLTK's cascading chunker as per Chapter 7 of the NLTK book. Unfortunately, I'm running into a few issues when performing non-trivial chunking measures.Let's start with this phrase:""adventure movies between 2000 and 2015 featuring performances by daniel craig""I am able to find all the relevant NPs when I use the following grammar:grammar = ""NP: {<DT>?<JJ>*<NN.*>+}""However, I am not sure how to build nested structures with NLTK. The book gives the following format, but there are clearly a few things missing (e.g. How does one actually specify multiple rules?):grammar = r""""""  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN  PP: {<IN><NP>}               # Chunk prepositions followed by NP  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments  CLAUSE: {<NP><VP>}           # Chunk NP, VP  """"""In my case, I'd like to do something like the following:grammar = ""MEDIA: {<DT>?<JJ>*<NN.*>+}           RELATION: {<V.*>}{<DT>?<JJ>*<NN.*>+}           ENTITY: {<NN.*>}""It occurs to me that a CFG might be a better fit for this, but I only became aware of NLTK's support for this function about 5 minutes ago (from this question), and it does not appear that much documentation for the feature exists.So, assuming that I'd like to use a cascaded chunker for my task, what syntax would I need to use? Additionally, is it possible for me to specify specific words (e.g. ""directed"" or ""acted"") when using a chunker?";[education, open-source];33;
5805;2;2015-05-16T00:28:32.713;;"Based on what I figured out from your problem:1You can easily convert your data to a graph using Networkx, igraph or any other tool/library/software. Then what you need is a Shortest Path Algorithm (Dijkstra is widely used and implemented in all graph/network analysis softwares). Once you created the graph you can simply calculate the average estimated time.For turning the problem into a Learning Problem, you can use historical time estimations for different paths and assign a weight to an edge proportional to the property of that edge (e.g. traffic jam probability, time conditions) and try to predict the ETA for a new query. 2You can also turn it into a Network Science Problem and use Graph Theoretc approaches to approach the question. You can start with statistical analysis of nodes and edges attributes e.g. passing time distribution, shortest path length distribution, probabilistic modeling of traffic jam and so on to see if some meaningful insight leads you the next step. Another idea is to use graph clustering algorithms to extract most connected parts of the town and go through the analysis of them i.e. calculate the ETA for different parts instead of whole the data and assign the estimated time to the members of corresponding cluster and reduce the computational complexty if your algorithm.3The last but not least is having a look at ArangoDB. It's a new database model which is based on graphs and you can run queries on millions of edges in an amazing speed! all what you need is a bit of javascript knowledge and even if you don't have it you can use AQL language designed for arangoDB. The interesting point is that it uses JSON files as the standard data format so you are already half way through ;)Hope i could help :)Good Luck!";;;
5806;2;2015-05-16T02:36:52.970;;I would suggest you to consider either direct dimensionality reduction approach. Check my relevant answer on this site. Another valid option is to use latent variable modeling, for example, structural equation modeling. You can start with relevant articles on Wikipedia (this and this, correspondingly) and then, as needed, read more specialized or more practical articles, papers and books.;;;
5807;2;2015-05-16T05:36:33.213;;I would like to add one more dimension to the answers given above. Usually we use cosine similarity with large text, because using distance matrix on paragraphs of data is not recommended. And also if you intend your cluster to be broad you tend to go with cosine similarity as it captures similarity overall. For example if you have texts which are two or three words long at max I feel using cosine similarity does not achieve the precision as achieved by distance metric. ;;;
5808;1;2015-05-16T16:58:08.090;Is it my Sales growth is depend on Commisions/Discounts?;I prefer this model in RWe are capturing sales data by time series (Month by month). Some of items have commissions and some have Discounts and others have both commissions and discounts. Is it Commissions or Discounts or commissions + Discounts have impact on my sales growth? Or is it my sales are growing because of those commissions or discounts or discounts +commissions Can you suggest me best model to solve my use case? I am thinking multiple regression. But I want to double check with experts like you.Thanks for your all your helpSample Data set: (5 variables)Year-Month -Product -Sales  -Commission -Discounts2013-01 Milk    300 No  Yes2013-02 Milk    400 No  Yes2013-03 Milk    200 No  Yes2013-04 Milk    150 No  Yes2013-05 Milk    500 No  Yes2013-01 Bread   800 Yes No2013-02 Bread   879 Yes No2013-03 Bread   790 Yes No2013-04 Bread   459 Yes No2013-05 Bread   600 Yes No2013-01 Cheese  400 Yes Yes2013-02 Cheese  350 Yes Yes2013-03 Cheese  600 Yes Yes2013-04 Cheese  590 Yes Yes2013-05 Cheese  720 Yes Yes;[education, open-source];49;
5809;1;2015-05-16T17:37:53.163;How to subset rows from a data frame with comparison operators in R;"I have a data frame (a csv file) with dimensions 100x6 and I need only the columns c(""X1"", ""X2"", ""X4"") and the rows in which the value of ""X1"" is greater than 30. So I did:  data_frame <- read.csv (""data_frame"")  data_frame [c(""X1"", ""X2"", ""X4"")]The column subset problem is solved but now I need to subset rows from data_frame [c(""X1"", ""X2"", ""X4"")] where the values of ""X1"" is greater than 30. I tried:  data_frame [c(""X1"" > 30), c(""X1"", ""X2"", ""X4"")] But it returned the same data frame as data_frame [c(""X1"", ""X2"", ""X4"")].Also tried using the function subset() with the same approach but got the same results.";[education, open-source];68;
5810;1;2015-05-16T18:09:15.630;Application of Control Theory in Data Science;I have recently completed an MSc in Control Systems from a top university. It seems to me that control theory must have an application within data science. I would like to apply my degree within this domain, but I want to be sure that it is relevant to the role of a data scientist.The topics which I have particular interest and experience in are State Space Control, Systems Identification, Model Predictive Control and Optimal Control. I imagine that effective management of any large dataset must involve modelling of the system in terms of transfer functions/state space models (based on large sets of historical input/output data). These models could then be used to predict the evolution of a market/variable over time, and therefore optimise a given cost function such as profit, risk etc.If this kind of role exists within data science/ other areas, can you please give me more information/ ideas of job roles/ industries to research.;[education, open-source];90;
5811;2;2015-05-16T18:52:10.650;;"You might be interested in this paper that explores a few of the questions you are asking: http://arxiv.org/pdf/1312.6184.pdf. It is aptly titled: ""Do Deep Networks Really Need to Be Deep?""The crux of the matter is that deep networks allow for a LOT of non-linearity in the data that is being described. For CIFAR-10, I suspect that something similar to what works well for MNIST will work well. Yan LeCunn's page about MNIST results gives a lot of clues about models with few parameters: http://yann.lecun.com/exdb/mnist/The double-problem of image classification is that in addition to the classification you need to extract features somehow. Often the parameters in deep networks go towards feature extraction rather than the classification itself, which is usually a softmax multi-class logistic regression classifier. ";;;
5812;2;2015-05-16T19:01:43.953;;Multiple regression sounds appropriate in this case. Real question is what variables to use. Definitely, you should include indicator variables commisions and discounts (and possibly their combination - commisions_and_discounts - as a separate variable). Date and time information may play important role as well, though treating them is a little bit harder. At the very least, it's worth to include year (because there may be a global trend), month and day_of_week (many patterns are repeated periodically). Product type is a little bit more complicated. Obviously, some products will always have higher sales than others, so first idea is to include a set of product_type_x dummy variables. But product type may affect sales not additively, but instead multiplicatively, i.e. not as sales ~ beta_0 + beta_1*product_type_x + ...but instead as sales ~ beta_1 * prodcut_type_x * (beta_0 + ...)In this case better solution would be to create separate regression models for each product type. ;;;
5813;2;2015-05-16T19:16:26.623;;"The offset parameter is sometimes called ""bias"" in classification tasks, and its intuitive understanding doesn't have to do with what kind of kernel is used. It is basically used to compensate for feature vectors that are not centered around 0.I will try to intuitively explain what the bias does with a toy example. Let's say you have some feature vector x who's parameters are always negative. The set of weights w you use in your SVM (let's say linear, just for clarity) will perhaps transform the features into the range [0, 1] - so they will always be negative. But, those elements that belong to class 1 fall in the range [0, 0.5], and the ones that are from class 2 fall into the range [0.5, 1]. To classify into a class, the SVM uses 0 as the threshold breakpoint - if greater than 0, it is an element of class 1, if less than 0, it is an element of class -1. But in this case, all the elements will be classified into class 2. However, with a bias of 0.5 (in this linear case), they will be classified correctly. This geometric interpretation doesn't quite work for more complex kernels, but the idea is the same: the bias term attempts to compensate for features that are not centered around zero. In practice, if the features are centered around zero the bias term isn't always needed. To get around the bias issue, you can either: center your features and forget about the bias term (doesn't always work)augment your feature data to include a 1 at the beginning. i.e.: [feat1 feat2 feat3] --> [feat1 feat2 feat3 1]for all of your feature vectors. Then the bias term will just be learned as another SVM weight parameter. Learn your weights, then calculate the bias based on the optimal prediction rule:That is, given all the support vectors $S$, the learned weights $\alpha_i$, the training class $y^{(i)}$, training features $x^{(i)}$, and features-to-use-for-prediction $x$, the predicted class will be $y(x)$. The optimal $w_0$ or bias will then be calculated as the average distance between the correct and calculated predicted class: Hope that helps. I think the easiest thing is to center your features ;)";;;
5814;2;2015-05-16T20:07:57.017;;"You wantdata_frame[data_frame$X1 > 30, c(""X1"",""X2"",""X4"")]that will just print it, you probably want to update data_frame or store it in something else:data_frame = data_frame[data_frame$X1 > 30, c(""X1"",""X2"",""X4"")]also you probably want to try asking this on StackOverflow, or reading a bit more basic R documentation because it should be well covered. Its a bit simple to be ""data science"".";;;
5815;2;2015-05-17T08:30:38.917;;Have you tried the Internet search? The results should be able to answer most, if not all, of your questions. The topics of your interest sound like rather general or high-level. I'm sure that they can, in one form or another, be applied in the data science context. In my opinion, those topics are more related to operations research (OR), therefore, I would recommend you to perform some research on the Internet on the intersections between control systems (theory) and data science.Having said that, first thing that comes to my mind is that the most likely candidate for use of control theory concepts and methods in data science context would be distributed systems and algorithms for data analysis, such as MapReduce (Hadoop, etc.), as well as other parallel processing systems. If there exist an intersection between OR's area of optimization and control theory, then it very well could be used for big data algorithms optimization, among other tasks.;;;
5816;2;2015-05-17T18:32:15.197;;I'd go for this:$S(object_i,object_j) = 1 - \frac{\sum_{k=1}^{N_c} |n_{object_i,cluster_k}-n_{object_j,cluster_k}|}{n_{object_i}+n_{object_j}}$where $N_c$ is the number of clusters (here 3), $n_{object_i,cluster_k}$ is the number of elements of object $i$ that belong to cluster $k$ (here $n_{object_2,cluster_2}=200$) and $n_{object_i}$ is the total number of elements in object $i$ (sum of $i^{th}$ row).I hope it helps :);;;
5817;1;2015-05-17T20:12:48.760;Airline Fares - What analysis should be used to detect competitive price-setting behavior and price correlations?;"I want to investigate price-setting behavior of airlines -- specifically how airlines react to competitors pricing.As I would say my knowledge about more complex analysis is quite limited I've done mostly all basic methods to gather a overall view of the data. This includes simple graphs which already help to identify similar patterns. I'am also using SAS Enterprise 9.4.However I'am looking for a more number based approach.Data SetThe (self) collected data set I'am using contain around ~54.000 fares.All fares were collected within a 60 day time window, on a daily basis (every night at 00:00).Hence, every fare within that time window occurs $n$ times subject to the availability of the fare as well as the departure date of the flight, when it is passed by the collection date of the fare.(You cant collect a fare for a flight when the departure date of the flight is in the past)The unformatted that looks basically like this: (fake data)+--------------------+-----------+--------------------+--------------------------+---------------+| requestDate        | price| tripStartDeparture | tripDestinationDeparture | flightCarrier |+--------------------+-----------+--------------------+--------------------------+---------------+| 14APR2015:00:00:00 | 725.32    | 16APR2015:10:50:02 | 23APR2015:21:55:04       | XA            |+--------------------+-----------+--------------------+--------------------------+---------------+| 14APR2015:00:00:00 | 966.32    | 16APR2015:13:20:02 | 23APR2015:19:00:04       | XY            |+--------------------+-----------+--------------------+--------------------------+---------------+| 14APR2015:00:00:00 | 915.32    | 16APR2015:13:20:02 | 23APR2015:21:55:04       | XH            |+--------------------+-----------+--------------------+--------------------------+---------------+""DaysBeforeDeparture"" is calculated via $I=s-c$ whereI & interval (days before departure)s & date of the fare (flight departure)c & date of which the fare was collectedHere is a example of grouped data set by I (DaysBeforeDep.) (fake data!):+-----------------+------------------+------------------+------------------+------------------+| DaysBefDeparture | AVG_of_sale | MIN_of_sale | MAX_of_sale | operatingCarrier |+-----------------+------------------+------------------+------------------+------------------+| 0               | 880.68           | 477.99           | 2,245.23         | DL           |+-----------------+------------------+------------------+------------------+------------------+| 0               | 904.89           | 477.99           | 2,534.55         | DL           |+-----------------+------------------+------------------+------------------+------------------+| 0               | 1,044.39         | 920.99           | 2,119.09         | LH               |+-----------------+------------------+------------------+------------------+------------------+What I came up with so farLooking at the line graphs I can already estimate that several lines will have a high correlation factor. Hence, I tried to use correlation analysis first on the grouped data. But is that the correct way? Basically I try now to make correlations on the averages rather then on the individual prices?Is there an other way?I'am unsure wich regression model fits here, as the prices do not move in any linear form and appear non-linear. Would I need to fit a model to each of price developments of an airlinePS: This is a long text-wall. If I need to clarify anything let me know. I'am new to this sub.Anyone a clue? :-)";[education, open-source];127;
5818;2;2015-05-18T02:32:19.643;;In addition to exploratory data analysis (EDA), both descriptive and visual, I would try to use time series analysis as a more comprehensive and sophisticated analysis. Specifically, I would perform time series regression analysis. Time series analysis is a huge research and practice domain, so, if you're not familiar with the fundamentals, I suggest starting with the above-linked Wikipedia article, gradually searching for more specific topics and reading corresponding articles, papers and books.Since time series analysis is a very popular approach, it is supported by most open source and closed source commercial data science and statistical environments (software), such as R, Python, SAS, SPSS and many others. If you want to use R for this, check my answers on general time series analysis and on time series classification and clustering. I hope that this is helpful.;;;
5819;1;2015-05-18T05:59:41.207;Looking for smallest set of rows that form a natural key in a data set;I have several sets of text files on hdfs that are exports from relations.  Unfortunately I do not know the structure of the table is, but I do know that each has a multi-part key that defines a row uniquely.  I know through domain knowledge that the key is multi-part (e.g. reporting-date, and item number) and I can identify some columns that are clearly not in the key (e.g. revenue from sale).  What is an effective way to identify potential sets of columns that are natural keys, because they are not duplicated in the observed data?  I can get several days of logs in a few Gig, so python or sql could work.  This seems like an great application for a dictionary, but I am not sure how to approach this.     ;[education, open-source];46;
5820;2;2015-05-18T09:20:51.007;;"You can consider that the clusters are defined by a multinomial distribution with 4 classes. Thus you would identify the cluster as a vector of 4 probabilities, one for each class, and the sum of probabilities being 1. In this form there are some tests and distance functions which can prove to be useful for you. You can ask questions like ""is the cluster 1 is different than the cluster 2?"". In order to do that you can employ hypothesis testing, and specifically chi square independence test (wikipedia page here). You select from your table only the first 2 columns, compute the marginal totals, compute the estimated counts as $E_{ij} = rowsum_i * colsum_j / grandtotal$. Denoting with $O_{ij}$ the cell counts you can compute a test statistic with formula: $$chi = \sum_{i=1}^4\sum_{j=1}^2\frac{(E_{ij}-O_{ij})^2}{E_{ij}}$$This is known to has a chi square distribution with $(rows-1)(cols-1)=3$ degrees of freedom. Thus using the cumulative distribution of chi square you can compute the p-value as $1-cdf_{\chi_{3}^2}(chi)$. Now if p-value is smaller than any critical value you choose before test (like 0.05, 0.01 or so), you have a statistical proof that the clusters have a different distribution (this follows from the fact that your procedure tests for the independence of cluster and object distributions, which with short derivation is the same as saying the clusters have the same distribution). To answer to the same type of question you can also employ G-test (wikipedia page here) or Fisher exact test (wikipedia page here).  Note also that all those tests can't prove that two clusters are the same. The logic behind the tests can be formulated as ""we consider the distributions the same, as a natural state, accepted without other proofs, and if we see some exceptionally obvious differences, we can bring that as a proof that they are different indeed"". A different approach would be to build a distance between those two distributions. The advantage of a distance function would be that it could be used for comparisons and eventually some further algorithms based on distances. The drawback of a distance function is that it gives you no hint to answer the first type of questions like ""are the differences between 2 distributions small enough in order to be explained by the natural noise in data?"". Thus I have two candidates in mind. First distance is the chi square distance. This type of distance gives a measure to the distance between two normalized histograms (or, put it simply, between two vectors of probabilities of the same size). Thus, if we denote with $cluster_i = (n_{i1}/n_i, n_{i2}/n_i, .., n_{ik}/n_i] = (p_{i1},p_{i2},..p_{ik})$, the distance between cluster i and j would be: $$d(i,j) = \frac{1}{2}\sum_{k=1}^{4}\frac{(p_{ik}-p_{jk})^2}{p_{ik}-p_{jk}}$$This distance function has the advantage that it is symmetric regarding $i$ and $j$. To be honest I rarely used it since I did not fully understand it's meaning. In general I use another function, described below.The second distance function is mutual information (see wikipedia page here). The idea is similar with Kulback-Leibler divergence, however the KL distance is an oriented measure (measures how a distribution can be expressed through another one). The MI distance is a measure of mutual independence between two distributions. In your case the two distributions are two clusters, and the MI measures how dependent they are. I give formula just to illustrate the idea behind it. $$MI(x,y) = \sum_{x,y}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$$You can see from the formula that when the two variables are independent, the quantity inside the logarithm is $1$, and as a consequence, the MI is $0$. As the difference grows, the function also grows. I will not derive what p means, see what I explained already for chi square test, since is very similar. ";;;
5821;1;2015-05-18T11:50:35.383;Convolutional neural network for sparse one-hot representation;I have some basic features which I encoded in a one-hot vector.Length of the feature vector equals to 400.It is sparse.I saw that conv nets is applied to a dense feature vectors.Is there any problems to apply conv nets to a sparse feature vectors?;[education, open-source];89;
5822;1;2015-05-18T13:07:17.983;Identifying sequences of behavioral interactions between multiple individuals;I'm wondering if anyone might have some novel insights as to the best way to analyze the following data.  It's a problem I've been thinking about in the back of my mind for a while, so I thought that I'd ask here. I have data that look like this:     day event actor recipient995    8   128     G         J996    8   129     G         K997    8   130     G         B998    8   131     B         G999    8   132     H         G1000   8   133     G         H1001   8   134     E         G1002   8   135     G         J1003   8   136     B         H1004   8   137     G         H1005   8   138     G         H1006   8   139     B         J1007   9     1     D         J1008   9     2     A         J1009   9     3     A         J1010   9     4     H         J1011   9     5     A         J1012   9     6     D         H1013   9     7     A         F1014   9     8     D         J1015   9     9     A         H1016   9    10     D         J1017   9    11     A         J1018   9    12     F         J1019   9    13     F         J1020   9    14     F         H1021   9    15     F         G1022   9    16     F         H1023   9    17     C         F1024   9    18     C         G1025   9    19     D         HWhat you see here is an extract of a R dataframe.  The first column being the rownumber of the df, then the four variables.    These data start at day1 and end at day 22.  There are between 13 and 215 'events' on each day - each event is a separate behavioral event. Higher number events occur later in time than earlier numbered events.  Individuals are in the 'actor' and 'recipient' variables.   The data are available in csv format here: There are 11 individuals (A - K). One thing you'll notice is that recipients tend to be lower down the alphabet, and actors tend to be higher up the alphabet.  A key question I'm interested in working out a methodology to address is to see if the likelihood of becoming an actor increases if an individual has recently been a recipient. You can see this on line 997 that G-B and then B-G occur followed by H-G and G-H.    An individual recipient doesn't have to appear on the next line to count as having an increased likelihood of appearing as an actor - I'm interested in the decay in the probability of this occurring over events (but not continuing to the next day).Further, I don't think this will be true for all individuals, so I am keen to test which individuals it is true for.     Lastly, I am interested to know if an individual who has recently been a recipient but is now an actor is paired with more often an individual of a higher or lower letter than themselves.I hope that these questions are clear and make sense.   I obviously don't expect a full analysis.  But I would be keen to hear of ideas for this type of analysis.  I believe that examining some Markov processes may be useful but I am interested in hearing about other ideas.;[education, open-source];29;
5823;1;2015-05-18T15:10:05.623;How do I cluster data that is a mix of text & categorical data?;I have a set of strings, each also has soem categorical information associated with it. The categorical information isn't always great though, so I need to cluster the messages based on the text content & the categories. What is the best way to do this? ;[education, open-source];48;1
5824;2;2015-05-18T17:36:21.813;;Depending on your technical ability and what you're trying to do, Qlik Sense might be a good fit as it is fairly easy to use and yet powerful, and comes with mapping functionality. If it's a commercial project though you will need to pay for (expensive) licenses, otherwise it's a free download for personal non-commercial use.;;;
5825;2;2015-05-18T17:50:19.717;;I would not apply convolutional neural networks to your problem (at least from what I can gather from the description).Convolutional nets' strengths and weaknesses are related to a core assumption in the model class: Translating patterns of features in a regular way either has a minor impact on the outcome, or has a specific useful meaning. So a pattern 1 0 1 seen in features 9,10,11 is similar in some way to the same pattern seen in features 15,16,17. Having this assumption built in to the model allows you to train a network with far fewer free parameters when dealing with e.g. image data, where this is a key property of data captured by scanners and cameras. With one-hot encoding of features, you assign a feature vector index from a value or category essentially at random (via some hashing function). There is no meaning to translations between indices of the feature vectors. The patterns 0 0 1 0 1 0 0  and 0 0 0 1 0 1 0 can represent entirely different things, and any associations between them are purely by chance. You can treat a sparse one-hot encoding as an image if you wish, but there is no good reason to do so, and models that assume translations can be made whilst preserving meaning will not do well.For such a small sparse feature vector, assuming you want to try a neural network model, use a simple fully-connected network.;;;
5826;1;2015-05-18T18:55:59.763;Master degree in Data Science;I am looking to change careers and would appreciate some advice. I have an undergraduate degree in English Literature and a JD. Needless to say, these were not the best decisions and I would like to change my career. I have always enjoyed math and science, and after months of research and self study, I have decided that I would like to pursue statistics. My question is basically this : would it be better to get another Bach. Degree in statistics or should I take calculus 1-3, linear algebra, probably and statistics and some computer science courses at a community college then try to get into a grad program instead? I am afraid that without the bachelors degree I will not have the required knowledge for a masters even after taking the courses at my local c.c. However, another bachelors degree may be a worthless waste of time. Please share your thoughts. Also, sorry if this is not the right place for this question. ;[education, open-source];123;
5827;2;2015-05-18T19:01:52.377;;First and foremost, there is no best way to do this task in general. It will require some creativity and problem-solvingHere is one potential idea. Clustering involves some kind of similarity metric. You can compute a similarity between text documents (after some term weighting like TF-IDF you could use the cosine similarity for example). You can also compute a similarity between objects with categorical data. Matching coefficients are good option for this. For example, http://en.wikipedia.org/wiki/Simple_matching_coefficient  is about as basic as it gets, but there are many other measures of similarity for categorical data that you can find with a simple google search.So if I have one similarity matrix A that gives the similarity based on the text, and one similarity matrix B that gives the similarity based on the categorical attributes, then I could take some weighted combination of A and B and use it as an overall similarity matrix for clustering. Perhaps a convex combination would work well and give you some flexibility to choose which measure of similarity you want to weight more heavily (since it sounds like the categorical information might not be as good):choose some parameter $0\leq \alpha\leq 1$ and compute an overall similarity matrix S as$$\textbf{S} = \alpha \textbf{A} + (1-\alpha)\textbf{B}$$Then you could use any number of clustering algorithms using the similarity information. For example, $k$-means should work just fine on a similarity matrix as input. Any type of graph/spectral clustering algorithm is also well suited for this type of input.;;;
5828;2;2015-05-18T19:07:45.150;;Graduate degrees in data science or business analytics provide a fast-track toward a job these days, even more so than a traditional statistics degree. Check out the graduate programs and what they require in terms of prerequisites. Some may not even require Calculus sequence, only a good foundation of stat and linear algebra. A bachelor's degree is necessarily going to waste your days with core courses like British Literature (no offense to english majors, but given your goals that time would be wasted).;;;
5830;1;2015-05-19T02:09:13.700;How do I best parse ECOG scores in cancer clinical trial results in java?;"I was informed of 5 java NLP libraries.Apache cTAKES™MetaMapLexEVS (http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/web/download.html)Apache OpenNLPI also plan to parallelize an NLP library via map-reduce with hadoop.However, I'm new to natural language processing, so I don't know how to approach the problem.The goal is to download a set of clinical trials on cancer from www.clinicaltrials.gov and parse eligibility criteria (both inclusion and exclusion criteria), identify the ECOG scores and annotate each CTA with the allowed ECOG scores.For example, in the following document, if ECOG is specified in inclusion criteria, it is not negated. If ECOG is specified in exclusion criteria, it is negated. If both exclusion criteria and inclusion criteria are not mentioned, then ECOG is not negated.Document:  Eligibility criteria of CTA ""NCT01572038"" Inclusion Criteria:  Patients must have histological proof of a primary non-small cell lung cancer  (bronchoalveolar carcinomas presenting as a discrete solitary radiological mass or  nodule are eligible) Patients must be classified post-operatively as stage IB, II or IIIA on the basis of  pathologic criteria At the time of resection a complete mediastinal lymph node resection or at least  lymph node sampling should have been attempted; if a complete mediastinal lymph node  resection or lymph node sampling was not undertaken, any mediastinal lymph node which  measured 1.5 cm or more on the pre-surgical computed tomography (CT)/magnetic  resonance imaging (MRI) scan or any area of increased uptake in the mediastinum on a  pre-surgical positron emission tomography (PET) scan must have been biopsied; note: a  pre-surgical PET scan is not mandatory   The nodal tissue must be labelled according to the recommendations of the  American Thoracic Society; surgeons are encouraged to dissect or sample all  accessible nodal levels; the desirable levels for biopsy are: Right upper lobe: 4, 7, 10 Right middle lobe: 4, 7, 10 Right lower lobe: 4, 7, 9, 10 Left upper lobe: 5, 6, 7, 10 Left lower lobe: 7, 9, 10  Surgery may consist of lobectomy, sleeve resection, bilobectomy or pneumonectomy as  determined by the attending surgeon based on the intraoperative findings; patients  who have had only segmentectomies or wedge resections are not eligible for this  study; all gross disease must have been removed at the end of surgery; all surgical  margins of resection must be negative for tumor No more than 16 weeks may have elapsed between surgery and randomization; for  patients who received post-operative adjuvant platinum-based chemotherapy, no more  than 26 weeks may have elapsed between surgery and randomization Patient must consent to provision of and investigator(s) must agree to submit a  representative formalin fixed paraffin block of tumor tissue at the request of the  Central Tumor Bank in order that the specific EGFR correlative marker assays may be  conducted The patient must have an Eastern Cooperative Oncology Group (ECOG) performance status  of 0, 1 or 2 Leukocytes >= 3.0 x 10^9/L or >= 3000/ul Absolute granulocyte count >= 1.5 x 10^9/L or >= 1,500/ul Platelets >= 100 x 10^9/L or >= 100,000/ul Total bilirubin within normal institutional limits Alkaline phosphatase =< 2.5 x institutional upper limit of normal; if alkaline  phosphatase is greater than the institutional upper limit of normal (UNL) but less  than the maximum allowed, an abdominal (including liver) ultrasound, CT or MRI scan  and a radionuclide bone scan must be performed prior to randomization to rule out  metastatic disease; if the values are greater than the maximum allowed, patients will  not be considered eligible regardless of findings on any supplementary imaging Aspartate aminotransferase (AST) (serum glutamic oxaloacetic transaminase [SGOT])  and/or alanine aminotransferase (ALT) (serum glutamate pyruvate transaminase [SGPT])  =< 2.5 x institutional upper limit of normal; if AST (SGOT) or ALT (SGPT) are greater  than the institutional upper limit of normal (UNL) but less than the maximum allowed,  an abdominal (including liver) ultrasound, CT or MRI scan must be performed prior to  randomization to rule out metastatic disease; if the values are greater than the  maximum allowed, patients will not be considered eligible regardless of findings on  any supplementary imaging Patient must have a chest x-ray done within 14 days prior to randomization; patient  must have a CT or MRI scan of the chest done within 90 days prior to surgical  resection if at least one of the following was undertaken:   A complete mediastinal lymph node resection; or Biopsy of all desired levels of lymph nodes - as specified above; or A pre-surgical PET scan within 60 days prior to surgical resection If none of  the above was undertaken then the CT or MRI scan of the chest must have been  performed within 60 days prior to surgical resection Note: a pre-surgical PET  scan is not mandatory  Patient must have an electrocardiogram (EKG) done within 14 days prior to  randomization Women of childbearing age and men must agree to use adequate contraception (hormonal  or barrier method of birth control) prior to study entry and while taking study  medication and for a period of three months after final dose; should a woman become  pregnant or suspect she is pregnant while she or her male partner are participating  in this study, she should inform her treating physician immediately Patients may receive post-operative radiation therapy; patients must have completed  radiation at least 3 weeks prior to randomization and have recovered from all  radiation-induced toxicity; patients who have received radiation therapy should also  be randomized within 16 weeks of surgery Patient consent must be obtained according to local institutional and/or University  Human Experimentation Committee requirements; it will be the responsibility of the  local participating investigators to obtain the necessary local clearance, and to  indicate in writing to either the National Cancer Institute of Canada (NCIC) Clinical  Trials Group (CTG) study coordinator (for NCIC CTG centers) or the Cancer Trials  Support Unit (CTSU) (for all other investigators), that such clearance has been  obtained, before the trial can commence in that center; a standard consent form for  the trial will not be provided, but a sample form is given; this sample consent form  has been approved by the National Cancer Institute (NCI) Central Institutional Review  Board (IRB) and must be used unaltered by those CTSI centers which operate under CIRB  authority; for NCIC CTG centers, a copy of the initial full board Research Ethics  Board (REB) approval and approved consent form must be sent to the NCIC CTG central  office; please note that the consent form for this study must contain a statement  which gives permission for the government agencies, NCI, NCIC CTG and monitoring  agencies to review patient records   NCIC-CTG Centers: the patient must have the ability to understand and the  willingness to sign a written informed consent document; the patient must sign  the consent form prior to randomization CTSU Centers: the patient, or in the case of a mentally incompetent patient his  or her legally authorized and qualified representative, must have the ability to  understand and the willingness to sign a written informed consent document; the  consent form must be signed prior to randomization  Patients must be accessible for treatment and follow-up; investigators must assure  themselves that patients registered on this trial will be available for complete  documentation of the treatment administered, toxicity and follow-up Initiation of protocol treatment must begin within 10 working days of patient  randomization Patients may have received post-operative adjuvant platinum-based chemotherapy;  patients must have completed chemotherapy at least 3 weeks prior to randomization and  have recovered from all chemotherapy-induced toxicity; patients who have received  adjuvant chemotherapy should also be randomized within 26 weeks of surgery  Exclusion Criteria:  Prior or concurrent malignancies; patients who have had a previous diagnosis of  cancer, if they remain free of recurrence and metastases five years or more following  the end of treatment and, in the opinion of the treating physician do not have a  substantial risk of recurrence of the prior malignancy, are eligible for the study;  patients who have been adequately treated for non-melanomatous skin cancer or  carcinoma in situ of the cervix are eligible irrespective of when that treatment was  given A combination of small cell and non-small cell carcinomas or a pulmonary carcinoid  tumor More than one discrete area of apparent primary cancer (even if within the same lobe,  T4, IIIB) Clinically significant or untreated ophthalmologic (e.g. Sjogren's etc.) or  gastrointestinal conditions (e.g. Crohn's disease, ulcerative colitis) Any active pathological condition that would render the protocol treatment dangerous  such as: uncontrolled congestive heart failure, angina, or arrhythmias, active  uncontrolled infection, or others A history of psychiatric or neurological disorder that would make the obtainment of  informed consent problematic or that would limit compliance with study requirements Patient, if female, is pregnant or breast-feeding Neoadjuvant chemotherapy or immunotherapy for NSCLC; however, patients may have  received pre-operative limited field, low dose (less than 1000 cGy) external beam  radiation therapy or endobronchial brachytherapy or laser therapy for short term  control of hemoptysis or lobar obstruction; full dose pre-operative radiotherapy of  curative intent is a cause for exclusion; patients may have received post-operative  adjuvant platinum-based chemotherapy however non-platinum-based chemotherapy is a  cause for exclusion History of allergic reactions attributed to compounds of similar chemical or biologic  composition to the agents used on this trial; patients with ongoing use of phenytoin,  carbamazepine, barbiturates, rifampicin, or St John's Wort are excluded Incomplete healing from previous oncologic or other major surgery I want to find documents with a query as below.Input  'Patients with Eastern cooperative Oncology Group (ECOG) performance status > 2' will be excludedOutput CTA: NCT01572038 Labels: ECOG 0, ECOG 1, ECOG 2What is the simplest way to approach this problem?";[education, open-source];59;
5832;2;2015-05-19T09:29:02.810;;Data science is a multi discipline subject.In addition to math you need some programming skills and ideally - understanding of the domain where you are going to solve problems.You can start from online-courses and some introduction literature.Look at the answers at current site:Data Science SertificationsBooks about Data ScienceData Science vs Data MiningData Science is a trend or a long term conceptPh.D. in Data Science?Statistics + Computer Science = Data Science?Starting career as Data Scientist, is Software Engineering experience required?Career switch to Big Data AnalyticsAnd more about career...I can't resist from placing this illustration for the millionth time:;;;
5833;1;2015-05-19T13:10:37.463;What is the best technique/algorithm to compare trees changes?;"I have a problem I would like to solve using machine learning. I would like to use some sort of classification to know if a just added change in a tree data structure is ""good"" or is ""bad"".Let's say I have this tree:        (A)         / \       /   \     (B)   (C)And I apply a change to it (a ""good"" change, so the algorithm should associate this change with the ""good"" changes). The updated tree would be like this:       (A)       / \      /   \    (D)   (C)    /   / (B)Added a certain node (D) above another node (B) would be classified as a ""good"" change.So when I have the learner with the correct data, the algorithm should be able to know that if I add a node of type D above a node of type B, it is a ""good"" change.I would like to work with XML files that keeps the tree structure, a simple classifier like a naive bayes would not work, because it wouldn't be able to recognise if a node is added above another one, it only would be able to know that a node has been added.I don't know how which algorithm/technique should I use and I don't know how should I pass the data to the learner, because the context in this scenario is important.I am new to machine learning, so sorry if this is a stupid question.Thanks";[education, open-source];54;1
5834;2;2015-05-19T15:46:55.280;;Here is an example of how to get pandas and sklearn to play nice say you have 2 columns that are both strings and you wish to vectorize - but you have no idea which vectorization params will result in the best downstream performance.create the vectorizer to_vect = Pipeline([('vect',CountVectorizer(min_df =1,max_df=.9,ngram_range=(1,2),max_features=1000)),                    ('tfidf', TfidfTransformer())])create the DataFrameMapper obj.full_mapper = DataFrameMapper([        ('col_name1', to_vect),        ('col_name2',to_vect)    ])this is the full pipelinefull_pipeline  = Pipeline([('mapper',full_mapper),('clf', SGDClassifier(n_iter=15, warm_start=True))])define the params you want the scan to considerfull_params = {'clf__alpha': [1e-2,1e-3,1e-4],                   'clf__loss':['modified_huber','hinge'],                   'clf__penalty':['l2','l1'],                   'mapper__features':[[('col_name1',deepcopy(to_vect)),                                        ('col_name2',deepcopy(to_vect))],                                       [('col_name1',deepcopy(to_vect).set_params(vect__analyzer= 'char_wb')),                                        ('col_name2',deepcopy(to_vect))]]}Thats it! - note however that mapper_features are a single item in this dictionary - so use a for loop or itertools.product to generate a FLAT list of all to_vect options you wish to consider - but that is a separate task outside the scope of the question.Go on to create the optimal classifier or whatever else your pipeline ends withgs_clf = GridSearchCV(full_pipe, full_params, n_jobs=-1);;;
5835;2;2015-05-19T16:59:34.567;;"You could do this a few ways...1) write a script that does the following.-pick a table,-get number of rows in the table, can do this as you go. (+=1 in loop)-select several fields(columns) in table that you think may form the key,-create empty Set()-burn through the file row by row, for each row grab the target fields and construct a string key str(field1)+'_'+str(field2)..etc.  Add this key to your set.-when you're done going through all rows of a table, check len(set).  If the key you picked is good then len(set) will equal # rows.2) Another way, and this will depend on what you want to do with your data and how often you will access it...is to import it into a sql database.  I usually use mysql. Once your data is in:select count(*) from table_name;  will give you rows in tableselect count( distinct field1, field2, field3) from table;  gives number of distinct combos of field1+field2+field3if the two selects give the same number then you have a key that works. It may not guarantee a join-key but it will be unique to that table and will help the linking process. 3) many people highly advocate the use of pandas. If you get the data imported into arrays you can create a dataframe and do some queries similar to SQL.  I'm not super familiar with pandas so I can't give an example.If it's a one-off, I'd make a python script.  But if you think you'll be exploring the data further, spending the time to put the data in a database could be useful.";;;
5836;2;2015-05-19T17:13:18.667;;"Although I agree with Neil Slater's response, you should keep a couple of things in mind.1) ""you never know!"" In data exploration, you never know what you may find.  If you have a ton of data, perhaps playing around with a 20x20 conv net will give you some decent results.  Of course, it would be helpful if there are more than just a few features for it to learn...if your 400 length vector is the result of one-hotting 4 different features then it's probably safe to say that a conv net won't give you much.2)  If you're looking for a reason to implement a conv net, then go for it. Even if your accuracy metrics are terrible you at least get to learn how to create your net, train, and predict using your own data...one cannot underestimate this learning experience!  So much more valuable that running yet another mnist example out of the box.3)  Comparison. Make a regular net and a conv net...then you get to compare the two. Not only that, compare it to a Random Forest, logistic regression, etc. etc.. Do this enough times and you start to develop intuition. I say do it! (unless somebody is paying you...in which case try the regular NN first)";;;
5837;2;2015-05-19T20:32:50.033;;"I like rvest in combination with the SelectorGadget chrome plug-in for selecting relevant sections.I've used rvest and built small scripts to paginate through forums by:Look for the ""Page n Of m"" object Extract m Based on the page structure,  build a list of links from 1 to m (e.g.    www.sample.com/page1)  Iterate the scraper through the full list of    links";;;
5838;1;2015-05-19T23:34:25.213;I am trying to classify/cluster users profile but don't know how with my attributes;"I have a dataset about users purchasing product from website. The attributes I have are  user id, region(state) of the user, the categories id of product, keywords id of product, keywords id of website, sales amount spent of the product. The goal is to use the information of product and website to identity who the users are, such as ""male young gamer"";""stay at home mom"". I attached a sample picture as below.There are totally 1940 unique categories and 13845 unique keywords for products. For the website, there are 13063 unique keywords. The whole dataset is huge as that's the daily logging data.I am thinking of clustering, as those are unsupervised, but those id are ordered number having no numeric meaning, then I don't know how to apply the algorithm. I also think of classification if I add a column of class based on the sales amount of product purchased. I think clustering is more preferred. I don't know what algorithm I should use for this case as the dimensions of the keywords id could be more than 10000 (each product could have many keywords, so does website). I need to use Spark for this project. Can anyone help me out with some ideas,suggestions? Thank you so much!";[education, open-source];143;2
5841;2;2015-05-20T09:59:47.257;;"Most of the machine learning algorithms are designed to work with data in a tabular format. That mean, each data instance is contained in a single row, and the values from each column are the observed values for a specific instance for a given variable. There are few reasons why the most ML algorithms are designed to work on this kind of data. An important factor is that the structure is very simple and various operations can be done with ease. A second reason is that even if looks like a inflexible structure, some sort of additional structure in your data still can be represented on tabular format (using redundancy). Another reason would be that an algorithm designed to work for a specific structure of the data will be constraint to work on a much smaller set of problems. So, the main point is that ""If the mountain won't come to Muhammad then Muhammad must go to the mountain"" (note there's nothing religion related here). So what you have to do would be to fabricate the features yourself in a tabular format. I will give you an example on how I see a starting point. Consider an instance a row in a table. Each row will be a change. A change has a label, it is good or bad. So, you can add a feature used as target feature called class. We go further by noting that a change is an insertion of a node. If your changes are of multiple type you can add a feature called operation-type having values: insert, delete, change, etc. Now, a node has a type also. You can add a new feature called node-type, which could be A, B, etc. What you have to do would be to invent those features by noting what is important for you or your business and eventually select only those features which would be relevant enough. I really hope it was clear enough.";;;
5842;1;2015-05-20T11:04:18.223;Index for efficient argmax(w.x) query ~ 20d;I'm looking for a spatial index that can efficiently find the most extreme n points in a certain direction, i.e. for a given w, find x[0:n] in the dataset where x0 gives the largest value of w.x and x1 the second largest value of w.x, etc... . Is there a name for this type of query? What would be an efficient data structure to use? x might have around 20 dimensions.Thankyou!;[education, open-source];17;
5843;2;2015-05-20T13:13:05.653;;Scrapy is a great Python library which can help you scrape different sites faster and make your code structure better. Not all sites can be parsed with classic tools, because they can use dynamic JS content building. For this task it is better to use Selenium (This is a test framework for web sites, but it also a great web scraping tool). There's also a Python wrapper available for this library. In Google you can find a few tricks which can help you use Selenium inside Scrapy and make your code clear, organized, and you can use some great tools for Scrapy library.I think that Selenium would be a better scraper for Linkedin than classic tools. There is a lot of javascript and dynamic content. Also, if you want to make authentication in your account and scrape all available content, you will get a lot of problems with classic authentication using simple libraries like requests or urllib.;;;
5844;1;2015-05-20T13:24:33.733;What are available methods for modeling startup survival rates?;I am interested in modeling startup companies failure and success rates to describe what is the representative startup. I have 40 companies in a dataset. Each company is represented as a list of all the investment financing rounds it has gone through. For example:Company 1: Seed Round, Series A RoundCompany 2: Seed Round, Series A Round, Series B RoundCompany 3: Seed Round, Series A Round, Series B RoundCompany 4: Seed Round, Series A Round, Series B Round, Series C RoundCompany 5: Seed RoundCompany 6: Series A Round, Series B RoundCompany 6: Series A RoundAnd so on.I can see that each round can be represented as a state in a Markov Chain, and transitions are only allowed from earlier stages to later stages. I can go from Seed to Series A, and from Series A to Series B, but not from Series B to Series A.So we have N-order markov chains (production data has N <= 4).The output I'm looking for is a binary tree chart showing each stage as a node and each node can either transition to the next node or to a final state meaning the company has failed.This problem can also be seen as a real options model...Any ideas on how to implement this model?I can code in Python or Ruby (but I am no expert).;[education, open-source];62;0
5845;2;2015-05-20T14:11:38.367;;The query is called a 'top-k' query, and you can answer it quickly using the ranking cube approach. http://www1.se.cuhk.edu.hk/~hcheng/paper/vldb06_rankcube.pdfThe paper does not derive the time complexity.;;;
5846;1;2015-05-20T17:41:15.180;Ideas for next step of Machine Learning;I have completely followed the machine learning course on coursera Machine Learning by professor Andrew Ng Now I want to put my knowledge to action. Some ideas that I have include : -Voice synthesis -Voice recognitionBut since the course did not focus specifically on application of machine learning in these domains, could some one point me to some other course or books that can get me started.;[education, open-source];55;1
5848;2;2015-05-21T05:33:57.283;;I would suggest you to check this excellent presentation by Li Deng (Microsoft Research). Many of the slides contain references to relevant research papers and even several interesting books on the topics of interest (it should be pretty easy to find). It might be also helpful to check references, listed in this research paper by Prof. Andrew Ng and his colleagues at Baidu Research. Finally, a focused Internet search will provide you with comprehensive list of resources for further research.;;;
5849;2;2015-05-21T08:17:15.573;;"Both previous answers take an interesting approach, but neither really tackles exactly what I was asking; how to compare a given terms frequency over time, and compare with other terms over the same period, in a statistically rigorous way that accounts for the power law distribution of term frequencies with collection size.The answer below is my work so far on the problem. It is not, by any means, a complete answer, however I post it here to elicit feedback and suggestions for improvement.Take the definition of Zipf's law,$$f(r) \sim z_{max} r^{-\alpha}$$here, $r$ is the rank of a given term, $z_{max}$ is the frequency of the most frequent term, $\alpha$ is Zipf's exponent, and $f(r)$ is the frequency of the r-th ranked term.Taking the log of both sides, you get a linear relationship between $log(f)$ and $log(r)$, with gradient $-\alpha$ and intercept $z_{max}$$log(f) \sim log(z_{max}) - \alpha log(r)$We want to find the expected frequency of a single term over a given period. First, split the period into equal sized windows. For the first window, $i$, we know the frequency of our term $f_i$, and the frequency of the largest term $z_{max,i}$ and $\alpha$. We can use these to find the rank of our term,$r \sim \left(\frac{f_i(r)}{z_{max,i}}\right)^{-\alpha}$This rank can then be used to calculate the 'expected' frequency for all subsequent time windows, which I'll indicate with $\hat{f}$,$\hat{f}_{i+1}(r) \sim z_{max,i+1} r^{\alpha(i+1)} $The ratio $\frac{\hat{f}}{f}$ represents the normalised frequency.Some observations:For a stationary underlying distribution, this approach identifies minor deviations from the expected frequency of a term. A moving window could be used if the underlying text distribution changes dramatically, or exhibits seasonal behaviour.The initial choice of $r$ could be calculated for the entire period, rather than the first window.$\alpha$ could be calculated from the raw term and total frequency counts using heap's law (see Lu et al. 2010 for a derivation of Heap's law from Zipf's).This approach assumes a linear relationship between $f$ and $r$ in log space.";;;
5851;2;2015-05-21T08:26:42.463;;Sounds like you could use a regression model to estimate the probabilities that a team wins/draws/loses vs. another team. Basically, for any outcome (win, draw and lose), you want this:P(A|B) = ...P(B|A) = ...Which means: the probability of outcome for team A given that it is matched against team B (and vice versa).Estimations could be represented like this:P(A > B) = 0.75 % A winsP(A = B) = 0.10 % A drawsP(A < B) = 0.15 % A losesP(B > A) = 0.20 % B winsP(B = A) = 0.10 % B drawsP(B > A) = 0.70 % B losesI think a logical step is to measure the bias towards a certain outcome. That would represent the confidence ratio of your algorithm. The more the probabilities of any outcome looks alike (i.e. P(B >/=/< A) = 0.33), the less confidence it has.;;;
5852;2;2015-05-21T08:46:49.783;;For orientation and exploration, I can recommend WeKa, which is a very nice toolkit for machine learning. It does take a certain input format (.ARFF) so you might need to look into that as well.As for the keyword dilemma, I recommend performing some feature selection in order to eliminate redundant or non-indicative keywords.;;;
5853;2;2015-05-21T12:51:35.033;;"Word of warning from a former airline Revenue Management analyst: you might be barking up the wrong tree with this approach. Apologies for the wall of text that follows, but this data is a lot more complex and noisy than might appear at first glance, so wanted to provide a short description of how it's generated; forewarned is forearmed.Airline fares have two components to them: all the actual fares (complete with fare rules and what have you) that an airline has available for a certain route, most of which are published the Airline Tariff Publishing Company (a few special-use ones are not, but those are the exception rather than the rule) and the actual inventory management performed by the airline on a day-to-day basis.Fares can be submitted to ATPCO four times a day, at set intervals, and when airlines do so, it will usually consist of a mixture of additions, deletions, and modifications of existing fares. When an airline initiates a pricing action (assuming their competitors aren't trying to make their own moves here), they usually have to wait until the next update to see if their competitors follow/respond. The converse goes when a competitor initiates a pricing action, as the airline has to wait until the next update before they can respond.Now, this is all well and good with respect to fares, but the problem is that, because this is all getting published in ATPCO, fares are the next best thing to public information... all your competitors get to see what you've got in your arsenal, so attempts to obfuscate are not unheard of, such as publishing fares that will never actually be assigned any inventory, listing all the fares as day-of-departure, etc. In many ways, the secret sauce comes down to the actual inventory allocation, i.e. how many seats on each flight will you be willing to sell for a given fare, and this information is not publicly available. You can get some glimpses by scraping web info, but the potential combinations of departure time/date and fare rules are quite numerous and may quickly escalate beyond your ability to easily keep track of. Typically an airline will only be willing to sell a handful of seats for a very low fare and the people who snag those have to book quite far in advance lest the fare rules lock them out, or other travelers simply beat them to the punch. The airline will be willing to sell a few more seats for a higher fare, and so on and so forth. They will be quite happy to sell all of the seats for the highest fare they've got published, but this is not usually feasible.What you're seeing with fares getting higher the closer you get to the day of departure is simply the natural process of having the cheap seats get booked farther out, while the remaining inventory gradually gets more expensive. Of course, there are some caveats here. The RM process is actively managed and human intervention is quite common as the RM team generally strives to meet its revenue goals and maximize revenue on each flight. As such, flights that fill up quickly may be ""tightened up"" by closing out low fares. Flights that are booking slowly may be ""loosened up"" by allocating more seats to lower fares.There is a constant interplay and competition between airlines in this area, but you are not very likely to capture the actual dynamics just from scraping fares. Don't get me wrong, we had such tools at our disposal, and, despite their limitations, they were quite valuable, but they were just one data source that fed into the decision-making process. You'd need access to the hundreds, if not thousands of operational decisions made by RM teams on a daily basis, as well as state-of-the-world information as they see it at the time. If you cannot find an airline partner to work with in order to get this data, you might need to consider alternate data sources.I'd recommend looking into getting access to O&D fare data from the Official Airline Guide (or one of their competitors) and try to use that for your analysis. It's sample-based (about 10% of all tickets sold) and aggregated at a higher level than would be ideal so careful route selection is imperative (I'd recommend something with plenty of airlines, flying non-stop multiple times a day, with large aircraft), but you may be able to get a better picture of what was actually sold (average fare) and how much of it was sold (load factor), vs. merely what is available for sale at a given point in time. Using that information you might be in better position to at least explore the outcomes of the airlines' pricing strategy, and make your inferences from there.";;;
5854;2;2015-05-21T15:23:44.270;;"If the data you show are the only data that you have, then the Markov Chain is really boring: it is a linear chain, going from Round A to Round B to Round C, and all those states are connected to a base state (which is Death, or something).You can directly calculate the transition probabilities from the data you have, since the number of companies that reached round N are all companies that could have reached round N (there is no alternative path).  The death probability at the previous stage is (1 - $P_{reaching N}$)In [1]: raw_data = """"""   ...: Company 1: Seed Round, Series A Round   ...: Company 2: Seed Round, Series A Round, Series B Round   ...: Company 3: Seed Round, Series A Round, Series B Round   ...: Company 4: Seed Round, Series A Round, Series B Round, Series C Round   ...: Company 5: Seed Round   ...: Company 6: Series A Round, Series B Round   ...: Company 6: Series A Round   ...: """"""In [2]: data_lines = raw_data.splitlines()[1:]In [6]: key_vals = {}In [12]: for line in data_lines:             key, val = line.split(':')             key = key.strip()             vals = [v.strip() for v in val.split(',')]             key_vals[key] = valsIn [13]: key_valsOut[13]:{'Company 1': ['Seed Round', 'Series A Round'], 'Company 2': ['Seed Round', 'Series A Round', 'Series B Round'], 'Company 3': ['Seed Round', 'Series A Round', 'Series B Round'], 'Company 4': ['Seed Round',  'Series A Round',  'Series B Round',  'Series C Round'], 'Company 5': ['Seed Round'], 'Company 6': ['Series A Round']}In [14]: transitions = ['Seed Round', 'Series A Round', 'Series B Round', 'Series C Round']In [19]: for transition in transitions:    summed = 0    for company, rounds in key_vals.iteritems():        if transition in rounds:            summed += 1    prob = float(summed) / float(len(key_vals.keys()))    death_prob = 1 - prob    print ""From previous to %s: probability %s"" % (transition, prob)    print ""Death rate at %s: probability %s"" % (transition, death_prob)From previous to Seed Round: probability 0.833333333333Death rate at Seed Round: probability 0.166666666667From previous to Series A Round: probability 0.833333333333Death rate at Series A Round: probability 0.166666666667From previous to Series B Round: probability 0.5Death rate at Series B Round: probability 0.5From previous to Series C Round: probability 0.166666666667Death rate at Series C Round: probability 0.833333333333However, if you had some more features of each company, like the amount of money they received at each stage, or the profit they were making, then you could train a decision tree, e.g. with this implementation in sklearn, that told you, in simple words, ""if a company arrived at round X with at least Y dollars raised and at least Z dollars of profit, then they are passing to the next round with 0.XX probability"".  Which is, I think, what you are aiming at.";;;
5855;1;2015-05-21T15:25:35.377;Gerrymandering - Geospatial optimization to maximize votes in R;"I am looking for a practical guide/tutorial preferably in R to show how to do gerrymandering. (I was looking for it also in CRAN but didn't find such package) Gerrymandering is the manipulation of the boundaries of electorial districts in order to gain political advantage for one party. If there is an analogy/similar process in another area it would be also interesting.I have detailed historical  election results. I can see the results from each ""voting office"" and also the geographical area they cover. These results are aggregated to electorial district level. I'd like to see how moving voting offices to different electorial districts can influence the results of the election and how this process can be optimized for one party. Certainly there should be some constraints not to create really weird shaped districts at the end.";[education, open-source];44;
5856;1;2015-05-21T17:57:35.003;How is Data Science related to Machine learning?;I went through this comparison of analytic disciplines and this perspective of machine learning, but I am not finding any answers on the following:How is Data Science related to Machine learning? How is it not related to Machine Learning? ;[education, open-source];255;2
5857;2;2015-05-21T19:00:55.473;;"Right now, I only have time for a very brief answer, but I'll try to expand on it later on.What you want to do is a clustering, since you want to discover some labels for your data.  (As opposed to a classification, where you would have labels for at least some of the data and you would like to label the rest).In order to perform a clustering on your users, you need to have them as some kind of points in an abstract space.  Then you will measure distances between points, and say that points that are ""near"" are ""similar"", and label them according to their place in that space.You need to transform your data into something that looks like a user profile, i.e.: a user ID, followed by a vector of numbers that represent the features of this user.  In your case, each feature could be a ""category of website"" or a ""category of product"", and the number could be the amount of dollars spent in that feature.  Or a feature could be a combination of web and product, of course.As an example, let us imagine the user profile with just three features:dollars spent in ""techy"" webs, dollars spent on ""fashion"" products, and dollars spent on ""aggressive"" video games on ""family-oriented"" webs (who knows).In order to build those profiles, you need to map the ""categories"" and ""keywords"" that you have, which are too plentiful, into the features you think are relevant.  Look into topic modeling or semantic similarity to do so.  Once that map is built, it will state that all dollars spent on webs with keywords ""gadget"", ""electronics"", ""programming"", and X others, should all be aggregated into our first feature; and so on.Do not be afraid of ""imposing"" the features!  You will need to refine them and maybe completely change them once you have clustered the users.Once you have user profiles, proceed to cluster them using k-means or whatever else you think is interesting.  Whatever technique you use, you will be interested in getting the ""representative"" point for each cluster.  This is usually the geometric ""center"" of the points in that cluster.Plot those ""representative"" points, and also plot how they compare to other clusters.  Using a radar chart is very useful here.  Wherever there is a salient feature (something in the representative that is very marked, and is also very prominent in its comparison to other clusters) is a good candidate to help you label the cluster with some catchy phrase (""nerds"", ""fashionistas"", ""aggressive moms"" ...).Remember that a clustering problem is an open problem, so there is no ""right"" solution!  And I think my answer is quite long already; check also about normalization of the profiles and filtering outliers.";;;
5858;2;2015-05-21T19:29:24.063;;Data science is much more broad. It's sort of a catch-all term that right now doesn't honestly have a very clear definition. But data-science includes all of the skills and techniques required to make sense of data which has high velocity (it's coming at you quickly), volume (there's a lot of it), or variability (it's messy, like natural language processing). This means that it certainly includes machine learning and AI, but that it's also about the tools one might use in a real-world situation such as SQL, Hadoop or Spark (and related information such as a knowledge of parallel programming). Additionally, data science may or may not include the communication aspect such as making good graphs and using Excel.Basically, Data Science is ML+.;;;
5859;2;2015-05-21T19:46:02.663;;Machine Learning tries to create systems that can learn from data. As such it can be used in a wide variety of settings, for example to make robots learn to walk or train virtual agents to play video games.Data science concerns itself with the extraction of knowledge from data. In order to do so it uses a bunch of different techniques from different disciplines. Machine learning includes some techniques that can be very useful for a data scientist such as deep learning, decision trees and different clustering algorithms. However, machine learning has more to offer than Data Science uses and Data Science does not solely rely on Machine Learning.;;;
5860;1;2015-05-21T22:26:59.077;Where to start, which books;What's a good book to start learning Artificial Intelligence ? What field to learn first ? What are the prerequisites ? Thanks in advance.;[education, open-source];173;2
5861;2;2015-05-22T09:22:12.163;;"Data science is much broader concept than machine learning.It starts from simple data visualization and descriptive statistics to get insights, manipulations like cleansing to prepare data. Before you can use some ML algorithms. Basically such huge stacks as bigdata, visualization and data preprocessing are out of machine learning scope. And they are all integral parts of ""Data Science"".Large resolution image:https://whatsthebigdata.files.wordpress.com/2013/07/datascientistmap.png";;;
5862;2;2015-05-22T11:28:01.467;;I would also go with beautifulsoup, if you know python. In case you rather code javascript/JQuery (and you are familiar with node.js), you may want to checkout CoffeeScript (Check out the Tutorial) I already used it successfully on several occasions for scraping web pages.;;;
5863;1;2015-05-22T13:21:44.143;Where does the sum of squared errors function in neural networks come from?;Training a basic multilayer perceptron neural network boils down to minimizing some kind of error function. Often the sum of squared errors is chosen as a this error function, but where does this function come from?I always thought this function was chosen because it makes sense intuitively. However, recently I learned that this is only partly true and there is more behind it.Bishop wrote in one of his papers that the sum of squared errors function can be derived from the principle of maximum likelihood. Furthermore he wrote that the squared error therefore makes the assumption that the noise on the target value has a Gaussian distribution.I am not sure what he means with that. How does the sum of squared errors relate to the maximum likelihood principle in the context of neural networks?;[education, open-source];132;
5864;2;2015-05-22T13:38:27.207;;"The standard textbooks that covers AI is ""Artificial Intelligence: A Modern Approach"" by Russel & Norvig. The book's website can be found here.I also recommend ""Artificial Intelligence: Foundations of Computational Agents"" by Poole and Mackworth. The book can be read online.";;;
5865;2;2015-05-22T14:01:28.910;;Your reference of Bishop is not entirely accurate. What he states in the paper you linked isIt should be noted that the standard sum-of-squares error, introduced here from a heuristic viewpoint, can be derived from the principle of maximum likelihood on the assumption that the noise on the target data has a Gaussian distribution [references cited]. Even when this assumption is not satisfied, however, the sum-of-squares error function remains of great practical importance.The important point with regard to your question is that there is no inherent assumption that there is Gaussian noise when training a Multilayer Perceptron (MLP). Therefore, for an MLP, the sum-of-squares error function is not derived from the principle of maximum likelihood.For example, consider training an MLP to learn the XOR function. There are four pairs of inputs with corresponding outputs but there is no noise in the data. Yet the sum-of-squares error is still applicable.The relevance of using sum-of-squares for neural networks (and many other situations) is that the error function is differentiable and since the errors are squared, it can be used to reduce or minimize the magnitudes of both positive and negative errors.;;;
5867;1;2015-05-22T21:22:15.133;What loss function does the 'multinomial' distribution with the gbm package in R use?;All distributions in the gbm package in R are associated with a loss function. For example, when we set distribution = 'binomial', the loss function chosen internally is the logistic loss function. Can anyone explain how multi-class classification works with gbm and the loss function that is being used for it i.e. when we set distribution='multinomial'? Is it using one-vs-all or all-vs-all under the hood for doing its multi-class classification? ;[education, open-source];82;
5868;2;2015-05-22T22:22:34.720;;Data Science is, as others have noted, a much broader term than machine learning. Applying Machine learning techniques is one aspect of data science. Data Science,  more generally, is the science of deriving knowledge from data. The term was coined back in 1960 and kept evolving to describe the flow and interplay of problem definition, data collection, data transformation, data modeling/ analysis,  and decision making. So to answer your question specifically: Machine learning aids data science by providing a suit of algorithms for data modeling/ analysis (through training of machine learning algorithms), decision making (through streaming, online learning, real-time testing that are all topics that come under machine learning), and even data preparation (machine learning algorithms automatically detect anomalies in the data). Data Science stitches together a bunch of ideas/ algorithms drawn from machine learning to create a solution and in doing so borrows a lot of ideas from traditional statistics, domain expertise and basic mathematics. In this way, data science is the process of solving a use case, providing a solution as opposed to machine learning that is an important cog in that solution. ;;;
5869;2;2015-05-22T22:29:22.153;;Lots of techniques out there. If your information system has a decision attribute or labels attached, the best way I found is to use rough set based attribute reduction. Check out the quick reduction algorithm by Qiang Shen and Richard Jensen.If you have unlabeled data, check Principal Component Analysis (PCA).;;;
5870;2;2015-05-22T22:39:53.793;;Well the text doesn't get converted into data points ...Let's say we are doing sentence level opinion mining..Features are extracted from a sentence .Now it depends on case to case as to what features to use...A common one is bag of words models in which features become distinct words in sentence and the value of features are the frequency it is repeated in a sentence. Those frequencies are your data points. ;;;
5872;2;2015-05-22T22:48:36.107;;Model is as a stochastic process . Markov chains is the way to go. Create a stochastic matrix where states could be  A team T  ...get all the combinations possible and use past data to get initial probabilities of winning ...and then use the beautiful property of Xn= XiP^nWhere Xn is probability vector of nth stage from now and Xi is initial vector and P is probability transition matrix .;;;
5874;1;2015-05-23T02:20:24.870;What techniques are used to understand call patterns?;I have customer data since 2013 and there is a file which has the customer unique id, a timestamp, and the reason for the call (a drop down from the person who handled the call).  I did some cumulative counts based on customer ID and the timestamp and I saw that one customer called in over 1000 times alone. What's the best way to make sense of the call driver data when I'm looking at millions of rows and around 200 categories of call types?  Is there a broader topic which looks into 'downstream'  issues or predicting the probability of future calls or events?  The end goal would be to visualize these calling patterns and focus on reducing the call backs. This is a specific problem but it seems like it should be common and I can learn about addressing it in a bigger picture manner. ;[education, open-source];31;
5875;1;2015-05-23T11:45:23.043;How to find the input variables for a classification problem?;I am working on a  classification problem. I have 1000+ features in this dataset. I don't know how to select the right variables/ features that can actually contribute to predicting the output. What are the different methods through which I can identify the important variables that can be used out of these 1000+ variables.;[education, open-source];131;
5877;2;2015-05-23T13:09:29.803;;Two approaches come to mind:Use all of them and perform feature selection to identify indicative features. Algorithms include Information Gain and Mutual Information.Hand-pick some features you deem indicative intuitively, or delete the ones you deem non-indicative.Generally, you have some sort of hypothesis that you are testing where you attest the capability of (a subset of) your features (and the algorithm you'll use) to uniquely identify the label/class they belong to. The features you use are one of the most important things in the classification task. So it is good to spend time to intelligently find out which features contribute. For this task, you do have a lot of possible features so the automatic feature selection approach might be better.;;;
5878;2;2015-05-23T20:45:02.473;;A somewhat popular introduction is Andrew Ng's Stanford  machine learning lectures;;;
5879;2;2015-05-23T20:56:03.840;;Start with neural nets. They are the basic building blocks of Deep Learning.Neural Nets for Newbies video is a well-explained introduction. CS-449 from Willamette University is an free and approachable class in neural networks.That should lay the foundation for more advanced Deep Learning material. Deep Learning by Bengio, Goodfellow, and Courville is a quality book on the subject.;;;
5880;2;2015-05-23T21:13:32.363;;Take a look at this free class Intro to Artificial Intelligence from Udacity. One of the instructors is Norvig. The class is more suitable for beginners than Norvig's book.Even though this class doesn't have programming exercise, it explains concepts so well.Its follow up class Artificial Intelligence for Robotics has programming exercises and does a fantastic job in explaining Partical Filters.;;;
5881;1;2015-05-23T21:49:17.603;Data prediction book;While I was studying, few years ago , one of the most interesting topic was evolution, genetic algorithms and neural networks. Many of the problems I faced could be solved by using that knowledge.I assume that since that time world has found more interesting algorithms, do you recommend some books worthy of reading in that domain ? Mainly I am looking a way to find patterns in huge amount of data. Ex. Having energy consumption for few years for one building.Lets have an algorithm that is trying to find all possible repeatings in many variations.Obviously at the beginning it should find that at the weekend energy consumption is less then average between Mon and Fri, but is that possible that an algorithm would tell me sth like this ? :Every Friday at third week in even months user sleeps for 3 hours and in uneven months 5 hours ?Or the algorithm finds it self that user like to save energy so if previous month he sees that he spent more then usual , next month he is trying to spent less, or lets assume that user eats breakfast at work but if he eats it at home then he will stay whole day at home, then lets assume that he has off day, then check energy usage after breakfast if is high, which means user is preparing to leave or is small which means that he wants to sleep and basically stays home. So I was wondering if this possible to auto detect this kind of patterns ? I am enthusiast of c# and interested in R.;[education, open-source];66;1
5882;2;2015-05-24T00:07:19.097;;you can trace the squared error in statistics through multivariate calculus all the way to Pythagorus. You are basically calculating the 'effective length' of the error, the hypotenuse, among errors from multiple variables $(X_1 - X_2)^2 + (Y_1 - Y_2)^2 + ...$ like in a triangle.But where did the square root go?Somebody realized that calculating roots of multiple variables over multiple iterations is computationally very expensive. So they decided to drop it. Checkout the squared Euclidean distance here for more detailsHow would a cubic error or a logarithmic error affect the outcome?It just takes more time to converge because they are not as accurate. But we do see logarithmic errors over squares such as logistic regression where it is more optimalAll in all it is a simple case of optimization;;;
5883;2;2015-05-24T00:42:07.003;;If you are looking specifically for books, the leaders in the industry have a variety of recommendations as posted on reddit AMA's of machine learning. One specific book I recommend, which is in preparation at the time, is Deep Learning by Yoshua Bengio et al There are plenty of resources outside books to learn from as posted here and hereDepends on how much time you have. If you are in it for the long haul, start by brushing up your knowledge on math, specifically Linear Algebra, Calculus, Probability and Statistics;;;
5884;2;2015-05-24T00:51:22.027;;please refer this question Books recommended by Yann Lecun can be found here and by G. Hinton here;;;
5885;1;2015-05-24T02:11:36.640;How do I normalize an array of positive and negative numbers so they are between 0 and 1?;"Using Brain to feed in an array of data with both positive and negative numbers; the output array will be in 0's and 1's and I believe Brain only allows inputs from 0 to 1. So how do I normalize an array of negative and positive numbers so it fits these requirements?";[education, open-source];652;3
5886;2;2015-05-24T03:12:06.020;;Find the largest positive number and the smallest (most negative) number in the array.Add the absolute value of the smallest (most negative) number to every value in the array.Divide each result by the difference between the largest and the smallest number.;;;
5887;2;2015-05-24T03:48:42.427;;"say you have a vector/array of values v = [1, -2, 3]minV = Math.min.apply(Math, v);;for(var i=0; i<v.length; i++) {v[i] -= minV;}maxV = Math.max.apply(Math, v);;for(var i=0; i<v.length; i++) {v[i] /= maxV;}Output at the end will be v = [0.6, 0, 1]. Explanation:Pushing the entire range of values to start from 0, so that we have no negativesDividing the values by max of range, so that max will be 1";;;
5888;2;2015-05-24T05:50:50.530;;This is called unity-based normalization. If you have a vector $X$, you can obtain a normalized version of it, say $Z$, by doing:$$Z = \frac{X - \min(X)}{\max(X) - \min(X)}$$ ;;;
5889;2;2015-05-24T10:04:25.767;;"In the third paragraph of the first section (page 1), they define $\#(w)$, $\#(w; c)$, and $\#(c)$. Quoting such paragraph: Word-context pairs are denoted as $(w; c)$, and $\#(w; c)$ counts all observations of $(w; c)$ from the corpus. We use $\#( w ) =  \sum_c \#(w; c)$ and $\#(c) = \sum_w \#(w; c)$ to refer to the count of occurrences of a word (context) in all word-context pairs. Either $\sum_c \#(c)$ or  $\sum_w \#(w)$ may represent the count of all word-context pairs.So, considering the equation you cited:$\#(w_i, c_j)$ counts the occurrences of a specific word $w_i$ over the context $c_j$;and $\frac{\#(c_)}{\sum_w \#(w)}$ accounts for the the probability of having context $c_j$ in the dataset.";;;
5890;1;2015-05-24T11:54:43.347;Different methods for clustering skills in text;"Consider a talent pool in which each member has some set of skills. Some of these talent are submitted to orders as potential candidates of which one is selected. It is reasonable to assume that the submitted talent have some dominant thing in common in their skill sets (let's call it a segment) that qualifies them for the order. Example segments are ""front end web-designer"" or ""brochure / sprint designer"".Given the total set of skills over all of the talent submitted to an order (like 2-5 with say 10 skills each, so 20 - 50 skills total), I am looking for the dominant segment. Then, I am looking for the dominant segment for each individual talent.My plan is to use latent Dirichlet allocation (LDA) such that the skills of all the talent submitted for an order are a ""document"" that contains some segments or ""topics"" with some probability. Likely, there will be one or two dominant topics depending on the total topic number. I will then use this model to predict the dominant segment for each talent where the individual talent skill set is a ""document"" with some segments or ""topics"" within.I am curious if anyone has feedback about my use of LDA or other ideas about how I might go about discovering these segments?";[education, open-source];90;
5892;2;2015-05-24T16:11:35.860;;"When you said ""a broader topic"", did you mean what algorithms to use to examine the event log with the goal of reducing future calls from the same customer on the same topic? In other words, a customer may call for help for a different topic. If a customer calls in for the same topic repetitively, something can be improved.You may get ideas from a Coursera class Processing Mining since the issue you're solving is similar to the example of a spaghetti process in lecture 6.7:""Spaghetti process describing the diagnosis and treatment of 2765 patients in a Dutch hospital. The process model was constructed based on an event log containing 114,592 events. There are 619 different activities (taking event types into account) executed by 266 different individuals (doctors, nurses, etc.).""By the way, you can click the drop down menu under ""Sessions"", choose ""April 1, 2015 to May 19, 2015"" then you can register and view the lectures. On the right of each lecture, there are some icons. The first icon is to download slides for the lecture. You may find reading the slides is faster than listening to the lecture.Suppose a customer called for installation of software release 1.5 then called a day later for running the new features of software release 1.5. Are these two issues logged as two of the 200 call categories? If so, we can use a time period (say one week) to judge whether it is about the same topic. Within a short time period, a customer is likely to work on the same topic, especially with the same key words such as ""software release 1.5"". We can coach the call center employees on solving possible follow up questions by saying something like ""now that we finished installation, let me show you a couple of new features. It'll save you time."" This will reduce the number of calls on the same topic from the same customer.";;;
5893;1;2015-05-24T21:45:02.207;How to create a good list of stopwords;I am looking for some hints on how to curate a list of stopwords. Does someone know / can someone recommend a good method to extract stopword lists from the dataset itself for preprocessing and filtering?The Data:a huge amount of human text input of variable length (searchterms and whole sentences (up to 200 characters) ) over several years. The text contains a lot of spam (like machine input from bots, single words, stupid searches, product searches ... ) and only a few % of seems to be useful. I realised that sometimes (only very rarely) people search my side by asking really cool questions. These questions are so cool, that i think it is worth to have a deeper look into them to see how people search over time and what topics people have been interested in using my website.My problem:is that i am really struggling with the preprocessing (i.e. dropping the spam). I already tried some stopword list from the web (NLTK etc.), but these don't really help my needs regarding this dataset. Thanks for your ideas and discussion folks!;[education, open-source];83;
5894;2;2015-05-25T06:32:29.107;;I came across this collection on Github. The collection is categorised as well.https://github.com/caesar0301/awesome-public-datasetsAnd for the part regarding  Can not a open-source model for sharing data sets devised for data scientists?you can refer The Leek group guide to data sharing;;;
5897;1;2015-05-25T07:54:09.283;How deep should ones linear algebra knowledge be before starting data science?;How important is linear algebra to being a data scientist? Are we talking college postgraduate level?;[education, open-source];58;
5898;2;2015-05-25T08:57:19.877;;"I think it truly depends on what you decide to specialize in. Data science is a very broad field, and you can actually work with data without knowing what eigenvalues and eigenvectors are. However, if you want to acquire a intermediate/advanced understanding of statistics or machine learning, you need at least an intermediate/advanced knowledge of linear algebra.I suggest to take an introductory class on linear algebra on MOOC - just to have a more precise idea of what linear algebra is - and then study some other topics that you are interested in. Linear algebra is a useful tool, but it can be very boring, especially if you are an ""applied"" kind of guy. Moreover, I think that some concepts like eigenvalues or eigenvectors are easier to understand when seen in an applied context, e.g. principal component analysis.";;;
5899;1;2015-05-25T12:37:20.030;Do I need to apply a Ranking Algorithm for this?;I have data of the form :Id1      A_Id2      B_Id2       C_Id2       D_Id2      E_Id2      F_Id21         6           3           9           23         20         51         4           7           8           9          11         56                                                   1         2           36          98          73         2          4     1         9           5           2           7          32         24           1         14          7           5           9          12         5                                                   2         34          4           7           10          7         12                                                       2         5           57          23          91          4         6                                                    2         7           .           .           .           .         .2         3           .           .           .           .         .2         .           .           .           .           .         ...100      .            .           .           .           .         .            Basically, I want to build a model such that I can get a best match of Id1, given top 5 Id2 matches of each attribute(A_Id2, B_Id2,...,F_Id2). Now every match should be computed keeping in mind A has highest priority, followed by B and C, followed by D and least priority to E and F. So the output will look like this :Id1    Match_Id21        92        73        .4        .I hope the problem is clear, if not please ask.How should I go about building a Machine Learning model for this? I was wondering if ranking algorithm will help ?;[education, open-source];38;
5901;2;2015-05-25T15:13:50.583;;The first idea in my mind is : view it forming a social graph where nodes are the email-ids (people) and 2 nodes are connected if they communicate with each other. You can also view it as a weighted graph where weight comes from the frequency of conversation and you can add the sense of direction as well using the sender-receiver information. Now you can apply all sort of social network analysis on this.;;;
5902;2;2015-05-25T15:19:21.227;;One approach would be to use tf-idf score. The words which occur in most of the queries will be of little help in differentiating the good search queries from bad ones. But ones which occur very frequently (high tf or term-frequency) in only few queries (high idf or inverse document frequency) as likely to be more important in distinguishing the good queries from the bad ones.;;;
5903;2;2015-05-25T15:19:34.493;;Using TFIDF (term frequency inverse document frequency) will solve your purpose. Get the TFIDF score for each word in your document and sort the words by their scores by which you can select the important words in your data.;;;
5904;2;2015-05-25T15:27:09.517;;A good introductory course for social network analysis : https://www.coursera.org/course/sna;;;
5907;2;2015-05-25T18:06:22.790;;You may want to consider preprocessing - convert different wording for the same type of talent to same wording. For example, a talent in machine learning is called data scientist at Coursera job site, data engineer at Udacity job site, or data analyst. This preprocessing is similar to stemming in concept.;;;
5908;2;2015-05-26T05:11:36.160;;The general approach in feature selection is to get a score of each feature in the data set and select top features. We can run algorithms like GBM or Random Forest on all the variables simply to get a ranking of variable importance. We can also use χ² (chi-squared) statistic with cross validation to select a user-specified percentile of features with the highest scoring. But, the disadvantage with these approaches is not detecting correlations between features. We can also use backward elimination: features are tested one by one and the ones that are not statically significant are deleted from data set. In forward selection that starts with out any variable in dataset and then adds variables that are statically significant. Hope this helps. ;;;
5909;2;2015-05-26T11:26:14.063;;One approach can be to use a no-SQL database running on top of a distributed store (like Cassandra or Hbase). Add an external index which support spatial indexing (say elastic) for fast search. This makes your solution scalable (due to distributed store) and fast enough for spatial search queries.;;;
5910;1;2015-05-26T12:36:54.197;Minimize correlation between input and output of black box system;"I am not sure if ""minimize correlation"" is the right title for this issue but I could not find a better sentence to describe what I would like to achieve.Let's say that I have a black box with multiple inputs and a single output. I know one of the inputs and the output and I have multiple example recordings of both. This known input modifies the output in a way that it is not desired, therefore, I would like to get rid of this ""noise"" caused by the known input. The transfer function for this input can be safely assumed as linear.What I am doing right now, it is to loop through the example recordings, creating a linear regression model to predict the unwanted outcome and subtracting it from the real measured output signal, for each example. Afterwards, I compute the average of all the fixed output signals to reveal meaningful data beyond noise.This strategy seems to work according to the following plot:X axis is the known input signal, Y axis is the output signal, blue and green dots represent the averaged data before and after applying the linear regression algorithm, respectively. Lines are the best fit for each data set. You can see that the green line (""cleaned"" dataset) has the smallest slope, meaning that the output variable is considerably less linearly correlated with the input than it was previously. Therefore, I assume that the regression technique explained before is working as expected.My question, looking at the plot, is there any mathematical procedure to directly ""project"" the original dataset in a way that the correlation between the input and output variables is minimized? Is there any math trick to avoid the use of the regression technique on all the example datasets to obtain a similar result? My written expression is not the best so please feel free to comment the question if you need further explanations.Any code is welcomed but python (pandas, numpy, etc.) and Matlab are preferred. Theoretical explanations are also very welcomed. ";[education, open-source];93;
5913;1;2015-05-26T22:11:54.650;How do you ascertain the quality of your data?;How do you assess the quality of your data? In data scientists' world, we come across several data. We often crunch numbers without formally assessing its quality due to various reasons. One such reason is we need to meet deadlines for reports & publications. I am wondering if anyone has adopted or come across a method/guidelines that help to find issues within the data (time-saving tips), so we can analyze the data efficiently. Please share your experience, tips, etc. ;[education, open-source];71;
5915;2;2015-05-27T03:38:45.067;;You can use Rexster to expose a REST api and then you can use this api via Requests module (there are many other modules to do this task - though Requests is what I prefer);;;
5916;1;2015-05-27T05:41:21.753;How to cluster a link traversal dataset;I'm using Google Analytics on my mobile app to see how different users use the app. I draw a path based on the pages they move to. Given a list of paths for say a 100 users, how do I go about clustering the users. Which algorithm to use? By the way, I'm thinking of using sckit learn package for the implementation.My dataset (in csv) would look like this :  DeviceID,Pageid,Time_spent_on_Page,Transition.<br> ABC,Page1, 3s, 1->2.<br>ABC,Page2, 2s, 2->4.<br>ABC,Page4,1s,4->1.<br>So the path, here is 1->2->4->1, where 1,2,4 are Pageids.;[education, open-source];92;2
5917;2;2015-05-27T06:12:44.633;;I have not worked with such a dataset myself but I think you can model this problem as a graph where the pages form the node and then you have directed edges based on transition. Add weights to nodes based on time spent on them and then use graph clustering algorithms. If you choose to use this, you can use the networkx library in python for graph based analysis.Edit : We can use the information about different possible paths and how frequently they are use to classify the users. Let's take example of Google search app. Suppose I want to search for images. One option is that I use the image search option, make the query and reach the results page. Other is that I make the query first and then switch to image option after getting the results. In both the cases I did up at the same page. I can use this information to classify my users. Now there can be quite a lot of paths possible so which all do I consider? The graph can be used to leveraged here along with the information about how you want to classify your users. Modeling it as a graph looks very intuitive to me as it lends itself to the concept of path.;;;
5918;1;2015-05-27T07:53:53.170;How to install rattle in centos;While running rattle in my system I am getting this errorrattle()Error: attempt to apply non-functionIn addition: Warning message:In method(obj, ...) : Unknown internal child: selectionI am using R version 3.1.0 (2014-04-10);[education, open-source];37;
5919;2;2015-05-27T08:02:10.797;;@Shagun's answer is right actually. I just expand it!There are 2 different approaches to your problem:Graph ApproachAs stated in @Shagun's answer you have a weighted directed graph and you want to cluster the paths. I mention again because it's important to know that your problem is not a Graph Clustering or Community Detection problem where vertices are clustered!Cunstructing a Graph in networkx using the last two column of the data, you can add time spent as weight and users who passed that link as an edge attribute. After all you'll have different features for clustering: the set of all vertices an individual ever met in the graph, total, mean and std of time spent, shortest path distribution parameters, ... which can be used for clustering the user behaviors.Standard DataAll above can be done by reading data efficiently in a matrix. If you consider each edge for a specified user as a single row (i.e. you'll have MxN rows where M is the number of users and N the number of edges in case you stick with 100 case!) and add properties as columns you'll probably able to cluster behaviors. if a user passed an edge n times, in the row corresponding to that user and that edge add a count column with value n and same for time spend, etc. Starting and ending edges are also informative. Be careful that node names are categorical variables.Regarding clustering algorithms you can find enough if you have a quick look at SKlearn.Hope it helped.Good Luck :);;;
5920;2;2015-05-27T09:41:38.917;;I got the answer.I have to install some of the packages in terminal . I installed it and it works.sudo yum install gtk+-devel gtk2-devel;;;
5921;1;2015-05-27T10:02:03.657;Is automatic feature detection feasible?;"I am searching for pointers to algorithms for feature detection. EDIT: all the answers helped me a lot, I cannot decide which one I should accept. THX guys!What I did:For discrete variables (i.e. $D_i, E$ are finite sets) $X_i : \Omega \to D_i$ and a given data table $$ \begin{pmatrix}{}  X_1 & ... & X_n & X_{n+1} \\  x_1^{(1)} & ... & x_n^{(1)} & x_{n+1}^{(1)} \\  ... \\  x_1^{(m)} & ... & x_n^{(m)} & x_{n+1}^{(m)} \\\end{pmatrix}$$(the last variable will be the 'outcome', thats why I stress it with a special index) and $X, Y$ being some of the $X_1, ..., X_{n+1}$ (so if $X=X_a, Y=X_b$ then $D=D_a, E=D_b$) compute$$H(X) = - \sum_{d \in D} P[X=d] * log(P[X=d])$$$$H(Y|X) = - \sum_{d \in D} {             P[X=d] * \sum_{e \in E} {               P[Y=e|X=d] * log(P[Y=e|X=d])             }           }$$where we estimate  $$P[X_a=d] = |\{j \in \{1, ..., m\} : x_a^{(j)} = d\}|$$and analogously  $$P[X_a=d \cap X_b=e] = |\{j \in \{1, ..., m\} : x_a^{(j)} = d ~\text{and}~ x_b^{(j)}=e\}|$$and then  $$I(Y;X) = \frac{H(Y) - H(Y|X)}{\text{log}(\text{min}(|D|, |E|))}$$ which is to be interpreted as the influence of $Y$ on $X$ (or vice versa, its symmetric).EDIT: A little late now but still:This is wrong:Exercise for you: show that if $X=Y$ then $I(X,Y)=1$.This is correct:Exercise for you: show that if $X=Y$ then $I(X,X)=H(X)/log(|D|)$ and if $X$ is additionally equally distributed then $I(X,X)=1$.For selecting features start with the available set $\{X_1, ..., X_n\}$ and a set 'already selected'$ = ()$ [this is an ordered list!]. We select them step by step, always taking the one that maximizes   $$\text{goodness}(X) = I(X, X_{n+1}) - \beta \sum_{X_i ~\text{already selected}} I(X, X_i)$$ for a value $\beta$ to be determined (authors suggest $\beta = 0.5$). I.e. goodness = influence on outcome - redundancy introduced by selecting this variable. After doing this procedure, take the first 'few' of them and throw away the ones with lower rank (whatever that means, I have to play with it a little bit). This is what is described in this paper.For computing the $I$ for continuous variables one needs to bin them in some way. More concretely, the inventors of 'I' suggest to take the maximal value over binning $X$ into $n_x$ bins, $Y$ into $n_y$ bins and $n_x \cdot n_y <= m^{0.6}$, i.e.  compute  $$ \text{MIC}(X;Y) = \text{max}_{n_X \cdot n_Y \leq m^{0.6}} \left( \frac{I_{n_X, n_Y}(X;Y)}{log(\text{min}(n_X, n_Y)} \right)$$where $I_{n_X, n_Y}(X;Y)$ means: compute the $I$ precisely as you did for discrete variables by treating $X$ as a discrete random variable after binning it into $n_X$ bins and analogously with $Y$.===ORIGINAL QUESTIONMore precisely: I have a classification problem for one boolean variable, let's call this variable outcome. I have lots of data and lots of features (~150 or so) but these features are not totally 'meaningless' as in image prediction (where every x and y coordinate is a feature) but they are of the form gender, age, etc. What I did until now: from these 150 features, I guessed the ones that 'seem' to have some importance for the outcome. Still, I am unsure which features to select and also how to measure their importance before starting the actual learning algorithm (that involves yet more selection like PCA and stuff). For example, for a feature f taking only finitely many values x_1, ..., x_n my very naive approach would be to compute some relation betweenP(outcome==TRUE | f==x_1), ..., P(outcome==TRUE | f==x_n) and P(outcome==TRUE)(i.e. the feature is important when I can deduce more information about the coutcome from it than without any knowledge about the feature).Concrete question(s): Is that a good idea? Which relation to take? What to do with continuous variables?I'm sure that I'm not the first one ever wondering about this. I've read about (parts of) algorithms that do this selection in a sort-of automated way. Can somebody point me into the right direction (references, names of algorithms to look for, ...)?";[education, open-source];103;1
5922;2;2015-05-27T10:08:56.653;;What you are describing makes sense and relates to the naive Bayesian classifier. A useful tool in that respect is mutual information (MI) (and all its derivatives). MI was first proposed for feature selection by Battiti. MI is based on marginal and joint density estimations so you will need some pdf estimators for your continuous variables, like histograms, or kernel-based density estimators.;;;
5923;2;2015-05-27T10:17:19.210;;This entirely depends on what you wish or aim to do with your data, and what you mean by your tag data-cleaning. This can involve the technical process of sanitising data i.e. fixing broken XML, but also the manual deletion of outliers etc.Are you going to perform statistics to measure the correlation between certain variables? Or:Are you going to include the data into a machine-learning solution?In any case, you can plot descriptive statistics to get an impression of the quality of your data, such as:Box-plots to detect outliersStandard deviation, mean, mode, median to assess if it's normally distributed or notMore, don't know from the top of my headRemember that those measures give you an impression of the quality of your data, but even good measurements can come from bad data, and possibly vice versa.In short, the best way of assessing the quality of your data is to assess the quality of your results, which are acquired generally after several experiments. Only then will you see how well your data is suited for the problem you are trying to solve. That - or you need to get your hands dirty and examine the data qualitatively before you use it.Data (in assumingly large quantities) are generally used to identify certain patterns within the phenomena described by the data. It is very important to describe your task in terms of a (research) question and a proposed solution or hypothesis. Then, you can assess whether the data is suitable for your particular (research) project. Keep in mind that, theoretically, you can try to answer numerous questions on the same data, or answer the same question on different data sets. The goal is the right balance.;;;
5924;1;2015-05-27T10:26:11.687;change in variable importance;I have a multi year dataset. Each time frame  of the data has different predictor importance. Say for example, I am slicing the data into two partions as follows:a dataset for the year 2014 (whole year)a 2015 Jan. When i look for the predictor importance, the predictor variables are different for both the partions. (1) Hence i am not able to arrive at one  unique decision tree which can explain the model better. (2). I am not able to train a model which can predict the new data correctly.Is there anything I am going wrong here.;[education, open-source];34;
5925;2;2015-05-27T11:07:38.020;;"Probably you can use multiple decision trees - each for slice of dataset. Then combine the results from each decision tree. You can also weigh the results from each decision tree. Eg the dataset for 2015 can be given more weight than dataset for 2014. I do not see much harm in spliting dataset and training different trees as it helps to account for the predictor values in a more accurate manner. If you could more details about the data or the ""variable"" you mentioned, people would be able to help more.";;;
5926;2;2015-05-27T13:27:44.253;;If you want to have some data science programming experience in R try https://www.datacamp.com/ Is' an interactive online coding environment for learning R programming. ;;;
5927;1;2015-05-27T15:20:16.180;Wordnet Lemmatization;"I tried finding about exception lists in wordnet lemmatizers. ""Morphy() uses inflectional ending rules and exception lists to handle different possibilities"" which I read from http://www.nltk.org/howto/wordnet.html . Can you explain what is an exception list. Thank you.";[education, open-source];31;
5928;2;2015-05-27T15:43:31.750;;The exception list files are used to help the processor find base forms from 'irregular inflections' according to the man page.  They mean that some words, when plural or a different tense, can't be algorithmically processed to find the base/root word.  More details can be found in the morphy man.  I'm not a language processing expert, but this is likely a result of English words that 'break the rules'.  If you think about the code like a human trying to learn English: the student learns rules to use (algorithm) and then has to memorize exceptions for the rules (exception lists).  An over-simplified analogy that does not involve endings/conjugation would a spell checking program.  An algorithm might check for 'i before e, except after c' but would first have to check the word against an exception list to make sure it isn't 'weird' or 'caffeine' - please don't start a linguistics fight about this rule, I am not commenting on the validity of it/that's not the point I'd like to make.;;;
5929;2;2015-05-27T22:39:35.300;;"Your question is very broad, and therefore I won't offer very specific answers.  You asked for ""time-saving tips"" but there are many and they depend on context. Instead, I'll offer a general set of heuristics that are useful most of the time.Start with a specification or definition of what ""quality"" means for your analysis, and therefore for your data.Use definition/specification in 1) to enumerate the types of errors, omissions, missteps, modifications, etc. that might arise in the course of data collection and recording.  This enumeration will always be provisional because there could be ways that quality could be degraded that you didn't think of originally.Use 2) to define inspection and test methods that might reveal the existence of any of these types of errors, omissions, etc.  The default method is ""eyeballing"" -- having a knowledgeable, experienced person looking at the data to see if it looks right. When you uncover errors, omissions, etc. try to determine the root causes and generating processes. E.g. erroneous data in dozens of columns resulted from an ""off-by-one"" bug in the ETL job that generated/translated the data from source files.Using 2), 3) and 4), define processes that might either correct erroneous data or mitigate the effects of data quality problems.  E.g. Soundex transformation of names can mitigate effects of misspelling, but can't mitigate people misusing the ""name"" field to enter notes: e.g. ""Nelson - DO NOT CALL"".  Be aware that any process you might use to correct or mitigate errors or problems might create new errors or problems (e.g. truncation of numerical data).After you perform your analysis, look back at the data and ask ""Could these results be the result of data quality problems rather than true/accurate/appropriate data?""  In other words, double check.";;;
5930;2;2015-05-27T23:25:19.413;;"Please allow me to paraphrase your question to make sure I get your question right.Suppose you have 1 million data entries. Each entry consists of three inputs X1, X2, X3 and one output Y. One of the three inputs, X1, is a noise. You want to find the relation between X1 and Y. So you can remove the impact of X1 on Y.Let's use a simplified example, Y = 2X1 + 3X2 + 4X3If you update each of the 1 million entries with:Y_new = Y - 2X1Now you can examine the updated 1 million entries to find the relation between input X2, X3 and Y_new:Y_new = 3X2 + 4X3The relation between X1 and Y can be computed with different algorithms, such as regression, decision tree. How to measure which algorithm generates the best result, i.e. truly reflects the relation between X1 and Y hence removes the impact of X1 on Y as much as possible?The problem you're solving is an interesting one. I would use a free tool such as Knime to find out which algorithm generates the best result in modeling the relation between X1 and Y. It's faster than coding in Python or Matlab.Details: to put the problem intuitively, suppose the price of a house is determined by location, size of the house, condition of the house. Assume these three factors are independent from each other. You want to remove the impact of ""condition of the house"" on ""price"" as much as possible. In essence, you want to find the relation between ""condition of the house"" and ""price"" which may not be linear.You can let Knime read the 1 million entries of ""condition of the house"" and ""price"", try Decision Tree, SVM, or even Ensemble Learning, and see which algorithm generates the best model of X1 and Y. In other words, try and error :)";;;
5932;1;2015-05-28T07:38:38.790;"getting error:-Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/io/Writable";"I am trying to connect to hive from java but getting error. I searched in google but not got any helpfull solution. I have added all jars also.The code is:-package mypackage;import java.sql.SQLException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.sql.DriverManager;/**** @author dsri*/public class HiveJdbcClient {private static String driver = ""org.apache.hadoop.hive.jdbc.HiveDriver"";public static void main(String[] args) throws SQLException,        ClassNotFoundException { Class.forName(""org.apache.hadoop.hive.jdbc.HiveDriver""); try { Class.forName(driver); } catch (ClassNotFoundException e) { e.printStackTrace();  System.exit(1);}Connection connect = DriverManager.getConnection(""jdbc:hive://master:10000 /default"", """", """");Statement state = connect.createStatement();String tableName = ""mytable"";state.executeQuery(""drop table "" + tableName);ResultSet res=state.executeQuery(""ADD JAR /home/dsri/hadoop_home/hive/lib /hive-serdes-1.0-SNAPSHOT.jar"");res = state.executeQuery(""create table tweets (id BIGINT,created_at     STRING,source STRING,favorited BOOLEAN,retweet_count INT,retweeted_status STRUCT<text:STRING,user:STRUCT<screen_name:STRING,name:STRING>>,entities STRUCT<urls:ARRAY<STRUCT<expanded_url:STRING>>,user_mentions:ARRAY<STRUCT<screen_name:STRING,name:STRING>>,hashtags:ARRAY<STRUCT<text:STRING>>>,text STRING,user  STRUCT<screen_name:STRING,name:STRING,friends_count:INT,followers_count:INT,statuses_count:INT,verified:BOOLEAN,utc_offset:INT,time_zone:STRING>,in_reply_to_screen_name STRING) ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe' LOCATION '/user/flume/tweets'"");String show = ""show tables"";System.out.println(""Running show"");res = state.executeQuery(show);if (res.next()) {  System.out.println(res.getString(1));}String describe = ""describe "" + tableName;System.out.println(""Running describe"");res = state.executeQuery(describe);while (res.next()) {  System.out.println(res.getString(1) + ""\t"" + res.getString(2));}}}I am getting these errors:-run:SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/dsri/hadoop/hive/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/dsri/hadoop/lib/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/dsri/GlassFish_Server/glassfish/modules/weld-osgi-bundle.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/io/Writable    at org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(HiveStatement.java:198)    at org.apache.hadoop.hive.jdbc.HiveStatement.execute(HiveStatement.java:132)    at org.apache.hadoop.hive.jdbc.HiveConnection.configureConnection(HiveConnection.java:133)    at org.apache.hadoop.hive.jdbc.HiveConnection.(HiveConnection.java:122)    at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:106)    at java.sql.DriverManager.getConnection(DriverManager.java:571)    at java.sql.DriverManager.getConnection(DriverManager.java:215)    at dp.HiveJdbcClient.main(HiveJdbcClient.java:35)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Writable    at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355)    at java.security.AccessController.doPrivileged(Native Method)    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)    ... 8 moreJava Result: 1";[education, open-source];202;
5933;1;2015-05-28T08:44:13.343;What possible data products can be built using this dataset;"I have a dataset which contains information about when do people enter and leave a premise. I have the following information in the dataset : Person IdTime of EntryTime of LeavingThe dataset has around 50 unique persons. Each person will have multiple entries corresponding to multiple visits. The data spans over a year so I have quite a lot of entries (around 1 million).These people can be classified on the basis of the department they work under (2 departments - mutually exclusive) or on basis of  their role (4 possible roles - all mutually exclusive)I was wondering what kind of data analysis can be done with this kind of dataset. I am not looking for straight-forward things like ""who spent the most time in building"". However things like finding correlation between visits of 2 people would be interesting. So if person A visits the premise, what is the probability that person B would also visit. Since I have only around 50 unique visitors, I think such an analysis is feasible. Another line of thought was to apply some interval-pattern mining techniques but I am not much familiar with them.Can someone give me some pointers/ideas about what kind of data products can be build using this or what kind of techniques can be used with such data.  Edit : As discussed in comments, I call it a product in the sense that I do not want some simple or trivial analysis. And I am not looking for any commercially viable idea - just some cool fun idea :)";[education, open-source];96;1
5934;2;2015-05-28T12:00:55.070;;I got the answer. One jar file was missing now it is solved. This file was missing.hadoop-common-2.1.0-beta.jar;;;
5935;2;2015-05-28T12:49:25.243;;Artificial Intelligence: A Modern Approach by Stuart Russel and Peter Norvig.;;;
5936;1;2015-05-28T15:23:27.847;Support vector regression and paremeters;I am doing load forecasting using SVR(kernel='rbf').How can I understand which is the best value for parameters C, epsilon and gamma?Thanks.;[education, open-source];37;
5937;2;2015-05-28T15:51:27.623;;"It looks like that you are using scikit-learn. In this case use Grid Search Cross Validation  or Randomized Search Cross Validation to find the best parameters. sklearn.grid_search.GridSearchCV(estimator, param_grid, scoring=None, cv=None, ...)In these approaches you basically loop over possible sets of your parameters, specified via param_grid. For each set you perform a cross-validation (default is 3-fold, but you can specify it via cv parameter). Cross-validation gives you the mean value and deviation of your 'scoring' parameter (e.g. 'accuracy' for classification, 'r2' for regression). The set of parameters with the best 'scoring' is the winner. See example here. It also show how to output not only the best set of parameters, but also ""top-N"". I find it very useful in building intuition about my model. ";;;
5938;2;2015-05-28T15:59:26.593;;"I don't know if this is the answer you are looking for, but generally start by asking ""what would I do differently""? So, if I am running a business (to apply some context), at how often do I not have coverage for certain positions. How can I schedule shifts (for example) to ensure that all critical positions are covered but I have as few people in the building as possible (minimising cost)? As a simple time series, you could apply any sort of forecasting to this data set - predicting who and how long they will be in the building, when they will arrive next, for example.  Look for seasonality in the arrival patterns. You could also look to predict when a given person will next be in the building. And can you detect outliers? Who has been leaving early or breaking from their usual pattern of behaviour?";;;
5939;2;2015-05-28T16:49:51.293;;It is a good idea to do automatic feature vector selection for classification, and is widely done for decision trees.If you are trying to find the relative importance of variables in classification schemes, a popular way of selecting variables is by performing entropy minimization.  This a technique that is often used for constructing decision trees.  Entropy is simply a way of numerically quantifying disorder of a system:For binary classification, it's simple to calculate entropy for every variable, moving the variables which minimize entropy to the top, As an example, consider a simple case where a game exists when one must choose between a right and left door, one of which is red, one of which is blue.    The color of the door changes from instance to instance (sometime left is red, sometimes right is red), but the prize is always behind the left door.  Suppose also that over many trials, both left-right, and red-blue have equal number of people choosing those options.  The entropy of the original system (P(prize)=.5, P(!prize)=.5) is 1.  Supposing that we select a door based on color, we have:Here the probability of red and blue are both .5, and the entropy of each system is 1, so the Entropy of the new system is still 1.  This means that no information was gained from choosing red or blue.  Suppose instead we select on left right:And since log_2(1)=0, there is no disorder (i.e. a perfect predictor).For continuous variables, it becomes a bit more complicated because you then have to select based on a floating threshold (or a few for a single variable).  While the basic entropy calculations hold fast, tuning many thresholds on many variables becomes resource intensive, so at that point you'll want to look at something like spark's mllib. ;;;
5940;2;2015-05-28T18:30:23.893;;You might be able to find individuals that act as anchors.  That if Abe is here, then everybody else is more likely to stay longer. Interesting to know if it was always the same people who stayed or not.There might also be some interesting patterns with groups that arrive, are present, or leave together.As a business owner you could look for individuals who not just work shorter hours, but arrive at their own time, and leave with a group.;;;
5941;1;2015-05-28T22:07:35.367;Need help with LDA for selecting features;"I am currently selecting features of products by using LDA to group 6000 keywords of product into topics. Here is the sample of my dataset after being organized into list of keywords for each product id.I consider each id as a ""document"" and each keywords as the ""word"" in a ""document"" for the case of LDA.It didn't work out as I expected as each topic have many identical keywords with different weight. I removed 100 most common keywords but there are still some identical keywords in the topics. Here is the sample output:How can I deal with the identical keywords in my topics? and also how to deal with the 100 most common keywords I removed?Here is the graph showing the frequency of the words in all ""documents""Each word only presents once in each document, but may present in different documents. I updated a new graph of the frequency of the wordsThank you so much. Any suggestion is appreciated.";[education, open-source];96;
5942;2;2015-05-29T00:41:17.407;;You can perform logistic regression (if your dependent variable has two classes) that penalizes based on the L1 norm. You can choose the correct sparsity parameter (typically $\lambda$) that chooses how strongly to seek sparsity based on cross-validation. The model will force many non-informative features to be 0. This is a form of feature selection. See here: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression ;;;
5943;2;2015-05-29T01:09:27.367;;"Here is the relevant source code. I'm pretty sure it's using cross-entropy for multiclass:     //  GBM by Greg Ridgeway  Copyright (C) 2003#include ""multinomial.h""CMultinomial::CMultinomial(int cNumClasses, int cRows){   mcNumClasses = cNumClasses;   mcRows = cRows;   madProb = new double[cNumClasses * cRows];}CMultinomial::~CMultinomial(){   if(madProb != NULL)   {      delete [] madProb;   }}GBMRESULT CMultinomial::UpdateParams(   double *adF,   double *adOffset,   double *adWeight,   unsigned long cLength){   // Local variables   unsigned long ii=0;   unsigned long kk=0;   // Set the probabilities for each observation in each class   for (ii = 0; ii < mcRows; ii++)   {      double dClassSum = 0.0;      for (kk = 0; kk < mcNumClasses; kk++)      {         int iIdx = ii + kk * mcRows;         double dF = (adOffset == NULL) ? adF[iIdx] : adF[iIdx] + adOffset[iIdx];         madProb[iIdx] = adWeight[iIdx] * exp(dF);         dClassSum += adWeight[iIdx] * exp(dF);      }      dClassSum = (dClassSum > 0) ? dClassSum : 1e-8;      for (kk = 0; kk < mcNumClasses; kk++)      {         madProb[ii + kk * mcRows] /= dClassSum;      }   }   return GBM_OK; }GBMRESULT CMultinomial::ComputeWorkingResponse(    double *adY,    double *adMisc,    double *adOffset,    double *adF,     double *adZ,     double *adWeight,    bool *afInBag,    unsigned long nTrain,    int cIdxOff){    unsigned long i = 0;    for(i=cIdxOff; i<nTrain+cIdxOff; i++)    {       adZ[i] = adY[i] - madProb[i];    }    return GBM_OK;}GBMRESULT CMultinomial::InitF(    double *adY,    double *adMisc,    double *adOffset,     double *adWeight,    double &dInitF,     unsigned long cLength){    dInitF = 0.0;    return GBM_OK;}double CMultinomial::Deviance(    double *adY,    double *adMisc,    double *adOffset,     double *adWeight,    double *adF,    unsigned long cLength,    int cIdxOff){    unsigned long ii=0;    double dL = 0.0;    double dW = 0.0;    for(ii=cIdxOff; ii<cLength+cIdxOff; ii++)    {        dL += -adWeight[ii] * adY[ii] * log(madProb[ii]);        dW += adWeight[ii];    }    return dL/dW;}GBMRESULT CMultinomial::FitBestConstant(    double *adY,    double *adMisc,    double *adOffset,    double *adW,    double *adF,    double *adZ,    unsigned long *aiNodeAssign,    unsigned long nTrain,    VEC_P_NODETERMINAL vecpTermNodes,    unsigned long cTermNodes,    unsigned long cMinObsInNode,    bool *afInBag,    double *adFadj,   int cIdxOff){      // Local variables    GBMRESULT hr = GBM_OK;    unsigned long iNode = 0;    unsigned long iObs = 0;   // Call LocM for the array of values on each node    for(iNode=0; iNode<cTermNodes; iNode++)    {        if(vecpTermNodes[iNode]->cN >= cMinObsInNode)        {         // Get the number of nodes here         double dNum = 0.0;         double dDenom = 0.0;         for (iObs = 0; iObs < nTrain; iObs++)         {            if(afInBag[iObs] && (aiNodeAssign[iObs] == iNode))                {               int iIdx = iObs + cIdxOff;                    dNum += adW[iIdx] * adZ[iIdx];               dDenom += adW[iIdx] * fabs(adZ[iIdx]) * (1 - fabs(adZ[iIdx]));                }         }         dDenom = (dDenom > 0) ? dDenom : 1e-8;         vecpTermNodes[iNode]->dPrediction = dNum / dDenom;        }    }    return hr;}double CMultinomial::BagImprovement(    double *adY,    double *adMisc,    double *adOffset,    double *adWeight,    double *adF,    double *adFadj,    bool *afInBag,    double dStepSize,    unsigned long nTrain){    double dReturnValue = 0.0;    double dW = 0.0;   unsigned long ii;   unsigned long kk;   // Calculate the probabilities after the step   double *adStepProb = new double[mcNumClasses * mcRows];   // Assume that this is last class - calculate new prob as in updateParams but   // using (F_ik + ss*Fadj_ik) instead of F_ik. Then calculate OOB improve   for (ii = 0; ii < mcRows; ii++)   {      double dClassSum = 0.0;      for (kk = 0; kk < mcNumClasses; kk++)      {         int iIdx = ii + kk * mcRows;         double dF = (adOffset == NULL) ? adF[iIdx] : adF[iIdx] + adOffset[iIdx];         dF += dStepSize * adFadj[iIdx];         adStepProb[iIdx] = adWeight[iIdx] * exp(dF);         dClassSum += adWeight[iIdx] * exp(dF);      }      dClassSum = (dClassSum > 0) ? dClassSum : 1e-8;      for (kk = 0; kk < mcNumClasses; kk++)      {         adStepProb[ii + kk * mcRows] /= dClassSum;      }   }   // Calculate the improvement    for(ii=0; ii<nTrain; ii++)    {        if(!afInBag[ii])      {         for (kk = 0; kk < mcNumClasses; kk++)         {            int iIdx = ii + kk * mcRows;                dReturnValue += adWeight[iIdx] * adY[iIdx] *                                (log(adStepProb[iIdx]) - log(madProb[iIdx]));            dW += adWeight[iIdx] * adY[iIdx];         }      }    }    return dReturnValue/dW;}";;;
5944;1;2015-05-29T02:55:18.203;How far from a specific state in a Markov chain;Problem:Lets say we have an irreducible Markov chain.Given a failure state or non desirable state F and a current state SIs it possible to find how far we are from the failure state F.Example: if we try to model an engine failure using Markov process and using historic data we have a state transition matrix P, one of which is a state where the engine was failed .Now , in a dynamic system, if the current state S and a state transition matrix are given, can we calculate how far we are from failure state F.Mathematically:x(n+1) = (x)P^nRight hand side of the equation gives us a vector v which consists of probabilities of transitioning to different states at stage n.With different value of n, the values in this vector v will changeone of the values in v will be the probability of going to state F.I want to find for what n , transitioning probability to F is maximum.;[education, open-source];33;
5945;1;2015-05-29T05:13:40.113;Compute utility of a notification;"I have no idea whether this is the right StackExchange flavor to post this question in, but here goes:I have an agent (an IRC bot that listens to an event stream) that I would like to add ""intelligent notifications"" to. Let me give a specific example.The bot monitors the results from our continuous integration system, so it basically sees a stream of test results associated with changes (and the changes are associated with users.) If I ask it to notify me of interesting events, then for each event it sees, it decides whether to tell me about it (or rather, about the current results so far.)I would like it to decide based on how interesting something is for me. Something is interesting if (1) it is a result of a change that I pushed to the continuous integration system, (2) it is a test failure, and (3) the information conveyed by that failure is a significant indicator of whether my change was bad.Tests have a baseline probability of failing for no good reason (""intermittent failures""). If one test fails, it does not necessarily mean my change was bad. It might just be a flaky test. (Any given push tends to result in a couple bogus failures, so this isn't some obscure edge case.) But the baseline probability of an intermittent failure varies according to the test suite running (we have several dozen different test suites that fire off for every change pushed.)So I don't want to be bothered with reports of failing tests that are probably just noise. And if my push is bad, I don't want to be flooded with notifications for every failed test. (If I break something, it'll probably show up in multiple test suites. So if the agent sees two failures and 20 successes, maybe it won't bother me, but if it then sees another failure or two it should.)I am imagining I could look at the change between the prior and posterior probabilities and use that to estimate the entropy of that new test result, and perhaps also compute my personal entropy (if that makes any sense -- as in, the bot should assume I don't know what any of the results are until it notifies me, at which point it should assume I am aware of the current full set of results and not bother me again until enough additional results have come in to substantially change the probability estimate of my change being bad.)Or something like that. I'm really looking for the right mathematical framework to compute things like this. I know next to nothing about machine learning or... well, mathematics in general.(The above is simplified; I would additionally like to do crazy things like take into account ""labeling"", where a third party looks at the test failures and decides whether they are real failures or not. But the time between the failure coming in and when that person labels it for me can also be modeled by a distribution, and I'd like to hold off notifications for interesting results if there's a good chance that this other person may tell me that the result is not interesting after all. But only if that's going to happen ""soon"". Also, we have an automated system for guessing whether something might be an intermittent vs real failure, and it'd be nice to take its opinion into account as well.)";[education, open-source];26;
5946;1;2015-05-29T07:39:35.713;Algorithm for deriving mutiple clusters;Suppose I have a set of data(with  2 diemensional feature space), and I want to obtain clusters from them. But I do not know how many clusters will be formed. Yet I want separate clusters(The number of clusters is more than 2). I figured that k means of k medoid cannot be used in this case. Nor can I use hierarchical clustering. Also since there is no training set hence cannot use Knn classifier to any others(supervised learning cannot be used as no training set). I cannot use OPTICS algorithm as I do not want to specify the radius(I dont know the radius) Is there any machine learning technique that would give me multiple clusters(distance based clustering) that deals well with outlier points too? This should be the output;[education, open-source];71;1
5947;2;2015-05-29T08:32:16.483;;"I think, even before doing LDA, you should remove words which appear in more than ""x"" percent of your documents. Try different ""x"" starting from 80% and then going down. The logic is that if the word is common for many documents, it  does not distinguished those and should be neglected.";;;
5949;2;2015-05-29T09:12:01.650;;"Feature selection is a very well established field in Machine Learning. The objective of feature selection algorithms is to select a subset of your feature set in order to maximize your system's prediction performance. There are two kind of feature selection approaches:Filter methods: filter methods select features without taking into consideration any specific prediction algorithm. They are based in applying different kinds of measures (like mutual information or correlation) in order to evaluate the relative information that a feature or a set of features can provide about the output variable. Filter methods are faster than wrapping methods, but do not perform as well as wrapping methods.Wrapper methods: these methods evaluate features or sets of features by using a specific classification/regression algorithm. I'd suggest to read one of the seminal papers in the feature selection field as a starting point in order to get to know the basics:Guyon, Isabelle; Elisseeff, André (2003). ""An Introduction to Variable and Feature Selection"". JMLR";;;
5950;2;2015-05-29T09:22:53.750;;"The fact is that you could use any of the algorithms you mentioned, and in general any algorithm that requires to set the number of clusters as a parameter (or any other parameter that indirectly sets the final number of clusters, like the threshold in a hierarchical clustering algorithm.)The solution to your problem is model selection. Model selection methods evaluate different clustering solutions, and select the one that optimizes a given criterion. For instance, in the case of K-means, you could find a solution for a range of k values, and keep the one that maximizes any cluster validation measure (see the Wikipedia entry for cluster analysis to read about some examples of cluster validation measures). There are automatic and more complex approaches (one specific example is ""Automatic Cluster Number Selection Using a Split and Merge K-Means Approach"" by Muhr, M. and Granitzer, M., but this is just an example). These methods use cluster validation measures to automatically split or merge clusters, but the idea is basically the same. ";;;
5951;1;2015-05-29T09:43:47.847;Kibana: How to add horizontal line which represents average value for selected time window?;I'm doing some analysis in ELK Stack. I have datetime histogram with average auto aggregation. Plot looks like that:.I would like to add automatic horizontal line which will represent average value of plotted points in selected time window. It should looks like that:How to do that?;[education, open-source];39;
5954;1;2015-05-29T14:53:36.960;What is the best editor for ontologies?;What is the best editor for ontologies?I am looking for editors to create my ontology, and I have seen that most publishers do not have current support.I consider that the following editors currently have a support, but would like to know your opinionPROTEGE standford http://protege.stanford.edu/Open semantic framework http://opensemanticframework.org/KAON2 Semantic turkey VITRO I could give more information about what I have researched each, but I am concerned to comply with the rules Stackexchange. So ask if you need any clarification in the comments.I apologize for my English.;[education, open-source];27;
5957;2;2015-05-29T20:40:51.467;;If the data are suitable, you can use Gaussian mixture modeling, fit via an EM algorithm to estimate various separate Gaussian clusters. When determining the number of clusters, you can use something like BIC (or other penalized likelihood criterion) to penalize based on the number of parameters that you are estimating. Then simply search over different numbers of clusters and choose the number with the lowest BIC. This is a form of model-based clustering. You should be able to use the mclust package in R to do this: http://cran.r-project.org/web/packages/mclust/index.html.  ;;;
5958;1;2015-05-29T23:04:54.580;Are there any unsupervised learning algorithms for time sequenced data?;Each observation in my data was collected with a difference of 0.1 seconds. I don't call it a time series because it don't have a date and time stamp. In the examples of clustering algorithms (I found online) and PCA the sample data have 1 observation per case and are not timed. But my data have hundreds of observations collected every 0.1 seconds per vehicle and there are many vehicles.Note: I have asked this question on quora as well.;[education, open-source];84;3
5959;2;2015-05-30T13:50:32.703;;What you have is a sequence of events according to time so do not hesitate to call it Time Series!Clustering in time series has 2 different meanings:Segmentation of time series i.e. you want to segment an individual time series into different time intervals according to internal similarities.Time series clustering i.e. you have several time series and you want to find different clusters according to similarities between them.I assume you mean the second one and here is my suggestion:You have many vehicles and many observations per vehicle i.e you have many vehicles. So you have several matrices (each vehicle is a matrix) and each matrix contains N rows (Nr of observations) and T columns (time points). One suggestion could be applying PCA to each matrix to reduce the dimenssionality and observing data in PC space and see if there is meaningful relations between different observations within a matrix (vehicle). Then you can put each observation for all vehicles on each other and make a matrix and apply PCA to that to see relations of a single observation between different vehicles. If you do not have negative values Matrix Factorization is strongly recommended for dimension reduction of matrix form data.Another suggestion could be putin all matrices on top of each other and build a NxMxT tensor where N is the number of vehicles, M is the number of observations and T is the time sequence and apply Tensor Decomposition to see relations globally.A very nice approach to Time Series Clustering is shown in this paper where the implementation is quiet straight forward.I hope it helped!Good Luck :)EDITAs you mentioned you mean Time Series Segmentation I add this to the answer.Time series segmentation is the only clustering problem that has a ground truth for evaluation. Indeed you consider the generating distribution behind the time series and analyze it I strongly recommend this, this, this, this, this and this where your problem is comprehensively studied. Specially the last one and the PhD thesis.Good Luck!;;;
5960;1;2015-05-30T08:33:06.713;Theano in deep learning research;How widely is Theano used in deep learning research? Is Theano a good start to learn the implementation of machine learning algorithms?Will learning the implementation of something like a feed forward network really help? Do graduate students implement neural networks or other algorithms at least once during their college days?Background:I have a reasonable idea about feed forward and recurrent networks, backpropagation, the general pipeline for a machine learning problem and the necessary mathematics. ;[education, open-source];165;1
5961;2;2015-05-30T12:48:52.713;;"Theano used to be very popular in the last few years. However, from seeing what the top research labs in the world currently use instead of theano (Facebook AI and Google Deepmind), I would either go for caffe or Torch7, with a stronger preference on the latter.The reason is that you can implement from Recurrent Neural Nets to Convolutional Neural Networks very easily and there is a wide range of examples on github (https://github.com/torch/demos).Graduate students in NYU, Stanford, Oxford definitely implement advanced models in their studies; your background will help you a lot, a great place to start and familiarise with such models, and implement them in Torch7, is the lessons of Prof. Nando de Freitas https://www.youtube.com/user/ProfNandoDF.";;;
5962;2;2015-05-31T09:47:23.370;;The radius in OPTICS is a maximum value, and it can be set to infinity!So you don't need to know it, and you should give OPTICS as well as DBSCAN a try. There are heuristics to choose their parameters, if you know your data.Similarly, try hierarchical clustering. There are good heuristics on how to extract flat partitions out of it.You want something that handles noise well - this calls for DBSCAN, OPTICS and HAC.;;;
5963;2;2015-05-31T11:22:46.940;;"I am assuming the context is office setting.Study of an Impact of a systemic event. e.g. how an organizational event impacted the attendance, or reverse of it (what happened which caused a shift in pattern). Build employee profile based on localized events e.g. school vacations, time-off patterns, response to extra workload. May not necessarily be ""fun"" (your question), but profiling can also be done on demographics (age, gender, ethnicity, etc.)Department or team analysis. e.g. generate networking index of a person, team or department (how correlated are their activities).Possible impact on internal org design (e.g. scale/reduce support staff  services, resource waste reduction, etc. ), use occupancy rate of property to decide on hotel office model, or relocation. ";;;
5964;1;2015-05-31T20:35:54.150;Brown clustering, graph partitioning, agglomerative clustering - libraries/software;I need to do some experimenting with Brown clustering, graph partitioning, agglomerative clustering.1) Are there Python/Matlab libraries for that? I know sklearn.cluster but it doesn't have algorithms I need.2) Is it possible to install graphical interface for Cluto on Mac OS? 3) Overall, are there useful tutorials on using Cluto?4) Other software for clustering that I could learn within a couple of hours?Thanks;[education, open-source];39;
5965;2;2015-05-31T21:52:47.957;;For graphs I definitely suggest Networkx and igraph libraries. They support many graph partitioning algorithms.;;;
5966;2;2015-06-01T06:03:05.510;;I don't think that EM clustering algorithms like k-means and Gaussian mixture models are quite what you're looking for. There are definitely other algorithms that don't require one to pick a number of clusters. My personal favorite (most of the time) is called mean-shift-clustering. You could find a great little blog post about it here, and it has a good implementation in python's scikit-learn library.;;;
5967;2;2015-06-01T06:37:06.680;;You should create /user folder at first. sudo -u hdfs hadoop fs -mkdir -p /user;;;
5968;1;2015-06-01T09:58:29.580;How to paste string and int from map to an array in hive?;"I am trying to paste a string and int from map in Hive to an array.For now, record looks like this:{""string1"":1,""string2"":1,""string3"":15}Is there a way to convert it to an array like this:[""string1:1"",""string2:1"",""string3:15""]";[education, open-source];11;
5969;1;2015-06-01T11:25:59.933;Making a labelled training data set;"We are developing a classification system, where the categories are fixed, but many of them are inter-related. For example, we have a category called, ""roads"" and another one called ""traffic"". We believe that the model will be confused by the text samples, which could be in roads category and also in traffic.Some of our text samples are suitable for multi class labelling too.  For example, ""There is a garbage dump near the footpath. The footpath is broken completely"". This text could be categorized into garbage bucket or footpath bucket. We are going to build a training set for this classifier, by manually annotating the text. So, can we put multiple labels for one issue? How should we deal with  text with multiple labels for it? Should they be added into all categories to which it is tagged to, as training sample ?For example, ""There is a garbage dump near the footpath. The footpath is broken completely"". This text could be categorized into garbage bucket or footpath bucket. So, should this text be added as a training sample for garbage and footpath? How should we consider the labels?Can you please give your insights?";[education, open-source];60;
5970;1;2015-06-01T11:51:45.187;Package for SMOTEBoost in R;I am solving a rare event cum classification problem. I have come across that there is a package called SMOTEBoost which oversamples the rare event and boost the results. But i am not sure is that supported in R. Could you please help me how can i use SMOTEBoost in R? Any examples?;[education, open-source];97;
5971;2;2015-06-01T11:55:49.633;;Good ideas there. But start with simple things first and then ramp up to a more complex system once you have something already working and some human feedback on the solution.Simpler solution is just a velocity check - if there is X number of failures per Y number of seconds, then send the notification. From your post it seems like 3 failures over 20 second time window should be a good start. It takes a fair bit of tinkering to get this kind of system running - work out the windowing functions, etc.Good luck :);;;
5972;1;2015-06-01T12:36:23.203;How to learn spam email detection?;I want to learn how a spam email detector is done. I'm not trying to build a commercial product, it'll be a serious learning exercise for me. Therefore, I'm looking for resources, such as existing projects, source code, articles, papers etc that I can follow. I want to learn by examples, I don't think I am good enough to do it from scratch. Ideally, I'd like to get my hand dirty in Bayesian.Is there anything like that that? Programming language isn't a problem for me.;[education, open-source];531;3
5973;1;2015-06-01T13:10:41.363;Choosing best methods for estimating the unknown parameters in a linear regression model;Given some dataset for prediction, for eg say I have different housing price prediction dataset:dataset  1 : 100 training and 100 testing sample, 50 featuredataset  2 : 100 training and 100 testing sample, 120 feature dataset  3 : 1000 training and 1000 testing sample, 50 featuredataset  4 : 1000 training and 1000 testing sample, 5000 featurehow should I choose the best methods for estimating the unknown parameters ( predict price) in a linear regression model from the following for each of these dataset?Ordinary least squaresStepwise regressionPrincipal component regression Partial least squares regressionShould I experiment with each of these one by one and compare the results or is there any rule of thump on when to use each of them based on the dataset ? Please help;[education, open-source];81;1
5974;2;2015-06-01T14:37:08.643;;"Assuming your map is called ""M"" and you want your array field to be called ""A""SELECT ...array(concat_ws("":"",""string1"",M[1]),      concat_ws("":"",""string2"",M[2]),      concat_ws("":"",""string3"",M[3]) as A....FROM table;";;;
5976;2;2015-06-01T15:14:53.047;;First of all check this carefully. You'll find a simple dataset and some papers to review.BUT as you want to start a simple learning project I recommend to not going through papers (which are obviously not basic) but try to build your own bayesian learner which is not so difficult.I personally suggest Andrew Moore's lecture slides on Probabilistic Graphical Models which are freely available and you can learn from them simply and step by step.If you need more detailed help just comment on this answer and I'll be glad to help :)Enjoy baysian learning!;;;
5977;2;2015-06-01T16:03:47.110;;In Andrew Ng's Machine Learning Course on Coursera (in someways the flagship course for Coursera) the programmers exercise for Support Vector Machines was an example doing a spam classifier. The lectures are great, famous even, and well worth watching.There is also this posted course from him:http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html;;;
5978;2;2015-06-01T16:32:03.140;;"There is a basic introduction to the Bayesian method for spam detection in the book ""Doing Data Science - Straight Talk from the Frontline"" by Cathy O'Neil, Rachel Schutt.The chapter is good, because it explains why other common data science models don't work for spam classifiers. The whole book uses R throughout, so only pick it up if you are interested in working with R.It uses the Enron email set as training data, since it has emails divided into spam/not spam already. ";;;
5979;2;2015-06-01T17:08:46.350;;"Generally with multiple classes you have to make a distinction between exclusive and inclusive groups. The simplest cases are ""all classes are exclusive"" (predict only one class), and ""all classes are compatible"" (predict list of classes that apply).Either way, label the classes as you would want your trained model to predict them. If you expect your classifier to predict an example is in both garbage and footpath, then you should label such an example with both. If you want it to disambiguate between them, then label with a single correct class.To train a classifier to predict multiple target classes at once, it is usually just a matter of picking the correct objective function and a classifier with architecture that can support it. For example, with a neural network, you would avoid using a ""softmax"" output which is geared towards predicting a single class - instead you might use a regular ""sigmoid"" function and predict class membership on a simple threshold on each output.You can get also more sophisticated perhaps with a pipeline model if your data can be split into several exclusive groups - predict the group in the first stage, and have multiple group-specific models predicting the combination of classes in each group in a second stage. This may be overkill for your problem, although it may still be handy if it keeps your individual models simple (e.g. they could all be logistic regression, and the first stage may gain some accuracy if the groups are easier to separate).";;;
5980;2;2015-06-01T18:27:41.960;;Use Algorithm 1 from this paper. The rest of paper is more about the analysis of algorithm and theoretical background which is very nice but not necessary for your purpose. Implementation of the algorithm in Python or R is pretty straightforward.Hope it helps :);;;
5981;1;2015-06-01T18:54:14.077;Iteratively construct Linear Classifier;"sorry if this question is out of place. I'm a begginer to machine learning, and I have use for a technique, and I don't even know where to look. The problem is:I have 5 features which are real valued (Parameters in a deterministic simulation). This features determine two aspects of the instance (model solution). Its feasibility (binary) and some measure of likelihood given certain experimental data (only for the instances that achieved feasibility). Since I want to avoid generating ""infeasible"" combination of features, what I devised was an algorithm that iteratively does the following:Generate Nc candidate feature vectorsEvaluate Feasibility and Likelihood for eachFind linear combination of features that involves a compromise between least amount of features / holds largest cluster of feasibility. Add this as constraints to the feature vector generation. In short, it detects and iteratively refines ""simple"" constraints that once added to the feature vector generation ""guarantee"" its feasibility to save computational time evaluating combination of parameters that lead to infeasible models. Afterwards, they could be tested by inverting them and looking for other ""regions"" (if any) of the feature vector where the model is feasible.  Any name of techniques and references I might look for ?Thanks!";[education, open-source];31;
5982;2;2015-06-01T19:06:51.007;;This is actually a really in-depth problem that many people and companies have worked on. Here are some basics:First, we need to represent the data well. This involves somehow representing each document as a vector in $d$-dimensional space. Ideally, in this space, we want samples that have the same label to be nearby in euclidean distance, and samples that are different labels to be far away in euclidean distance. This step can be really hard, but one tried-and-true representation is called Term Frequency-Inverse Document Frequency (tf-idf). Here, each dimension in the space represents a particular word, and the value in that dimension for a particular sample basically represents the normalized number of times that word occurs in the document. You could read more about that here. There's a pretty good scikit-learn implementation of this representation if you want to try it out.Now the data is in a useful space, but a really high-dimensional space. I'd recommend reducing this dimensionality somehow, but that's a whole subject for another thread.Finally you could train some algorithm to classify the samples (which is what the other answers are about). There are lots of great choices - neural networks, adaboost, SVMs, Naive Bayes, and graphical classification models will all give you good results. Many of these also have implementations in scikit-learn. But the best algorithms leverage the fact that this problem is actually a transfer learning one. That is, the distributions from which the training and testing data come might not be exactly the same - because the sorts of things one person thinks are spam might be different than the sorts of things another person thinks are spam.;;;
5983;1;2015-06-02T08:39:41.997;Random Forest, Type - Regression, Calculation of Importance Example;I am trying to use a Random Forest Model (Regression Type) as a substitute of logistic regression model. I am using R - randomForest Package. I want to understand the meaning of Importance of Variables (%IncMSE and IncNodePurity) by example. Suppose I have a population of 100 employees out of which 30 left the company. Suppose in a particular decision tree, population is split by an attribute (say location) into two nodes. One node contains 50 employees out of which 10 left the company and other contains 50 employees from which 20 left the company. Can someone demonstrate me a calculation of %IncMSE and IncNodePurity. (if Required for averages etc., please consider another decision tree)This may look like a repeated question but I could not find a worked out example.;[education, open-source];64;
5984;2;2015-06-02T13:13:55.013;;MSE is measure of error of the overall regression model, $\frac{1}{n}\sum\|y_i-\hat y_i\|^2$.For an important variable, if it is replaced with random noise, you would imagine MSE with the faulty data to increase. IncMSE (Incremental MSE) for a particular variable is how much the MSE will increase if the variable is completely randomized.This is usually computed on the out-of-bag data.Node purity is a measure of how homogeneous a node is. An example of node purity is information entropy, i.e. $-p_1\log p_1-p_0\log p_0$ if there are two classes.For regression models, node impurity is usually taken as the variance in a node.Everytime you split a node, you do it to make the new nodes homogeneous, hence the purity increases.IncPurity of a variable is weighted average of incremental purity because of each split by this variable was used to split, with the node population as the weight.;;;
5985;1;2015-06-02T13:45:36.703;How to analyze which site has most numbers;I am trying to determine which site in our organization is in greater need of upgrades to SEP 12, so when I run a query to count, I get these type of numbersGroup       Windows_SEP_11  Mac_SEP_11  Windows_SEP_12  Mac_SEP_12Arizona\A   417                  29              219         6Arizona\B   380                  20              282        15Arizona\C   340                  30              383        507Arizona\D   310                  104             186        857Arizona\E   307                  74              403        243Arizona\F   285                  171             522        14Arizona\G   269                  1               559        41However, when I find percentages, I get these numbersGroup          Win_Sep_11_%   Mac_SEP_11_%  Windows_SEP_12_%    Mac_SEP_12_%Boston/Site 1   100               0                0               0Boston/Site 2   100               0                0               0Boston/Site 3   94                0                0               5And obviously, percentage isn't good indicator because Boston/Site 1 has only 3 computers, Boston/Site 2 only has 4 computers, etc.What is the best way to analyze data? I ultimately need a visual of sites that havemany computers, anda great need for upgrades to SEP 12, i.e. if there are more computers with SEP 11 than SEP 12.Please point me in the right direction.;[education, open-source];33;
5986;2;2015-06-02T16:31:29.623;;Try before doing LDA look at the data - like doing TF, IDF and TFIDF analysis to identify such words which happen in all subject. If You have some taxonomy of Your product definition - consider using it. In my case it was really helpful.I experimented with LDA topic modeling for recommendation systems purposes. I've few runs for offers in our marketplace service. It is in some aspect similar to Your problem with products and a list of keywords. But in our case, we do not have a product definition and an offer description is created by a seller. So You can imagine what tokens soup You will get :-)The key point for me wast to analyze TF, IDF and TFIDF inside categories where I've tested LDA. E.g. for Toys category (http://allegro.pl/zabawki-11818?ref=simplified-category-tree) ~ .5M items, LDA based only on its names results in:TOPIC:lego    0.02282500803373657puzzle  0.016679246682968853gra     0.01092812347216676klocki  0.010650330607355207trefl   0.006797740719581059zabawka 0.006365688767908592lalka   0.0063546887775893105maskotka        0.006111593588558912dzieci  0.00588884699554650124h     0.005736876716620104TOPIC:lego    0.02296167186908205puzzle  0.016613064812376045gra     0.010928234683331952klocki  0.010607432835126579trefl   0.006689061156804526lalka   0.006317220992078405zabawka 0.0062838718987015575maskotka        0.006121026383268814dzieci  0.005856944392576014424h     0.00578544965708585TOPIC:lego    0.02285507528449409puzzle  0.016539723998111246gra     0.010892673154789407klocki  0.010624244094063881trefl   0.006683424205358961lalka   0.006350949499009139zabawka 0.006324510823409019maskotka        0.006148070280704085dzieci  0.005840158016370256524h     0.005782372531889951...And the rest looks almost the same. Before running LDA I just did basic text preprocessing - our dictionary based item-name stop words removal and text tokenization (but without steaming). Even giving more terms for topic descriptions results doesn't look promising. Then, after exporting word frequencies with its offer categories by looking at data and plots I've decided to remove terms with TF-IDF above some threshold. Yes, above - I've used the calculation provided by Spark 1.3.1 implementation (HasingTF + IDF from mllib, no ml). After doing this I received:TOPIC:interaktywny    0.0026985965530064884pony    0.0026721637823727343dmuchany        0.0015777211309733249baterie 0.0012447186564456534pojazdy 0.0011171017074143481thomas  9.476877418459519E-4dinozaury       8.823212274401862E-4monsters        7.426822230409613E-4heroes  7.365256561824247E-4ninjago 7.344344320326593E-4TOPIC:pony    0.0026867880330479327interaktywny    0.0026803244251279693dmuchany        0.0015797940843537916baterie 0.001263424692131187pojazdy 0.0010685733429671523thomas  9.87888159025782E-4heroes  8.26066846602493E-4ptaszek 7.952454816383102E-4dinozaury       7.849838129629457E-4batman  7.408185413186286E-4Where results started to differ from each other. Taking like last time only 10-terms for every subject description. Still, it's not perfect but better. So, for me doing the experiments which few categories, the way to overcome problem was to remove it based on TF-IDF value. But, for every category the threshold was calculated separately. Mainly: mean(tf-idf) + 3*sd(tf-idf). I know that Idf multiplication factor in Tf-Idf should resolve this problem by itself and the term should be punished for occurring in every document. But, using Spark implementation (idf = log((m + 1) / (d(t) + 1))) and our data it was simpler during a quick and dirty experiment to filter it that way. When I find few sec. I will get back on this and place results with code and online to share this.;;;
5987;1;2015-06-02T20:16:43.627;How do I calculate the delta term of a Convolutional Layer, given the delta terms and weights of the previous Convolutional Layer?;I am trying to train an artificial neural network with two convolutional layers (c1, c2) and two hidden layers (c1, c2). I am using the standard backpropagation approach. In the backward pass I calculate the error term of a layer (delta) based on the error of the previous layer, the weights of the previous layer and the gradient of the activation in respect to the activation function of the current layer. More specifically the delta of layer l looks like this:delta(l) = (w(l+1)' * delta(l+1)) * grad_f_a(l)I am able to compute the gradient of c2, which connects into a regular layer. I just multiply the weights of h1 with it's delta. Then I reshape that matrix into the form of the output of c2, multiply it with the gradient of the activation function and am done.Now I have a the delta term of c2 - Which is a 4D matrix of size (featureMapSize, featureMapSize, filterNum, patternNum). Furthermore I have the weights of c2, which are a 3D matrix of size (filterSize, filterSize, filterNum). With these two terms and the gradient of the activation of c1 I want to calculate the delta of c1.Long story short:Given the delta term of a previous convolutional layer and the weights of that layer, how do I compute the delta term of a convolutional layer?;[education, open-source];39;1
5988;2;2015-06-02T22:14:07.250;;The most obvious way of visualizing this is to have the number of computers on the Y-axis and the size of the dots representing the percentages. The categories (or sites in your case) can be represented on the X-axis. The image below shows an example where the Y-axis represents a continuous value (can be mapped to number of computers in your case), the X-axis represents a discrete value (can be mapped to sites in your case), and the size of the dots represents another attribute (like percentage in your case). I have used the R package ggplot2 for this. ;;;
5989;2;2015-06-03T00:58:36.530;;There are two packages in R that should be able to use SMOTE to up sample the minority class: unbalanced package, and DMwR package It doesn't seem like they provide the boosting that is described in the paper that you might be referring to. But it should be straightforward to sew in SMOTE with the idea of boosting using some base tree packages in R like rpart (for example). ;;;
5990;1;2015-06-03T05:56:50.243;What is a good way to transform Cyclic Ordinal attributes?;I am having 'hour' field as my attribute, but it takes a cyclic values. How could I transform the feature to preserve the information like '23' and '0' hour are close not far. One way I could think is to do transformation: min(h, 23-h)Input: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]Output: [0 1 2 3 4 5 6 7 8 9 10 11 11 10 9 8 7 6 5 4 3 2 1]Is there any standard to handle such attributes?Update: I will be using superviseed learning, to train random forest classifier!;[education, open-source];49;
5991;1;2015-06-03T07:24:32.630;Where can I download a tagged dataset of text related to finance, programming, analytics etc.?;I want to create and train a model which classifies a new text content into finance, programming, analytics, design etc. Where can I get a relevant dataset to train my models? TIA. ;[education, open-source];67;
5992;1;2015-06-03T12:19:59.147;Could one algorithm fetch keywords from texts of different natural languages?;"I am new to practicing NLP and most topics related, but I want to make a program that can gather and extract data for me on its own.To be more specific, I want to tell the program ""I want more information on this topic(i.e heart attacks)"", and then the program shall find, gather and extract meaningful texts on the topic from around the www.I happen to live in Norway, which means that most interesting data will be in English, but I also want to fetch interesting data found in Norwegian.One challenge is the differences in stop words. For instance, ""are"" and ""and"" are both stop words in English and subjects in Norwegian.Other challenges are also likely to occur.So my question is: Would I need to create separate algorithms for every natural language to be interpreted?";[education, open-source];56;
5993;2;2015-06-03T16:55:37.517;;The question is very interesting and I do not remember to read about interesting answers. Because of that I dare to give you one possible solution even if it looks crazy enough. Usually one avoids having the same information in multiple features, since many algorithms can't handle that. But this is not the case of random forest. Contrasting linear regression (and all models based on similar ideas), random forests test all the features, by taking into consideration each feature one at a time. This way it is possible to code the same information in multiple ways without affecting learning performance, onyly space and running time.So my suggestion would be to create 24 features, each of the form $(h+offset)%24$. It's like when you encode the time in local time zones. Thus you give the occasion to rf to detect using the same units some interesting agglomerations around some hours, because each possible hour has the chance to be encoded properly in at least 1 of 24 features. It waste some space and time, but I would giv it a try to see how that works.;;;
5995;2;2015-06-03T20:05:43.563;;"There are a few ways to deal with this issue.  Python has a package called NLTK which contains stop word lists for several languages (including English and Norwegian).  You can simply use this package, it's usage is as follows:>>> from nltk.corpus import stopwords>>> stop = stopwords.words('english')>>> sentence = ""this is a foo bar sentence"">>> print [i for i in sentence.split() if i not in stop]  ['foo', 'bar', 'sentence']Alternatively, a method for automatically suppressing stop words is called tf-idf;  tf-idf is commonly used in search engines so that the most important words are promoted to the forefront.  In your case, I would suspect you'd want to have IDF scores for both English and Norwegian and apply only the appropriate one on a language to language basis.http://en.wikipedia.org/wiki/Tf%E2%80%93idf";;;
5996;1;2015-06-03T20:29:40.490;Which is faster: PostgreSQL vs MongoDB on large JSON datasets?;I have a large dataset with 9m JSON objects at ~300 bytes each. They are posts from a link aggregator: basically links (a URL, title and author id) and comments (text and author ID) + metadata.They could very well be relational records in a table, except for the fact that they have one array field with IDs pointing to child records.What implementation looks more solid?JSON objects on a PostgreSQL database (just one large table with one column, namely the JSON object)JSON objects on a MongoDBExplode the JSON objects into columns and use arrays on PostgreSQLI want to maximize performance in joins, so I can massage the data and explore it until I find interesting analyses, at which point I think it will be better to transform the data into a form specific to each analysis.;[education, open-source];222;0
5997;2;2015-06-03T20:59:43.400;;For data load, Postgre outperforms MongoDB.MongoDB is almost always faster when returning query counts.PostgreSQL is almost always faster for queries using indexes.Check out this website  and this one too for more info. They have very detailed explanations. ;;;
5998;2;2015-06-03T23:19:42.263;;"http://en.wikipedia.org/wiki/Named-entity_recognition#Formal_evaluation : To evaluate the quality of a NER system's output, several measures  have been defined. While accuracy on the token level is one  possibility, it suffers from two problems: the vast majority of tokens  in real-world text are not part of entity names as usually defined, so  the baseline accuracy (always predict ""not an entity"") is  extravagantly high, typically >90%; and mispredicting the full span of  an entity name is not properly penalized (finding only a person's  first name when their last name follows is scored as ½ accuracy). In academic conferences such as CoNLL, a variant of the F1 score has  been defined as follows:  Precision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when  [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was  required, precision for the predicted name is zero. Precision is then  averaged over all predicted entity names. Recall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions. F1 score is the harmonic mean of these two.  It follows from the above definition that any prediction that misses a  single token, includes a spurious token, or has the wrong class,  ""scores no points"", i.e. does not contribute to either precision or  recall.";;;
5999;2;2015-06-04T03:31:21.937;;"Have you tried Random Forest to do feature selection for categorical features. Random Forest uses proximity calculation(information gain) on deciding to split on a particular feature.Random Forest:""After each tree is built, all of the data are run down the tree, and proximities are computed for each pair of cases. If two cases occupy the same terminal node, their proximity is increased by one. At the end of the run, the proximities are normalized by dividing by the number of trees. Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data""For more check this link out:http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp";;;
6000;2;2015-06-04T07:32:02.597;; So my question is: Would I need to create separate algorithms for every natural language to be interpreted?Yes, I believe so. But building a model for detecting the used language is not hard: usually taking n-grams (n-shingles) and then doing classification on them works quite well in practice. By the way, for the start you can use stop words to detect a language, e.g. like it's described here.Then once the language is detected, I'd do the NLP stuff for each language separately. ;;;
6001;2;2015-06-04T08:27:43.573;;If I understood your question correctly, you want to be able to extract keywords from texts in different languages. One thing for sure, you will need a list of stopwords for each language. As someone else mentioned, that can be also obtained with TF-IDF. Algorithm for extracting keywords that I came across and seems promising is ToPMine - http://web.engr.illinois.edu/~elkishk2/papers/ToPMine.pdf It is fairly well explained in this text.Hope this helps.;;;
6004;1;2015-06-04T15:30:18.877;How to select features from text data?;"I have a data set of questions belonging to ten different categories namely (definitions, factoids, abbreviations, fill in the blanks, verbs, numerals, dates, puzzle, etymology and category relation).The categories are briefly described as follows: Definition – A question that contains a definition of the answer. Category Relation – The answer has a semantic relation to the question where the relation is specified in the category.FITB – These are generic fill in the blank questions – some of them ask for the completion of a phrase. Abbreviation – The answer is an expansion of an abbreviation in the question. Puzzle – These require derivation or synthesis for the answer. Etymology – The answer is an English word derived from a foreign word. Verb – The answer is a verb. Number – The answer is a numeral. Date – The question asks for a date or a year. Factoid – A question is a factoid if its answer can be found on Wikipedia. I used the Stanford core NLP package called shiftreducer to find out the Part-Of-Speech (POS) values for each question in a category. I thought of using this POS pattern as a discriminant among the classes but it turned out to be generalized since: All the classes follow a similar patternNouns top the POS count followed by Determinants, Prepositions, Adjectives, Plural nouns and finally verbs.What could be the other ways in which I could differentiate among the question categories? Or as my question was in its first place, ""What kind of features do I select for efficient categorization?""";[education, open-source];85;1
6005;1;2015-06-04T18:37:59.000;Plotting multiple datasets on a 3D plot;"I want to make a 3D scatter plot of multiple data selections on a single plot (i.e same axes). I know that in 2D this is possible by using par() function like so:  plot(6:25,rnorm(20),type=""b"",xlim=c(1,30),ylim=c(-2.5,2.5),col=2) par(new=T) plot(rnorm(30),type=""b"",axes=F,col=3) par(new=F)(source: http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_addat.html)Can I do something like that on a 3D plot, preferably an interactive 3D plot, like the ones created using plot3D from 'rgl' package? ";[education, open-source];83;
6006;1;2015-06-04T19:22:50.990;R Programming, how to replicate for districts in a city;I am new to R Programming and just learned basics through codeschool.comOur network spans the city, and it is divided into districts.I would like to create a map that assigns a value (based on ratio of outdated software and new software) to each district.This website has sample of 3-D maps that were created by R Programming, and I see one I am very interested in replicating, but for our city only. But when I see the source code, I don't see any mention of latitude or longitude. My head is spinning, trying to figure out how I will input this, i.e latitude and longitude of a district in our city, versus an assigned ratio, which I believe will be read from a spreadsheet.Thanks for any guidance.;[education, open-source];31;
6007;2;2015-06-04T19:38:40.603;;The latitude and longitude are stored in a text file that is read into a table, Dat.  It has the form:lat long climate-groupFor your particular case, you would calculate Dat[, 3].;;;
6008;2;2015-06-04T21:53:54.960;;"If I understand you correctly, you're looking to take the text of these questions and train classifiers to identify which of 10 categories they belong to. And you'd like to come up with a decent feature representation in order to do this. I think your finding about part-of-speech is intuitive. It makes sense that in grammatical English (assuming your question data is written in English), most questions would follow similar part-of-speech sequences since grammatically correct questions follow a particular syntactic form (at least when posed interrogatively as in the case of ""When was George Washington born?"")So, you've ruled something out - which you should actually view as progress. If you haven't tried it already, one simple thing you might do is use the actual words within the questions as features. You could use any order n-gram you like, but unigrams stick out as an immediate linguistic feature to try. It seems likely to me that while the POS-tags are similar across classes, making them difficult to distinguish between, the actual words being used in the questions may vary from class to class, giving your model a better shot of differentiating between classes.That is, maybe words like ""time"", ""year"", and ""when"" co-occur more highly with the Date class while words like ""numerical"" and ""quantity"" co-occur more with the Number class (obviously, this is speculation - I haven't seen your data). You might also look at bigrams, trigrams, or any other number n-gram for this feature set as well.Finally, there may be other features you could generate using NLP methods that may be useful. I'm not familiar with the Shiftreducer software, but Named-Entity Recognition could be helpful in generating features for the Factoid class if there are many questions about proper nouns. Other really simple features such as length of the question (counted in number of tokens). A final thought would be to use only the tagged verbs from your POS-tagger, tab these up and to see whether they differ between classes. This may be a useful feature for identifying questions present in your Verb class. Hopefully, those are some ideas to get you started.";;;
6009;2;2015-06-05T02:49:48.877;;Your algorithm is very similar to a Genetic Algorithm, which has almost the same 3 iterative steps.You would be missing a Termination condition to end the search for an answer, once your reach an acceptable one.You can probably adapt your algorithm to fit under a formal GA search heuristic.;;;
6011;1;2015-06-05T04:21:09.853;Comparing accuracy of models in ordinal regression / classification;"I am looking into creating a model to predict whether an item is ""Very Good"", ""Good"", ""Bad"" or ""Very Bad"".After I fit the training data to the models, comparing the accuracy of the models during test stump me: should it matter if a model misclassified a G to VG while the other G to VB? What about a model that has two misclassifications of one level away versus another model with only one misclassification but three levels away (eg VG to VB)?Any guideline on what is the common approach? Also, my thinking at the moment is that this should be a regression problem, but I'm happy to be corrected if I should approach this labeling of datasets more as a classification problem.";[education, open-source];43;
6013;1;2015-06-05T04:34:42.440;Automating lists on Facebook;How could one automate and improve the efficiency of suggesting lists (Close friends, Family and Acquaintances) on Facebook? Are there some community detection or specific machine learning algorithms that I could use directly for accomplishing the same? 1. Currently this option is manual on your Facebook profile2. There are suggestions but they are not close enough.;[education, open-source];42;1
6014;2;2015-06-05T05:10:10.423;;As you accept vague answers:Sranford NLP tools are strong for this kind of stuff. NER, POS Tagger maybe, Parsers, etc. Now for machine learning itself, I would try looking up at WEKA, it's got a lot of filtering, classifiers and clustering methods, including StringToWordVector filters, that are, in my opinion, fundamental to text classification. Mostly, the tags you should be looking for are Text Categorization, Natural Language Processing and even Sentiment Analysis if you will. ;;;
6015;1;2015-06-05T05:18:26.460;Assumptions/Limitations of Random Forest Models;What are the general assumptions of a Random Forest Model? I could not find by searching online. For example, in a linear regression model, limitations/assumptions are:It may not work well when there are non-linear relationship between dependent and independent variables. It may not work if the dependent variables considered in the model are linearly related. Therefore one has to remove correlated variable by some other technique. It assumes that model errors are uncorrelated and uniform (No hetroscedasticity). Are there any assumptions/limitations on similar lines.;[education, open-source];43;
6016;1;2015-06-05T05:19:23.083;Machine Learning Startup Ideas;I just started using machine learning and was wondering if anybody have cool ideas for a Startup project, I've seen this website Treato  and was amazed by it. Thanks.;[education, open-source];120;
6017;2;2015-06-05T09:47:34.947;;"Your classes express a certain order. You can classify apples to, say, ""green"", ""red"" or ""yellow"", and then every disagreement with a reference set is equal. After all, colours express no order. So as you already suggested, I would certainly use regression. Assume that the classes could be distributed as something like this:Very bad = 0 - 0.25Bad = 0.25 - 0.50Good = 0.50 - 0.75Very good = 0.75 - 1.00Now, the mismatch of Very good vs. Bad is at least 0.25, where is must be at least 0.50 with Very good vs. Very bad, which gives a better and more honest impression of the performance of your model.";;;
6018;1;2015-06-05T11:02:43.003;Create a prediction formula from data input;I have an algorithm which have as an input about 20-25 numbers. Then in every step it uses some of these numbers with a random function to calculate the local result which will lead to the final output of A, B or C.Since every step has a random function, the formula is not deterministic. This means that with the same input, I could have either A, B or C.My first thought was to take step by step the algorithm and calculating mathematically the probability of each output. However, it is really difficult due to the size of the core.My next thought was to use machine learning with supervised algorithm. I can have as many labeled entries as I want.So, I have the following questions:How many labeled inputs should I need for a decent approach of the probabilities? Yes, I can have as many as I want, but it needs time to run the algorithm and I want to estimate the cost of the simulations to gather the labeled data.Which technique do you suggest that works with so many inputs that can give the probability of the three possible outputs?As an extra question, the algorithm run in 10 steps and there is a possibility that some of the inputs will change in one of the steps. My simple approach is to not include this option on the prediction formula, since I have to set different inputs for some of the steps. If I try the advanced methods, is there any other technique I could use?;[education, open-source];37;
6019;1;2015-06-05T11:37:57.630;What is the definition of knowledge within data science?;"""Knowledge"" is crucial within several fields like Knowledge Discovery, Knowledge Distraction, Natural Language Processing, Data Mining, Big Data, etc etc etc.What is the definition of knowledge within these fields?Is there 1 common definition, or does it depend on the exact context?";[education, open-source];50;
6021;1;2015-06-05T15:03:41.860;How do I deal with non-IID data in gradient boosted random forest (for stock market)?;"I am working on a stock market decision system. I have currently centered on gradient boosting as the likely best machine learning solution for the problem. However, I have 2 fundamental issues with my data owing to it being from the stock market having to do with it not being IID. First, because of the duration of average in some indicators use, some data-points are highly correlated. For example, the 2-year trailing return of a stock is not very different if measured a month ago. My understanding is that this requires a sampling (for ensembles) where I choose datapoints that are ""far away"" in time to make trees more independent. From what I can tell so far, Matlab does not have functionality to pick a random subspace with this criteria. When I was previously thinking of using simple bagging, I figured I would just build the trees myself from custom subspaces and aggregate them into an ensemble, but this won’t work if I want to do gradient boosting. Now, on this point I am not totally sure that it is so critical to have samples “far away.” My intuition is that it is better if they are, but even if they are not perhaps by right-sizing the percent of data sampled and having enough trees it gives the same result. I would love any insight on that issue and how I might be able to use LSboost in matlab on custom samples.  (One idea I have is to just to create a small number, like 5-10 custom samples, use LS-Boost on each, and then average them.) The second fundamental problem is that data from a given stock is correlated/related to itself. I realized after thinking about it that this is of critical importance. Consider, it would likely be better, if there is enough data, to make a prediction for stock A from training data only or mostly from stock A than to use the entire market. Thus, I had been thinking of a “system” where I train on stock-specific data, stock-group data (where I use a special algorithm to group stocks), and the entire market, and then use a calculation (I can elaborate if interested) that determines which of these models is more likely to give the better result. If the input looks very different from the stock-specific training data, for example, then it will use the group or entire market. I am pretty convicted that some form of taking into account which stock the system is looking at is important to optimizing performance. Now, on the second issue the question is what is the best way to organize this. Thinking naively, it would be great to simply feed categories to the predictor that indicate what stock it is looking at. However, my belief here from what I know about these algorithms is that this will have poor results on new data, because this predictor will assume that it has seen the full universe of potential outcomes for each stock, when many times this isn’t the case. (Say there is a stock with only a one year history with a big rally – the system will think the rally will continue regardless of how different the new data looks). So I feel like I have to do something like in the previous paragraph. I don’t know if there is some way for the system to “automatically” recognize when new data is sufficiently similar to stock-specific data to focus on a stock-specific prediction vs. when it is different and it should go to the default system with multiple stocks.If you have any insights on these issues and/or how to address them in Matlab or otherwise, I would very much appreciate. Thanks in advance. Best,Mike";[education, open-source];17;
6022;1;2015-06-05T15:23:25.213;+'s and -'s of transformation/normalization for tree-based decision systems;"I am having a confusion about the possible benefits of data transformation for tree-based decision systems. Specifically, I am doing stock market analysis. In this particular problem, assume I use as inputs the 1-week (1w), 2w, 3w and 4w historic returns of stocks. Lets say the output to target is the 1-month (1m) forward return. One possibility I am considering is to for each stock ""normalize"" the returns so that they have the same standard deviation of 1m returns...essentially adjust for volatility. To get the output of the ""transformed"" system, i'll just re-adjust the output by volatility. The inputs would also be adjusted in the same way as the output (they are all stock returns). My sense is that I am in a way INCREASING the amount of training data I have in doing so, since the inputs and outputs will look much more like each other than if I don't do the transformation. Let me see if a numerical example helps, even though it is contrived. Suppose a volatile stock has inputs of -5% -7% -10% -12%. A low-volatility stock may never have inputs like this, but it might have something like -2.5% -3.5% -5% -6% (these are half of the other returns). With the transformation, it is as if the system gets to see the data twice. My sense is there may be a trade-off here. If the underlying nature of the data is such that the pattern followed by different stocks is consistent after the transformation, then I likely will get a gain in results with the transformation (i will have more data effectively). If on the other hand it is important to treat more volatile-looking inputs (like the first numbers) separately from less volatile, then I may reduce my variance with the transformation, but increase bias. The prior paragraph brings up yet another question: if there are advantages of both approaches, is there a way to combine them somehow? I honestly can't think of how to, because the output is different in the two cases. Maybe I could feed the output from the transformed system to the non-transformed one? Or, maybe I could feed volatility as an input to a non-transformed system?Any help on how to address this issue would be very much appreciated. I'm really just trying to understand the trade-off here, not looking for any advice given my specific analysis (my analysis is actually a lot more complex!). Thanks so much.Mike";[education, open-source];7;
6023;2;2015-06-05T15:23:58.380;;I'm not sure if I understood your question! Probably better to plot a scheme at least. But according to what I guess from your question:Q2- You probably need a simple MLP (Multilayer Perceptron)! it's a traditional architecture for Neural Networks where you have $n$ input neurons (here 20-25), one or more hidden layers with several neurons and 3 neurons as output layer. If you use a sigmoid activation function ranged from 0 to 1, the output for each class will be $P(Y=1|X=x)$.Q1- So your question probably is: how many training data you need for learning a model? and to the best of my knowledge the answer is as many as possible!and about the last question, I really could not figure out what you mean. You apparently have a very specific task so I suggest to share more insight for sake of clarification.I hope I could help a little!;;;
6024;2;2015-06-05T15:32:19.307;;This link contains an amazing amount of deep learning literature.Summarizing it here(going in the order a beginner ideally should)-NOTE: All these resources mainly use python.1) First of all, a basic knowledge of machine learning is required. I found Caltech's Learning from data to be ideal of all the machine learning courses available on the net.Andrew Ng's Coursera course is pretty good too.2) For Neural networks, nobody explains it better than Dr.Patrick Winston.The assignments should be tried out for better understanding. They are in python.3) For a better understanding of Neural Networks, Michael Nielsen's course should be done(as suggested by Alexey). It is pretty basic but it works.4) For deep neural networks, and implementing them faster on GPUs, there are multiple frameworks available, such as Theano, Caffe, Pybrain, Torch,etc.Out of these Theano provides a better low level functionality that allows its user to create custom NNs. It is a python library, so being able to use numpy,scikit-learn, matplotlib, scipy along with it is a big plus.The deep learning tutorial written by Lisa Lab should be tried out for a better understanding of theano.5) For Convolutional Neural Networks, follow andrej karpathy's tutorial.6) For unsupervised learning, follow here and here.7) For an intersection of deep learning and NLP, follow Richard Socher's class.8) For LSTMs, read Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780 and Graves, Alex. Supervised sequence labelling with recurrent neural networks. Vol. 385. Springer, 2012.Here is LSTM's Theano code.;;;
6025;2;2015-06-05T19:59:43.367;;First of all, I would get acquainted with some core concepts of Machine Learning before building websites:Common algorithms, differences between them (Naive Bayes, KMeans, Regression, Support Vector Machines, Decision Trees, you name it...)Supervised vs. unsupervisedFeatures and feature selection Classification evaluation (precision, recall, accuracy)Machine learning is a huge field which is applied in many different areas. I don't know how proficient you are at this stage, but I would certainly play around with these things first in order to get a solid understanding of what you're dealing with. After that, the options are virtually limitless in a world of Big Data (forgive me for using the annoying buzz-word). What is truly valuable is someone knows how to do machine-learning rather than what (because there's enough to go about).;;;
6027;1;2015-06-05T21:52:05.920;project data from S^n to S^2;"I have few points in $S^n$, i.e., the $n$-dimensional unit sphere embedded in $\mathbb{R}^{n+1}$, and I would like to project them down to $S^2$, i.e., the 2-dimensional unit sphere (embedded in $\mathbb{R}^3$) to visualize it with the constraint that neighboring points should be close by. I spent some time playing with t-sne but of course, the points no longer lie on $S^2$. I normalized the projections but that introduces weird distortions, for instance, if the variance of one dataset is very small in $S^n$ as compared to other, I expect the same to hold in their $S^2$ projections; that is not the case upon normalizing t-sne. Any ideas? I would really like something that makes the previous statement hold.";[education, open-source];51;
6028;1;2015-06-06T03:30:10.683;"Python - Increasing performance speed of permutation calculation of Itertools; repeat=15";"What methods of increasing calculation when repeat is greater then 10-25.permutation.pyinputRepeat = input('>>> Enter repeat value. (int): ')L = 'ABCDEFGHIJKLMNOPQRSTUVWYZ'i = list(''.join(x) for x in itertools.product(L', repeat=inputRepeat))print iI'm running this on a Mac and working on data science project. It calculates when repeat value is >= 9, but needs increased amount of time before the script begins to print after that value; it could be that this machine needs more CPU/memory, or can the current code be optimized for performance? As this becomes a problem when I need to generate the entire list of repeat=26.I'm thinking there are alternative solutions that could employ the use of a matrix list and combinations/permutations to arrive at the same results, and result in decreases of runtime/increased performance.";[education, open-source];29;
6029;2;2015-06-06T06:51:08.527;;You may benefit more from the schemaless design of Mongodb. This means its very easy to modify data structures on the fly.There is no such thing as a join in Mongodb. So how one thinks about data and how to use it needs to be modified to account for document based and schemaless db environments.Maybe speed becomes less important as perspective and priorities change.I hope that helps.-Todd;;;
6030;1;2015-06-06T07:00:13.363;Using Vowpal Wabbit for NER;The Vowpal Wabbit apparently supports sequence tagging functionality via SEARN. The problem is that I cannot find anywhere detailed parameter list with explanations and with some examples. The best I could find is Zinkov's blog entry with a very short example. The main wiki page barely mentions SEARN.In the checked out source code I found demo folder with some NER sample data. Unfortunately, the script running all the tests does not show how to run on this data. At least it was informative enough to see what is the expected format: almost the same as standard VW data format, except that entries are separated by blank lines (this is important).My current understanding is to run the following command:cat train.txt | vw -c --passes 10 --searn 25 --searn_task sequence \--searn_passes_per_policy 2 -b 30 -f twpos.vwwhere--searn 25 - the total number of NER labels (?)--searn_task sequence - sequence tagging task (?)--searn_passes_per_policy 2 - not clear what it doesOther parameters are standard to VW and need no additional explanation. Perhaps there are more parameters specific to SEARN? What is their importance and impact? How to tune them? Any rules of thumb?Any pointers to examples will be appreciated.;[education, open-source];32;
6031;1;2015-06-06T11:59:55.387;Geometric weighting of temperature;I have a number of weather stations, and I know their positions. I would like to interpolate measurements from them for various other positions as a weighted average of these stations, but of course I need weights for this. I am thinking the most logical choice here from a physics point of view will be weighting by the inverse of the distance to the station, but I haven't quite convinced myself that that is right, and am not sure if I should maybe be using the distance squared, or maybe something in between.Any comments? Are there any other reasonable alternatives I should be considering?;[education, open-source];18;
6032;1;2015-06-06T15:50:34.333;Iterative normalization of data;I'm working on a data processing framework (in Java) that implements various machine learning and meta-heuristic optimization algorithms to data streams iteratively in real-time.I'm working with massive data sets that transition from database to live feeds. I want the same algorithms to be running on both data sets without interruption when the switch from database to live data happens.Are there normalization methods that work well for noisy timeseries data and that can be implemented iteratively (i.e. without using the average and stdev of the entire data set)? Perhaps using rolling/tumbling windows?;[education, open-source];23;
6033;2;2015-06-06T21:49:54.407;;The definition of knowledge varies based on the context, but can be broadly defined as actionable information based on further analysis (statistical, heuristic, or otherwise) of those patterns, associations, or relationships identified in the raw data.Put another way, knowledge is defined by what information will most prove actionable based on the needs of the client or user (sometimes called the semantics). ;;;
6034;2;2015-06-07T05:38:32.923;;Knowledge is a general term and I don't think that there exist definitions of knowledge for specific disciplines, domains and areas of study. Therefore, in my opinion, knowledge, for a particular subject domain, can be defined just as a domain-specific (or context-specific, as mentioned by @JGreenwell +1) perspective (projection) of a general concept of knowledge.;;;
6036;1;2015-06-07T18:42:27.227;How can i draw a plot like this?;I want a plot like below one, which software i should to use and how?;[education, open-source];74;
6037;1;2015-06-07T23:26:19.183;rpart and rpart2;I'm not sure if this is more appropriate for SO or DS in Stack Exchange since technically it's not about coding: in caret package for training in R, it's possible to train the model using rpart or rpart2 as the method.I understand that rpart is an implementation of CART. What is rpart2 and how is it different from rpart?My eventual aim is actually to compare the difference between the tree generated by rpart and rpart2, because my result seems to imply rpart2 has better accuracy for my dataset, but I have no clue how to view the rpart2 tree.;[education, open-source];67;
6038;2;2015-06-08T02:16:02.677;;You could do that easily using Microsoft Excel. Select your data and click suggested charts. EXCEL provides you with an array of options to display your data graphically. If you want to try your luck with other packages, I would suggest Tableau or Gephi. Although Tableau is paid, you could easily download the most stable and current version of Gephi for free. Just make sure you have the correct versions of Java JDK on your system for Gephi. ;;;
6040;1;2015-06-08T08:16:38.667;What kind of analysis can I do in this dataset?;I have a dataset of clients (their city, name, age, gender, number of children) and another dataset about the products that they have bought. i have been asked to do:extract knowledge about client profiles.I don't know what knowledge should I extract.we are studying clustering and classification so they should connect to the question. what i thought about is to make clusters of clients. but i don't know what criterisas should I depend on.should I just use a clustering algorithm like k means and let it give me the clusters ?any suggestion would be appreciatedUpdateThese are the dataset that I have:Card: CardID, City, Region, PostalCode, CardStartDate, Gender, DateOfBirth, MaritalStatus, HasChildren, NumChildren, YoungestChildItem: ItemCode, ItemDescription, CategoryCode, SubCategoryCode, BrandCode, UpmarketFlagTransaction: Store, Date, Time, TransactionID, CardID, PaymentMethodCategory: CategoryCode, CategDescriptionTransaction_Item: Store, Date, Time, TransactionID, ItemNumber, ItemCode, AmountSubCategory: SubCategoryCode, SubCategDescripThe teacher said that we should categories the clients and then indicate which products best suit for each customer.;[education, open-source];59;
6042;1;2015-06-08T10:52:39.433;Poor performance shown on Rare event modeling;I am working on a Rare event classification problem. I Have 95% of the data as a majority class and 5% of the data as the minority class. I use classification trees algorithm. I am measuring the goodness of the model using confusion matrix.As the i have the minority class just 5% of the total data, even though my prediction performance of minority class is close to 70%, the total number of errors are high.For example, here is my confusion matrix.             0           1     0     213812      7008     1     29083       16877Though the Minority class(class 1) has predicted 16877 times correctly(70% and the misclassifcation is just 30%, but the absolute value of the misclassifcation is very high(29083) comparing to the correctly predicted minotriy class (16877). Which makes the solution less usable for the business. Is there any idea on handling these kind of  issues in such rare event modelling.Kind note: I have  balanced the target variable using the SMOTE algorithm before applying Classification tree.;[education, open-source];85;1
6043;2;2015-06-08T11:10:44.397;;You may go through two different approaches:Unsupervised Learning (Clustering)You can choose attributes who make someone's profile and try to cluster it (e.g. using k-means). If you look at clusters according to different attributes it may give you some insight about the data. Do not forget to exclude names as they are meaningless for analysis.Supervised Learning (Classification)You can use relevant attributes which may affect an output e.g. age and city may affect gender or Nr of children so if u put these as targets you can classify different users according to their age and city.The point is that your data is not suitable for more sophisticated analysis as your features are not many and too complicated (i.e. answering the questions like age distribution among a certain gender or the relation between cities, age and number of children or something like them are the only questions can be answered)I hope it helped and if there are more questions please comment here.UPDATE 1You can look at your data from different point of view so first try to choose one. For example clients can be clustered via their transactions (when they bought, what they bought, how expensive they bought, etc) or by their personal info (gender, age, payment method, etc). It gives you a first impression of your client data. Then go through clustering using all those information i.e. put personal and transactional features together. It gives you an overview of customers in general and might result in some significant clusters (categories).After that you can look at the problem from product point of view meaning you look at the transactional features and try to see different distributions and histograms to get an overview of what's happening to the product e.g. you can set time as the x axis and try to extract different time series like how many product are sold at this point of time?, what kind of people (age, gender, etc) bought at this point of time? and so on. For general analysis a dimensionality reduction (e.g. PCA) might reveal some information and give you insight. Please note that for any kind of analysis ONLY use relevant features e.g. IDs are not informative but categories.The most important point for recommending product to a customer (or predicting what they buy) is to use your input/output pair properly so the customer info (age, city, when does he/she usually buy something?, etc) are inputs and products (their categories, their type, whatever u know about them) are outputs.Good Luck!PS1: From experience of doing an industrial version of exactly the same project I would say payment method is not much informative.PS2: StackExchange provides an upvote and an accept button for good answers!;;;
6044;1;2015-06-08T19:50:16.510;What is a XML dataset?;What are xml datasets? Is it possible to convert them to csv files?I'm working on a Java program and I sometimes download datasets wich are in a binary format, are those xml? Thank you.;[education, open-source];39;
6045;2;2015-06-08T21:40:20.450;;"XML is a markup language similar to html. One uses tags with attributes to build data structures. For example,<sampleXML>  <Menu>    <Food>      <item1>Spaghetti Bolognese</item1>      <item2>Spaghetti Carbonara</item2>    </Food>    <Drinks>      <item1 class = 'drinks'>Sprite</item1>    </Drinks>  </Menu></sampleXML>As you can see XML employs tags such as <Food></Food> and attributes such as class = 'drinks' which is exactly what HTML has. To access the XML data in java you got couple choices. You can read it in as a string and parse it using built in DOM parser. Or you can use JAXB to map XML directly to Java objects. Surely you can convert XML to a csv file. There are free websites online for that. Just google ""XML to csv converter.""Binary files are not XML, but can be. This needs little explanation. The letters and words you see here are ASCII characters. This is human readable text. Each ASCII character has a binary representation. For example, j in binary is 1101010. A binary file is any file in computer language (0 an 1). A binary file can also be a combination of text and binary. You can convert binary to ASCII and those files you download may indeed be XML I described above. To convert binary to ASCII just google it.";;;
6048;1;2015-06-09T09:37:11.537;Decision tree or logistic regression?;I am working on a classification problem. I have a dataset containing equal number of categorical variables and continuous variables. How will i know what technique to use? between a decision tree and a logistic regression?Is it right to assume that logistic regression will be more suitable for continuous variable and decision tree will be more suitable for continuous + categorical variable?;[education, open-source];171;1
6050;1;2015-06-09T10:34:14.970;Encoding for k-level qualitatative variable;I have a qualitative variable, e.g. userId, which could take around 30,000 different coded values ($k$). I would like to represent this variable as a dummy variable. Coding this into a vector of size $k$ doesn't seem to be a good approach. Is there a more compact method for coding for this variable?;[education, open-source];36;0
6051;1;2015-06-09T11:27:38.940;Do I need an Artificial Intelligence API?;I'm wondering if programmers tend to use AI apis. And if so, what are they like? And where can I find a nice one for JAVA?Sorry if my question is too vague or out of place, I'm just a beginner. Thank you.;[education, open-source];57;
6052;1;2015-06-09T13:49:57.683;"How to recognize a two part term when the space is removed? (""bigdata"" and ""big data"")";"I'm not a NLP guy and I have this question. I have a text dataset containing terms which go like, ""big data"" and ""bigdata"". For my purpose both of them are the same. How can I detect them in NLTK (Python)?Or any other NLP module in Python?";[education, open-source];62;2
6053;2;2015-06-09T13:51:35.770;;I don't use Java much but have you looked into machine learning with the Weka Java API?  You might also try Encog, mentioned here, and if you search for Practical Artificial Intelligence Programming With Java by Mark Watson, you might still be able to find a PDF version that's licensed for free non-commercial use.;;;
6054;1;2015-06-09T14:39:04.210;In SVM Algorithm, why vector w is orthogonal to the separating hyperplane?;I am a beginner on Machine Learning.In SVM, the separating hyperplane is defined as $y = w^T x + b$.Why we say vector $w$ orthogonal to the separating hyperplane?;[education, open-source];83;0
6055;2;2015-06-09T15:12:19.093;;Geometrically, the vector w is directed orthogonal to the line defined by $w^{T} x = b$. This can be understood as follows: First take $b = 0$. Now it is clear that all vectors, $x$, with vanishing inner product with $w$ satisfy this equation, i.e. all vectors orthogonal to w satisfy this equation. Now translate the hyperplane away from the origin over a vector a. The equation for the plane now becomes: $(x − a)^{T} w = 0$, i.e. we find that for the offset $b = a^{T} w$, which is the projection of a onto to the vector $w$. Without loss of generality we may thus choose a perpendicular to the plane, in which case the length $\vert\vert a \vert\vert = \vert b \vert /\vert\vert w\vert\vert$ which represents the shortest, orthogonal distance between the origin and the hyperplane.Hence the vector $w$ is said to be orthogonal to the separating hyperplane. ;;;
6056;2;2015-06-09T15:16:41.170;;Your question seems vague without the sample data you are using. How does your data-set look like? If there are delimiters within your data, you could get rid of only spaces between all words and then 'big data' & 'bigdata' would be the same, if that is what you want to do. ;;;
6057;2;2015-06-09T15:21:27.990;;Try using both regression and decision trees. Compare the efficiency of each technique by using a 10 fold cross validation. Stick to the one with higher efficiency. It would be difficult to judge which method would be a better fit just by knowing that your dataset is continuous and, or categorical.  ;;;
6058;2;2015-06-09T15:34:12.437;;To use Decision Tree, you should transform the continuous variable into categorical.One more thing, Logistic Regression is usually used to predict result according to the probability.;;;
6059;2;2015-06-09T16:58:44.353;;Long story short: do what @untitledprogrammer said, try both models and cross-validate to help pick one.Both decision trees (depending on the implementation, e.g. C4.5) and logistic regression should be able to handle continuous and categorical data just fine. For logistic regression, you'll want to dummy code your categorical variables.As @untitledprogrammer mentioned, it's difficult to know a priori which technique will be better based simply based on the types of features you have, continuous or otherwise. It really depends on your specific problem and the data you have. (See No Free Lunch Theorem)You'll want to keep in mind though that a logistic regression model is searching for a single linear decision boundary in your feature space, whereas a decision tree is essentially partitioning your feature space into half-spaces using axis-aligned linear decision boundaries. The net effect is that you have a non-linear decision boundary, possibly more than one. This is nice when your data points aren't easily separated by a single hyperplane, but on the other hand, decisions trees are so flexible that they can be prone to overfitting. To combat this, you can try pruning. Logistic regression tends to be less susceptible (but not immune!) to overfitting.Lastly, another thing to consider is that decision trees can automatically take into account interactions between variables, e.g. $xy$ if you have two independent features $x$ and $y$. With logistic regression, you'll have to manually add those interaction terms yourself.So you have to ask yourself: what kind of decision boundary makes more sense in your particular problem?how do you want to balance bias and variance?are there interactions between my features?Of course, it's always a good idea to just try both models and do cross-validation. This will help you find out which one is more like to have better generalization error.;;;
6060;2;2015-06-09T17:46:48.943;;There is always clustering techniques you could do that would help you determine how to group userId into a dummy variable.;;;
6061;2;2015-06-10T02:10:50.280;;"There is a nice implementation of this in gensim: http://radimrehurek.com/gensim/models/phrases.htmlBasically, it uses a data-driven approach to detect phrases, ie. common collocations. So if you feed the Phrase class a bunch of sentences, and the phrase ""big data"" comes up a lot, then the class will learn to combine ""big data"" into a single token ""big_data"". There is a more complete tutorial-style blog post about it here: http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/";;;
6062;2;2015-06-10T02:19:16.067;;"If you are willing to use the caret package in R and use random forests, you can use the method in the following blog post for downsampling with unbalanced datasets: http://appliedpredictivemodeling.com/blog/2013/12/8/28rmc2lv96h8fw8700zm4nl50busepBasically, you just add a single line to your train call. Here is the relevant part: > rfDownsampled <- train(Class ~ ., data = training,+                        method = ""rf"",+                        ntree = 1500,+                        tuneLength = 5,+                        metric = ""ROC"",+                        trControl = ctrl,+                        ## Tell randomForest to sample by strata. Here, +                        ## that means within each class+                        strata = training$Class,+                        ## Now specify that the number of samples selected+                        ## within each class should be the same+                        sampsize = rep(nmin, 2))I have had some success with this approach in your type of situation.For some more context, here is an in-depth post about experiments with unbalanced datasets: http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/";;;
6063;2;2015-06-10T03:53:36.337;;One needs to use an artificial intelligence (AI) API, if there is a need to add AI functionality to a software application - this is pretty obvious. Traditionally, my advice on machine learning (ML) software includes the following two excellent curated lists of resources: this one and this one.However, keep in mind that ML is just a subset of AI domain, so if your tasks involve AI areas beyond ML, you need more AI-focused tools or platforms. For example, you can take a look at ai-one's AI platforms and APIs as well as interesting general AI open source project OpenCog.In addition to the above-mentioned AI-focused platforms, IBM's Watson AI system deserves a separate mention, as quite cool and promising. It offers its own ecosystem for developers, called IBM Watson Developer Cloud, based on IBM's BlueMix cloud computing platform-as-a-service (PaaS). However, at the present time, I find this offering to be quite expensive as well as limiting, especially for individual developers, small startups and other small businesses, due to its tight integration with and reliance only on a single PaaS (Blue Mix). It will be interesting to watch this space as competition in AI domain and marketplace IMHO will surely intensify in the future.;;;
6064;1;2015-06-10T04:57:02.477;Multivariate - Time series data pattern changes;New to R. In my example, my customers have restricted allocation of budget for Milk. I have more than 5 brands of milk in my store. Here my objective is how I know my customer is shifting from one brand to other? (Example: Customer is replacing Brand 1 with Brand 2 in my time series data). I would like to compute that shifting pattern every quarter and observe the trend quarter by quarter.Sample Quarter Data:Date    Milk-Brand1 Milk-Brand2 Milk-Brand3 Milk-Brand4 Milk-Brand51/1/2015    200 140 190 220 1501/2/2015    204 138 195 226 1441/3/2015    208 136 200 232 1261/4/2015    212 134 205 238 1082/2/2015    216 132 210 244 901/6/2015    220 130 215 250 721/7/2015    224 128 220 256 541/8/2015    228 126 225 262 361/9/2015    232 124 230 268 183/1/2015    236 122 235 274 03/2/2015    240 120 240 280 133/3/2015    244 118 245 286 333/4/2015    248 116 250 292 1520/3/2015   252 114 255 298 3320/3/2015   256 112 260 304 15Do you suggest compute correlation between each 'brand of milk' and compare those correlations from one quarter to other quarter? Or Cross-correlation? Or others? I am open.Thanks for your advice.;[education, open-source];47;
6065;2;2015-06-10T11:13:37.083;;"By using a parse tree, you divide your sentence into parts. Suppose, in the example of sentiment analysis, you can use those parts to assign a positive/negative sentiment to each part and then take the cumulative effect of those parts.This image will help you understand more. The first half has a negative sentiment(mainly because of the word ""dry"") but because of the word ""but"" and the usage of the word ""enjoyed"", the negative sentiment is turned into a positive sentiment.As for using them, you can simply generate a word vector representation of the individual words in the sentence and use neurons in place of the parent nodes. Each neuron should be connected to another neuron through weights. All the leaf nodes will be the word vector representations of words of the sentence. The top parent neuron(in this case the top blue + symbol) should generate a positive/negative sentiment according to the sentence. This tree structure can be trained in a supervised manner.Read this paper for a more through understanding.Image credits: cs224.stanford.edu";;;
6066;1;2015-06-10T11:33:17.250;Text classification: Choosing training and testing set for each category;I am very new to machine learning.I have a text classification problem in hand. I have a tagged dataset of around 750 documents( short texts), categorized manually into 16 buckets. I want to train a classifier on this data. I know that there should be a training set and a test set (an option could be 80-20 ). In my understanding, this should be for the complete set( 80% of my 750 documents- training, 20% of 750 documents - testing ). 1. They should be randomly generated or is there some condition for category? ie. if category A constitutes 60%,category B 5%, C 7% etc. how to choose the training set?;[education, open-source];63;
6067;2;2015-06-10T13:00:47.707;;The most commonly used option is 2/3rd of the data as training and 1/3rd as testing sets respectively. What kind of software or tool are you using for this categorization process? For your classifier to work efficiently at the end of all the hard work, have you considered cross validating it using a multiple fold cross validation before using and assigning the training and testing sets? As in for the skewed data categories, try finding instances for the classes/ categories which have fewer samples (~10%).;;;
6068;1;2015-06-10T13:16:17.530;How can an undergraduate learn more about deep learning?;I am an undergraduate who needs to submit a thesis for graduation. I am fairly interested in deep learning, and am working on a project that uses deep learning methods extensively (rCNNs to be precise). The caveat here is that I am working alone, with little help from my advisor. There are hardly any experts on deep learning in my university. How do I go about finishing my project. It is specially intimidating to see that most papers in deep learning are published by multiple accomplished scientists with very little involvement of undergraduate level students. PS: I am really interested in this topic, so please try not to advise me to switch to an easier topic.;[education, open-source];136;
6069;1;2015-06-10T14:25:05.840;Rearange input data - unknown size of input;I have some data of form showed below to squezze through some neural network.Customer Step Var0  ... VarN0        0    some  ... stuff0        1    again ... stuff0        2    foo   ... bar0        3    bla   ... blub1        0    other ... stuff1        1    and   ... ongoing2        0    and   ... so on.        .    .     ... ..        .    .     ... ..        .    .     ... .As the neural network has a fixed input layer size, but the customers step count can differ at each customer, I have following question:What is the best practise (if exists) to rearange the given data for using as input of a NN? ;[education, open-source];7;1
6070;1;2015-06-10T17:27:05.703;Statistical distances for time series of distributions;"I am interested in clustering $N$ time series of $T$ 'values' each. These values are distributions (which can be represented by their cumulative distribution functions (cdf), or their probability density functions (pdf), or more convenient forms such as square-root pdfs yielding a simple spheric geometry).For comparing given distributions, there is an extensive literature on statistical distances (KL, Hellinger, Wasserstein, and so on), but for comparing given time series of distributions, I am not sure whether there is any literature at all?Such distances should somehow take into account dynamics information besides the distribution proximity at time t. Ideally, I wish I could have a kind of information factorization similar to this result.I am wondering if such distances already exist and whether this kind of problem has already been formulated in the literature?--edit for further precisions and answer to comments:Thanks for your answer, but dynamic time warping does not suit to my need. This dp technique only captures a rough similarity of shapes by allowing non-linear time distortion. But, it does not amount for the whole information in these time series, e.g. what about the distribution of distortions? Do the distributions of a given time series vary smoothly through time or violently? DTW is not always the solution, for instance, when working with random walks, it does not make sense to use a DTW since there are no time patterns! In this case, the only information is ""correlation"" and ""distribution"" (cf. Sklar's theorem in Copula Theory), and the paper cited above.-- edit 2 Here are the papers that are somehow related to my question:Predicting the Future Behavior of a Time-Varying Probability DistributionClustering on the unit hypersphere using von Mises-Fisher distributionsUnsupervised clustering of multidimensional distributions using earth mover distanceHilbert space embeddings of conditional distributions with applications to dynamical systems";[education, open-source];89;
6071;2;2015-06-10T18:47:11.447;;Coursera and EdX have a courses on Neural Networks (which are the basis of Deep Learning).  Stanford also has a good series on Deep Learning.This question also has some good tips. For me, what is most helpful is to program the algorithms to help me understand them step by step.  Python is good for that purpose.;;;
6072;1;2015-06-10T18:53:25.013;CSV to PKL for Theano;"I am trying to make a pkl file to be loaded into theano from a csv starting point    import numpy as np    import csv    import gzip, cPickle    from numpy import genfromtxt    import theano    import theano.tensor as T    #Open csv file and read in data    csvFile = ""filename.csv""    my_data = genfromtxt(csvFile, delimiter=',', skip_header=1)    data_shape = ""There are "" + repr(my_data.shape[0]) + "" samples of vector length "" + repr(my_data.shape[1])    num_rows = my_data.shape[0] # Number of data samples    num_cols = my_data.shape[1] # Length of Data Vector    total_size = (num_cols-1) * num_rows    data = np.arange(total_size)    data = data.reshape(num_rows, num_cols-1) # 2D Matrix of data points    data = data.astype('float32')    label = np.arange(num_rows)    print label.shape    #label = label.reshape(num_rows, 1) # 2D Matrix of data points    label = label.astype('float32')    print data.shape    #Read through data file, assume label is in last col    for i in range(my_data.shape[0]):        label[i] = my_data[i][num_cols-1]        for j in range(num_cols-1):            data[i][j] = my_data[i][j]    #Split data in terms of 70% train, 10% val, 20% test    train_num = int(num_rows * 0.7)    val_num = int(num_rows * 0.1)    test_num = int(num_rows * 0.2)    DataSetState = ""This dataset has "" + repr(data.shape[0]) + "" samples of length "" + repr(data.shape[1]) + "". The number of training examples is "" + repr(train_num)    print DataSetState    train_set_x = data[:train_num]    train_set_y = label[:train_num]    val_set_x = data[train_num+1:train_num+val_num]    val_set_y = label[train_num+1:train_num+val_num]    test_set_x = data[train_num+val_num+1:]    test_set_y = label[train_num+val_num+1:]    # Divided dataset into 3 parts. split by percentage.    train_set = train_set_x, train_set_y    val_set = val_set_x, val_set_y    test_set = test_set_x, val_set_y    dataset = [train_set, val_set, test_set]    f = gzip.open(csvFile+'.pkl.gz','wb')    cPickle.dump(dataset, f, protocol=2)    f.close()When I run the resulting pkl file through Thenao, (as a DBN or SdA) it pretrains just fine, which makes me think the data is stored correctly. However when it comes to finetune I get the following error:        epoch 1, minibatch 2775/2775, validation error 0.000000 %    Traceback (most recent call last):      File ""SdA_custom.py"", line 489, in         test_SdA()      File ""SdA_custom.py"", line 463, in test_SdA        test_losses = test_model()      File ""SdA_custom.py"", line 321, in test_score        return [test_score_i(i) for i in xrange(n_test_batches)]      File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 606, in __call__        storage_map=self.fn.storage_map)      File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 595, in __call__        outputs = self.fn()    ValueError: Input dimension mis-match. (input[0].shape[0] = 10, input[1].shape[0] = 3)    Apply node that caused the error: Elemwise{neq,no_inplace}(argmax, Subtensor{int64:int64:}.0)    Inputs types: [TensorType(int64, vector), TensorType(int32, vector)]    Inputs shapes: [(10,), (3,)]    Inputs strides: [(8,), (4,)]    Inputs values: ['not shown', array([0, 0, 0], dtype=int32)]    Backtrace when the node is created:      File ""/home/dean/Documents/DeepLearningRepo/DeepLearningTutorials-master/code/logistic_sgd.py"", line 164, in errors        return T.mean(T.neq(self.y_pred, y))    HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.10 is the size of my batch, if I change to a batch size of 1 I get the following:    ValueError: Input dimension mis-match. (input[0].shape[0] = 1, input[1].shape[0] = 0)I think I am storing the labels wrong when I make a pkl, but I can't seem to spot what is happening or why changing the batch alters the error Hope you can help!";[education, open-source];15;
6073;2;2015-06-10T19:06:56.943;;Both rpart and rpart2 implement a CART and wrap the rpart function from the rpart library.  The difference is the constraints on the model each enforces.  rpart uses the complexity parameter, cp, while rpart2 uses the max tree depth, maxdepth.See: train_model_list section of http://cran.r-project.org/web/packages/caret/caret.pdf  and rpart.control section of http://cran.r-project.org/web/packages/rpart/rpart.pdf.;;;
6074;2;2015-06-10T19:43:17.277;;If you are using tree based methods, you can replace the qualitative variables with random numbers or with the counts of each class.;;;
6075;1;2015-06-10T20:38:49.410;Cross selling + Classifier modeling strategy;"Product A or B can be offered to a customer in a cross-selling setting. Through A/B testing, data was collected about the customers that buy A and those that buy B.Now there are different ways to model this. We could have two models that model P(buysA; X) and P(buysB; X) and offer the product that we believe has the highest probability of being bought. The problem with this is that it requires well calibrated probabilities. Another possibility is to use a model that directly chooses between showing A or B. The problem with this case is that there is no direct data where the customer HAD to choose between both, either they were presented with one OR the other. What would be the sane approach to model this?";[education, open-source];30;
6076;1;2015-06-10T20:39:15.903;Most important part of feature standardization and how is standardization affected by sparsity?;I am thinking of preprocessing techniques for the input data to a convolutional neural network (CNN) using sparse datasets and trained with SGD. In Andrew Ng's coursera course, Machine Learning, he states that it is important to preprocess the data so it fits into the interval $ \left[ 3, 3 \right] $ when using SGD. However, the most common preprocessing technique is to standardize each feature so $ \mu = 0 $ and $ \sigma = 1 $. When standardizing a highly sparse dataset many of the values will not end up in the interval.I am therefore curious - would it be better to aim for e.g. $ \mu = 0 $ and $ \sigma = 0.5 $ in order for the values be closer to the interval $ \left[ 3, 3 \right] $? Could anyone argue based on a knowledge of SGD on whether it is most important to aim for $ \mu = 0 $ and $ \sigma = 1 $ or $ \left[ 3, 3 \right] $?;[education, open-source];60;1
6078;1;2015-06-11T06:20:43.703;Topic models for Relevance Prediction;Suppose I have data in the following form :  Query - Document Pairs along with their relevance score( or class ). Is there a way to use topic modeling to devise a model so that later given a query and a document, we can predict its relevance score ?;[education, open-source];23;
6079;2;2015-06-11T07:42:17.480;;From a geometric point of view, if all your data are unitary, $\forall x, ||x||^2 = \langle x,x \rangle = 1$, then the scalar product of two vectors defines an angle $\phi$, $\langle x,y \rangle = \cos \phi$, and you have a distance $\phi = \arccos \langle x,y \rangle$. Visually, all your data live on a unit sphere. Using a dot product as a distance will give you a chordal distance, but if you use this cosine distance, it corresponds to the length of the path between the two points on the sphere. That means, if you want an average of the two points, you should take the point in-between on this path (geodesic) rather than the mid-point obtained from the 'arithmetic average/dot product/euclidean geometry' since this point does not live on the sphere (hence essentially not the same object)!;;;
6080;1;2015-06-11T07:53:26.120;metric learning and information retrieval;I am interested in parsing semi-structured text.Assume that I have a text with labels of the kind: year_field, year_value, identity_field, identity_value, ..., address_field, address_value, and so on.These fields and their associated values can be everywhere in the text, but usually they are near to each other, and more generally the text in organized in a (very) rough matrix, but rather often the value is just after the associated field with eventually some non-interesting information in between. The number of different format can be up to several dozens, and is not that rigid (do not count on spacing, moreover some information can be added and removed).I am looking toward machine learning techniques to extract all those (field,value) of interest.I think metric learning and/or conditional random fields (CRF) could be of a great help, but I have not practical experience with them.Does anyone have already encounter a similar problem?Any suggestion or literature on this topic?;[education, open-source];34;
6081;2;2015-06-11T12:14:38.347;;I am not sure about what you are trying to do. But generally speaking, you can look at this paper for the relation between k-means clustering and PCA, especially Theorem 3.3.;;;
6082;1;2015-06-11T15:40:45.990;visualize a horizontal box plot in R;"I have a dataset like this. The data has been collected through a questionnaire and I am going to do some exploratory data analysis.windows <- c(""yes"", ""no"",""yes"",""yes"",""no"")sql     <- c(""no"",""yes"",""no"",""no"",""no"")excel  <- c(""yes"",""yes"",""yes"",""no"",""yes"")salary <- c(100,200,300,400,500 )test<- as.data.frame (cbind(windows,sql,excel,salary),stringsAsFactors=TRUE)test[,""salary""] <- as.numeric(as.character(test[,""salary""] ))I have an outcome variable (salary) in my dataset and a couple of input variables (tools). How can I visualize a horizontal box plot like this:";[education, open-source];106;
6083;2;2015-06-11T16:48:26.687;;You are going to need to make a column that contains software info-- for example name it software and the salary column has the corresponding salary so something like Software   Salary Microsoft  100 Microsoft  300 Microsoft  400 SQL        200and so on...then you can plot with the code belowp <- ggplot(test, aes(factor(software), salary))p + geom_boxplot() + coord_flip();;;
6084;1;2015-06-11T16:58:53.380;How do I create a complex Radar Chart?;So, I want to create a Player Profile Radar Chart something like this https://twitter.com/mixedknuts/status/575984119739498496Not only the scale of each variable different, but also I want a reversed scale for some statistics like the 'dispossessed' stat, where less actually means good.One solution for the variable scale for each statistic maybe is setting a benchmark and then calculating a score on a scale of 100? But, How do I display the actual numbers on the chart then? Also, how do I get the reversed scale for some of the statistics.Currently working in Excel. What is the most powerful tool to create a complex chart like this?;[education, open-source];47;1
6086;1;2015-06-11T21:21:55.473;MinHashing vs SimHashing;"Suppose I have five sets I'd like to cluster. I understand that the SimHashing technique described here:https://moultano.wordpress.com/2010/01/21/simple-simhashing-3kbzhsxyg4467-6/could yield three clusters ({A}, {B,C,D} and {E}), for instance, if its results were:A -> h01B -> h02C -> h02D -> h02E -> h03Similarly, the MinHashing technique described in the Chapter 3 of the MMDS book:http://infolab.stanford.edu/~ullman/mmds/ch3.pdfcould also yield the same three clusters if its results were:A -> h01 - h02 - h03B -> h04 - h05 - h06      |C -> h04 - h07 - h08                  |D -> h09 - h10 - h08E -> h11 - h12 - h13(Each set corresponds to a MH signature composed of three ""bands"", and two sets are grouped if at least one of their signature bands is matching. More bands would mean more matching chances.)However I have several questions related to these:(1) Can SH be understood as a single band version of MH?(2) Does MH necessarily imply the use of a data structure like Union-Find to build the clusters?(3) Am I right in thinking that the clusters, in both techniques, are actually ""pre-clusters"", in the sense that they are just sets of ""candidate pairs""?(4) If (3) is true, does it imply that I still have to do an $O(n^2)$ search inside each ""pre-cluster"", to partition them further into ""real"" clusters? (which might be reasonable if I have a lot of small and fairly balanced pre-clusters, not so much otherwise)";[education, open-source];78;1
6087;2;2015-06-11T22:34:19.643;;"Let's start by creating some fake dataset. software = sample(c(""Windows"",""Linux"",""Mac""), n=100, replace=T) salary = runif(n=100,min=1,max=100) test = data.frame(software, salary)This should create a dataframe test that will look like somewhat like:     software    salary1    Windows 96.6972172      Linux 29.7709053    Windows 94.2496124        Mac 71.1887015      Linux 94.0283266      Linux  7.4826327        Mac 98.8416898        Mac 81.1526239    Windows 54.07376110   Windows  1.707829EDIT based on comment Note that if the data does not already exist in the above format, it can be changed to this format. Let's take a data frame provided in the original question and lets assume the dataframe is called raw_test.    windows sql excel salary1     yes  no   yes    1002      no  yes  yes    2003     yes  no   yes    3004     yes  no    no    4005      no  no   yes    500Now, using the melt function/ method from the reshape package in R, first create the dataframe test (that will be used for final plotting) as follows: # use melt to convert from wide to long format test = melt(raw_test,id.vars=c(""salary""))# subset to only select where value is ""yes""test = subset(test, value == 'yes')# replace column name from ""variable"" to ""software"" names(test)[2] = ""software""   Now, you will get a datframe test that looks like:   salary software value1     100  windows   yes3     300  windows   yes4     400  windows   yes7     200      sql   yes11    100    excel   yes12    200    excel   yes13    300    excel   yes15    500    excel   yesHaving created the dataset. We will now generate the plot. First, create the bar plot on the left based on the counts of software that represents usage rate. p1 <- ggplot(test, aes(factor(software))) + geom_bar() + coord_flip()Next, create the boxplot on the right. p2 <- ggplot(test, aes(factor(software), salary)) + geom_boxplot() + coord_flip()Finally, place both these plots next to each other. require('gridExtra')grid.arrange(p1,p2,nrow=1)This should create a plot like: ";;;
6088;1;2015-06-12T00:35:25.937;Training and testing AdaBoost for low probability classification;I have a dataset that I want to classify as fraud/not fraud and I have many weak learners. My concern is that there is much more fraud than not fraud, so my weak learners perform better than average, but none perform better than 50% accuracy in the complete set.My question is whether I should set up testing and training sets that are half fraud and half not fraud or if I should just use a representative sample.;[education, open-source];39;
6089;1;2015-06-12T02:11:51.483;How to fit an odd relationship with a function?;Let's say there is a function $f$ such that $y = f(x)$. However, if $f$ is a piecewise function such that:$$y = \begin{cases} 0 \quad x \leq 0 \\ 1 \quad x >0\end{cases} $$How do I fit $f$ in that case?Many thanks, guys.;[education, open-source];29;
6090;2;2015-06-12T02:35:04.070;;"The definition you gave is the definition of the function. This is called the Heaviside Step Function. There is not a simple analytic way to express it (like as a ratio, product, or composition  of trigonometric functions, exponentials, or polynomials). Note that it is neither continuous nor differentiable at x = 0. There are a couple of cool ways to represent it. The coolest and most intuitive way is as an integral of a Dirac Delta Function:$$ H(x) = \int_{-\infty}^x { \delta(s)} \, \mathrm{d}s $$Note, though, that a Dirac Delta Function is itself not an ""official"" function, since it is not well-defined at x = 0. Check out Distribution Theory for some cool info on weird ""functions"" like this.Now, I think you may be trying to approximate this function, because you asked how to ""fit"" it. Taken straight from Wikipedia:For a smooth approximation to the step function, one can use the logistic   function$$H(x) \approx \frac{1}{2} + \frac{1}{2}\tanh(kx) = \frac{1}{1+\mathrm{e}^{-2kx}},$$where a larger k corresponds to a sharper transition at x = 0.";;;
6092;1;2015-06-12T10:25:12.457;How to generate ratings without training data?;I am working on generating restaurant ratings automatically and I have various feature values like delivery time, cost estimate, etc. I want to generate a rating for each restaurant between 0 to 5. But I don't have any training data or ground truth to validate. This rating might vary with user. Most of the related work, mostly related to the Yelp data challenge, have some relevance score as training data. I though of using reinforcement learning to learn the rating with user feedback, but not sure how to do that. Can anyone please suggest a relevant technique or algorithm for this problem?;[education, open-source];51;
6093;2;2015-06-12T10:57:34.290;;There is another interesting technique in this paper called the decoupling normalization method. I've used it and found that the results are good. This finds the affinity of user to a particular item and then you can scale it to a scale of 5 or 10, whichever you want. Hope it helps.;;;
6094;2;2015-06-12T11:25:02.010;;If you model your system by means of reinforcement learning, you will make your system learn from the user's feedback. The system will provide a rating based on the input features (it may be just a random rating during the first stages, since you will not have any prior information), and then the user will tell the system how well it predicted such rating. Based on the difference between the suggested and the actual user's ratings, the reinforcement learning algorithm will refine the recommendation system's model in order to provide more accurate ratings in the future.  Sutton's book on reinforcement learning (http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html) is a good introduction to the reinforcement learning field. ;;;
6095;2;2015-06-12T12:44:06.463;;Training set must represent the dataset your application/algorithm is actually going to face. I suggest you to take a representative sample instead of dividing the training and test set with exactly half fraud an half non-fraud. But please make sure that the training set contains both positive and negative example for fraud for your classifier to perform better.;;;
6096;2;2015-06-12T12:57:10.240;;There are many resources online where you can learn more about deep learning. Specially this link has a great collection. I recently forked this repository. You should go through the resources that interest you. Implementing is the best way to learn, where you get to see the results. As @Christopher mentioned, Pyhton is the best to learn for a beginner. Cheers!;;;
6097;1;2015-06-12T13:20:15.063;How to extract the values used in columns by a Classification model in R?;"I want to find the variables (and its values) used to build a  classification model?I came to know that the following code does the sameget_all_vars(model_built, dataset_used_for_building_a_model)However I would not be able to use the above code as I am going to use the ""model_built"" alone in the R code and i will not be having the ""dataset_used_for_building_a_mode.Simply put i want to use this ""Model_built"" alone and fetch the variables used along with the values. ";[education, open-source];24;1
6098;1;2015-06-12T13:39:15.880;how can I generate a Bernoulli block mixture model in matlab?;Good morning.I am trying to write the code of a Bernoulli block mixture model in matlab..But, a small mistake is showing up every time I run the function,especially how to relate the distribution parameter(alpha) to the latent variable Z and W.In other words, Z follows a multinomial dist. of parameter (pi) such that dim(pi)=(1,g) where g is the number of clusters of rows and sum(pi)=1. W follows a multinomial dist. of parameter (rho) such that dim(rho)=(1,m), where mis the number of column clusters and sum(rho)=1 .Z is a matrix of dimension (N,g), and W is  a matrix of dimension(d,m) where N is the number of observations,p is the number of variables,g is the number of row clusters and m is the number of column clusters.X follows a bernoulli distri of parameter (alpha_ZW) where the notation alpha_ZW means that the values of alpha depend on Z and W and alpha is (g,m) matrix..please find attached a graphical representation of the model.My problem sir, is only how to write the code of (alpha_ZW) in order to get generate the model.May you please help me to achieve the correct code.Best regards,Thank you;[education, open-source];33;
6099;1;2015-06-12T14:31:49.337;Sampling from a multivariate von Mises-Fisher distribution in Python;I am looking for a simple way to sample from a multivariate von Mises-Fisher distribution in Python. I have looked in the stats module in scipy and the numpy module but only found the univariate von Mises distribution. Is there any code available? I have not found yet.  -- edit. Apparently, Wood (1994) has designed an algorithm for sampling from the vMF distribution according to this link, but I can't find the paper.;[education, open-source];62;
6100;1;2015-06-12T15:11:33.597;Complete a Hungarian stem to a real word;I'm quite new to the NLTK package of Python and to NLP too (I usually work in R but for NLP purposes and scraping maybe Python is more able). I scrap articles from Hungarian newsportals and want to make a wordcloud out of it to show what are the current trending news topics. First I filter out stopwords and then stem the remaining words. (nltk has Hungarian stemmer) So I'm able to make a frequency table which can be the base of the wordcloud. My problem comes afterwards because stems are usually meaningless chunks (and not lemmas) of real words. I want to somehow complete the stem to a real word.My first idea was to assign the most common word or the shortest one (or some combination of this 2 rules) to the stem and represent that in the wordcloud.Is there a better solution for stem completion or should I follow a different workflow?;[education, open-source];29;
6101;1;2015-06-12T18:33:57.383;Price optimization for tiered and seasonal products;Assuming I can collect the demand of the purchase of a certain product that are of different market tiers. Example: Product A is low end goods. Product B is another low end goods. Product C and D are middle-tier goods and product E and F are high-tier goods.We have collected data the last year on the following1. Which time period (season - festive? non-festive?) does the different tier product reacts based on the price set? Reacts refer to how many % of the product is sold at certain price range2. How fast the reaction from the market after marketing is done? Marketing is done on 10 June and the products are all sold by 18 June for festive season that slated to happened in July (took 8 days at that price to finish selling)How can data science benefit in terms of recommending1. If we should push the marketing earlier or later?2. If we can higher or lower the price? (Based on demand and sealing rate?)Am I understanding it right that data science can help a marketer in this aspect? Which direction should I be looking into if I am interested to learn about it.;[education, open-source];58;1
6102;2;2015-06-12T20:01:55.923;;You should be able to use linear regression to find correlation between the factors which cause your products to sell better (or worse).There are many correlations you can test against in this data set. Some examples are:If a product has been marketed aggressively, does it sell more quickly?If a low tier item is available, do fewer high-tier items sell?If multiple high-tier items are available, are fewer sold of each item?Keep in mind that correlation does not necessarily imply causation. Always think about other factors which may cause sales to go up and down. For example, you may sell more high tier items in a season one year than another year. But, this could be due to changes in the overall economy, rather than changes in your pricing.The second thing you can do is perform A/B tests on your product sales pages. This gives you clear feedback right away. Some example tests could be:Show the user one high-tier product and one low-tier product (A). Show the user two high-tier products and no low-tier products(B). Which page generates more revenue?Send out marketing emails for a seasonal sale 5 days in advance to one group of users (A). Send the same email to a different set of users 1 day in advance (B).There are many possibilities. Use your intuition and think about previous knowledge you have about your products.;;;
6103;1;2015-06-12T20:55:33.353;AWS machine learning prediction schema problems;"I've trained an AWS Machine Learning model with the training data from here : https://www.kaggle.com/c/titanic/dataI'm now trying to run a batch prediction with the test data from the same source but I get the following error when I try to load the data : ""  The schema in this data file must match the datasource used to create the ML model ml-xxxxxxxxx. Ensure that the data file you are using matches the schema structure.""The schema, as far as I can see, is identical. I have tried it with and without the 'survived' column which is the value I'm trying to predict. I even tried it with the same training set which obviously has an identical schema and got the same error.What am I doing wrong?";[education, open-source];52;
6104;1;2015-06-12T22:03:46.950;Working with docker image on an oracle virtual box;"I downloaded the docker application and installed it on my Windows machine. After the installation, I tried to access the boot2docker application to work with the docker. But, it wasn't work in the way it was supposed to work. So, I opened the oracle virtual box. I found the boot2docker vm image there and accessed it. I have a spark application installed using the docker image.Now, I am trying to execute a simple spark application that I wrote usings scala. During the execution, I am getting the error "" There is an insufficient memory for the Java Runtime Environment to continue"". I have been trying different options to resolve this issue and could not resolve it. Can someone please help me with this issue. I am fairly new to the big data concepts and I haven't gotten a complete sense over spark, docker and virtual box. ";[education, open-source];15;
6105;2;2015-06-13T02:06:13.920;;It looks like you can sample the von Mises-Fisher distribution with that R package. Have you thought about calling R from within Python using the rpy2 package? I haven't tried this for myself, but could you try the following?from numpy import *import scipy as spfrom pandas import *from rpy2.robjects.packages import importrimport rpy2.robjects as roimport pandas.rpy.common as comfrom rpy2.robjects.packages import importr# import the movMF R packagemovMF = importr('movMF')# call the rmovMF sampling function from the R packageprint(movMF.rmovMF(10, 3 * c(1, -1) / sqrt(2)));;;
6107;1;2015-06-13T09:56:45.397;What are deconvolutional layers?;"I recently read http://arxiv.org/abs/1411.4038. I don't understand what ""deconvolutional layers"" do / how they work.The relevant part is 3.3. Upsampling is backwards strided convolution Another way to connect coarse outputs to dense pixels  is interpolation. For instance, simple bilinear interpolation  computes each output $y_{ij}$ from the nearest four inputs by a  linear map that depends only on the relative positions of the  input and output cells.  In a sense, upsampling with factor $f$ is convolution with  a fractional input stride of 1/f. So long as $f$ is integral, a  natural way to upsample is therefore backwards convolution  (sometimes called deconvolution) with an output stride of  $f$. Such an operation is trivial to implement, since it simply  reverses the forward and backward passes of convolution.  Thus upsampling is performed in-network for end-to-end  learning by backpropagation from the pixelwise loss.  Note that the deconvolution filter in such a layer need not  be fixed (e.g., to bilinear upsampling), but can be learned.  A stack of deconvolution layers and activation functions can  even learn a nonlinear upsampling.  In our experiments, we find that in-network upsampling  is fast and effective for learning dense prediction. Our best  segmentation architecture uses these layers to learn to upsample  for refined prediction in Section 4.2.I don't think I really understood how convolutional layers are trained. What I think I've understood is that convolutional layers with a kernel size $k$ learn filters of size $k \times k$. The output of a convolutional layer with kernel size $k$, stride $s \in \mathbb{N}$ and $n$ filters is of dimension $\frac{\text{Input dim}}{s^2} \cdot n$. However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).So if my understanding of convolutional layers is correct, I have no clue how this can be reversed.Could anybody please help me to understand deconvolutional layers?";[education, open-source];120;2
6108;1;2015-06-13T10:19:56.420;Caffe net.predict() outputs random results (GoogleNet);"I used pretrained GoogleNet from https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet and finetuned it with my own data (~ 100k images, 101 classes). After one day training I achieved 62% in top-1 and 85% in top-5 classification and try to use this network to predict several images.I just followed example from https://github.com/BVLC/caffe/blob/master/examples/classification.ipynb,Here is my Python code:import caffeimport numpy as npcaffe_root = './caffe'MODEL_FILE = 'caffe/models/bvlc_googlenet/deploy.prototxt'PRETRAINED = 'caffe/models/bvlc_googlenet/bvlc_googlenet_iter_200000.caffemodel'caffe.set_mode_gpu()net = caffe.Classifier(MODEL_FILE, PRETRAINED,               mean=np.load('ilsvrc_2012_mean.npy').mean(1).mean(1),               channel_swap=(2,1,0),               raw_scale=255,               image_dims=(224, 224))def caffe_predict(path):        input_image = caffe.io.load_image(path)        print path        print input_image        prediction = net.predict([input_image])        print prediction        print ""----------""        print 'prediction shape:', prediction[0].shape        print 'predicted class:', prediction[0].argmax()        proba = prediction[0][prediction[0].argmax()]        ind = prediction[0].argsort()[-5:][::-1] # top-5 predictions        return prediction[0].argmax(), proba, indIn my deploy.prototxt I changed the last layer only to predict my 101 classes.layer {  name: ""loss3/classifier""  type: ""InnerProduct""  bottom: ""pool5/7x7_s1""  top: ""loss3/classifier""  param {    lr_mult: 1    decay_mult: 1  }  param {    lr_mult: 2    decay_mult: 0  }  inner_product_param {    num_output: 101    weight_filler {      type: ""xavier""    }    bias_filler {      type: ""constant""      value: 0    }  }}layer {  name: ""prob""  type: ""Softmax""  bottom: ""loss3/classifier""  top: ""prob""}Here is the distribution of softmax output:[[ 0.01106235  0.00343131  0.00807581  0.01530041  0.01077161  0.0081002   0.00989228  0.00972753  0.00429183  0.01377776  0.02028225  0.01209726   0.01318955  0.00669979  0.00720005  0.00838189  0.00335461  0.01461464   0.01485041  0.00543212  0.00400191  0.0084842   0.02134697  0.02500303   0.00561895  0.00776423  0.02176422  0.00752334  0.0116104   0.01328687   0.00517187  0.02234021  0.00727272  0.02380056  0.01210031  0.00582192   0.00729601  0.00832637  0.00819836  0.00520551  0.00625274  0.00426603   0.01210176  0.00571806  0.00646495  0.01589645  0.00642173  0.00805364   0.00364388  0.01553882  0.01549598  0.01824486  0.00483241  0.01231962   0.00545738  0.0101487   0.0040346   0.01066607  0.01328133  0.01027429   0.01581303  0.01199994  0.00371804  0.01241552  0.00831448  0.00789811   0.00456275  0.00504562  0.00424598  0.01309276  0.0079432   0.0140427   0.00487625  0.02614347  0.00603372  0.00892296  0.00924052  0.00712763   0.01101298  0.00716757  0.01019373  0.01234141  0.00905332  0.0040798   0.00846442  0.00924353  0.00709366  0.01535406  0.00653238  0.01083806   0.01168014  0.02076091  0.00542234  0.01246306  0.00704035  0.00529556   0.00751443  0.00797437  0.00408798  0.00891858  0.00444583]]It seems just like random distribution with no sense.Thank you for any help or hint and best regards, Alex";[education, open-source];134;1
6109;2;2015-06-13T18:09:22.033;;It really depends on the structure of the underlying distribution of your data. If you have strong reason to believe that the data approximate a Bernoulli distribution, multinomial logistic regression will perform well and give you interpretable results. However if there exist nonlinear structures in the underlying distribution, you should seriously consider a nonparametric method. While you could use a decision tree as your nonparametric method, you might also consider looking into generating a random forest- this essentially generates a large number of individual decision trees from subsets of the data and the end classification is the agglomerated vote of all the trees. A random forest helps give you an idea of the share each predictor variable contributes to the response.Another factor to keep in mind is interpretability. If you are just trying to classify data, then you probably don't care about the underlying relationships between explanatory and response variables. However, if you are interested at all in interpretability a multinomial logistic regression is much easier to interpret, parametric methods in general, because they make assumptions about the underlying distribution, tell you more intuitively interpretable relationships.;;;
6110;1;2015-06-13T18:36:49.023;Relationship between VC dimension and degrees of freedom;I'm studying machine learning and I feel there is a strong relationship between the concept of VC dimension and the more classical (statistical) concept of degrees of freedom.Can anyone explain such a connection?;[education, open-source];74;
6111;1;2015-06-13T18:45:53.480;Probability tree in SPSS/ R/ others?;Similar questions have been asked in this context but this one seems a bit different.[Aim]We would like to find out what the probability is of a person purchasing a 4USD column D (see below) price given that he has purchased C-2, B-2, A[Context]We have a sizeable dataset of about 500,000 observations of this pattern. E.g. customer decides to buy A), bought B-1, then C-2, D-3.[Issue]We don't know how deep the events reach. E.g. it can be to column E, F, G. That's why we would like to auto-generate a probability tree in preferably SPSS but are open to other solutions as well.How does this look like mathmatically? Can we just extend bayes conditional probability? If so, how does it look? Any potential issues?Any other comments/ ideas are always welcome :)Thanks!;[education, open-source];16;
6112;2;2015-06-14T08:56:13.250;;I would say Teradata should be good choice. However, when you make join you have to take careful consideration of choosing right joining keys.;;;
6113;1;2015-06-14T10:19:31.923;Algorithm for segmentation of sequence data;I have a large sequence of vectors of length N. I need some unsupervised learning algorithm to divide these vectors into M segments. E.g.:k-means is not suitable, because it puts similar elements from different locations into a single cluster.UPD:real data looks like this:I see here 3 clusters: [0..50], [50..200], [200..250]UPD 2:I use modified k-means and get this acceptable result:borders of clusters: [0, 38, 195, 246];[education, open-source];80;1
6114;2;2015-06-14T10:32:51.277;;As stated by Prof Yaser Abu-Mostafa- Degrees of freedom are an abstraction of the effective number of parameters. The effective number is based on how many dichotomies one can get, rather than how many real-valued parameters are used. In the case of 2-dimensional perceptron, one can think of slope and intercept (plus a binary degree of freedom for which region goes to +1), or one can think of 3 parameters w_0,w_1,w_2 (though the weights can be simultaneously scaled up or down without affecting the resulting hypothesis). The degrees of freedom, however, are 3 because we have the flexibility to shatter 3 points, not because of one way or another of counting the number of parameters.;;;
6115;1;2015-06-14T11:28:46.043;How to convert a text to lower case using tm package?;I am using the below R code to convert text to lower case:movie_Clean <- tm_map(movie_Clean, content_transformer(tolower))However I end up getting the below error: Error in FUN(content(x), ...) :    invalid input 'I just wanna watch  Jurassic World í ½í¸«' in 'utf8towcs'.Please help how to overcome this error.;[education, open-source];48;
6116;1;2015-06-14T14:27:36.443;How do you manage expectations at work?;With all the hoopla around Data Science, Machine Learning, and all the success stories around, there are a lot of both justified, as well as overinflated, expectations from Data Scientists and their predictive models.My question to practicing Statisticians, Machine Learning experts, and Data Scientists is - how do you manage expectations from the businesspeople in you company, particularly with regards to predictive accuracy of models? To put it trivially, if your best model can only achieve 90% accuracy, and upper management expects nothing less than 99%, how do you handle situations like these?;[education, open-source];192;4
6117;2;2015-06-14T19:07:42.017;;Please see my comment above and this is my answer according to what I understood from your question:As you correctly stated you do not need Clustering but Segmentation. Indeed you are looking for Change Points in your time series. The answer really depends on the complexity of your data. If the data is as simple as above example you can use the difference of vectors which overshoots at changing points and set a threshold detecting those points like bellow:As you see for instance a threshold of 20 (i.e. $dx<-20$ and $dx>20$) will detect the points. Of course for real data you need to investigate more to find the thresholds.Pre-processingPlease note that there is a trade-off between accurate location of the change point and the accurate number of segments i.e. if you use the original data you'll find the exact change points but the whole method is to sensitive to noise but if you smooth your signals first you may not find the exact changes but the noise effect will be much less as shown in figures bellow:ConclusionMy suggestion is to smooth your signals first and go for a simple clustering mthod (e.g. using GMMs) to find an accurate estimation of the number of segments in signals. Given this information you can start finding changing points constrained by the number of segments you found from previous part.I hope it all helped :)Good Luck!UPDATELuckily your data is pretty straightforward and clean. I strongly recommend dimensionality reduction algorithms (e.g. simple PCA). I guess it reveals the internal structure of your clusters. Once you apply PCA to the data you can use k-means much much easier and more accurate.A Serious(!) SolutionAccording to your data I see the generative distribution of different segments are different which is a great chance for you to segment your time series. See this which is probably the best and most state-of-the-art solution to your problem. The main idea behind this paper is that if different segments of a time series are generated by different underlying distributions you can find those distributions, set tham as ground truth for your clustering approach and find clusters. For example assume a long video in which the first 10 minutes somebody is biking, in the second 10 mins he is running and in the third he is sitting. you can cluster these three different segments (activities) using this approach.;;;
6118;2;2015-06-14T20:18:40.730;;"This seems like an encoding error. Try adding the line Encoding(movie_Clean)  <- ""UTF-8""before you lowercase the data. Check out this answer for a little context: http://stackoverflow.com/a/28340080/4539807";;;
6119;2;2015-06-15T06:23:32.277;;The VC dimension is very well explained in this paper in Section 2.1 and further, with the basic lemmas and proofs given. You can go through this.;;;
6120;1;2015-06-15T12:24:03.440;MCMC vs Network Theory for Big Data and Data Science applications;[Apologies if this post sounds naive, I'm fairly new to the world of data science/big data and very unsure where I'm heading career-wise]I'm currently an undergraduate MMath [integrated master's] Mathematics student in the UK who has finished the third year of the course [out of four years].As I have been considering the possibility of doing further research in Mathematics/Statistics/Operational Research/Data Science, I have decided to stay on and complete the Master's component of the course [as it is the only Master's course I can get funding for at this stage]. After the Master's I may continue on and do a PhD.There are currently two projects that appeal to me that seem to have relevant applications. The first one is on improved MCMC [Markov Chain Monte Carlo] methods, in particular MCMC using Hamiltonian Dynamics. There is scope for some big data applications here.The other project that I could take part in is one on the centrality/communities detection of networks within network science. This could possibly be useful with applications in Operational Research.Does anyone have an idea as to which project will be more relevant to data science/analytics?;[education, open-source];39;
6122;1;2015-06-15T13:15:30.957;How to draw attention to sites with high levels of outdated software;I'm totally new to Data Science and even enrolled for R programming online course.I have some data that I would eventually plot on a map of Texas using an R package, similar to But the first step is determining how to portray these figures.Should I create percentage of outdated Windows SEP 11 vs up-to-date Windows SEP 12? Or should I create ratios? Or is there some other metric?;[education, open-source];29;
6123;1;2015-06-15T15:07:43.607;Implementing sklearn.linear_model.SGDClassifier using python;I have an excel file that contains details related to determining the quality of a wine and I want to implement the linear model concept using the function sklearn.linear_model.SGDClassifier(SVM => Hinge loss) and (Logarithmic regression =>log loss) using python. I learned the basics about these function through the scikit learn website and I am not able to implement the model using excel file. I am very new to python and machine learning and I finding it hard to implement the model. I opened the excel file in python and tried to take two columns [randomly] from the file and use that as an input to call the fit function available in the model. But, I got an error stating Unknown label type: array. I tried a couple of other methods too, but, nothing worked. Can someone guide me with the implementation process? from xlrd import open_workbookfrom sklearn import linear_modeli = 0fa = []ph = []book = open_workbook('F:/BIG DATA/winequality.xlsx')sheet = book.sheet_by_name('Sheet1')num_rows = sheet.nrows - 1num_cols = sheet.ncols - 1curr_row = 0while curr_row <num_rows:    curr_row += 1    cell_val = sheet.cell_value(curr_row,0)    cell_val1 = sheet.cell_value(curr_row,10)    fa.append([float(cell_val),float(cell_val1)])    cell_val2 = sheet.cell_value(curr_row,8)    ph.append(float(cell_val2))model = linear_model.SGDClassifier()print(model.fit(fa,ph))The error message screenshot:;[education, open-source];56;
6124;2;2015-06-15T15:43:22.097;;I think that this is the same issue as in this question: http://stackoverflow.com/questions/24923143/x-and-y-have-incompatible-shapesThe shape of X must be (n_samples, n_features) as explained in the SVC.fitdocstring. A 1-d array is interpreted as a single sample (for convenience when      doing predictions on single samples). Reshape your X to (n_samples, 1).That means you should use numpy.reshape to reshape the X column. If the data frame has n rows, you should useX_new = X.reshape(n, 1)Then use the fit method with X_new. Note: you probably don't need to do this if you use two or more X columns for your model fitting.;;;
6125;2;2015-06-15T16:12:59.267;;Gather competitive counterparts. Try and determine a state-of-the-art and see how your models compare with that. It also heavily depends on how long your team has been working on it. Science-driven models are not created statically, they develop dynamically because a good scientist will always try to find ways to improve it.Upper management personnel should know that a data scientist explores new methods, sometimes/often without knowing their quality. They should know that machine learning techniques do not produce perfect models right away. If they did, it wouldn't be challenging anyway.A data scientist ought to be evaluated by how he justifies and discusses his results and how he plans the future. A way for the management personnel to handle with their expectations is to not have unrealistically high ones.Still, if reasonable results are expected in the field of context, think about these questions:Did/will results get better over time?Are future expectations positive?How well are results compared to similar systems (from competitors)?;;;
6126;2;2015-06-15T19:02:12.293;;Both MCMC methods and network analysis play an important role in data science. I think you should go for the project you like more. However, in my experience, community detection in networks is a niche (applied math/graph theory) of network analysis, while MCMC methods involve lots of statistical and computational concepts. Personally, being a statistician, I would go for the MCMC methods.;;;
6127;2;2015-06-15T20:36:40.303;;It is so called because it classifies the linear sequence, and not because the structure of the graph.;;;
6129;2;2015-06-16T03:48:30.780;;Main references:Courses on deep learning:Andrew Ng's course on machine learning has a nice introductory section on neural networks.Geoffrey Hinton's course: Coursera Neural Networks for Machine Learning (fall 2012)Michael Nielsen's free book Neural Networks and Deep LearningYoshua Bengio, Ian Goodfellow and Aaron Courville are writing a book on deep learningHugo Larochelle's course (videos + slides) at Université de SherbrookeStanford's tutorial (Andrew Ng et al.) on Unsupervised Feature Learning and Deep LearningOxford's ML 2014-2015 courseNVIDIA Deep learning course (summer 2015)NLP-oriented:Stanford CS224d: Deep Learning for Natural Language Processing (spring 2015) by Richard SocherTutorial given at NAACL HLT 2013: Deep Learning for Natural Language Processing (without Magic) (videos + slides)Vision-oriented:CS231n Convolutional Neural Networks for Visual Recognition by Andrej Karpathy (a previous version, shorter and less polished: Hacker's guide to Neural Networks).Toolkit-specific tutorials:DL4J (Java): http://deeplearning4j.org/documentation.htmlTheano  (Python, Y. Bengio): http://deeplearning.net/Machine Learning with Torch7 (Lua, LeCun): http://code.madbits.com/wiki/doku.phpH2O Deep Learning (Java): http://0xdata.com/product/deep-learning/ Caffee (C++, UCB): http://caffe.berkeleyvision.org/ ;;;
6130;2;2015-06-16T09:28:02.837;;Thanks to your help. I finally got my code working, plus some bibliography.I put my hands on Directional Statistics (Mardia and Jupp, 1999) and on the Ulrich-Wood's algorithm for sampling. I post here what I understood from it, i.e. my code (in Python), with a 'movMF' flavour.The rejection sampling scheme:def rW(n,kappa,m):dim = m-1b = dim / (np.sqrt(4*kappa*kappa + dim*dim) + 2*kappa)x = (1-b) / (1+b)c = kappa*x + dim*np.log(1-x*x)y = []for i in range(0,n):    done = False    while not done:        z = sc.stats.beta.rvs(dim/2,dim/2)        w = (1 - (1+b)*z) / (1 - (1-b)*z)        u = sc.stats.uniform.rvs()        if kappa*w + dim*np.log(1-x*w) - c >= np.log(u):            done = True    y.append(w)return yThen, the desired sampling is $v \sqrt{1-w^2} + w  \mu$, where $w$ is the result from the rejection sampling scheme, and $v$ is uniformly sampled over the hypersphere.def rvMF(n,theta):dim = len(theta)kappa = np.linalg.norm(theta)mu = theta / kapparesult = []for sample in range(0,n):    w = rW(kappa,dim)    v = np.random.randn(dim)    v = v / np.linalg.norm(v)    result.append(np.sqrt(1-w**2)*v + w*mu)return resultAnd, for effectively sampling with this code, here is an example:import numpy as npimport scipy as scimport scipy.statsn = 10kappa = 100000direction = np.array([1,-1,1])direction = direction / np.linalg.norm(direction)res_sampling = rvMF(n, kappa * direction);;;
6131;2;2015-06-16T11:39:51.880;;"I like this question because it gets at the politics that exist in every organization. In my view and to a significant degree, expectations about model performance are a function of the org culture and degree to which an organization is ""technically literate."" One way to make clear what I mean is to consider the differences between the 4 big ""data science"" entities -- Google, FB, Amazon and Yahoo -- versus the 4 big agency holding entities -- WPP, Omnicon, Interbrand and Publicis. Google, et al, are very technically literate. The agencies, on the other hand, are known to lean towards tech phobia. What's the evidence for this? First off, the technically literate group was founded or are run by engineers, computer scientists, geeks and people with strong tech backgrounds. Who runs the tech illiterate companies? Marketers who have risen to prominence by virtue of their soft communication and people skills. And not only that, having worked in some of these shops in NYC, I can testify that these organizations systematically punish and/or push out the highly technically literate types as not a ""fit"" with the culture.Next, consider their aggregate (stock) market caps, The tech literate group adds up to about 800 billion dollars while the tech illiterate group amounts to 80 billion. Tech literate entities are 10x bigger than the others in market cap. This is a clear statement of the market's expectations and it's not high for the illiterates. So, by extrapolation, what kind of hope can you have for challenging the ""predictive accuracy"" expectations of bozos like these?So, given that cultural breakout and depending on where you fall, you should have more or less realistic expectations. Of course, different ""tech illiterate"" entities will have managers who know what they're doing, but for the most part, these entities are dominated by the idiocy of the lowest common denominator in tech skills, i.e., people who are at best technical semi-literates (and dangerous) or, more commonly, totally innumerate but don't know it. Case in point, I worked for a guy who wanted words like ""correlation"" scrubbed from c-suite decks. This is an extreme case: after all, every secretary knows what a ""correlation"" is.This raises the issue of how one deals with the maddeningly naive and innumerate when they ask a really dumb question like, ""Why aren't you getting 99% predictive accuracy?"" One good response is to reply with a question like, ""Why would you assume such an unrealistically high PA is even possible?"" Another might be, ""Because if I actually got 99% PA, I would have assumed that I was doing something wrong."" Which is highly likely to be true, even with 90% PA.The there's the more fundamental question of the insistence on PA as the sole criterion for model value. The late Leo Breiman left many footprints on the statistical and predictive modeling community of which PA is one. His primary concern with PA was to address the many criticisms being made in the 90s regarding the instability and error inherent in running a single CART tree. His solution was to motivate “random forests” as an approximate and provisional method that would maximize accuracy and reduce instability by eliminating tree structure.  He benchmarked the lower MSE from ~1,000 iterative RF “mini-models” against the error from a single logistic regression model. The only problem was that he never bothered to mention the glaring apples to oranges comparison: what might the logistic regression MSE have been if it had also been performed 1,000 times? The 2008 Netflix Prize offered a sizeable monetary reward to any statistician or team able to improve upon the MSE of their recommender system. At the time, Netflix was spending $150 million a year on this system, convinced that the costs were more than recovered in customer loyalty and purchase of movies that would otherwise never have been chosen. The eventual winners used a complex ensemble of 107 different models.As Netflix learned however, the real problem was that, from a fully loaded cost perspective, the actual improvement in error over their current model was a mere 0.005% reduction in the 5 point ratings. Not to mention that the IT costs in time, heavy-lifting and maintenance of the winning ensemble of 107 models more than nullified any gains from the error reduction. Given this, Netflix eventually abandoned the pursuit of MSE and no more Netflix Prizes have been awardedAnd this is the point: minimizing predictive error can be easily gamed or p-hacked and is prone to analyst fraud (i.e., finding a solution that glorifies the analyst’s modeling skills, positively impacting his potential end-of-year bonus). Moreover, it is a completely statistical solution and goal set in an economic and business vacuum. The metric provides little or no consideration of ancillary, collateral costs -- the very real operational consequences evaluated from A to Z that should be an integral part of any fully-loaded, trade-off based decision-making process.This has become one of those issues that is embedded in organizations and is very, very difficult to change. In other words, I am fully aware that I am tilting at windmills with this rant about the caveats with the use of PA.";;;
6132;2;2015-06-16T13:42:47.060;;Rational business people don't pay for accuracy, they pay to either save money on a profitable process (thereby making it more profitable), or bycreating new money (creating new profitable processes). So any project that is undertaken has to be couched in terms that   reflect this. The first step is always understanding which of the two   processes you are working on, and you should have a clear idea of how   that might be achieved, while keeping in mind that as you make   progress the details of how you do that might change.If you can improve the accuracy of a process, you can probably make money for the firm and the business people will invest in your progress. The only rational reason a business person could have for insisting on 99 percent accuracy and reject 90 percent is if they already had a way of doing it that was better than the 90 percent.  If that is the case they are of course justified in their position.  Understanding and presenting the business case of the projects you are working on in terms that the Business People understand is part of the maturing process of any Engineer. It is not unique to Data Science at all, though Data Science has some unique aspects (like lower maturity but higher probability of fortuous serendipity discovery - at least in today's environment.A relevant process that is close to Data Science that makes this step explicit can be found here:https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining But most Enterprise Architecture Frameworks are similarly applicable. ;;;
6133;1;2015-06-17T09:45:49.593;Over-fitting issue in a classification problem (unbalanced data);I am working on a rare event (unbalanced target variable) classification problem using decision trees. My dataset comprises of 95% non-event and 5% minority (events) class. I used decision tree over logistic regression because I had many categorical variables comparing to continuous variables. I get a  good performance for training data with the decision tree C5.0. However I get poor results for the new data. I use the confusion matrix as a measure of performance. Training model is over-fitting. I did pruning to reduce the over-fitting caused by the decision tree.  I used the following code to build the modelClassifi_C5.0 <- C5.0(TARGET ~., , data = training_data_SMOTED, trails = 500,                      control = C5.0Control(minCases = mincases_count,                                            noGlobalPruning = FALSE))I balanced the minority and majority class using the following code:training_data_SMOTED <- SMOTE(TARGET ~ ., training_data,                              perc.over = 100, k = 5, perc.under = 200)Any sort of advice will be helpful. ;[education, open-source];63;2
6134;2;2015-06-17T11:25:19.117;;Why would you need a distance when you have the pdf? Time series can be assigned to clusters based on their fit to the cluster's pdf. This also means the method works for ragged timeseries, for which distance methods would break down.;;;
6137;1;2015-06-17T19:36:26.493;Is it ok to interprete PCA plot this way?;"I have several samples (C2, C4, C5) and want to check if they are at a certain stage. I included some known samples (D0 - D77) which were generated at different stages by another lab. In the PCA plot, my samples cluster together on the left and the known samples are dispersed on the right. I think the major difference among all the samples is different experimental protocols (PC1) and the second is different stages (PC2). So my samples are at the same stage. Is that right? And can we say my samples are at a stage between D12 and D19 (when projected to known samples, my samples are located between D12 and D19)? I have no strong mathematical background. Hope someone with math background can give some explanation. Thanks![ UPDATE1 ]I did this analysis using prcomp in R. The input is a 2D numeric matrix with 17436 rows and 42 columns. Each row represents a gene and each column represents a sample. The number is the gene expression level for a gene in a sample. The gene expression level is normalized using DESeq2 and thus the numbers are comparable across genes (rows) and samples (columns). For the 42 columns, 18 are from my experiments and the rest from published datasets. Besides different protocols, it is possible there are other differences. Generally, the table is a combination of two sources of data. In my data , C2, C4 and C5 are three cell lines which we processed in parallel in experiments. In published datasets, they sampled the cells at different time points (Day 0 to Day 77).[ UPDATE2 ] R Code for PCA and plotting# normCounts: normalized count from DESeq2, with 17436 rows and 42 columnsnormCounts0 <- normCounts[ rowSums(normCounts) > 0, ]tab <- t(normCounts0)pca <- prcomp(tab, scale = TRUE)tmp.x <- as.data.frame(pca$x)tmp.x$sample<-c(rep(""C2_Con"",3),rep(""C2_KD"",3),rep(""C5_Con"",3),rep(""C5_KD"",3),rep(""C4_Con"",3),rep(""C4_KD"",3), rep(""D0"",4),rep(""D7"",4),rep(""D12"",2),rep(""D19"",4),rep(""D26"",2),rep(""D33"",2),rep(""D49"",2),rep(""D63"",2),rep(""D77"",2))require(""ggplot2"")p <- ggplot(tmp.x, aes(x=PC1, y=PC2, color=sample))p + geom_point() + scale_color_discrete(breaks=c(""C2_Con"",""C2_KD"",""C5_Con"",""C5_KD"",""C4_Con"",""C4_KD"", ""D0"",""D7"", ""D12"",""D19"",""D26"",""D33"",""D49"",""D63"",""D77""))";[education, open-source];135;
6139;1;2015-06-18T07:09:08.140;Can we use HDFS and big data Analytics for processing huge log files being processed through some application on some central server?;Detailed Question Explanation:Suppose say our application X is processing huge logs (size varying from MBs to GBs) and giving insight results in these logs(NOT A Social Data logs or Security Logs)now this logs are in format say log.y with different variety, using C++ as Engine to process these huge logs.(It generates imp. insights about data but need to be processed using our application X only and we don't want to change core way processing of application X)If this processing happens on some server it under or over utilizes resources (That I already know).If we use cloud computing for this processing we get that processing power with optimum usage. How do we see help of BIG data analytics in this particular sort of usage?Any help or suggestion is very deeply appreciated  ;[education, open-source];55;1
6140;1;2015-06-18T09:04:22.453;Reading Persian Characters in R;"I have a csv file with characters in Persian and I cannot view them in R correctly. Also, I cannot subset based on Persian characters values. Here is a sample code:list1 <- c(""x"",""y"")list2 <- c(""ب"",""الف"")list1 list2 ##OK-readablewritedf <- as.data.frame(cbind(list1,list2),encoding=""UTF-8"")write.csv(writedf,""test.csv"")testdf <- read.csv(""test.csv"",encoding=""UTF-8"")testdf  ## not readabletestdf[,testdf$list2==""ب""]## data frame with 0 columns and 2 rows  ???";[education, open-source];53;
6141;1;2015-06-18T10:43:22.037;Working with text files in Excel;"I have an excel file containing a long text in column A. I am looking for the words starting by ""popul"" such as popular and populate . I can find these cells by the formula:    =SEARCH(""popul"",A1,1)I want a function that returns the whole words starting by popul such as popular and populate.";[education, open-source];54;
6142;2;2015-06-18T12:41:01.350;;All what I'd say is that you are most probably right! Your plot is compatible to the explanation you have for your data so most probably that is the case. I assume you know what features to use (see example) but still better to post some info about input data -e.g. the dimensionality of the data- to be sure everything is right there. The only reason which my ruin the explanation is presence of a confounder.ExampleI'm not an expert of your field but just imagine there is a difference between materials you and the other lab used to get the data. Then I'll ask you how would you know that the difference on PC1 is not  according to the difference of materials but difference of protocoles? you can confidently say that PC1 is capturing the difference of protocols if you are sure there is nothing else affecting it.Hope it helped!;;;
6143;2;2015-06-18T15:47:18.920;;First, $x_{ij}$ is not a bernoulli random variable. In the text, $x_{ij}$ is described as real valued. Also, the dimensions of $z$ and $w$ are incorrect.The text indicates that we're drawing a matrix of data call it $X (n\times d)$ such that $X_{i,j}=x_{ij}$.A particular entry has distribution $x_{ij} \sim f(\alpha_{z_i, w_j})$. Where $\alpha$ is some given matrix of parameters. The maximum row index of $\alpha$ is $g$. The maximum column index of $\alpha$ is $m$. So $\alpha$ is $g \times m$.All that remains is to pin down the indices $z$ and $w$. These are vectors rather than matrices since they have a categorical distribution, for example $z_i$ should be an integer between $1$ and $g$.Putting this together we need to:Draw $z$, a vector of length $n$, by $z_i \sim \text{Cat}(\pi), i=1,...,n$Draw $w$, a vector of length $d$, by $w_j \sim \text{Cat}(\rho), i=1,...,d$Draw $X$, an $(n\times d)$ matrix, by $x_{ij} \sim f(\alpha_{z_i, w_j})$ ;;;
6144;1;2015-06-18T16:41:43.320;Sensitivity to scaling of features in a multivariate gaussians;I'm using the HMMLearn python package for hidden markov models.  That implementation is build on multivariate gaussian distributions.So I have a string of features.  How sensitive are gaussians to vastly different feature scales?  Will it be really skewed if one feature is scaled between 0 and 1, and another is scaled between 0 and 1e8?;[education, open-source];57;
6145;2;2015-06-18T17:38:19.883;;"I'm no Excel expert, as I generally use Python or R instead, but this might get you started until an Excel expert comes along.  In the meantime, it would help if you clarified your question.  And you should be aware that search will only find you the index of the first match, not all matches in the string.  If you only need the first hit, you can use=MID(A1,SEARCH(""popul"",A1,1),IFERROR(FIND("" "",A1,SEARCH(""popul"",A1,1)),LEN(A1)+1)-SEARCH(""popul"",A1,1))although I cannot claim this is the best way to do this.  You really didn't specify where you want the results to appear, how they should look, or if you only have one cell you need to search in.  It would also help to know the version of Excel you have.  I'll also present a crude way to return all the hits in the string:Cell A1 contains the string, B1 has no formula, and if you run out of ""n/a""s you can extend columns B, C, and D by filling down.  The formulas are as follows:B3 and below use=IF(C2+1<LEN($A$1),C2+1,""n/a"")C2 and below use=IFERROR(FIND("" "",$A$1,SEARCH(""popul"",$A$1,B2)),LEN($A$1)+1)D2 and below use=IFERROR(MID($A$1,SEARCH(""popul"",$A$1,B2),C2-SEARCH(""popul"",$A$1,B2)),"""")As you can see, there's little to no error checking except to deal with the match at the end of the string.  In the end though, if you're going to use Excel for this you should probably create a user defined function or utilize VBA instead of in-cell formulas.";;;
6146;1;2015-06-18T18:23:07.763;Best way to store large data set using R from Twitter?;I am working on a project that aims to retrieve a large data-set (i.e., tweet data which is a couple of days old) from Twitter using the twitteR library on R.  have difficulty storing tweets because my machine has only 8 GB of memory. It ran out of memory even before I set it to retrieve for one day. Is there a way where I can store the tweets straight to my disk without storing into RAM? I am not using the streaming API as I need to get old tweets. ;[education, open-source];115;
6147;2;2015-06-18T18:57:08.203;;I worked on a Twitter data project last Fall wherein we used Java libraries to pull in tweet data from the streaming and the rest API's. We used Twitter4J (an unofficial Java library) for the  Twitter API. The tweet data was fetched and directly written onto text files on our hard drives. Yes, we did increase the memory and heap. I believe R studio will have a similar option. An alternative would be to pull in lesser amounts of tweet data with more number of repetitions. ;;;
6148;2;2015-06-18T20:36:47.597;;"Find a way to make your program write to disk periodically. Keep count of the number of tweets you grab and save after that number is high. I don't write R but psuedocode might look like:$tweets = get_tweets();$count = 0;$tweet_array = array();for each ($tweets as $tweet) {  $tweet_array += $tweet;  $count++;  if ($count > 10000) {    append_to_file($tweet_array, 'file_name.txt');    clear_array($tweet_array);  }}";;;
6149;2;2015-06-18T21:50:08.290;;"Use subset instead i.e. subset(testdf, testdf$list2==""ب"") results in what you want.  ";;;
6150;1;2015-06-19T05:41:26.783;How to import a tsv file to Matlab;"I am really new to data science. Please don't mark me down as this website is my only hope of progress.I have set of data I obtained from NASA website. When I saved it, it saved as ""tsv' file. (Tab separated values). I want to open it on Matlab as a Matrix as I have a code to run on that matrix. Basically I want to import that file to matlab and start running the code on it. Can someone please help me or guide me in the right direction.I tried various things such as import data, tdfread but so far nothing has worked for me. I was first trying to export the tsv file to MS Excel and then go from Excel to Matlab. That too I don't know how to do. I will give you the link of my data which I want to import on to Matlab. The link for my data is the following.Please take a look at http://vizier.u-strasbg.fr/viz-bin/VizieR?-source=J%2FApJS%2F209%2F31If you just click on submit at the lower right corner, one will see the data.a = importdata('J_ApJS_209_31_table3-150618.tsv') [This the command I used].Error message on matlab is     ??? Error using ==> importdata at 136    Unable to open file.    Error in ==> data at 1Then I wrote a script.    tdfread(J_ApJS_209_31_table3-150618,'\t')Error message I get is         ??? Undefined function or variable 'J_ApJS_209_31_table3'.    Error in ==> data at 1";[education, open-source];56;
6151;1;2015-06-19T07:12:07.153;NER on Twitter data;"What are the best method/library/data available to extract named entities [Names and Location] from Twitter data ? [Other than dictionary lookup]I tried with Python-Stanford NER, But it seems to fail when named entities is not capitalized. I also tried to predict NER after converting text to upper case eg :  text = ""david beckham played for england"" stanford.NERTagger.tag(text) [(u'david', u'PERSON'), (u'beckham', u'PERSON'), (u'played', u'O'), (u'for', u'O'), (u'england', u'O')] stanford.NERTagger.tag(text.upper()) output : [(u'DAVID', u'PERSON'), (u'BECKHAM', u'PERSON'), (u'PLAYED', u'O'), (u'FOR', u'O'), (u'ENGLAND', u'LOCATION')]";[education, open-source];58;
6152;2;2015-06-19T07:30:04.703;;Conditional Random Fields (CRFs) can be used for segmenting/labeling sequential problems. Try CRF++: Yet Another CRF toolkit,  a simple, customizable, and open source implementation of Conditional Random Fields (CRFs) You can label and create a tagged training corpus and use CRF++ You also need to create a feature templateRefer : http://taku910.github.io/crfpp/ for more details. Check the example from data for CoNLL shared task (PoS tagging).;;;
6153;1;2015-06-19T07:37:44.407;Importance of Random initialisation VS number of hidden units;"A question crossed my mind not so long ago: I am doing experiments on Language Model with RNN (always with the same network topology: 50 hidden units, and 10M ""directs connections"" that are emulating N_grams models) and different fraction of corpus (10,25,50,75,100%) (9M words).I noticed that while perplexity seems to decrease when the training data become more abundant, certain times it does not.Last example : 143 118 109 106 112My first thought was network initialization, so I began testing with a smaller corpus and 20 hidden units (for technical reasons. Even with 10% corpus, learning can take up to 30h, which is problematic for me), and I found after 50 tries that all nets converged on values within 3% of each other.But I thought that maybe the importance of this initialization is a function of the number of hidden units? I mean the more hidden units the more parameters to tune.Also maybe my stop criterion is too sensitive (It stops if evolution of perplexity between two iterations is inferior to a certain number).Do you think it would make an impact to allow it to run one of two iterations after the criterion was met to see if it was just a local thing ?Thanks,Marc";[education, open-source];19;
6154;2;2015-06-19T10:17:17.027;;This tutorial by Andrej Karpathy does an excellent job of explaining convolutional neural networks.Reading this paper should give you a rough idea about deconv networks.These slides are great for deconv networks.;;;
6156;2;2015-06-19T15:14:58.097;;"make sure the files are saved in UTFtry Sys.setlocale(""LC_ALL"", locale_code) and have a look at the documentation of this function ";;;
6157;2;2015-06-19T15:28:51.003;;Yes it is a challenging task to extract named entities in tweets. Give a go at NLTK NER and also Alan Ritter's Twitter specific NER and evaluate on their performance and compare to Stanford NER and which one fits in your use. Maybe you want to use more than one to get more named entities if you don't mind so much of false NEs..;;;
6158;2;2015-06-19T16:39:48.010;;"Given the information provided, I do not think it's enough to say that your samples are between stages D12 and D19. I admit knowing nothing about your domain, but with what you're showing it is very conceivable that other factors are causing the spread of your samples from the others on PC1 and it's too much of a stretch to say that your samples ""project"" into the others in between D12 and D19. Why does it have to be a projection orthogonal to the axis described by PC1? Remeber that PC1 is a linear combination of all your inputs. That is, it describes an arbitrary line through your input space that is probably not aligned with any one thing in particular. For example, all three of the following projections could be envisioned for your problem:Another red flag for me is that you say PC1 is related to differences in experimental protocols. You haven't posted a scree plot or listed the eigenvalues for the different PCs but obviously PC1 soaks up the most variance so it's a pretty big deal. If you're saying that PC1 is correlated with experimental protocol, then that would seem good because hopefully the impact of experimental protocol is minimized in other PCs. However, that's never completely the case, really.Again, not knowing your domain it's hard to tell what the impact of experimental protocol is on the distribution of your data in the PC plot, but I'd be VERY wary of comparing these data in this way if your data was produced using a different experimental design and, as you noted, ""there may be other differences."" My hunch is that you would need to continue minimizing differences in the protocols between your experiments and the published experiments before you could really use this type of analysis. Others might disagree but if you posted the sources of your data, the eigenvectors, and possibly a subset of the data, experts in your field may be able to chime in on this methodology. The short answer is: you could be right in your interpretation but at this point you don't have enough evidence to suggest your samples are ""in between"" D12 and D19 in terms of the stage. And I worry about differences in methodology completely swamping your ability to interpret things this way. ";;;
6159;2;2015-06-19T16:46:00.387;;"tdfread displays the File Open dialog box for interactive selection of a data file, then reads data from the file. The file should have variable names separated by tabs in the first row, and data values separated by tabs in the remaining rows. tdfread creates variables in the workspace, one for each column of the file. The variable names are taken from the first row of the file. If a column of the file contains only numeric data in the second and following rows, tdfread creates a double variable. Otherwise, tdfread creates a char variable. After all values are imported, tdfread displays information about the imported values using the format of the tdfread command.tdfread(filename) allows command line specification of the name of a file in the current folder, or the complete path name of any file, using the string filename.tdfread(filename,delimiter) indicates that the character specified by delimiter separates columns in the file. Accepted values for delimiter are:' ' or 'space''\t' or 'tab'',' or 'comma'';' or 'semi''|' or 'bar'The default delimiter is 'tab'.s = tdfread(filename,...) returns a scalar structure s whose fields each contain a variable.";;;
6162;2;2015-06-20T06:12:08.537;;I think you are in a better position to train your own NER model. You can start with CRFSuite as a package.;;;
6165;1;2015-06-21T08:22:39.630;Implementing Complementary Naive Bayes in python?;ProblemI have tried using Naive bayes on a labeled data set of crime data but got really poor results (7% accuracy). Naive Bayes runs much faster than other alogorithms I've been using so I wanted to try finding out why the score was so low. ResearchAfter reading I found that Naive bayes should be used with balanced datasets because it has a bias for classes with higher frequency. Since my data is unbalanced I wanted to try using the Complementary Naive Bayes since it is specifically made for dealing with data skews. In the paper that describes the process, the application is for text classification but I don't see why the technique wouldn't work in other situations. You can find the paper I'm referring to here. In short the idea is to use weights based on the occurences where a class doesn't show up.After doing some research I was able to find an implementation in Java but unfortunately I don't know any Java and I just don't understand the algorithm well enough to implement myself.Questionwhere I can find an implementation in python? If that doesn't exist how should I go about implementing it myself?;[education, open-source];63;1
6166;2;2015-06-21T09:03:26.883;;Naive Bayes should be able to handle imbalanced datasets. Recall that the Bayes formula is$$P(y \mid x) = \cfrac{P(x \mid y) \, P(y)}{P(x)} \propto P(x \mid y) \, P(y)$$So $P(x \mid y) \, P(y)$ takes the prior $P(y)$ into account.In your case maybe you overfit and need some smoothing? You can start with +1 smoothing and see if it gives any improvements. In python, when using numpy, I'd implement the smoothing this way:table = # counts for each feature PT = (table + 1) / (table + 1).sum(axis=1, keepdims=1)Note that this is gives you Multinomial Naive Bayes - which applies only to categorical data. I can also suggest the following link: http://www.itshared.org/2015/03/naive-bayes-on-apache-flink.html. It's about implementing Naive Bayes on Apache Flink. While it's Java, maybe it'll give you some theory you need to understand the algorithm better.;;;
6167;2;2015-06-21T10:57:07.127;;Don't just mix the data of two different sources.It looks as if the two sources may simply be using different scales. But that makes the whole PCA analysis meaningless.All your plot may be visualizing is that the other samples use a different way of measuring things. It seems all the variance is in the other samples. In particular this means that you cannot draw any conclusions about how your samples relate to the reference samples.;;;
6168;2;2015-06-21T17:11:39.947;;It depends on the usage of various packages (Numpy/Scipy etc), which are written in C and be incredibly fast also Python can be complied using JIT.  Here is an excellent comparison between R and Python : https://learnanalyticshere.wordpress.com/2015/05/14/clash-of-the-titans-r-vs-python/;;;
6169;1;2015-06-21T17:31:05.457;How to run a pyspark application in windows 8 command prompt;"I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called spark-submit command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/binAfter that, in cmd prompt: spark-submit script.py";[education, open-source];118;
6170;2;2015-06-21T18:52:37.563;;IntelliJ supports R via this plugin:https://plugins.jetbrains.com/plugin/6632It's a recent project, so RStudio is still more powerful, including its focus on data-friendly environment (plots and data are always in sight).;;;
6172;1;2015-06-22T02:13:40.850;What kind of research can be done with genomic data?;It's well known that science has given us large amount of free accessible data, such as http://www.1000genomes.org and http://www.ncbi.nlm.nih.gov/genbank. How can we play around with the data and apply data science/machine learning to it? What could be some ideas?My own ideas:Biological data visualisationGene prediction using hidden-markov-modelAny more?;[education, open-source];47;
6173;2;2015-06-22T12:00:29.197;;"First of all I should mention that this is an OS specific question. You will see this problem in Windows but not in Linux (I'm not sure about OS X, but maybe no problem there too.)The problem with your code is not with the reading part.If you open your test.csv in Notepad++ you will see something like this:If you edit it in Notepad++ so that it looks like and then you run this:testdf <- read.csv(""test.csv"",encoding=""UTF-8"")testdf[1,3]testdf$list2plot(1:10,main=testdf2[2,3])You will see that there is no problem.So in fact there are two problems:When writing Unicode text, R changes it back to system locale and then again to unicode, that's why writing data to csv file does not work correctly.R has problems showing Unicode data in data.frame so even after reading it correctly, testdf would print those codes there not the Unicode characters.see this question on Writing Unicode Text into Text File from R (in Windows) in R mailing list archives,and also this question on UTF-8 file output in R on stackoverflow.";;;
6174;1;2015-06-22T13:22:57.950;Rstudio using 2.5% of 250GB RAM. how to Increase it;I am working on Rstudio on a server which has 250GB ram. But its taking too much time to handle a 2GB data file. how should i speed up my work?;[education, open-source];91;
6175;1;2015-06-22T14:41:12.013;Which open-source sgdb for kind of large data;"I have a 7 giga confidential dataset which I want to use for a machine learning application.I tried :Every package recommanded for efficient dataset management in R like :data.table, ff and sqldf with no success. Data.table needs to load all the data in the memory from what I read, so it's obvious that it will not work since my computer has only 4g RAM. Ff leads to a memory error too.So I decided to turn to sgdb and I tried :Mysql which managed to load my dataset in 2 hours and 21'. Then I began my requests (I have a few requests to do to prepare my data before I export a smaller set in R for machine learning application), and then I had to wait for hours before I got the following message ""The total number of locks exceeds the lock table size"" (my request was just an update to extract the month from a date for each tuple). I read that postgre was similar to mysql in performance so I didn't tryI read that redis was really performant but not at all adapted to massive importation like I want to do here so I didn't tryI tried mongoDb, the nosql upraising solution that I heard everywhere about. Not only I find rather disturbing that mongoimport is so limited in options (I had to change all semi-colon in commas using sed before I can import the data), but It seems to be less performant that mysql since I launched the loading yesterday and it is still running. What I can't try : data are confidential so I don't really want to rent some space on Azure or Amazon clouding solution. I am not sure that it is that big that I have to turn to Hadoop solution but maybe I am wrong about that. Is there an open-source performant solution that I didn't try that you would recommend to perform some sql-like requests on a biggish dataset ?Edit : Some more details about what I want to do with these data for you to visualize. These are events with a timestamp and a geolocalisation. I have 8 billions of lines. One example of what I want to do : standardize series identified by geolocalisation (I need to compute mean grouping by geolocalisation for example), compute average count of events by type of season, day... (usual group by sql request)... Edit As a beginning of answer for those who have limited hardware like me, rSQLite seems to be a possibility. I am still interested in other people experiences. ";[education, open-source];52;
6176;1;2015-06-22T15:26:36.693;Debugging Neural Network for (Natural Language) Tagging;I've been coding a Neural Network for recognizing and tagging parts of speech in English (written in Java). The code itself has no 'errors' or apparent flaws. Nevertheless, it is not learning -- the more I train it does not change its ability to predict the testing data. The following is information about what I've done, please ask me to update this post if I left something important out.I wrote the neural network and tested it on several different problems to make sure that the network itself worked. I trained it to learn how to double numbers, XOR, cube numbers, and learn the sin function to a decent accuracy. So, I'm fairly confident that the actual algorithm is working.The network using using the sigmoid activation function. The learning rate is .3, Momentum is .6. The weights are initialized to rng.nextFloat() -.5) * 4I then got the Brown Corpus data-set and simplified the tagset to 'universal' with NLTK. I used NLTK for generating and saving all the corpus and dictionary data. I cut the last 15,000 sentences out of the corpus for testing purposes. I used the rest of the corpus (about 40,000 sentences of tagged words) for training. The neural network layout is as follows: There is an input neuron for each Tag. Output Layer: There is one output neuron for each tag. The network is taking inputs for 3 words: first: the word coming before the word we want to tag, second: the word that needs to be tagged, third: the word that follows the second word. So, total number of inputs are 3x(total number of possible tags). The input values are numbers between 0 and 1. Each of the 3 words being fed into the input layer is searched for in a dictionary (made up by the 40,000 corpus, the same corpus that is used for training). The dictionary holds the number of times that each word has been tagged in the corpus as what part of speech.  For instance, the word 'cover' is tagged as a noun 1 time and a verb 3  times.Percentages of being tagged are computed for each part of speech that the word is associated as, and this is what is fed into the network for that particular word. So, the input neuron designated as NOUN would receive .33 and VERB would receive .66. The other input neurons that hold tags for that word receive an input of 0.0. This is done for each of the 3 words to be inputted. If a word is the first word of a sentence, the first group of tags are all 0. If a word is the last word of a sentence, the final group of input neurons that hold the tag probabilities for the following word are left as 0s.I've been using 10 hidden nodes (I've read a number of papers and this seems to be a good place to start testing with)None of the 15,000 testing sentences were used to make the 'dictionary.' So, when testing the network with this partial corpus there will be some words the network has never seen. Words that are not recognized have their suffix stripped, and their suffix is searched for in another 'dictionary.' Whatever is most probable for that word is then used as inputs for it.This is my set-up, and I started trying to train the network. I've been training the network with all 40,000 sentences. 1 epoch = 1 forward and backpropagation of every word in each sentence of the 40,000 training-set. So, just doing 1 epoch takes quite a few seconds. Just by knowing the word probabilities the network did pretty well, but the more I train it, nothing happens. The numbers that follow the epochs are the number of correctly tagged words divided by the total number of words.First run 50 epochs: 0.928218786100 epochs:        0.933130661500 epochs:       0.928614499 took around 30 minutes to train this                   Tried 10 epochs:         0.928953683 Using only 1 epoch had results that pretty much varied between .92 and .93So, it doesn't appear to be working...I then took 55 sentences from the corpus and used the same dictionary that had probabilities for all 40,000 words. For this one, I trained it in the same way I trained my XOR -- I only used those 55 sentences and I only tested the trained network weights on those 55 sentences. The network was able to learn those 55 sentences quite easily. With 120 epochs (taking a couple seconds) the network went from tagging 3768 incorrectly and 56 correctly (on the first few epochs) to tagging 3772 correctly and 52 incorrectly on the 120th epoch. This is where I'm at, I've been trying to debug this for over a day now, and haven't figured anything out.;[education, open-source];28;
6177;2;2015-06-22T19:21:53.897;;Determine the function of genes and the elements that regulate genesthroughout the genome.Find variations in the DNA sequence among people and determine theirsignificance. The most common type of genetic variation is known as asingle nucleotide polymorphism or SNP (pronounced “snip”).  Thesesmall differences may help predict a person’s risk of particulardiseases and response to certain medications.Discover the 3-dimensional structures of proteins and identify theirfunctions.Explore how DNA and proteins interact with one another and with theenvironment to create complex living systems.Develop and apply genome-based strategies for the early detection,diagnosis, and treatment of disease.Sequence the genomes of other organisms, such as the rat, cow, andchimpanzee, in order to compare similar genes between species.Develop new technologies to study genes and DNA on a large scale andstore genomic data efficiently.Continue to explore the ethical, legal, and social issues raised bygenomic research.Source;;;
6178;2;2015-06-22T20:44:59.480;;"You may or may not already know this, but here are a few basics about how R and Rstudio work and use resources.Rstudio is a graphical user interface to R, not the interpeter/runtime environment.  There is a separate ""R session"" that actually executes your R programs and returns results for Rstudio to display.  Therefore, having Rstudio use more memory won't make any difference in the execution speed of your program.Second, memory allocation and cleanup (a.k.a. ""garbage collection"") is handled automatically by the R runtime environment.  ""R holds objects it is using in virtual memory"". Virtual memory is a combination of physical main memory and secondary storage, the mixture determined by the server OS configuration.  You can use command line to change the amount of virtual memory allocated to processes. (Consult with your favorite Linux expert on this.)Third, your speed of your program may or may not be limited by memory speed. You may be compute-bound.  Do some testing to find out what is constraining performance.Fourth, you should first ask yourself whether your program implements an efficient algorithm.  Even if you aren't programming with loops and branches, the functions you call may use them, and maybe not efficiently for your application.  For example, there are dramatic performance gains to be had by switching to data.table from data.frame.Fifth, once you have chosen an efficient algorithm, you can put effort into parallel execution.  The simplest way to do this is by using functions that automatically vectorize operations.  A bit more complicated is to recode your program use the packages doParallel and foreach.  With doParallel, you can specify the number of CPU cores to use, which on a server may range from 32 to 64 or more.  Finally, if your server has a Graphics Processing Unit (GPU), it's possible for some algorithms to reprogram using the GPU commands and get massive parallelism.  This option takes the most effort and has the most constraints.";;;
6179;2;2015-06-23T02:37:33.750;;analyzing 8 billion lines on a 4gb computer is pretty silly, but you can tryhttp://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html;;;
6181;1;2015-06-23T08:36:13.723;What kinds of data other than geographical are topologically spherical?;"I'm trying to think of a data set that is essentially topologically spherical. It's easier to think of cylindrical datasets (two dimensions, one periodic) or toroidal datasets (two dimensions, both periodic). Obvious candidates are geographical and astronomical, ground and sky; but I think the only thing spherical about the sky is its projection onto the ground, so it really just comes back to Earth.I find it helpful to think about in terms of a circle's fundamental polygon:";[education, open-source];62;
6182;1;2015-06-23T09:20:27.740;Is there an interface for bootstrap sampling in rattle in R?;I am using rattle in R for predictive models and am trying to see whether there is a difference in different sampling methods. The split function at the start of rattle for splitting into training and testing (optional validation) I take it is split validation. Is there a way to do bootstrap validation and cross validation directly in rattle?;[education, open-source];14;
6183;1;2015-06-23T12:59:13.607;"What is an alternative name for ""Unstructured Data""?";"I'm writing my thesis at the moment, and for some time - due to a lack of a proper alternative - I've stuck with ""unstructured data"" for referring to natural, free flowing text, e.g. Wikipedia articles.This nomenclature has bothered me from the very beginning, since it opens a debate that I don't want to get into. Namely, that ""unstructured"" implies that natural language lacks structure, which it does not - the most obvious being syntax. It also gives a negative impression, since it is the opposite of ""structured"", which is accepted as being positive. This is not the focus of my thesis, though the ""unstructured"" part itself plays an important role.I completely agree with the writer of this article, but he proposes no alternative except for ""rich data"", which doesn't cover my point. The point I'm trying to make that the text lacks a traditional database-like (e.g. tabular) structure of the data, with every piece of data having a clear data type and semantics that is easy to interpret using computer programs. Of course I'd like to condense this definition into a term, but so far I've been unsuccessful coming up with, or discovering an acceptable taxonomy in literature.";[education, open-source];48;
6184;1;2015-06-23T13:54:03.990;Finding aggregated information of data;I am new to data science. I have a dataset of around 200,000 records, having 5 columns. There is a field called, class. For each class, there are one or many divisions. I have to do this:1. Filter the dataset, such that only those classes with at least 5 divisions turn up.For each division, I have to calculate attendance from another column.There is a minimum attendance value for each class. I have to find the percentage of divisions in each class with the minimum attendance.I started with importing the data in python using Pandas and started writing loops for processing this. But I am sure this is not the right way to do. Can you please give some idea.Can I do this in Excel pivot table?;[education, open-source];34;
6186;1;2015-06-23T14:40:47.173;Is there a difference between on-line learning, incremental learning and sequential learning?;"What I mean is the following: Instead of processing all the training data at once and calculating a model, we process one data point at a time and update the model directly afterwards. I have seen the terms ""on-line (or online) learning"" and ""incremental learning"" for this. Is there a subtle difference? Is one term used more frequently? Or does it depend on the research community?Edit: The Bishop book (Pattern Recognition and Machine Learning) uses the terms on-line learning and sequential learning as synonyms but does not mention incremental learning. ";[education, open-source];51;2
6187;2;2015-06-23T15:38:09.920;;I'm not seeing where the relationship between hypotheses and research paper is. I know what the relationship is in reality, but your question is a bit unorganized. Perhaps we could think about it in a different way... I would first create a definition of possible hypotheses (i.e., $H_{0}$ and $H_{1}$ are my two possible hypotheses, and every hypothesis will have both a null and an alternative), and then associate a $H_{0}$ with a $H_{1}$ such that a combination of both null and alternative hypotheses equate to one hypothesis. With this, it's a matter of iteratively counting.With $p \in P$, where $p$ is a paper in the set of papers $P$,With $w \in W$, where $w$ is a word in a set of words $W$ that indicate whether $p$ is a research paper or not,With $i$ as the number of times a $w$ occurred in a $p$, count the number of times a $w$ occurs in each $p$ and iterate $i$ accordingly. ;;;
6188;1;2015-06-23T16:29:23.320;What is the best way to propose an item from a set based on previous choices?;The goal of this question is to be able to propose a user further choices based on his past experiences: like Amazon's book advices.From a set of mp3 files, I assume that a set of mp3 tags data is already filled, based on the music he/she has alredy listened to  : what is the easiest way to  implement a machine learning that is able to propose a list of music choices  based on the user's set ?NB : I'm a Machine learning novice I'd appreciate if the answer could be based on Orange, Weka or these kind of tools.Update: removed the classification tag as recommanded.For new comers as me:- the book Predictive Analytics For Dummies is a nice general introduction about this subject- a next step would be the the paper Finding Clusters of Similar Artists which is really interesting especially for the K-means approach- The millions songs dataset which is a gold mine with its huge dataset as well as tutorials with Python codes to use with it- special thanks sheldonkreger for his answer with neo4J graph usage idea wich is really interesting ;[education, open-source];77;1
6189;1;2015-06-23T16:36:41.873;Bigdata cluster compatible distributed predictive model;I might be asking a dumb question but my question is can I write a python program (lets say a classifier) using some library that scales in hadoop (not only using a simple parallel processing).The reason I am confused is 1)The scikit learn python codes don't scale in big data.2)The spark mlib is written in scala and not in python.3)Although h2o is open sourced I don't think it is written in python.;[education, open-source];82;2
6190;1;2015-06-23T19:15:08.960;What is the right algorithm to detect segmentations of a line chart?;"To be concrete, given 2D numerical data as is shown as line plots below. There are peaks on a background average movement (with small vibrations). We want to find the values of pairs (x1, x2) if those peaks drops down to average; or (x1) only if the line doesn't back to the average.There are thousands of such 2D data.What is the right statistic or machine learning algorithm to find x1 and x2 above without plotting?";[education, open-source];57;
6191;2;2015-06-23T19:22:12.353;;I'm not sure what you are saying in the second paragraph, but if your assumptions are that all the mp3s are already tagged, then the easiest way to suggest further is to suggest mp3s with the highest overlap in tags.;;;
6192;1;2015-06-23T19:26:12.690;algorithm and validation ideas of customers profiling (kmeans clustering);The intention of my project is to create customer profiles and it is related to advertisements. There are two types of activities: one is the advertisements that customers solely clicked on, and the other one is the advertisements that customers clicked on and purchase the product afterwards. I have lists of keywords describing the advertisements and those are the information for me to cluster customers in groups for creating customer profiles. I had a post about the second type of the activities before:I am trying to classify/cluster users profile but don't know how with my attributes  By logc 's suggestion, I used the sales amount of the product customer purchased to create the customers profile, such as customer #1234 purchased a bracelet for $20 and the bracelet has keywords: fashion, gold and accessories, here is the way to represent this customer as followed:I used the dataset like this to do the kmeans clustering after applying PCA and it worked well, but now I was told to add the information about the first activity about solely clicking, then I am lack of ideas of the algorithm. As there is no numeric meaning of those advertisements solely clicked on, I tried LDA by considering each customer as a document and combining all the keywords of advertisements customer clicked on or purchased about into this document. The result is more about topics other than customers. I also tried taking the keywords in activity one as the dummy variables and then combining this with the numeric variables of activity two to do the clustering. I can't tell whether it's good and I am thinking that whether the dummy variables part made the data very sparse,so that it wouldn't work well for Kmeans.The WSSSE of this upadated combination case is very large.   Any suggestions for my case?   Thanks a lot!   ;[education, open-source];32;1
6193;2;2015-06-23T20:44:49.510;;Take a look at map(),reduce(),filter() functions in Python.;;;
6194;2;2015-06-23T21:07:20.510;;"One way to do what you're talking about is called ""change point analysis."" There is an R package for this called changepoint that you might want to check out. In Python, you could try changefinder.";;;
6195;2;2015-06-23T22:52:02.133;;"""Raw data"" is what we say in NLP.";;;
6196;2;2015-06-24T01:22:17.710;;"It is a bad idea to counterpose ""unstructure data"" to, say, tabular data (as in ""non-tabular data""), as you will have to elliminate other alternatives as well (e.g., ""non-tabular and non-graph and ... data""). ""Plain text"" (-- my choice) or ""raw text"" or ""raw data"" sound fine.";;;
6197;2;2015-06-24T01:29:39.120;;"Partition the time axis (the horizontal axis) into regions, and for each region compute the variation of the function value (or its deviation from the mean) as well as its finite differences (e.g., 1st and 2nd). The time points corresponding to ""anomalies"" in each of the obtained series are what you are looking for. If the number of points does not satisfy you, repeat the process hierarchically, altering the time granularity.";;;
6198;2;2015-06-24T02:17:05.160;;"Any data consisting of a three element vector of numbers  $v =\{x,y,z\}$ could be viewed as topologically spherical if, after converting to polar coordinates, there is only one datum for every value of the 3D angle $\theta$. Thus, every point in the data represents an exterior of a topological sphere. This is taking the term ""topological"" literally and formally.There are also graph structures (acyclic) that have neighborhood structures that allows the graph to be mapped topologically to a sphere. For example, all graphs that can be mapped to Platonic Solids (i.e. vertexes to vertexes, edges to edges, 1:1) are topological spheres. This can include irregular neighborhood structures, too.As far as practical examples of these, none come to mind immediately.";;;
6200;1;2015-06-24T11:23:03.077;What are the basic approaches for balancing a dataset for machine learning?;Some times we come across datasets in which classes are imbalanced. For eg. class A may have 2000 instances but class B has only 200. How can we train a classifier for such datasets?;[education, open-source];69;1
6201;1;2015-06-24T12:22:46.497;Resources on Data Science for Football / Soccer?;I am searching for resources ( Websites , Twitter-Profiles, Blogs , Paper etc.) on data science projects for football , soccer data.Thanks for sharing. ;[education, open-source];62;
6202;1;2015-06-24T12:24:03.540;SPSS style custom tables in orange;Is there a way to draw custom tables like in SPSS where you put some attributes to rows and some to column to visualize clustering results?;[education, open-source];10;
6203;2;2015-06-24T15:12:37.963;;What you are describing is not a classification task. You need to use a recommender engine to do this - that's what they are actually developed for. I suggest you to google 'Mahout in action' book which is brilliantly covers the development of recommender engines. After reading the theory you will be able to find a tool suitable for your task.;;;
6204;2;2015-06-24T15:17:38.227;;Well, you just train it on unbalanced dataset, it is not a problem. I don't think you need to apply some special techniques.The only case where you may want to do something special is if your classes are skewed by their nature (if the skewness is the property of class itself, not of just your specific dataset). For example, if you build a classifier which will tell if someone has a malignant tumor in X-ray image or not. By nature of the task absolute majority of patients will not have a malignant tumor. In such case you may want to tune your classifier algorithm a bit, for example introduce a weighting into your SVM or smth like it.A special case is if you have EXTREMELY unbalanced classes. For example 100000 of positive examples and 20 negatives. In such case you will want to go away from classification task to the approach called 'anomaly detection'. ;;;
6205;1;2015-06-24T15:21:29.590;Word taxonomies for Facebook likes categories;"I query the Facebook graph API to get some users' likes, which come with a ""category"" field, which can be, for instance, Italian Restaurant or Health & Wellness website and so on.I need to draw a profile of the user so I was thinking of retrieving the top level root in a taxonomy graph of the aforementioned categories in such a way that the first example falls into something like Food and the second one into something like Health. This way I'd know, by a majority rule, who is a sporty person, who likes fashion etc.Is there a free public API I could use to get word taxonomies? I found Alchemy, an IBM project which is pretty cool but unfortunately limited to 1000 API requests per day and I'd need way more than that given the big amount of users and categories I have.I know I'm probably asking for too much (a free and unlimited tool!) so the question might become ""do you have better ideas about how to solve this""?";[education, open-source];31;
6206;1;2015-06-24T15:46:52.680;How many observations in a neural networks dataset?;I started to study and programming in neural networks for a little while now, but I never read about the minimum number of observations one must collect in a dataset to get robust results. Of course, more observations better results, but, Does exist an empirical or theoretical relationship between variables and observations number?I mean, neither in econometrics you can compute the minimum number of observations, but it does exist some rule of thumbs that relies the number of exogenous variables to the target variable.I wonder if there is something similar to that in neural networks too, but, till now, browsing on the internet, I did not find anything of useful.Any ideas, advises or hint will be appreciated.;[education, open-source];44;
6207;2;2015-06-24T16:04:32.073;;There's this website called Sportsdatamart which allows you todownload CSV and XLS file formats for a number of Football serieslike the England Premier League, the Italian Serie A, the GermanBundesliga and the Spanish La Liga. On Football Data UK, you can find EXCEL and CSV data files touse for quantitative testing of betting systems in spreadsheetapplications. League tables, head2head statistics and information ongoalscrores, first scorers and top scorers could be accessed throughthe Livescore service.Soccer Wiki is a collaborative database and anyone can createand edit data. This community driven database contains informationon players, clubs, stadiums, managers, referees, leagues and otherdata related to the world of soccer.If you are looking for data sets centered on European Football,check this out.;;;
6208;1;2015-06-24T16:36:32.730;Working with inaccurate (incorrect) dataset;"This is my problem description:""According to the Survey on Household Income and Wealth, we need to find out the top 10% households with the most income and expenditures. However, we know that these collected data is not reliable due to many misstatements. Despite these misstatements, we have some features in the dataset which are certainly reliable. But these certain features are just a little part of information for each household wealth.""Unreliable data means that households tell lies to government. These households misstate their income and wealth in order to unfairly get more governmental services. Therefore, these fraudulent statements in original data will lead to incorrect results and patterns.Now, I have below questions:How should we deal with unreliable data in data science?Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms? -How can we evaluate our errors in this study? Since we have unlabeled dataset, should I look for labeling techniques? Or, should I use unsupervised methods? Or, should I work with semi-supervised learning methods?Is there any idea or application in Machine Learning which tries to improve the quality of collected data?Please introduce me any ideas or references which can help me in this issue.Thanks in advance.";[education, open-source];102;1
6209;1;2015-06-24T20:05:17.320;Regular Expressions in Word;I've OCR'ed a historic directory and am trying to clean the text up via Word. Specifically I need some help in terms of writing a Regular Expression to combine two lines together. For example something that isJohn Smith, 87 BankBldgshould actually beJohn Smith, 87 Bank BldgI've been playing around with some expressions but haven't been successful at all. Any help on this? Thanks in advance!;[education, open-source];36;
6210;2;2015-06-24T20:26:24.657;;I have a solution that might not be very standardized but will suffice your need. Copy all of your data into any advanced text editor like, Notepad++ or Sublime Text. Next, Use CTRL+H to toggle the find and replace feature. Find : '\n' and Replace with ''. ;;;
6211;2;2015-06-24T20:52:38.970;;A neural network is nothing but a set of equations. And the basic rule of any set of equations is that you must have as many data points as the number of parameters.The parameters of any neural network are its weights and biases.So that means that as the neural network gets deeper and wider, the number of parameters increase a lot, and so must the data points.This being said, the more proper and detailed way to know whether the model is overfitting is to check if the validation error is close to the training error. If yes, then the model is working fine. If no, then the model is most likely overfitting and that means that you need to reduce the size of your model or introduce regularization techniques.;;;
6212;1;2015-06-25T12:37:48.030;How to optimising caret for sensitivity;I am currently using caret which optimises for accuracy. Is it possible to optimise for sensitivity. I see documentation mentioning that metric = 'roc' has been used, but sensitivity does not appear to be one of the options.;[education, open-source];26;
6213;1;2015-06-25T12:50:03.597;How can I show the relations between travel destinations?;I'm trying to do a project about email marketing. I'm working on a tourism company and I want to make a best destination suggestion for the clients. But I need to see the relations between destinations.Example: How many people visited Dublin and then visited London?My question: How can I best analyse this relation between the cities, given data about traveler itineraries?I want to send email offers to clients who went to London and didn't go to Dublin (assuming a strong relation between London and Dublin).;[education, open-source];27;
6214;2;2015-06-25T15:54:24.020;;Check if this link could help you out. ;;;
6216;1;2015-06-25T19:12:04.583;learning rate in reinforcement learning;Does anyone know how to get the learning rate from participant data?I'm computing all the expected values for all trials (=200)V(S)t = V(S)t−1 + alpha * error t (error t = Rt − V(S)t−1)and then computing alpha at each trial usingalpha= (V(S)t  - V(S)t−1 )/ error tbut it seems not the right way!!!!Suggestions?Thanks,;[education, open-source];22;
6217;1;2015-06-25T21:24:37.237;Is there a method that is opposite of dimensionality reduction?;"I am new to the field of machine learning, but have done my share of signal processing. Please let me know if this question has been mislabeled.I have two dimensional data which is defined by at least three variables, with a highly non-linear model way too complicated to simulate.I have had varying level of success at extracting the two main components from the data using methods like PCA and ICA (from the python library Scikit-Learn), but it seems these method (or at least, these implementation of the methods) are limited to extracting as many components as there are dimensions in the data, for example, 2 components from a 2D point cloud.When plotting the data, it is clear to the trained eye that there are three different linear trends, the three color lines show the directions.When using PCA, the main component is aligned to one of the color lines, and the other is at 90°, as expected. When using ICA, the first component is aligned with the blue line, and the second is somewhere in between the red and green ones. I am looking for a tool which could reproduce all three components in my signal.EDIT, Additional info: I am here working in a small subset of a bigger phase plane. In this small subset, each input variables produce a linear change on the plane, but the direction and amplitude of this change is non-linear and depends on where exactly on the bigger plane I am working. At some places, two of the variables can be degenerate: they produce change in the same direction. for example, say the model depends on X, Y, and Z. A change in the variable X will produce a variation along the blue line; Y causes a variation along the green line; Z, along the red one.";[education, open-source];169;2
6218;2;2015-06-26T03:10:04.370;;Check out the Github repositories for Christopher Long (octonion):Football: https://github.com/octonion/football-publichttps://github.com/octonion/footballSoccer: https://github.com/octonion/soccer-mhttps://github.com/octonion/soccerMany others:https://github.com/octonion?tab=repositories;;;
6219;2;2015-06-26T03:49:58.613;;You can try Graph databases (Neo4j/Orient DB etc). Store location and connection between the location as nodes and edges. Then do analysis over graph data. Based on your need, you can use additional attributes (like count) and assign weights for edges etc. Neo4j supports collaborative filtering also.;;;
6220;2;2015-06-26T07:32:27.147;;I think you might have to keep overall Data pipelines and Machine learning pipelines in mind. For which you need a robust framework to move data between table like and graph like storage apart from powerful distributed processing. From my understanding Spark GraphX is promising to build these pipelines. Joseph Gonzalez's (one of the creator of GraphLab from CMU) talk on GraphX on youtube is worth watching.;;;
6221;2;2015-06-26T07:41:55.480;;Good info @Sean Owen. Would like to add one additional. Spark may help to build Unified data pipelines in Lambda architecture addressing both Batch and Streaming layers with an ability to write to common serving layer. It is huge advantage to reuse the logic between batch and Streaming. Also Streaming K-Means algorithms in Spark1.3 is an added plus to ML apart from excellent job monitoring and process visualizations in 1.4.;;;
6222;2;2015-06-26T07:54:24.640;;For comprehending the derivation of Back propagation algorithm, I suggest Ryan Harris youtube video which is less daunting. You may find second video as well.;;;
6223;2;2015-06-26T08:07:36.270;;You may build models to classify genomes by population.Run unsupervised learning (clustering) to see if populations are reconstructed in the model.Build models to infer missing genotypesTo do a Scalable DNA analysis you may check Adam software based on Apache Spark;;;
6224;1;2015-06-26T08:07:59.093;What does the relative sum of squares error signify in neural network analysis in SPSS?;What range of values of the relative sum of sqaures error is acceptable for a good neural network? I am getting around 0.9 of the relative error for 1 model and around 0.4 for another. Are both the networks significant?;[education, open-source];22;
6225;2;2015-06-26T10:17:42.997;;I think total number of default categories in Facebook is somewhat static. There is around 140-160 categories. ref: http://stackoverflow.com/questions/4216648/facebook-pages-authoritative-list-of-categories/8576572#8576572 and http://www.marketinggum.com/types-of-facebook-pages-for-business/ You can cluster (manually) them in to 10-12 generic groups. eg: Sports Event,     Sports League,     Amateur sports team,    Sports Venue,    Athlete,    Sports/Recreation/Activities etcall belongs to Sports category. Similarly cluster and form generic groups.And then you can assign a user to any of these generic group based on the like category.;;;
6226;1;2015-06-26T12:19:16.267;How to combine two different random forest models into one in R?;I have two different dataset with same variables. I have built training models separately using random forest. Now i want to combine both these models. Could anyone tell me how can i this be achieved? Do we have something called combine() function in R?Regards,Arun;[education, open-source];29;
6227;1;2015-06-26T12:56:55.410;Traversing trees in SQL: JOINs vs imperative algorithm;I have a table representing posts on a message board. Posts may or may not have parents.What is the most common way to get all posts starting from a given post, or to find the root post of any given post? I can think of using JOINs to join parents and children, but how can I know how many joins I need before I stop?;[education, open-source];21;
6228;2;2015-06-26T13:29:43.747;;You might want to go through the code snippet on this webpage. The forum there has chalked out a solution to a similar kind of a query.;;;
6229;2;2015-06-26T13:33:43.820;;This question has already been answered here. It addresses the problem in hand. ;;;
6230;2;2015-06-26T13:58:42.547;;The short answer is yes. Essentially you will be performing some sort of feature engineering. This means constructing a series of functions of your data, often:$\phi_j(x): \mathbb R^p \rightarrow \mathbb R \ ,\ \  j=1,...,K$Which, strung together, define a transformed data vector $\boldsymbol \phi(x)$ of length $K$.There are a number of ways, better and worse, of doing this. You may want to look up terms like:Splines and generalised additive models.The kernel trick (how to make a model where $K\rightarrow \infty$).Feature engineering (of the manual variety, e.g. adding an $x^2$ column to your data).Deep learning, representation learningAs you might guess from such a varied bag of techniques, this is a large area. It goes without saying really but care has to be taken to avoid overfitting.This paper Representation Learning: A Review and New Perspectives deals with some of the issues around what makes a particular set of features 'good', from a deep learning perspective.;;;
6232;1;2015-06-26T20:48:12.417;How to find similarity between different factors in a dataset;"IntroductionLet's say I have a dataset of different observation of different people and I want to group people together to know which person is closest to the other one. I also want to have a measure to know how close they are to each others and know the statistical significance.Data       eat_rate drink_rate   sleep_rate    play_rate  name   game1  0.0542192259 0.13041721 5.013682e-03 1.023533e-06  Paul Rayman4  0.0688171511 0.01050611 6.178833e-03 3.238838e-07  Paul  Mario6  0.0928997660 0.01828468 9.321211e-03 3.525951e-07  Jenn  Mario7  0.0001631273 0.02212345 7.061524e-05 1.531270e-07  Jean   FIFA8  0.0028735509 0.05414688 1.341689e-03 4.533366e-07  Mark   FIFA10 0.0034844717 0.09152440 4.589990e-04 5.802708e-07  Mark Rayman11 0.0340738956 0.03384180 1.636508e-02 1.354973e-07  Mark   FIFA12 0.0266112679 0.20002020 3.380704e-02 4.533366e-07  Mark  Sonic14 0.0046597056 0.01848672 5.472681e-04 4.034696e-07  Paul   FIFA15 0.0202715299 0.16365289 2.994086e-02 4.044770e-07 Lucas   SSBMReproduce it:structure(list(eat_rate = c(0.0542192259374624, 0.0688171511010916, 0.0928997659570807, 0.000163127341146237, 0.00287355085557602, 0.00348447171120939, 0.0340738956099744, 0.0266112679045701, 0.00465970561072008, 0.0202715299408583), drink_rate = c(0.130417213859986, 0.0105061117284574, 0.0182846752197192, 0.0221234468128094, 0.0541468835235882, 0.0915243964036772, 0.0338418022022427, 0.200020204061016, 0.0184867158298818, 0.163652894231741), sleep_rate = c(0.00501368170182717, 0.00617883308323771, 0.00932121105128431, 7.06152352370024e-05, 0.00134168946950305, 0.000458999029040516, 0.0163650807661753, 0.0338070438697149, 0.000547268073086768, 0.029940859740489), play_rate = c(1.02353325645595e-06, 3.23883801132467e-07, 3.52595117873603e-07, 1.53127022619393e-07, 4.53336580123204e-07, 5.80270822557701e-07, 1.35497266725713e-07, 4.53336580123204e-07, 4.03469556309652e-07, 4.04476970932148e-07), name = structure(c(5L, 5L, 2L, 1L, 4L, 4L, 4L, 4L, 5L, 3L), .Label = c(""Jean"", ""Jenn"", ""Lucas"", ""Mark"", ""Paul""), class = ""factor""), game = structure(c(3L, 2L, 2L, 1L, 1L, 3L, 1L, 4L, 1L, 5L), .Label = c(""FIFA"", ""Mario"", ""Rayman"", ""Sonic"", ""SSBM""), class = ""factor"")), .Names = c(""eat_rate"", ""drink_rate"", ""sleep_rate"", ""play_rate"", ""name"", ""game""), row.names = c(1L, 4L, 6L, 7L, 8L, 10L, 11L, 12L, 14L, 15L), class = ""data.frame"")QuestionGiven a dataset as fellow (with continuous and categorical feature), how can I know if a person (a categorical answer) identified by a name is more correlated to another person?";[education, open-source];96;1
6233;1;2015-06-26T23:30:40.847;Simple Path to Route Algorithm;I'm currently conducting research into subway paths, but stand in front of a problem, with no programming knowledge and an extremely short time-limit. Probably the question I am asking is trivial, but I would be very happy if you could point me in the right direction or to the right forum!Having generated all possible routes a passenger can take underground for a specific city subway system with a starting, transfer and exit station, I now have to find the on-vehicle travel time for each route. I have excel-style data for each single link of the route, but these have to be ordered and added up for each vector of (starting, transfer, exit) to give me the total travel time.Take for example the following route: A -> B -> C and I have information on the traveltime for each line, e.g. A4 -> A5, A5 -> A6, A6 -> A, A -> ... -> B and B3 -> B4, B4 -> B5, B5 -> B6, B6 -> ... ->  C, where each k = {An, Bm, C} is a station and A is the starting point, B the transfer point and C the exit point for the route.How can I write a program, which goes through the excel observations and adds up all the relevant travel time for each of the routes?With about 50000 routes considered, this can be hugely time-intensive to be done manually.Any help on this would be greatly appreciated! If anything is unclear, I will be glad to try and clarify what I mean.Best;[education, open-source];26;
6234;2;2015-06-27T02:28:24.020;;"One way is to normalize your quantitative  values (play, eat, drink, sleep rates) so they all have the same range (say, 0 -> 1), then assign each game to its own ""dimension"", that takes value 0 or 1. Turn each row into a vector and normalize the length to 1. Now, you can compare the inner product of any two people's normalized vectors as a measure of similarity. Something like this is used in text mining quite oftenR Code for Similarity MatrixAssumes you've saved your dataframe to the variable ""D""#Get normalization factors for quantitative measuresmaxvect<-apply(D[,1:4],MARGIN=2,FUN=max)minvect<-apply(D[,1:4],MARGIN=2,FUN=min)rangevect<-maxvect-minvect#Normalize quantative factorsD_matrix <- as.matrix(D[,1:4])NormDMatrix<-matrix(nrow=10,ncol=4)colnames(NormDMatrix)<-colnames(D_matrix)for (i in 1:4) NormDMatrix[,i]<-(D_matrix[,i]-minvect[i]*rep(1,10))/rangevect[i]gamenames<-unique(D[,""game""])#Create dimension matrix for gamesNgames<-length(gamenames)GameMatrix<-matrix(nrow=10,ncol=Ngames)for (i in 1:Ngames) GameMatrix[,i]<-as.numeric(D[,""game""]==gamenames[i])colnames(GameMatrix)<-gamenames#combine game matrix with normalized quantative matrixPeople<-D[,""name""]RowVectors<-cbind(GameMatrix,NormDMatrix)#normalize each row vector to length of 1 and then store as a data frame with person namesNormRowVectors<-t(apply(RowVectors,MARGIN=1,FUN=function(x) x/sqrt(sum(x*x))))dfNorm<-data.frame(People,NormRowVectors)#create person vectors via addition of appropriate row vectorsPersonMatrix<-array(dim=c(length(unique(People)),ncol(RowVectors)))rownames(PersonMatrix)<-unique(People)for (p in unique(People)){  print(p)  MatchIndex<-(dfNorm[,1]==p)*seq(1,nrow(NormRowVectors))  MatchIndex<-MatchIndex[MatchIndex>0]  nclm<-length(MatchIndex)  SubMatrix<-matrix(NormRowVectors[MatchIndex,],nrow=length(MatchIndex),ncol=dim(NormRowVectors)[2])  CSUMS<-colSums(SubMatrix)  NormSum<-sqrt(sum(CSUMS*CSUMS))  PersonMatrix[p,]<-CSUMS/NormSum}colnames(PersonMatrix)<-colnames(NormRowVectors)#Calculate matrix of dot productsSimilarity<-(PersonMatrix)%*%t(PersonMatrix)";;;
6235;1;2015-06-27T03:41:16.967;Data repositories like UCI;Are there any other data repositories like UCI and mlData, for biological data?? I want to know about mostly biological data set.;[education, open-source];47;
6236;1;2015-06-27T06:34:46.957;Data produced as an output to Dumbo API of Python not getting distributed to all the nodes of cluster;The node from which I run Dumbo commands,all the files produced as an output to the Dumbo command are produced in the same node.For example to say ,suppose there is a node having name hvs and I ran the script:dumbo start matrix2seqfile.py -input hdfs://hm1/user/trainf1.csv -output hdfs://hm1/user/train_hdfs5.mseq -numreducetasks 25 -hadoop $HADOOP_INSTALL I ran the above scipt from the node hvs.When I observe my file system,I found that all the files produced are accumulated in node hvs.Ideal situation when the files get distributed throughout the cluster.Data is not getting balanced throughout the cluster.How to fix the above situation?;[education, open-source];15;
6237;2;2015-06-27T08:46:32.010;;I do biological network analysis and here are some networks.;;;
6238;2;2015-06-27T09:32:14.997;;Try any graph database (neo4j/orientdb etc). You can upload spreadsheet data to Neo4j check : http://neo4j.com/blog/importing-data-to-neo4j-the-spreadsheet-way-in-neo4j-2-0/ Create places as nodes and routes as edges, assign weights (eg :travel time) to edges and then try different graph algorithms as your use-case needs. ;;;
6239;2;2015-06-27T15:40:36.653;;There're lots and tons of data sets for biological data.http://www.ncbi.nlm.nih.gov/genbank/http://www.1000genomes.org/http://www.ddbj.nig.ac.jp/https://en.wikipedia.org/wiki/List_of_biological_databases;;;
6240;2;2015-06-27T17:14:44.760;;Despite normalized euclidean distance you can also have a look at the pearson distance as a similarity measure. Here is a neat description : http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sphilip/pear.html ;;;
6242;1;2015-06-28T06:44:01.293;GATE with Python;I am working on named entity recognition project and find GATE very useful. But I couldn't find a Python connector for GATE yet. Is it possible or is there a way to integrate GATE work flows into Python ? Or how can I write JAPE rules for extracting entities in python ?;[education, open-source];64;
6243;1;2015-06-28T14:26:12.743;Self-designed objective for linear regression learning;A multiple linear regression is to use several predictor variables to predict the outcome of a response variable, like the following relationship:$y_{i}=\beta_{1}x_{i1}+...+\beta_{p}x_{ip}+\epsilon_{i},   i=1,...,n$I understand the typical objective to learn the $\beta$ paramters is least-squares, which means to minimize the sum of the sqaure of $\epsilon_{i}$. Now I want other kinds of objective, for example to maximize the Shannon entropy of the sequences of $\epsilon$ (or other self-specified objective). I googled towards this direction but no luck. I am wondering if there is any problem (and tool to solve it if possible) I can look into to do that?Thank you for your help.;[education, open-source];39;
6244;1;2015-06-28T15:55:34.763;How does the test data gets collected?;I'm learning the linear regression now. I used R to build linear model upon a set of train model and try to predict() data based on the test data.My question: I understand how train data gets collected. But what's with the test data? How did it get collected? Is the test data built, or is it collected, or is it predicted or what?PS: I'm learning data science by method of self-learning, so I lack the structural in my knowledge. I might know something on one place while lack the knowledge at another place. Please forgive and guide. Thanks. :);[education, open-source];18;
6245;2;2015-06-28T16:14:24.527;;"In general, you randomly splits the available data into following 3 sets. Train, validation and test. [Different ratios could be used depending on total amount of data at hand and the difficulty of the problem.] You can start with a simple 80%/10%/10% split.You use the first two sets to build your model. So you would use the 80% split of your data to lets say build a logistic regression model. Now you use the 10% validation set to see how good your model is. You can iterate on this process until you are satisfied with the validation data set performance of the model.Now, you use the last set (the ""test"" split) to see how your model would generalize on an ""unseen"" data set. [Here unseen means, your model never had access to ""see"" this data while in the learning phase]. You never use use this 3rd set to glean knowledge about how your model could be improved. Think of this set as a set your customer has, and you don't have access to, and to whom you are going to deliver your model.Once you get a hang of this concept also learn more about how you can do cross-validation if you have limited data and to improve confidence on your model.";;;
6247;1;2015-06-28T16:56:58.467;Can you use clustering to pick out signals in noisy data?;As my first project into data science, I would like to pick out the main clusters in noisy data. I think a good example would be trying to pick out certain links on a given StackExchange question that has a number of answers. The most common type of link is a link to a question on the SE network. The next common is either tag links, or links to user profiles. The remaining links might be random links included in posts, which is considered noise.  Ideally, I'm looking for a solution where I don't know how many clusters of links there will be ahead of time.I've implemented my first attempt using scikit-learn and KMeans. However, it's not ideal because I appear to have to specify the number of clusters ahead of time, and I think the random, noisy links get grouped improperly. I also think it's more effective on a larger corpus compared to the relatively small one of URL tokens (though that's just a guess).Is there a way to do this type of clustering, where the number of clusters is unknown or where one of the clusters is a sort of miscellaneous cluster containing objects that don't closely match the other clusters?;[education, open-source];35;
6249;2;2015-06-28T22:47:23.027;;There are a few of other approaches you can take to try to balance your class distribution..Subsample Majority ClassYou can balance the class distributions by subsampling the majority class.Oversample Minority ClassSampling with replacement can be used to increase your minority class proportion.Adding NoiseA more sophisticated scheme is to add Gaussian, or other suitable, noise to the existing instances of the minority class in an effort to create a greater number of representative, but diverse,  instances.SMOTEA popular method to synthesiseminority class instances with greater complexity than pure noise addition is SMOTE ( Synthetic Minority Oversampling TEchnique ). This uses a K-member neighbourhood in feature space to impute new instances.WEKA has a filter for this. Though there is some evidence that this technique is not overly beneficial with high dimensional data here.;;;
6251;2;2015-06-29T04:11:05.683;;Have you looked at DBSCAN? It is a density-based spatial clustering of data with noise that can define non-linear clusters (unlike k-means).It doesn't require knowing the number of clusters. However, it does require two parameters (minimum cluster size and neighborhood size) that measure density. But you may be able to estimate them in your particular domain.;;;
6252;2;2015-06-29T04:27:24.677;;"One method I've used successfully is the Triangle Method used for thresholding in image processing. Basically is looks for the ""elbow"" in the data by drawing a line between the data peak and the minimum at one end and defining the ""elbow"" point as being the farthest from that line. Checkout the link and the picture there will clearly explain what the algorithm does.";;;
6253;1;2015-06-29T04:30:30.007;OCR Spell Check based upon previously found words;"I have a bunch of OCR produced text files with numerous spelling errors. I want to automatically correct these spelling based upon the prevalence of the closest correctly spelled words. For example, if the text reads ""junp"" and the options are ""jump"" or ""june"", then it would pick between the two based upon both context and the frequency of the two words. Is there any software like this?";[education, open-source];11;
6254;1;2015-06-29T09:36:45.270;Parameters for OnlineLogisticRegression function in Mahout;"Can anyone tell me where do I find any documentation for parameters like:-stepOffset-alpha-decayExponentin an OnlineLogisticRegression function in Mahout?I am interested in what do they change in calls like this one:int FEATURES = 10000;       OnlineLogisticRegression learningAlgorithm = new OnlineLogisticRegression(20, FEATURES, new L1())                        .alpha(1).stepOffset(1000).decayExponent(0.9).lambda(3.0e-5).learningRate(20);";[education, open-source];11;
6255;1;2015-06-29T13:09:52.317;root mean square error - significance of square root;what is the significance of the square root in root-mean-square-error? In effect, my question is what is the difference between (rms error) and (rms error)^2 ?;[education, open-source];73;2
6257;1;2015-06-29T14:33:18.370;Predict set elements based on other elements;Sorry if this has been answered before but could someone help me with solving the following problem:Each symbol in a dataset has a set of labels. Given a set of labels how can we predict more labels for that set?Or to attempt a more formal wording: let a set of sets $S = \{ s_1, s_2, s_3,  ... \} $ where $ s_i \subseteq L $ and $ L = \{ l_1, l_2, l_3,...,l_m\} $ where $L$ is the finite set of labels. Given a set of labels $Y = \{ y_1 ,..., y_n \} $ where $Y \subseteq X $ what is the probability $ P(r = l_i) \forall l_i \in L$ so that $Y \cup \{r\} \in S $.;[education, open-source];30;1
6258;2;2015-06-29T16:02:18.727;;"Maybe you could use some algorithm that solves ""Market Basket Analysis"". The problem is explained here:  Market Basket Analysis is a modelling technique based upon the theory  that if you buy a certain group of items, you are more (or less)  likely to buy another group of items. For example, if you are in an  English pub and you buy a pint of beer and don't buy a bar meal, you  are more likely to buy crisps (US. chips) at the same time than  somebody who didn't buy beer.http://albionresearch.com/data_mining/market_basket.phpOne example of such an algorithm is: https://en.wikipedia.org/wiki/Association_rule_learning";;;
6259;1;2015-06-29T16:05:19.520;Web Scraping - a scientific database;I am searching a scientific database for abstracts of papers containing the words project management. Here is the link:http://en.journals.sid.ir/SearchPaper.aspx?str=project%20managementFor getting abstracts, I need to click on any paper and open a new page. How can I do that for 68 papers? I program in R and bash.;[education, open-source];95;
6260;2;2015-06-29T16:15:42.947;;"I tried that, and didn't have enough rep. (50) to comment, thus posted in ""answer"".Sean Owen, feel free to delete this comment as well...I still dont see how this question is data science related..";;;
6261;2;2015-06-29T16:32:44.140;;It depends on what you are using the RMSE for. If you are merely trying to compare two models/estimators, then there is no significance to the square root. However, if you are trying to plot the error in terms of the same units as you made the measurements/estimates, then you need to take the square root to transform the squared units to the original units (much like variance vs standard deviation);;;
6264;1;2015-06-29T17:32:31.397;User profiling with Mahout from categorized user behavior;"I'm trying to cluster and classify users with Mahout. At the moment I am at the planning phase, my mind is completely mixed with ideas, and since I'm relatively new to the area I'm stuck at the data formatting.Let's say we have two data table (big enough). In the first table there are users and their actions. Every user has at least one action and they can have too many actions, too. About 10000 different user_actions and millions of records are in the table.user        - user_actionu1          - au2          - bu3          - au1          - cu2          - cu2          - cu1          - bu4          - fu4          - eu1          - eu1          - du5          - dIn the other table, there're action categories.  Every action may have none or multiple categories. There are 60 categories.user_action - categorya           - cat1b           - cat2c           - cat1d           - NULLe           - cat1, cat3f           - cat4I'm going to try to build a user classification model with Mahout but I've no idea what I should do. What type of user vectors should I create? Or do I really need user vectors?I think I need to create something like;u1 (a, c, b, e, d)u2 (b, c, c)u3 (a)u4 (f, e)u5 ()Problem in here, some users performed more than 100000 actions (some of them are same actions)So; this is more useful, I think;u1 (cat1, cat1, cat2, cat1, cat3)u2 (cat2, cat1, cat1)u3 (cat1)u4 (cat4, cat1, cat3)u5 ()The things I also worry about areHow should I weight categories for users? For example u1 has at least three action that related with cat1, while u3 has only 1. These one should be different?How can I decrease the difference between active users and passive ones? Like u1 has too many actions and so categories, u3 has only 1.Any guidance are welcome.";[education, open-source];80;
6267;1;2015-06-29T20:26:54.153;How to speed up optimization using Differential Evolution?;My application is high frequency trading.  My data are time series of the bid and ask prices of a stock recorded on every tick (change in price). For each data point I also have a certain indicators that predict the future movement of the price. The indicators have different horizons of the predictions, some being optimal at few second intervals and others few minutes. I need to assign these predictors weights and based on whether the linear combination crosses a threshold, the decision will be taken to buy of sell the stock. So far I have tried the Differential Evolution (DE) method to figure out the weights. I use a black box model with the weights vector $w_i$ and threshold as inputs. For each data point I have a vector of indicators $\alpha _i$. $$total\_alpha = \sum\alpha _i*w_i$$If$$total\_alpha > threshold, BUY$$Else If$$total\_alpha < -threshold, SELL$$The output of the model is the sum of difference between each between the price of each consecutive buy and sell. This output is being optimised by the DE algorithm. I am having trouble with the computational aspects. My data is very large (~$7 \times 10^8$ rows by 20 columns), and thus the execution time for the DE algorithm is unacceptable. My question: Is there a better and a faster way to solve this problem? ;[education, open-source];42;
6268;1;2015-06-29T22:55:28.100;How to convert categorical data to numerical data in Pyspark;I am using Ipython notebook to work with pyspark applications. I have a CSV file with lots of categorical columns to determine whether the income falls under or over the 50k range. I would like to perform a classification algorithm taking all the inputs to determine the income range. I need to build a dictionary of variables to mapped variables and use a map function to map the variables to numbers for processing. Essentially, I would my dataset to be in a numerical format so that I can work on implementing the models. In the data set, there are categorical columns like education, marital status, working class etc. Can someone tell me how to convert them into numerical columns in pyspark?  workclass = {'?':0,'Federal-gov':1,'Local-gov':2,'Never-  worked':3,'Private':4,'Self-emp-inc':5,'Self-emp-not-inc':6,'State-gov':7,'Without-pay':8}I created a sample dictionary with key value pairs for work class. But, I don't know how to use this in a map function and replace the categorical data in the CSV file with the corresponding value. wc = pd.read_csv('PATH', usecols = ['Workclass'])df = pd.DataFrame(wc)wcdict = {' ?':0,' Federal-gov':1,' Local-gov':2,' Never-worked':3,' Private':4,' Self-emp-inc':5,' Self-emp-n-inc':6,' State-gov':7,' Without-pay':8}df_new = df.applymap(lambda s: wcdict.get(s) if s in wcdict else s)print(df_new)This is the code I have written in normal python to convert the categorical data into numerical data. It works fine. I want to do the conversion in spark context. And, there are 9 categorical columns in the data source. Is there a way to automate the dictionary update process to have a KV pair for all 9 columns?;[education, open-source];155;
6270;1;2015-06-30T03:22:59.463;I want to extract name from CV;I want to extract name from CV. I need high level if accuracy more than 95 %. I have started with taking assumptions that it is highly likely to be found in 10% top lines or if not there then in some section similar to Personal details.Can u plz  provide any help/guidance ?;[education, open-source];20;
6271;2;2015-06-30T04:34:50.527;;Since CV data is mostly structured you can use Stanford NER for named entity recognition or use CRF to train and model your own named entity recognizer.;;;
6272;2;2015-06-30T04:46:02.770;;workclass = {'?':0,'Federal-gov':1,'Local-gov':2,'Never-  worked':3,'Private':4,'Self-emp-inc':5,'Self-emp-not-inc':6,'State-gov':7,'Without-pay':8}try defining a mapper fuction which return key :def mapr(dict_key):    return workclass[dict_key]print list(map(mapr,workclass));;;
6273;2;2015-06-30T04:53:38.793;;Check out Clearbit.com -- https://dashboard.clearbit.com/docs#company-apiSuper comprehensive company lookup. Not free, but 50 free calls a month. ;;;
6274;1;2015-06-30T05:28:31.537;Kmeans on mixed dataset with high level for categ;My retail dataset contains 3 numeric attributes and two categorical attributes  Time and ID with 50,000 records. Both categorical attributes have more than 20 thousand levels and their format is 1/11/2011 11:54 and 1TD10051 respectively.How do I do kmeans on these dataset? Converting categorical to binary will give very sparse dataset?How to proceed?;[education, open-source];44;
6275;2;2015-06-30T06:44:20.367;;There are plenty of methods, variations of k-means for the case of mixed dataset: k-modes, k-protoypes etc.It has been discussed already.;;;
6276;2;2015-06-30T08:05:31.830;;Check out PySemantic - http://github.com/motherbox/pysemanticThere's a video too - https://www.youtube.com/watch?v=6z-18zP4hOA;;;
6277;2;2015-06-30T10:52:18.617;;The square in RMSE is used because it always gives a positive value for error, so avoiding errors cancelling each other out, and affords greater weight to values further from the target function, so emphasising points for which the estimator is poor.The square root is used to remove the effects of the squaring.You could look at using the Mean Absolute Error ( MAE ) which does not have the distance weighting effect of the RMSE and just takes the average of the absolute value of the errors. ;;;
6279;1;2015-06-30T13:39:03.970;How to include class as a feature;"I am currently experimenting with the idea of including the class of a feature vector as a separate feature. My work is about preposition selection in learner language use. I want to train a classifier that, given a certain position of a sentence, uses relevant context features around that position to build vectors. The classifier detects if there should be a preposition or if it should remain empty. Some features include:Neighbouring N-grams, where N={2,3}, at Part-Of-Speech and token-levelNeighbouring nouns and verbsThe class expresses a binary distribution: 1 for preposition presence and 0 for preposition absence.Consider that if a language learner opted to select a preposition in a given sentence, but the classifier thinks there should NOT be one, it can be interpreted as the classifier marking a language error: The learner inserted a preposition in a prohibited context. Conversely, if a learner did NOT use a preposition at a given position in a sentence, but the classifier does expect one, it is also an error.The model is trained on correct negative (no preposition) and positive (""yes"" preposition) events (context features around a certain position: a whitespace or a preposition). This means that the assigned labels are the gold standard, similarly for the test data, because they are trained on *native *language use which is assumed to be error-free. The same assumption cannot be made when running the classifier on learner's language - after all, learners make mistakes, which is what I am trying to detect. I want to include the decision (yes/no) of the learner into the classifier rather than just as the assigned label. Because in practice, the chance on either error is very small, so the classifier should be very confident before marking it as such. But when I do this, the classifier always selects the value of that decision feature as the class. After all, it is trained so that there is a 100% correlation between said feature and the assigned class. How do I turn this learner's decision - which is NOT an absolute indication of the actual answer - into an indication instead?I'm experimenting with Linear Support Vector Classifier and Logistic/Linear Regression.Thanks a bunch, if more details are needed, please ask.";[education, open-source];32;1
6280;1;2015-06-30T14:40:03.720;"Error in R:: Error: Unexpected '}' in "" }""";"I am getting a error as ""Error: Unexpected '}' in ""   }"" when i run the following code. I have put closing brackets for every open bracket. But i am not sure why i get this error. find number of Columns & Rowsnumcol <- ncol(training_main)  numrow <- nrow(training_main)attach(training_main)  NA_count <- as.data.frame(sapply(training_main, function(x) sum(is.na(x))))  for(i in 1:numcol)    {       if (NA_count > 0)    {    for(j in 1:numrow)    {        if(is.na(training_main[j,i])          {            training_main[j,i] <- as.character(training_main[j,i])             training_main[j,i] <- ""Empty""             training_main[j,i] <- as.factor(training_main[j,i])              print(""empty printed"")           }       }     }    }   Is the nested for -if - for - if as mentioned below  is allowed in R? That is the logic i have to use. Is there any other way i can run this code?";[education, open-source];53;
6281;1;2015-06-30T14:48:40.600;Is it possible to use central imputation in R for categorical variable?;Could you please let me know how can i use the centralImputation in R for categorical variable? What value should the empty values should be replaced ideally while handling missing values?;[education, open-source];14;
6282;2;2015-06-30T15:04:10.570;;"try RSelenium. with phantomjs since the date is requested and filled in by ajax calls. so any static web scraping tools wont work.I managed to get the list on the first page. http://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-headless.htmlsample of what i managed to pull.remove( mopub, m, run , rx, x , first1)library(RSelenium)pjs<- phantom( pjs_cmd=""C:/Users/bhavin.patel/Downloads/phantomjs-2.0.0-    windows/bin/phantomjs.exe"")Sys.sleep(5)remDr <- remoteDriver( browserName = 'PhantomJS')dsurl <- ""http://en.journals.sid.ir/SearchPaper.aspxstr=project%20management""remDr$open()remDr$navigate(dsurl)allt3 <-remDr$findElements('id', 'Table3')lapply( allt3 , FUN=function(dst){ dst$getElementText(); })[[1]][[1]][[1]][1] "" 1 :   EFFECTIVE FACTORS ON RURAL PEOPLE’S NON-PARTICIPATION OF     MAHABAD’S DAM CATCHMENT IN WATERSHED MANAGEMENT PROJECTS\nAuthor(s): RASOULIAZAR SOLEIMAN*,FEALY SAEID\nJournal: INTERNATIONAL JOURNAL OF AGRICULTURAL MANAGEMENT AND DEVELOPMENT (IJAMAD)\nNumber: MARCH 2015 , Volume  5 , Number  1 ; Page(s) 19 To 26.\nKeyword(s): NON-PARTICIPATION, CATCHMENT, WATERSHED MANAGEMENT, MAHABAD TOWNSHIP, IRAN\nReference(s):  (0)      Citation(s):  (0) FullText:""";;;
6283;2;2015-06-30T17:45:52.297;;There are many great ways to handle this problem. It is a recommendation problem, not a classification problem, as pointed out by others. There are many ways to do recommendation with a data set like this. I'll point out a few methods and you can choose one or try them all.The first method is called user-based collaborative filtering. The basic idea is to give users recommendations based on the tastes of like-minded users. So, you'd be trying to recommend music based on the listening history of users who have listened to the same songs. Such data can be modeled as a graph or sparse matrix. Then, you choose the exact algorithm depending on how you want to model your data. The second method is called item-based collaborative filtering. Rather than associating users together, this strategy looks at the set of items a user has 'rated' (the songs a user has listened to) and calculates how similar they are to a specific target item (song), or even to all the songs in your data set. It grabs the set of most-similar items and uses various methods to predict how much a user will like the song. In this case, you only have binary data (user listened to it or they did not). These calculations tend to work best with actual rating scores (like a 5 star system) because this gives more detailed variation amongst items in the data set. The third option is to model your data in a graph database like Neo4J and write graph traversal queries in order to find similar items. If you like graph theory, this can be a lot of fun. The sky is the limit in regards to what kinds of traversals will return good results. To get started, think of the users and songs as nodes in the graph, and 'listened' as the edge. $user->listened->$song  Because of ratings and item-based filtering, and because there are probably many songs in your data set, and each user only listens to a very small portion of them, I'd first try a user-based collaborative filtering method which uses sparse matrix operations to calculate recommendations. If your data set is large, these computations scale horizontally so you can leverage parallel processing if you run into performance issues.You can find more detail about collaborative filtering in this paper: http://files.grouplens.org/papers/www10_sarwar.pdf;;;
6287;2;2015-06-30T20:09:09.820;;Error is due to missing ')' in the second if statement in your code.if(is.na(training_main[j,i]) change to if(is.na(training_main[j,i]));;;
6288;2;2015-06-30T20:15:50.307;;Probably.  It depends on what you are doing exactly, and in particular which summaryFunction you are using.  If it's twoClassSummary for common binary classification then you should be able to specify metric='Sensitivity'.  If that's not the case you also have the option of overriding the default performance summaryFunction with your own implementation that calculates the metrics you desire.I highly recommend reading through the documentation at http://topepo.github.io/caret/training.html#control ;;;
6289;2;2015-06-30T21:52:02.733;;Another workaround is to get the listing by POST requests using curl in bash.You can get the curl post statement from Firebug ( Firefox F12 ) under Network , filter for XHR requests and copy the last statement which requests SearchPaper.aspx?str=project+management (right-click -> copy curl-adress).In this post request statement you have to increase the parameter ctl00$ContentPlaceHolder1$txtPageNo  to a desired pagination number (1-6 in this case).Then parse the output to a static xml parsing tool to get your data.;;;
6290;1;2015-06-30T22:01:36.153;Denormalise data in Neural Networks;I wrote a Neural Networks prediction model in Python.My data has a few inputs and two outputs. In order to make it work, I have to normalise every column on data for good prediction results.However, I had an issue. I run several times the prediction model with the same inputs to get the average and standard deviation of it. But obviously, those are also normalised to 0-1. On the test data I knew the min and max, so I could denormalised them. On the prediction values, I cannot the min and max real value.How do you solve this kind of problem and if you cannot, are there any other decent prediction techniques without the need of normalisation?;[education, open-source];53;
6291;2;2015-07-01T01:45:12.260;;Why not use the same statistics to denormalize the results? If the test data is good enough to train a neural network then why would its statistics not be good enough to denormalize the results?However, I would suggest not using min and max as the scale factors. The min and max can be very sensitive. I suggest using robust estimates of the mean and standard deviation to normalize. ;;;
6292;1;2015-07-01T06:13:41.820;Is there any working alternatives to Netvizz?;I'd like to download my friends data as a graph/network from Facebook. In Feb 2015, Facebook banned the awesome application Netvizz. Do you know any alternative application for my purpose?;[education, open-source];24;
6293;2;2015-07-01T06:16:19.477;;Do you run your analysis algos in batch or live? Which programing language under which environment do you use?At a first naive look, i would recommend to parallelize your code as each indicator calculation seems to be independent of the other's.;;;
6294;1;2015-07-01T06:18:06.550;Algorithm/Analysis that utilises incremental information?;I am looking for techniques which utilise information in an incremental manner. Example: A day with inclement climate is likely to be followed by another day with inclement climate. Or when an entire dataset sorted by date is available, an algorithm able to identify that a person is likely to call in sick when the previous day had bad weather.Is there any effective analysis which utilises this prior information where not all events are completely independent?;[education, open-source];33;
6295;1;2015-07-01T08:44:01.663;Finding both count and average of a column in R data.table, after group by;I have a data table of 5 columns. I want to do a group by on one column  and want to get the count of another column, id.I do this in this way:  dt[,length(id),by=dt$cid].If I also want to get sum of id, how do I proceed?I tried this:dt[,(sum(id) length(id)),by=dt$cid]but it did not work. Could you please help out.Thanks;[education, open-source];89;
6297;1;2015-07-01T10:52:34.003;NA/NaN/Inf in foreign function call (arg 1) while running RFE;"I am trying to find the important variables using RFE package using the below code:results <- rfe(training_main_part1[,1:313], training_main_part1[,314], sizes=c(1:50), rfeControl=control)  I am getting the below error.Error in { :   task 1 failed - ""NA/NaN/Inf in foreign function call (arg 1)""I made sure that I dont have NA/NaN/Inf values using is.na/is.nan/is.infinite. Any suggestion on this please? Did i miss out something?";[education, open-source];14;1
6298;2;2015-07-01T10:56:58.213;;Go for NodeXL SocialNetImporter pluggin;;;
6300;2;2015-07-01T14:44:13.520;;Use tapply,with(dt,tapply(id, cid, FUN=sum));;;
6301;1;2015-07-01T15:52:15.467;python - Will this data mining approach work? Is it a good idea?;"I need to extract fields like the document number, date, and invoice amount from a bunch of .csv files, which I believe are referred to as ""unstructured text."" I have some labeled input files and will use the NLTK and Python to design a data extraction algorithm.For the first round of classification, I plan to use tf-idf weighting with a classifier to identify the document type - there are multiple files that use the same format.At this point, I need I way to extract the field from the document, given that it is X type of document. I thought about using features like the ""most common numbers"" or ""largest number with a comma"" to find the invoice amount, for example, but since the invoice amount can any numerical value I believe the sample size would be smaller than the number of possible features? (I have no training here, bear with me.)Is there a better way to do the second part? I think the first part should be okay, but I'm not sure that second part will work or if I even really understand the problem. How is my approach in general? I'm new to this kind of thing and this was the best I could come up with.";[education, open-source];84;
6303;2;2015-07-01T20:15:53.563;;You can use tapply or aggregate along with a custom function. aggregate(Id~cid, data=dt, FUN=function(x) c(mn=mean(x),ln=length(x)))You may like to see my blog where I discuss few other scenario.http://r4sqlminded.blogspot.com/2015/06/selecting-data-from-data-frame.html;;;
6304;1;2015-07-01T20:18:20.783;Cross validation for C5.0 algorithm;I want to try K-fold cross validation in R for C5.0 algorithm,The following is the code i use. Can someone suggest me how can i include k-fold as well?Classifi_C5.0 <- C5.0(TARGET ~., , data = training_data_SMOTED, trails = 500,                      control = C5.0Control(minCases = mincases_count,                                            noGlobalPruning = FALSE))Is it required to do k-fold cross validation for Random forest? ;[education, open-source];23;
6305;2;2015-07-01T20:34:48.350;;I am not sure if using a classifier is the best way to approach this problem.  If it is something which can be easily extracted using regex, then that is the best way to do it. If however, you want to use classifiers, here are two questions you need to ask yourself.One, what does the unlabelled data look like and can you design good features from it? Depending on the kind of feature vector you design, the complexity of the classification task may range from very easy, to impossible. (A perceptron cannot solve XOR usually, except when you provide it with specific linear combinations of the input variable).Two, what does the labelled data look like? Is it representative of the entire dataset or does it only contain very specific types of format? If it is the former, then your classifier will not work well on files which are not represented in the labelled data. If you just want to test run a classifier first, you can solve the problem of having more features than training samples by using Regularization. Regularization forces the training algorithm of the classifier to accept the simplest possible solution (think occam's razor). Almost all Machine Learning related packages in Python will have regularization options you can use, so enjoy. ;;;
6306;2;2015-07-01T20:50:14.187;;It looks like you are looking for time series based Machine Learning (or Sequential Machine Learning as Emre pointed out). This means that you have an entire dataset available to you beforehand. You train on this dataset and then use it to predict next day's weather given the weather of the last n days. Please note that you do not learn incrementally here, i.e. you do not learn from the new data that is rolling in every day. The learning is limited to the dataset that you had beforehand. Widely used algorithms for this kind of learning are Conditional Random Fields(CRF), Hidden Markov Models etc. Statistical methods like Autoregressive Models can also be used sometimes. A good starting point would be CRFs. If on the other hand you want to learn incrementally, read about online learning.  ;;;
6308;1;2015-07-01T21:12:52.027;How to convert a SQLContext Dataframe to RDD of vectors in Python?;I have a SQLContext data frame derived from pandas data frame consisting of several numerical columns. I want to perform multivariate statistical analysis using the pyspark.mllib.stats package. The statistics function expects a RDD of vectors. I could not convert this data frame into RDD of vectors. Is there a way to convert the data frame? Code: rdd = sqlCtx.createDataFrame(df_new) summary = Statistics.colStats(rdd)I am getting df_new from  df_new = df.applymap(lambda s: dic.get(s) if s in dic else s) #df is a pandas dataframeI am getting a PY4JJava error at the summary line. The issue is with the format of rdd. ;[education, open-source];97;
6309;2;2015-07-01T21:38:16.940;;"I think one solution for this might be using Matrix factorization, to perform some kind of collaborative Filtering. This way you will have not have to deal with the two concern points that you have, explicitly.To do this, create a [User X Action] Matrix W, where rows are users and columns are actions. W(i,j) i.e. the Matrix entry on i row and j column would be the number of times the user i has performed action j. The matrix entries where no data is given are treated as missing data values by most collaborative filtering algorithms. When you factorize this Matrix, you factorize it into two parts W= A*B. The A matrix will have rows are users and columns as latent dimensions. From this A, you get a feature vector for each User. You can then run clustering on these feature vectors to cluster the users. For more intuition about the factorization approach, look up ""Matrix Factorization for collaborative Filtering"" on google. Mahout has matrix factorization implementations that you can use.Using the action categories is another matter altogether. It depends on what a category means. Does it mean that every action inside a category is very similar to each other or that they just have one or two aspect of similarity. A very easy approach would be to factorize a [User X (Action + Category)] Matrix. This can be solved using the same implementation mentioned earlier. You can also have more fancy schemes, like fixing part of the Matrix B to be very similar to the [Action X Category] matrix. But these schemes will require you to write optimization code yourself.";;;
6310;2;2015-07-02T01:07:56.313;;"I cannot comment directly on how to accelerate the DE algorithm you are using, because I have not personally used it. I would however, like to suggest breaking up your problem into two different parts. The first part is to calculate the weights, by trying to predict the price on the next data-point given prices of ""n"" previous data-points. The second part of the problem is given the weights that you have determined, get the threshold which will allow you to get the best output ( i.e. sum of earnings). These two problems are somewhat decoupled (in my opinion), i.e. solving each optimally or suboptimally should provide you with an good overall solution. The first part is just standard least squares, you can find many references to accelerate it or parallelize different versions of it. In the second part, you can just run a grid search, to find upper-threshold (to sell) and lower-threshold(to buy). Each run in the grid search with a different set of parameters should have O(n) complexity. Grid search itself, can be parallelized very easily.";;;
6313;2;2015-07-02T09:55:32.537;;So, the following answer is just based on different opinions of collegues and professors from the field. I want to try to summarize it briefly:Sequential and online learning is mostly associated with Bayesian updating. The sequential learning is used widely for an order in time of the data, meaning that $x_1$ is coming always first, then $x_2$, then $x_3$ and so on. The Dataset each has a certain order in that sense. In contrast to that incremental may be a whole block of data at time x and another block of data at time y. While the block internally may be randomly ordered. Concerning online learning, the people mostly referred to a data stream, hence a online learning is always incremental learning but incremental learning does not have to be online. These are quite fuzzy definitions, and in my opinion there is not clear definition though. I still hope that helps.;;;
6314;1;2015-07-02T11:12:44.083;Improving data point accuracy using multiple observation at single point;Say at each point Pi, I am taking m GPS observations. Even though all readings are supposed to be the same, it is varying due to errors. I need to verify the integrity of these values at each point and improve the accuracy. Assume O11, O12 and O13 are observations at point P1 using devices D1, D2 and D3 resp. As an example My aim is to improve the accuracy of the observation or IOW how to arrive at the conclusion which observation is the nearest correct?What is the best algorithm or method to achieve the same.;[education, open-source];19;
6315;2;2015-07-02T13:14:48.217;;In hadoop etc folder,hdfs-site.xml was having number of replication units as 1.That is why,all the files were getting saved on one single node.I changed it and the problem resolved.;;;
6316;1;2015-07-02T13:16:43.333;Weka class attribute suggestion;We are trying to run J48 on a classified data set. Our class attribute has two possible values ( 0,1) when running J48 the tree terminates at the very first node and doesnt process any further.Instead of considering (0- false) as the starting point of J48. How can we consider running J48 by selecting (1-true) as the starting point of the tree?Any suggestion will be greatly appreciated.;[education, open-source];11;
6318;1;2015-07-02T14:36:59.107;Data science projects explained step by step?;I am looking for a website or book where several practical examples are given step by step, explaining how they choose the relevant features, the model selection procedure, etc...;[education, open-source];224;5
6321;2;2015-07-02T15:46:37.280;;I can recommend this collection of Ipython Notebooks which includes Data Science, Statistics and Machine Learning commented notebooks.https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-NotebooksCheers,Thomas;;;
6322;1;2015-07-02T16:45:35.833;Clustering uncertain data with independent uncertainty per dimension;I have $n$ objects located in a $d$ dimensional space, however I do not know their exact coordinates. For each object and each dimension, I have a set of noisy measurements of the coordinate.I would like to cluster this data in the For example, with $n = 3$ and $d = 2$, I could have access to the following data:object $a$, dim 1: 0.8 0.7 0.6object $a$, dim 2: 1.0 1.0 1.0 0.9object $b$, dim 1: 0.4 0.3object $b$, dim 2: 0.2 0.1object $c$, dim 1: 0.9 0.6object $c$, dim 2: $\emptyset$In my current approach, I take the for each (object, dimension) pair that has data the average value, and I imput the average value of the dimension for the missing data.So I would get$a$ [0.7 0.975]$b$ [0.35 0.15]$c$ [0.75 0.6] (where $0.6 = \frac{1+1+1+0.9+0.2+0.1}{6}$)Then I use the scikit-learn python library to run the mean-shift algorithm and obtain clusters.I am not completely satisfied with this methods for two main reasons (maybe they're the same):an (object, dimension) pair with a single observation is treated as as reliable as one with numerous observationsthere is a discontinuity between how a pair with zero observations and a pair with some observations are treated. In the second case, the value of the other objects does not influence the attribute at all.My questions are: What would be a more principled approach to this problem? If I need to use another algorithm, are there open source decent quality libraries available that would implement it?I currently use $d = 256$ and $n = 10$ for my testing, but I aim to use $d = 16000$ and $n = 1000$ (but maybe a smaller $n$ if it's unrealistic) in my target application.;[education, open-source];36;1
6323;2;2015-07-02T16:58:29.833;;I would say cross validation is unnecessary here since the multiple partitioning of the data and variables is already implicit in Random Forests.   But it's still a good practice to hold out a testing set that is distinct from the training set.  This is mostly because you may introduce changes in your random forest to improve the performance on the test sets overall, thereby introducing the bias that the random forests are trying to overcome.   So if you withheld a portion of your data and judged the final performance of the RF on that withheld set only in the predict step, then it's fine.;;;
6324;2;2015-07-02T18:51:51.377;;One place you might find some interesting step-by-step explainations is the Kaggle tutorial and winner's interviews. Often people will post a detailed summary of their approach.;;;
6325;1;2015-07-02T20:41:18.900;Joint spatial clustering: How to force clusters to minimally contain datapoints from all datasets;I want to jointly cluster datapoints coming from different datasets (50 datasets with around 2000 points each). I would like to then extract information associated to the datapoints belonging to the different clusters to compare aspects of the datasets.Now my problem is that if I just place the datapoints originating from the different datasets into the same feature space, too few clusters comprise datapoints coming from all datasets simultaneously, so that after filtering I'm left with too few clusters (I tried with kmeans and similar methods).My question is: Which is the best way to jointly cluster my points while imposing the condition that a given cluster should contain points from all datasets? The ideal solution would also allow some outliers to not fulfill this condition. The first thing I could think about is to define distances between points and clusters which are updated depending on whether a point belonging to the same dataset is already present in the cluster? Seems too far fetched though.I would appreciate any ideas, thanks a lot in advance!Edit: Three of my six features are spatial coordinates and ideally I would also want the clusters to be connected within a given dataset.;[education, open-source];36;
6326;2;2015-07-02T22:13:46.977;;There are two separate issues and you should be able to combine the two following ideas to solve your problem.Multiple measurementsWhat if you treat all the measurements as separate data points during the clustering and just cluster all the data together. Run your favorite clustering algorithm. Then, if one of your objects has measurements that get clustered in different clusters that will provide you with a measure of the fuzziness of the object. Assign the object to the cluster with the most measurements assigned to it.Incomplete measurementsThis one is trickier. You don't just have missing data points, but you have missing dimensions. Here is a dissertation that starts to address some of these issues. The literature review may point you in the right direction.My initial idea would be to cluster the data sequentially in order of subspace rank. That is, first cluster all data points with similar rank d together, then use those cluster labels as the initial clustering to rank those with rank d-1, then rank d-2, etc.;;;
6327;2;2015-07-03T01:57:51.177;;I had the same question a few weeks ago.I personally found O'Reilly's Python for Data Analysis very useful in learning the basics. The book assumes you have some python programming experience, but it also has an appendix in the back to go through the basics.The author gives you a wide variety of real world (not Monty Python) examples in the beginning you can create within the first few chapters, then goes into detail about each thing as the book goes on, building your knowledge.I found the instructions very easy and step by step. My professor who is my guide in all this was impressed how quickly I learned. I also have heard good things about Kaggle.;;;
6328;1;2015-07-03T02:27:59.847;matplotlib geostrophic wind plot;"I am working on an assignment that is teaching how to plot and label using matplotlib using Python. Science or math is not my background. I have been given the formula for calculating the geostrophic wind and we are to plot it (on the y-axis) versus the latitude on the x-axis.I know how to plot give an x and a y. Beyond that, the formula is not making sense to me given my lack of background in the area.Furthermore, there is a key to translate the symbols to the correct python code, but I don't know the English names to the symbols, making it very difficult.For example, I am given r'$x^{10}$'r'$R_^{final}$'r'$alpha^{\eta}$' The first two are superscript and subscript. That I understand. But how this helps with the formula calculations I do not know. I am given the values to put into the formula as well. An explanation of the order of operations would help.If,g0=9.81 ms-­‐2;ΔZ=60m;Δn=2x10^5m;andf=2Ωsin(φ) My question is how do I put the values into the formula and then plot them in matplotlib? is it as easy as x and y?";[education, open-source];42;
6329;2;2015-07-03T05:38:17.023;;If the set that you are using the RMSE on is a linear space, a good reason to use the square root is that you turn the set into a metric space. The square root ensures the right scaling property. Essentially, the RMSE is equivalent to the Euclidean norm. As a benefit, it is possible to use results of the general theory of metric spaces.;;;
6330;2;2015-07-03T06:47:43.017;;Model the measurement processFor each dimension you could attempt to model the distribution of the measurement process variance. Perhaps every dimension has the same measurement process or perhaps it is distinct. Combine all measurements from all dimensions if the process is the same.If you have enough data then you can build an empirical model of the distribution of errors using perhaps a kernel based estimator.Otherwise you might choose to use a Gaussian distribution formed from the mean value of each measurement and an overall measure of the variance of the measurement process. Then you replace observations you have with a sample drawn from a  distribution with the observation mean and overall measurement process variance.;;;
6331;1;2015-07-03T08:52:36.247;Storing Sensor Data for Analysis of the Office;I have currently been tasked with designing an application that tracks several different measurements around the office, eg. the temperature, light, presence of people, etc. Having never really worked on data analysis before, I would like some guidance on how to store this data (which database design to use).What we're looking at currently are around 50 sensors that only send data when an event of interest occurs: if the temperature changes by 0.5 degrees or if the light turns on/off or if a room becomes occupied/vacant. So, the data will only be updated every few seconds. Also, in the future, I'd like to analyse some of the data. Hence, the data must be persistent in the database. What kind of technologies would you suggest to carry out this task?;[education, open-source];21;
6333;2;2015-07-03T12:08:21.970;;"Let me start with: I don't know how appropriate this is for Data Science StackExchange. But I can't comment yet and think I can provide an answer, so here goes.Putting the values into the formula. The quantities are:$g_0$: Standard gravity (9.81 m/s2);$\Delta Z$: Change in geopotential height over some distance;$\Delta n$: The distance over which you calculated $\Delta Z$; and$f$: The Coriolis parameter, measuring the rate of revolution at$\phi$: The chosen latitude.You wrote the equation that links the Coriolis parameter $f$ to the latitude and to $\Omega=7.29\times 10^{-5}$ s$^{-1}$. Let's calculate this part first. I'm assuming that a) you can use numpy along with matplotlib, and b) you were asked to plot the geostrophic wind as a function of latitude. One way to enter this into Python is by writing:> import numpy as np> phi = np.arange(10, 91)> Omega = 7.29e-5> coriolis = 2 * Omega * np.sin(phi * np.pi/180)The second line gives you a numpy array of latitudes, in degrees, from 10N to 90N. I left out the tropics, since $f=0$ at the equator; the geostrophic wind isn't well-defined there. The third line defines Omega, and the last line gives you a numpy array of values for $f$. Note that the argument of np.sin should be in radians, not degrees.After doing this, the rest of the values are easy to input. For the order of operations, make sure everything in the numerator is grouped together and same for the denominator. You get:> grav, dz, dn = 9.81, 60.0, 2.0e5> VGeo = (grav * dz) / (coriolis * dn)The first line uses a sequence of values to set grav ($g_0$), dz, and dn. The second line defines a numpy array for $V_g$, with the numerator and denominator grouped.Plotting out the result. I don't know how familiar you are with the workings of matplotlib. Let me know if this doesn't make sense and I can expand on it. Many plots of something that varies in latitude have latitude on the y-axis, since that's the way globes and maps are oriented. So, when you make a plot of $(x,y)$ values, $y$ should be $\phi$ and $x$ should be $V_g$. A working example with some labels, using pyplot, might be:> import matplotlib.pyplot as plt> fig, ax = plt.subplots()> ax.set_ylim(0,90)> ax.set_xlim(0,125)> ax.plot(VGeo, phi, linestyle='-', color='k')> ax.set_ylabel('Latitude $\phi$')> ax.set_xlabel('Wind $V_g$')> ax.set_title('Geostrophic wind $V_g = \frac{g_0 \Delta z}{f \Delta n}$')> fig.savefig('test.png', format='png')I'm doing several things here. The second line creates a figure and axes using plt.subplots(); without any arguments, this returns a single axis. Next, I'm setting the x- and y-limits so that we see all the data and so that the y-axis starts at the equator (even though we don't plot values south of 10N, I think it looks better). Then I'm just plotting $(x,y)=(V_g,\phi)$, with a solid black line. The next three lines involve creating labels using TeX formatting. The last line is saving the figure to a local PNG file.Let me know how it works out!";;;
6335;2;2015-07-04T02:27:33.023;;The most logical way to transform hour is into two variables that swing back and forth out of sink. Imagine the position of the end of the hour hand of a 24-hour clock. The x position swings back and forth out of sink with the y position. For a 24-hour clock you can accomplish this with x=sin(2pi*hour/24),y=cos(2pi*hour/24).  You need both variables or the proper movement through time is lost.  This is due to the fact that the derivative of either sin or cos changes in time where as the (x,y) position varies smoothly as it travels around the unit circle.Finally, consider whether it is worthwhile to add a third feature to trace linear time, which can be constructed my hours (or minutes or seconds) from the start of the first record or a Unix time stamp or something similar.  These three features then provide proxies for both the cyclic and linear progression of time e.g. you can pull out cyclic phenomenon like sleep cycles in people's movement and also linear growth like population vs. time.Hope this helps!;;;
6336;2;2015-07-04T05:44:08.447;;I have been doing similar project in my college. I have classroom and I'm supposed to collect data like temp, humidity, light, occupancy, etc. Assuming that you have worked with sensors and motes to use, I'm going to explain rest of the structure.You need sensor network setup and like you said you have done it. These sensor networks generally do not send data directly over internet so you need a Gateway that can collect data from sensors and send it over internet to local server.On server side you need REST API and you could use any language to develop it and I use PHP. I find it very easy to use and develop using PHP. This REST API shall receive data from Gateway and store it into database. I use mysql database because amount of data is not so big for us. But if your data is big enough you can use big data Nosql tool like mongoDB or so. Whatever type of database you use structure remains same. For sending data from Gateway to server you can use protocols like HTTP or MQTT whichever you feel comfortable. What I do is I have WSN controller that sends data over USB to Gateway then Gateway sends data to server over Ethernet. So I had to develop USB to Ethernet Gateway. If you can just take two UART terminals out of your controller you can build UART to Ethernet Gateway using any microcontroller or even Arduino Ethernet shield would work in that case.In my case data is sensed periodically but as you said you are sensing data when event of interest occurs then you can use poisson distribution method over periodically collected data to predict what is average number of events per day and then you can decide if your data is big or not.;;;
6338;2;2015-07-04T06:26:36.840;;Your answer was incorrect because the axes were flipped and it didn't plot the entire line.This was the correct answer I was looking for.import matplotlib.pyplot as plt import numpy as np# define the parametersg = -9.81 # m/s^2dZ = 60 # mdx = 2e5 # momega = 7.2921e-5 # rad/sphi = np.linspace(10,90) # degf = 2 * omega * np.sin(np.radians(phi)) # coriolis frequency, s^-1# compute geostrophic wind, x-componentu_g = -1. * g/f * dZ/dx# plot phi vs V_gfig, ax = plt.subplots()ax.plot(phi, u_g, color='#FF33CC')ax.set_xlabel('$\phi$')ax.set_ylabel('$V_{g}$ (m $s^-1$)')plt.title('Geostrophic Wind vs. Latitude')plt.show();;;
6339;1;2015-07-04T11:08:18.030;Questions for data analysis of a class room;I have Wireless Sensor Network setup in a classroom. I'm sensing data like temp, humidity, light, occupancy, etc. Classroom can accomodate 150 students and has 6 AC with light. From one side of classroom has big windows and that side is towards west and other 3 sides have thick walls and doors always closed. I have been looking for some questions that can be answered by analysing data collected. Some of the questions I can think of are,Predicting optimum time to turn on AC before class starts so that class is attains required temp with least energy consumed.As of now I have 9 sensors on all walls and ceiling too. So to decide best position for sensor such that I can co-relate and predict other sensor values from only one or two sensor values. Decide optimum amount of light needed by occupants to be comfortable in classroom.Please suggest me more questions that can asked about this situation. Also suggest if I can use somehow machine learning in WSN I have deployed.;[education, open-source];29;
6340;1;2015-07-04T12:18:51.560;What are the performance measures in the neural networks field?;I constructed a neural networks in R using neuralnet package.I want to test that using cross-validation, that is a technique based on using 4/5 of the dataset to train the network and the fifth one as the test set.I wonder about what measures I should use to measure the neural networks performance in terms of predictability.Could you suggest what measures are commonly used in the field and explain me why?Any hint and ideas about that will be appreciated.;[education, open-source];58;
6341;1;2015-07-04T21:55:51.210;How are clusters from DBSCAN sometimes non-convex?;"I've been using clustering in my bag of ML techniques for quite some time now, and  I've never found a satisfying answer to this question.  In DBSCAN, we define a maximum radius with which to form clusters.  The algorithm will scan the space and group together points that are ALL reachable from one another.  However, we can sometimes end up with a non-convex cluster.  My confusion is around how the notion of a ""radius"", which describes a convex object, can be an input to an algorithm which results in a non-convex object?";[education, open-source];44;
6342;1;2015-07-05T01:07:42.470;Spatial clustering based on response to inputs and building a reduced model;"Stats version: I have a few measurements of a function that takes three inputs and produces a few 2D fields of outputs: $f_i(a,b,c;x,y)$, with $f$ being a vector of several quantities. I would like to cluster the points in $(x,y)$-space into regions that have a similar response to the input variables $(a,b,c)$. If I treat the points independently, applying a clustering algorithm is fairly easy. (I can elaborate on which one I'm using if you think it's relevant, but I'm open to suggestions on your favorites.) But there's no guarantee that the clusters are near each other spatially. Having contiguous clusters of roughly equal size is definitely a plus. What are some ways to modify the approach to add in that constraint?Science version: I'm running an atmospheric chemistry model and varying emissions over the US. In that setup, $(a,b,c)$ are the national-total emissions of NOx, SO2, and NH3; $(x,y)$ are grid points over the US; and $f_i$ is a vector of the various chemical species I'm tracking. I've varied $(a,b,c)$ in a number of simulations. Now I want to divide the country into regions that have broadly similar responses to the emissions changes, and create simplified models for a manageable number of regions. I'm particularly interested in nonlinearities/mixed-effects/second-order derivatives in the chemistry response. It's possible that different regions of the country do respond similarly to emissions even if they're not near each other, but the different regions should have some sort of spatial agglomeration. Thoughts?Side question: I originally played around with this a few years ago with Matlab code but have since migrated to Python. Suggestions on how to efficiently use Python stats packages for this sort of problem?";[education, open-source];26;1
6343;2;2015-07-05T08:44:07.217;;I think it's non-convex because the particular cluster assignment you get when applying DBSCAN depends on the order you traverse the data.Let's try to illustrate it with an example. Consider this dataset:You want to run DBSCAN with radius $r = 3$ and $\text{min_pts} = 4$, so you get this: The point in the center is not a core point because it has only 3 points, not 4, and we have only two core points. And depending on how you traverse the data points, you may get different cluster assignments: The top picture shows the result we'd get by traversing left-to-right, and the bottom picture - by traversing right-to-left.Apparently both these results would correspond to the same value of the cost function, thus the cost function has several minima and it is not convex. ;;;
6344;2;2015-07-05T13:22:46.627;;"A cluster in DBSCAN consists of multiple core points.The radius is the area covered by a single core point, but together with neighbor core points the shape will be much more complex. In particular, they can be much larger than epsilon, so you should choose a small value, and rely on this ""cover"" functionality.Wikipedia has an example of a non-convex cluster";;;
6345;1;2015-07-05T15:10:56.790;Pivoting a two-column feature table in Pandas;How can I transform the following DataFrame into one with cities as rows and each cuisine as a column, and 1 or 0 as values (1 if the city has that kind of cuisine)? I think this turns out to be a very common problem in transforming data into features for machine learning.I am aware of the Pandas pivot_table functionality, but it asks for a value column, and in this case we don't have any.import pandas as pddata = {    'city': ['NY','NY', 'SF','SF','SF'],    'cuisine': ['Japanese', 'Chinese', 'French', 'Japanse', 'German']}df = pd.DataFrame(data);[education, open-source];27;
6346;2;2015-07-05T15:50:02.373;;If you want an application-oriented book, consider Christopher Bishop's Model-Based Machine Learning. He has more technical books that are well regarded.If you are looking for lots of code, Probabilistic Programming & Bayesian Methods for Hackers is an option.Another introductory book with a more statistical bent is An Introduction to Statistical Learning with Applications in R. Again, the authors have a well-regarded technical version of the book.;;;
6347;2;2015-07-05T17:10:14.417;;The best choice for storage technologies will depend largely on how much data (in terms of bytes) you expect to accumulate over the lifetime of your project, so the first thing i would do is try to get some sample data, or make some educated guesses (e.g. how many bytes does 1 temperature recording take up X how many change events am I expecting per day X how many temperature sensors X how many days worth of data you want to store and analyse over time).Once you have a rough idea of how much data you need to store and analyse, you can use that to start narrowing down your choices. There's no right answer, and others may disagree, but I would suggest that if you're dealing with anything less than terabytes of data, you don't need hadoop (I noticed that's a tag in your question) - hadoop is not really a data storage solution (although it does have it's own file system called HDFS or just DFS), it's more of a framework for processing and transforming huge quantities of data. Also if you don't have thousands of events per second to record, you probably don't need NoSQL solutions either.For storage of structured data, given that you've never really done data analysis before, SQL databases are probably the way to go if you have gigabytes or less, and SQL will be easier and more useful to learn - it's mature, been around for ages and is still the go-to standard in most industries, so there are plenty of learning resources. Maybe try out MySQL Community Edition server (free, open source) as a start, I would also recommend the MySQL Workbench to help you get started (a bunch of GUI tools you can use to mess around with SQL when learning)PS I don't know anything about capturing signals from sensors, so maybe there are more appropriate technologies which I'm not aware of!;;;
6348;2;2015-07-06T06:49:46.487;;Typical predictive performance measures used to compare accuracy of ANN models are:RMSE - (root mean squared error) measures the distance between estimated and actual outcomes. Other metrics that measure the same concept are MSE, MAE or MPE.R square - (R^2 or coefficient of determination) measures the reduction of variance when using the model.When comparing two different ANN models for performance, metrics that take into account the complexity of the model may be used, such as AIC or BIC.;;;
6349;1;2015-07-06T09:40:59.947;What should I care about while stacking as an ensemble method?;I'm using SMO, Logistic Regression, Bayesian Network and Simple CART algorithms for classification. Results form WEKA: Algorithm               Sensitivity (%)       Specificity (%)         Overall accuracy (%)Bayesian Network            57.49                 76.09                    65.24Logistic Regression         64.73                 69.86                    66.87SMO                         54.32                 79.20                    64.69Simple CART                 71.88                 61.51                    67.56SMO gives the best result for my classification problem, since it correctly classify the 79.20% of the class which is important for me. I want to increase this accuracy by stacking. I tried to combine some of them. In most of the cases I couldn't increase the accuracy but stacking SMO with Logistic Regression made a little increment in accuracy. How can I explain why stacking SMO with Logistic Regression is better than others? Is there any generalization such as combining tree classifiers gives good result in stacking? What should I care about while stacking? EDIT:                                Bayesian Network    Logistic Reg.   SMO         CART Kappa statistic                   0.3196             0.3367         0.3158      0.3335 Mean absolute error               0.3517             0.4164         0.3531      0.4107 Root mean squared error           0.5488             0.4548         0.5942      0.4547 Relative absolute error (%)      72.3389              85.65        72.6299      84.477 Root relative squared error (%) 111.3076            92.2452       120.5239     92.2318 Weighted Avg. of F-Measure        0.653               0.671          0.676     92.2318 ROC Area                          0.725               0.727          0.668       0.721Total number of instance is 25106. 14641 of them is class a, and 10465 of them belong to class b.=== Confusion Matrix of Simple CART ===     a     b   <-- classified as 10524  4117 |     a = 0  4028  6437 |     b = 1=== Confusion Matrix of SMO ===    a    b   <-- classified as 7953 6688 |    a = 0 2177 8288 |    b = 1=== Confusion Matrix of Logistic Regression ===    a    b   <-- classified as 9477 5164 |    a = 0 3154 7311 |    b = 1Since SMO is successful at class b and CART is successful at class a, I tried to ensemble these two algorithms. But I couldn't increase the accuracy. Then I tried to combine SMO with Logistic Regression, the accuracy is increased a little bit. Why ensembling SMO with Logistic Regression is better than ensebling SMO with CART, is there any explanation?;[education, open-source];109;1
6350;1;2015-07-06T10:10:34.600;"Definition of ""inside"" in K-means?";"After conducting a cluster analysis using K-means, I have new data coming online that I need to detect anomalies with.  Anomalies are assumed to not be within the clusters.So, how is one to define ""inside a cluster"" in K-means?";[education, open-source];69;1
6354;2;2015-07-06T13:13:24.633;;"It's inside a cluster if it is part of the partition of the respective Voronoi diagram. This is a visual explanation that translates to ""a point is inside cluster A if it is closest to the centroid of A (compared to all other centroids).""If your clusters don't have infinite boundaries and outliers shouldn't be in any cluster at all, you might need to refine your approach to something else than k-means that detects outliers.";;;
6355;1;2015-07-06T13:42:12.230;Theoretical bound - regression error;The Bayes error rate is a theoretical bound that determines the lowest possible error rate for a classification problem, given some data. I was wondering whether an equivalent concept exists for the case of regression algorithms. My aim is to determine how far my regression algorithm's error is from that theoretical bound, as a way to assess how far am I from the best possible solution. Is there any way to obtain a bound of the lowest regression error for a given dataset?;[education, open-source];80;1
6356;2;2015-07-06T17:11:49.963;;"If there is no value column - introduce it yourself!df[""value""]=1pd.pivot_table(df, values=""value"", index=[""city""], columns=""cuisine"", fill_value=0) For your example I got (after fixing the misprint in 'Japanse' to 'Japanese')cuisine  Chinese  French  German  Japanesecity                                      NY             1       0       0         1SF             0       1       1         1";;;
6357;2;2015-07-06T20:12:07.840;;"With the Voronoi cells that runDosrun brings, it is true that they are likely unbounded, so your new data that is ""outside of the cluster"" isn't actually outside the cluster.  You should probably look at a different clustering algorithm if that is indeed what you are looking to do.  If you are intent on using k-means, you could attempt to classify anomalous data into its own cluster.There are libraries out there that you can use to constrain the boundaries of the k-means during cluster analysis to the initial domain.  Without knowing what environment you are trying to do this in or what the context of the data is, it would be really difficult to define ""inside a cluster"" with out you specifically choosing certain boundaries.";;;
6358;2;2015-07-06T20:24:27.203;;"Very difficult question for someone to answer as ultimately your examples and many questions contain certain constraints you won't get from your sensors.For example:Your first question is predicting ""optimum"" time to turn on AC.  who defines optimum?  Optimum for the classroom at full capcity, half capacity?Your second question looks like it one, requires a map of the classroom and where the sensors are, two you won't be able to predict 7 sensor values from 2 sensor values, I don't know why you would want to do this.Your third questions is a social sciences problem.  Define ""light needed to be comfortable"".  I imagine you would get 150 answers if you had 150 students.Your scenario lacks context/comparison.  Let's say you had sensors outside the classroom for your various data points.  Then you could do things like, can I predict the temp inside the room based on my other room and external data points.Finally, yes machine learning can be used with your wireless sensors.";;;
6359;2;2015-07-06T20:42:57.077;;I don't understand the purpose of imposing a condition that requires any cluster to contain at least some (let's say even 1) point from every dataset but at the same time find a solution that also allows that condition to be broken by outliers (I assume these are specific cluster outliers or data set outliers?).  Have you considered implementing an overlapping cluster algorithm?  You don't get the full distinctness of a traditional single membership but your might better fit your data to an algorithm that achieves the desired condition.  It will probably require some exploration of the data and testing for cluster membership after the fact.;;;
6360;2;2015-07-06T21:14:53.163;;Data Joy - Online Python & R EditorCollaborate with other people in real timePrivate VM's for each projectWork via  your browserIn built track changes/history featureDropbox & Github SyncChat to other usersPublish your project to its own page you can share, with comments rendered using markdownOpen source https://github.com/sharelatex/web-sharelatex/tree/datajoySource - I helped build this;;;
6361;1;2015-07-06T21:16:52.867;How to simulate customer walk-ins for a given period in a fast food chain in R;I would really appreciate it if someone could tell me where I would start in tackling the following tasks in R.I'm not a data-science expert and am trying to teach myself data-analysis from a background in physics. I have introductory statistics and have just finished Udacity's course in R programming and exploratory data analysis.Tasks:1 )Simulate customer walk-ins for a given period. The simulations should take into account peak volumes at different times of the days and differences in week days and weekends.2) Simulate customer purchases based on the customer walk-ins. Make assumptions on the average dollar price and variations for different times of the day.Thank you;[education, open-source];22;
6363;2;2015-07-06T21:26:01.297;;"You need either an initial data set or a large set of constraints to ""fake"" a data set.  if you have an initial data set you could make somewhat valid assumptions regarding traffic flow, purchasing behaviors and the likes.If not, you are going to need to research these different metrics from existing companies that are like what you want data for and then I assume put together a little program with those metrics as constraints and simulate your data.  Once you have those constraints you could use simple averaging or weighted options for developing a constraint per variable.Make sure they are semi accurate.  For example, if you were modeling company growth for a ""really good idea"" research growth trends for like companies and for companies you want to emulate and then scale BACK from there.You can also always purchase a dataset:http://datamarket.azure.com/browse/data";;;
6364;2;2015-07-06T22:37:50.287;;"Strictly speaking, the k-means algorithm does not have a definition for ""inside the cluster"" and is therefore not a great candidate for anomaly detection.  In k-means, every point is assigned to one of k clusters and then a new cluster centroid is calculated.  But as previous uses have pointed out, you could construct some sort of ad-hoc system where you process a set of data and then define new data as anomalous when its extends beyond 2 standard deviations of the centroid location. DON'T DO THIS! K-Means will not work well for this ad-hoc method either.  If k is poorly chosen, then the distribution within a cluster will not be normally distributed.  You very, very frequently see natural distributions of points which are split between two clusters.  I suggest you use another clustering method.  The first option that comes to mind is DBSCAN.  This allows one to set a threshold for noise and the cluster numbers are not set a-priori.  DBSCAN is therefor much more likely to return normal distributions within a cluster.Finally, I'll point out that the method you are proposing is not as good as other novelty and anomaly detection methods.  You should consider doing novelty detection using a single (or possibly even multiple) class support vector machine (SVM) with a nonlinear kernel. The nonlinear kernel will allow you to recover multiple ""clusters"" while the SVM will do much better at predicting which points are inside the class.";;;
6365;1;2015-07-07T00:21:53.520;Question about (Python/Orange) Apriori associative algorithm;"I'm trying to wrap my head around Association rules and frequent itemsets.So I threw my data in, instead of the samples one and sometime it works, sometimes it doesn't.  rules = Orange.associate.AssociationRulesSparseInducer(data, support = 0.3)print ""%5s   %5s"" % (""supp"", ""conf"")for r in rules:    print ""%5.3f   %5.3f   %s"" % (r.support, r.confidence, r)inducer = Orange.associate.AssociationRulesSparseInducer(support = 0.2, store_examples = True)itemsets = inducer.get_itemsets(data)print itemsetsprint data.domainprint [data.domain[i].name for i in itemsets[4][0]]More often than not, itemsets[4][0] shows an IndexError: list index out of range error.So I start playing around with support = 0.3, support = 0.5, support = 0.2 and itemsets[2][0] or itemsets[3][0].From the docs: support  Minimal support for the rule. Depending on the data set it should be set to sufficiently high value to avoid running out of working memory (default: 0.3).True - I tried 0.2 and it quickly blasted my memory on a 800 rows data file.Any idea what I should do best there or which are viable values for a shopping cart analysis?800 rows of data (800 orders)1 to x item(-categories) per order15 different item-categories in the file, so my data looks like:  ItemCat1ItemCat2, ItemCat2, ItemCat2, ItemCat2, ItemCat7, ItemCat7, ItemCat7,     ItemCat7, ItemCat7ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1,     ItemCat2ItemCat4, ItemCat4ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1, ItemCat1,     ItemCat1, ItemCat2ItemCat5";[education, open-source];26;
6366;2;2015-07-07T01:15:04.840;;If the size of your logs are still growing then a distributed data system is definitely the right way to go. I have been using Mesos in production for almost a year now and it solved the problem of if this processing happens on some server it under or over utilizes resources. I probably would look into some stacks like this:Mesos as your fault-tolerant and elastic distributed systems Spark or some Hadoop-based solutions for log processing and store the output in a DFS like HDFSHave your applications consume data stored in HDFS as final steps;;;
6367;1;2015-07-07T03:23:12.333;Similar Candidate Recommendations;I've just embarked on a project to recommend similar candidates to employers for a website that offers employers the ability to search its CV database. My background is in engineering research on biomedical images - from simple predefined feature extraction to CNNs.I'm about to start with a simple bag of words approach standardised by the frequency of appearance and see where that gets me.Has anyone done anything similar before and has suggestions for which algorithms might give good results?The data is very well structured, separated into seperate sections for each previous employment etc.;[education, open-source];15;
6368;2;2015-07-07T03:46:55.903;;Data Science in the Cloud with Microsoft Azure Machine Learning and R is a free textbook which works through an example in great detail. Don't be put off by the particular tools used as you don't need them to get some benefit out of the book. Another one which I enjoyed is Programming Collective Intelligence which also goes through a number of projects in detail, including the web scraping part which most books gloss over.;;;
6369;2;2015-07-07T06:50:32.313;;Read the following by MLWave:http://mlwave.com/kaggle-ensembling-guide/This is very good starting point to stacking / ensembles.;;;
6370;1;2015-07-07T09:23:45.250;How to use REST API to execute Map-Reduce Task?;Let's say I have one Text File (Size 1 GB). I want to search particular word from file and if it is found the Line number should be returned.I can Execute my java program using command line in linux. But what I want is some Interface using REST API. When I submit that word in GUI(My Interface) and submit request it should be able to call HDFS , commands should be provided by itself and it should return me the result. How Can you approach this problem ? (I have very primitive knowledge about REST API)Referred https://hadoop.apache.org/docs/r1.0.4/webhdfs.html#ParameterDictionary[]https://www.youtube.com/watch?v=7YcW25PHnAA;[education, open-source];38;
6372;2;2015-07-07T12:37:14.897;;Try content based filtering, you can cluster candidates based on similarity of keywords and by assigning similarity scores. Check tf-idf /cosine similarity measures. Additionally you can customize your algorithm by giving extra weightage for section wise similarities since your data is very well structuredAlso check collaborative filtering on Mahout ;;;
6374;2;2015-07-07T14:45:11.840;;It is not available in Orange, but i would try FP-Growth. Its way faster in combination with low support values than apriori!;;;
6375;1;2015-07-07T15:33:41.323;Trying to come up with a feature to improve emotion classifier based on facial movement using facial landmarks;I managed to create an emotion recognition system that uses dense optical flow on each entire frame. While the accuracy range is within 80-90% with cross-validation, I am aiming to improve the accuracy of the program. There are four emotions: Neutral, happy, surprised, and angry. So far my classifier works pretty well, though it tends to over guess 'neutral' when  the answer is 'happy' or 'surprised'. This tends to happen when the mouth is only slightly opened, but the subject is visible smiling or have mouth opened in shock while the classifier still thinks the mouth is closed.Confusion Matrix for Dense Optical Flow:   [[27 22  0  0]   [ 0 57  1  0]   [ 0 12 60  0]   [ 0  9  3 68]]  Accuracy: 80-90% rangeThere is something I want to try in order to solve this though.I have the ability to get the position of facial landmarks, though I don't know what I can do to turn this information into an effective additional feature I can use that would increase the accuracy. I was thinking of just simply getting the face landmark coordinates at the end of each video, but I feel like that it would not be the solution to differentiate between a closed mouth and slightly opened one (the difference in coordinate values will be small I think and am guessing that machine learning won't notice the difference).I considered the possibility of just simply taking a still image of the subject's mouth and just analyzing that, but rejected it as it vulnerable to factors like lighting and people's appearance, and inconsistent matrix sizes. Plus I want my additional feature to take advantage of facial movement tracking.I was wondering if there is a smart way to implement facial landmark tracking into a feature that would increase the accuracy of my classifier by dealing with my classifier's tendency to over predict the emotion 'neutral'. Any ways I can accomplish that?;[education, open-source];19;
6376;2;2015-07-07T16:06:07.573;;I'm going to agree with AN6U5 in general. However, in practice what I've done in the past is used k-means with cross validation and and Silhouetting to find a 'good' clustering of historical data. What I then did was take the clusters and calculated their covariance matrix so I could use the Mahalanobis distance. This allows me to take into account the actual shape of the distribution along each dimension in determining whether a point lies to far outside my previous examples. Since the Mahalanobis distance is in standard deviations, if its say 3 sigma outside each cluster its usually flagged for further study.  I've been able to get away with this since I know the general process for whats generating my data from testing and I have a rather large amount of data for known operating conditions, so I have pretty good faith in my historical data clusters.;;;
6377;2;2015-07-07T16:10:09.023;;I'm not sure I'm following your question completely, but for spatial type clustering maybe a Self-Organizing Map would meet your needs. Basically it maps out the space based on nearby activations. Side answer: I haven't used python;;;
6378;1;2015-07-07T16:26:18.230;Interpreting the evaluation result of multiple linear regression;I am learning the multiple linear regression model.I've built a model and using r command:summary(model)I got this result: Signif. codes:  0 ‘’ 0.001 ‘’ 0.01 ‘’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 253.2 on 44 degrees of freedom Multiple  R-squared:  0.3336,   Adjusted R-squared:  0.2579  F-statistic: 4.405 on  5 and 44 DF,  p-value: 0.002444How can I interpret this result in order to have a decision regarding the goodness of the model? In specific, what is the 44 degrees of freedom means for this case?Also, why do we have adjusted and multiple r squared parameters?;[education, open-source];12;
6379;1;2015-07-07T16:48:10.223;What is the proper way way to -log10 transform ggplot2 axis?;What is the 'proper' way to -log10 transform an axis in ggplo2 using the scale functions?  I can write my own transform function using the scales library, but something cleaner would be just combining scale_x_log10 + scale_x_reverse.I'd like to figure out something a bit more concise than the following: scale_x_continuous(trans=trans_new('nlog10_trans', transform=function(x) { -log10(x) }, inverse=function(x) { 10**x }) );[education, open-source];13;
6380;1;2015-07-07T18:05:23.903;How to avoid overfitting in random forest?;I want to avoid overfitting in random forest. In this regard, I intend to use mtry, nodesize and maxnodes etc. Could you please help me how to choose values for these parameters. I use R.Also if possible please tell me how can i use k-fold cross validation for random forest. in R.;[education, open-source];150;4
6381;2;2015-07-07T18:07:39.507;;It seems like you are overthinking this and getting caught up in your own head a little.We can define a restricted set of time series as those which are well behaved (non-asymptotic) and single valued.  These time series then form a time series algebra which have additive, multiplicative, commutative, and associative properties.  There is an identity element (straight line with value 1) and a zero element (straight line with value 0).  We even have a well defined inverse member for all time series not crossing zero.  I believe this even forms an Abelian group.Further, previous posters are correct that this group can be perfectly represented by various orthogonal vector spaces such as polynomials and Fourier space.  This is true whether we are talking about Lagrange polynomials, Fourier series, or Gauss-Lobatto-Legendre polynomials.The point then becomes... where does this get you? You got caught up in the suggestion to employ Fourier series in that you didn't want to remain in frequency space, but this is entirely unnecessary.  You can, for instance, Fourier transform into frequency space, apply a high frequency filter, and then transform back into the time domain.  This is incredibly effective in removing noise (although band pass filters tend to work better).Stepping back from the question a little, it seems like you might really be asking:  Given typical time series and typical time series operations (smoothing, de-seasoning, de-trending, etc) do the time series and their typical operations constitute a group with a well defined algebra?  The answer is, yes.  All of these operations just involve finding other time series which can be added, subtracted, multiplied, or divided by the original time series in order to yield a more physically tractable (interpretable) representation.;;;
6384;1;2015-07-07T23:15:34.657;What is the actual output of Principal Component Analysis?;"I'm trying to understand PCA, but I don't have a machine learning background.  I come from software engineering, but the literature I've tried to read so far is hard for me to digest.As far as I understand PCA, it will take a set of datapoints from an N dimensional space and translate them to an M dimensional space, where N > M.  I don't yet understand what the actual output of PCA is.For example, take this 5 dimensional input data with values in the range [0,10):// dimensions:// a  b  c  d  e[[ 4, 1, 2, 8, 8],      // component 1 [ 3, 0, 2, 9, 8], [ 4, 0, 0, 9, 1], ... [ 7, 9, 1, 2, 3],      // component 2 [ 9, 9, 0, 2, 7], [ 7, 8, 1, 0, 0]]My assumption is that PCA could be used to reduce the data from 5 dimensions to, say, 1 dimension.Data details:There are two ""components"" in the data.One component has mid a levels, low b and c levels, high d, and nondeterministic e levels.The other component has high a and b levels, low c and d levels, and nondeterministic e levels.This means that the two components are most differentiated by b and d, somewhat differentiated by a, and negligibly differentiated by c and e.  Outputs?I'm making this up, but say the (non-normalized) linear combination with the highest differentiating power is something like5*a + 10*b + 0*c + 10*d + 0*eThe above input data translated along that single axis is:[[110], [105], [110], ...etcIs that linear combination (or a vector describing it) the output of PCA?  Or is the output the actual reduced dataset?  Or something else entirely?";[education, open-source];123;
6385;2;2015-07-08T00:50:49.643;;"Have you tried reading the Intuition section of the PCA page on wiki? Also, the Further Considerations section, I think, explains what the various components represent and addresses your questions.A short summary of what those sections describe is that the original components (i.e., axes) of the data do not properly represent the relationship inherit in the data. Instead, the components can be combined to yield a new set of components, or axes, that better describe how the data is distributed.The wiki link states that ""PCA can be thought of as fitting an n-dimensional ellipsoid to the data"". In that line of thinking, the output of PCA is the eigenvectors and eigenvalues that define the orientation and length of the ellipsoid that best fits the data. What you do with that ellipsoid is up to you and your application of PCA.";;;
6386;2;2015-07-08T05:29:27.883;;Data Science is a field demanding a variety of skills. Having knowledge of Hadoop is one of them. The main tasks of a Data Scientist include:Gathering data from different resources.Cleaning and pre-processing the data.Studying statistical properties of the data.Using Machine Learning techniques to do forecasting and derive insights from the data.Communicating the results to decision makers in an easy to understand way.Out of the above points knowledge of Hadoop is useful for points 1,2 and 3, but you also need to have strong mathematical/statistical background and strong knowledge of Computational techniques to work in data science field. Also Hadoop is not the only framework that is being used in Data Science. Big Data ecosystem has a range of frameworks, each specific to a particular use case. This article gives introductory material regarding major Big Data frameworks that could be used in Data Science:http://www.codophile.com/big-data-frameworks-every-programmer-should-know/;;;
6387;1;2015-07-08T07:35:31.750;Why ConLL is not in XML format;I am new to NLP, I found a format named ConLL which seems a tab-separated file,like ID FORM LEMMA PLEMMA POS PPOS FEAT PFEAT HEAD PHEAD DEPREL PDEPREL I found a dependency Treebank in this format for Persian. I would like to do some experiment on it. I am familiar with C# and C# has great libraries for XML document files. I just wonder why such a corpus is not in XML format! I don't say a sophisticated one but they could put each line in a node of XML. Then at least, I would know what is the tag for each element.However, I can convert it to XML.;[education, open-source];22;
6388;2;2015-07-08T07:51:00.077;;One reason for sure is, that you can easialy open it in spreadsheet viewer.;;;
6389;2;2015-07-08T07:51:17.783;;"ConLL is a format optimised for processing efficiency (both speed and memory usage).XML formats for NLP often have the annotation layers one after the other; therefore you have to hold the whole file in memory for processing it.A drawback of ConLL is that you have to put in fillers (e.g., 0) for sparse annotations.";;;
6390;2;2015-07-08T08:47:16.220;;"I agree with dpmcmlxxvi's answer that the common ""output"" of PCA is computing and finding the eigenvectors for the principal components and the eigenvalues for the variances, but I can't add comments yet and would still like to contribute. Once you hit this step of calculating the eigenvectors and eigenvalues of the principal components, you can do many types of analyses depending on your needs.I believe the ""output"" you are specifically asking about in your question is the resultant data set of applying a transformation or projection of the original data set into the desired linear subspace (of n-dimensions). This is taking the output of PCA and applying it on your original data set. This PCA step by step example may help. The ultimate output of this 6 step analysis was the projection of a 3 dimensional data set into 2 dimensions. Here are the high level steps:  Taking the whole dataset ignoring the class labels Compute the d-dimensional mean vector Computing the scatter matrix (alternatively, the covariance matrix) Computing eigenvectors and corresponding eigenvalues Ranking and choosing k eigenvectors Transforming the samples onto the new subspace Ultimately, step 4 is the ""output"" since that is where the common requirements for performing PCA are fulfilled. We can make different decisions at steps 5 and 6 and produce alternative output there.  A few more possibilities:You could decide to project the observations with outliers removedAnother possible outcome here would be to calculate the proportion ofvariance explained by one or any combination of principal components. For example, the proportion of variance explained by the first two principal components of K components is (λ1+λ2)/(λ1+λ2+. . .+λK).After plotting the projected observations into the first two principal components (as in the given example), you can impose a plot of the loadings of each of the original dimensions into the subspace (scaled by the standard deviation of the principal components). This way, we can see the contribution of the original dimensions (in your case a - e) to principal component 1 and 2. The biplot is another common product of PCA.";;;
6391;1;2015-07-08T12:05:49.663;R - Interpreting neural networks plot;"I know there are similar question on stats.SE, but I didn't find one that fulfills my request; please, before mark the question as a duplicate, ping me in the comment.I run a neural network based on neuralnet to forecast SP500 index time series and I want to understand how I can interpret the plot posted below:Particularly, I'm interested to understand what is the interpretation of the hidden layer weight and the input weight; could someone explain me how to interpret that number, please?Any hint will be appreciated.";[education, open-source];56;
6393;1;2015-07-08T15:22:09.047;Modules on Python which are useful for missing Word/Letter prediction in text paragraphs from a coprpus;Can someone please recommend me some Python packages which I can use for missing word prediction in a paragraph of text. The missing words are a part of existing semantic list of words.( List of missing words are a common set of words like was ,were etc). If there are better packages in R which can help me with this  it would be very useful if you post it here.Thank you.;[education, open-source];21;
6394;1;2015-07-08T18:58:25.897;any reason for this project to use hadoop/spark?;I am setting up for a couple self study projects to explore machine learning techniques.1st project has 10,000 time series with 24 float data points each day for 10 years (876 million points). I will be creating a bunch of calendar and weather features for the data, then trying to forecast using a variety of machine learning techniques.2nd is some 13 million rows of text data (several paragraphs for each row)  for classification. (currently in solr database)My compute rig is 6-core, 32g ram, gforce GPU. I plan to install Ubuntu 14.2.I expect to be using python for file processing, scilearn, pylearn2 and word2vec for general exploration and training. R for getting a taste of the language.Clearly data set 1 will require joining weather and calendar data to date/time and aggregation across time and location. I know how to stuff it all into a MySQL database and do the aggregations and joins there, but I have been reading about spark and wondering.......If I take the time to simulate a cluster using virtual box/hadoop/spark (for my learning experience, not performance), can/should I do the aggregations there and write the results to the distributed data store? Since deep learning can not be run on spark, does that mean I would need to copy the aggregated data back out to the local file system to use some of those techniques?For data set 2, I want to run the word2vec algorithm as found in the kaggle tutorial https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors. In that example, that is a deeplearning method, so I should just leave the data in the solr.. right?In general I am looking for appropriate applications and insight into data flow from app to app to help me get to the part where I start trying various ML techniques.Thanks for helping me along;[education, open-source];43;
6395;1;2015-07-08T20:04:16.703;How flexible is the link between objective function and output layer activation function?;"It seems standard in many neural network packages to pair up the objective  function to be minimised with the activation function in the output layer.For instance, for a linear output layer used for regression it is standard (and often only choice) to have a squared error objective function. Another usual pairing is logistic output and log loss (or cross-entropy). And yet another is softmax and multi log loss.Using notation, $z$ for pre-activation value (sum of weights times activations from previous layer), $a$ for activation, $y$ for ground truth used for training, $i$ for index of output neuron.Linear activation $a_i=z_i$ goes with squared error $\frac{1}{2} \sum\limits_{\forall i} (y_i-a_i)^2$Sigmoid activation $a_i = \frac{1}{1+e^{-z_i}}$ goes with logloss/cross-entropy objective $-\sum\limits_{\forall i} (y_i*log(a_i) + (1-y_i)*log(1-a_i))$Softmax activation $a_i = \frac{e^{z_i}}{\sum_{\forall j} e^{z_j}}$ goes with multiclass logloss objective $-\sum\limits_{\forall i} (y_i*log(a_i))$Those are the ones I know, and I expect there are many that I still haven't heard of.It seems that log loss would only work and be numerically stable when the output and targets are in range [0,1]. So it may not make sense to try linear output layer with a logloss objective function. Unless there is a more general logloss function that can cope with values of $y$ that are outside of the range?However, it doesn't seem quite so bad to try sigmoid output with a squared error objective. It should be stable and converge at least.I understand that some of the design behind these pairings is that it makes the formula for $\frac{\delta E}{\delta z}$ - where $E$ is the value of the objective function - easy for back propagation. But it should still be possible to find that derivative using other pairings. Also, there are many other activation functions that are not commonly seen in output layers, but feasibly could be, such as tanh, and where it is not clear what objective function could be applied.Are there any situations when designing the architecture of a neural network, that you would or should use ""non-standard"" pairings of output activation and objective functions?";[education, open-source];28;1
6396;2;2015-07-08T20:18:27.417;;It sounds like you're describing optimal interpolation (AKA Gauss-Markov analysis, objective analysis, probably others too). This is a solid intro, and this is a powerpoint on the subject. It's a hard process to summarize quickly, but roughly speaking, you're on the right track.Optimal interpolation is fairly common in meteorology and other environmental sciences, but I'm not convinced it's really the best tool for the job. Any statisticians want to take a crack at it?;;;
6398;1;2015-07-08T20:57:09.627;Can I conclude my finding with just one linear regression result?;"I'm trying to learn Linear Regression and I'd like someone to look at my notebook to see if I miss out anything at all. How how else I can tune my model better. Here's my notebook.I get the dataset from here which is really simple. http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/frame.html%matplotlib inlineimport pandas as pdimport numpy as npfrom scipy import statsimport matplotlib.pyplot as pltdf = pd.read_csv('slr12.csv', names=['annual', 'cost'], header=0)df.describe()df.plot(kind='scatter', x='X', y='Y');slope, intercept, r_value, p_value, std_err = stats.linregress(df['X'], df['Y'])plt.plot(df['X'], df['Y'], 'o', label='Original data', markersize=2)plt.plot(df['X'], slope*df['X'] + intercept, 'r', label='Fitted line')plt.legend()plt.show()So from this trend we can predict that if you annual fee is high then you need your startup cost will be high as well.";[education, open-source];26;
6399;2;2015-07-09T00:15:00.987;;I like Blocks, which is also built on top of Theano. Way more approachable than PyLearn2, and more feature rich than Lasagne. Neatly written, too.;;;
6400;2;2015-07-09T00:59:20.503;;This problem seems to be exactly (or at least very similar to) that in the Billion Word Imputation (https://www.kaggle.com/c/billion-word-imputation) challenge at Kaggle. Some of the approaches discussed in the forums might help you out. My initial thoughts for useful tools/methods would be word2vec, the stanford NLP tools and sklearn.;;;
6401;2;2015-07-09T01:42:27.827;;Spark does have a pretty good Python API, check out this tutorial.For traditional Hadoop stack, take a look at mrjob, it lets you write MapReduce jobs in Python and run them on several platforms.;;;
6403;2;2015-07-09T02:21:55.417;;Hey is Python or other tools an option here? Since you mentioned it is a large dataset, you might want to iterate over it instead of loading all of them at once. Here is a solution in Python:import pandas as pdfrom collections import defaultdictinputs = ['cat1/: Topic1_Variable1','cat2/: Topic1_Variable2','cat3/: Topic1_Variable3','cat4/: Topic1_Variable4','cat1/: Topic2_Variable1','cat2/: Topic2_Variable2','cat3/: Topic2_Variable3','cat4/: Topic2_Variable4','cat1/: Topic3_Variable1','cat2/: Topic3_Variable2','cat3/: Topic3_Variable3','cat4/: Topic3_Variable4',]outputs = defaultdict(list)for item in inputs:    cat, topic = item.split('/: ')    outputs[cat].append(topic)print pd.DataFrame(outputs)Output:               cat1              cat2              cat3              cat40  Topic1_Variable1  Topic1_Variable2  Topic1_Variable3  Topic1_Variable41  Topic2_Variable1  Topic2_Variable2  Topic2_Variable3  Topic2_Variable42  Topic3_Variable1  Topic3_Variable2  Topic3_Variable3  Topic3_Variable4;;;
6404;1;2015-07-09T02:29:10.683;What Framework To Use for Asynchronous Algorithms?;I have a problem with an extremely large dataset (who doesn't?) which is stored in chunks such that there is low variance across chunks (i.e., the chunks are sort of representative). I wanted to play around with algorithms to do some classification in an asynchronous fashion but I wanted to code it up myself.A sample code would look like start a masterdistribute 10 chunks on 10 slaveswhile some criterion is not met  for each s in slave:  classify the data inexactly using some kind of iterative algorithm and return to master master waits for any 2 slaves to report the classifier, averages the classifier and sends it back for the slaves to continue What framework do I use? Hadoop, Spark, Other?If I was doing this in pure-C, I would use pthreads and have a very fine control over threads, locks and mutexes. Is there any analogous framework in this distributed data-science environment?;[education, open-source];19;1
6405;1;2015-07-09T03:10:08.480;Temporal Difference Learning Getting Stuck;I'm trying to implement a temporal difference algorithm that learns the maximum revenue over a period of time using prices as the actions, inventory as the state, and revenues realized as the reward. The problem I'm having is that I can't seem to get it to converge on an optimal policy. It seems like it gets stuck at a revenue somewhere around 60% of optimization and then won't budge anymore. Are there some common pitfalls that might be causing this? I've tried playing with the rate of exploration a little bit, but that hasn't seemed to help it much.EDIT:Ok, so I went back through everything, and it seems like the problem is that, when it stops exploring, it just keeps increasing the Q-values of states that it's already deemed the best in the past. So for example, it visits the price 5 at a certain state and time and gets a reward. Then, in the next several episodes, it continues to visit 5, and continues getting a reward, adding that reward to the Q-value until it's pretty high. At that point, even if it were to explore at the same state and time, the reward it gets isn't enough to overcome the inflated Q-value, so it just goes right back to 5 in the next episode. Here are the steps I'm trying to follow.;[education, open-source];28;
6406;1;2015-07-09T04:44:42.500;How to evaluate clustering success in a completely unsupervised system?;The algorithm in question is Kohonen's SOM. But the question could also apply to PCA and some others.When the umatrix (or the codebook?) is examined, is there a way to tell how successful clustering was?And would it be a good idea to apply GA's to optimize size, lattice structure, learning rate, and the learning degradation functions as well as the epoch count for clustering, or is there a danger of overfitting in this instance?Assume that the SOM data is coming from a demonstrably weak PRNG and that the very first attempt shows a distinct structure. Is there some statistical property or algorithm that can evaluate the presence and degree of structure to be used for a GA fitness function?;[education, open-source];92;1
6408;2;2015-07-09T10:50:15.607;;"Sounds like an interesting application. To debug RL applications, I like to perform rollouts. First identify a state where the current policy appears to be clearly wrong; (maybe sample some states and look at them individually). Then switch off the learning and run a sample from that state for both the current RL policy and the action you think is correct. This should give you the true action values. Hopefully, that will give you a hint as to what's going wrong. If it's an exploration problem, then the current policy won't be trying any actions similar to the action you think is correct. If the form of the value function is wrong, then it won't fit the values coming from your samples.";;;
6409;2;2015-07-09T13:25:24.823;;"First, PCA is not a clustering method. It is a dimensionality reduction scheme.  You can assess the performance of PCA through analysis of the percent of variance in the dataset that is retained as you decrease the number of dimensions.  Retaining 99%,95%, or 90% is usually ideal, depending on your problem. With regards to clustering, you probably want to start with the Silhouette Coefficient.  This combines assessments of both the cohesion (how tight a cluster is) and separation (how well separated each cluster is from other clusters).Calculate a = average distance of i to the points in its clusterCalculate b = min (average distance of i to points in another cluster) The silhouette coefficient for a point is then given by:s =1 – a/b, if a < bIt is typically between 0 and 1 with larger numbers being ""better"".  You can average the coefficients over a cluster or the entire region to get an assessment of the cluster or the entire clustering procedure for your data.More generally, try googling ""assessing clustering"" or ""cluster validity"" to read about all of the other ways that you can score your clustering algorithm.  Here is a very complete treatment of the topic.Hope this helps!";;;
6410;1;2015-07-09T16:12:00.940;Does the network learn based on previous training or does it restart? Matlab, neuralnetworks;"In Matlab, if you build a simple network and train it: OP = feedforwardnet(5, 'traingdm');inputsVals = [0,1,2,3,4];targetVals = [3,2,5,1,9];OP = train(OP,inputsVals,targetVals);then you train it again so another OP = train(OP,inputsVals,targetVals);What is happens to the network? Does it train again based on what it learned the first time you did OP = train(OP,inputsVals,targetVals); or does it train as if it were the first time training the network. ";[education, open-source];24;
6412;2;2015-07-09T17:47:34.793;;"To directly answer your question about stacking: you should care about minimizing 1) bias, and 2) variance. This is obvious, but in practice this often comes down to simply having models which are ""diverse"". (I apologize that link is behind a paywall, but there are a few others like it and you may well find it other ways)You don't want ensembles of like-minded models - they will make the same mistakes and reinforce each other. In the case of stacking, what is happening? You are letting the outputs of the probabilistic classifiers on the actual feature input become the new features. A diverse set of classifiers which can in any way give signals about edge cases is desirable. If classifier 1 is terrible at classes A, B, and C but fantastic at class D, or a certain edge case, it is still a good contribution to the ensemble. This is why neural nets are so good at what they do in image recognition - deep nets are in fact recursive logistic regression stacking ensembles! Nowadays people don't always use the sigmoid activation and there are many layer architectures, but it's the same general idea. What I would recommend is trying to maximize the diversity of your ensemble by using some of the similarity metrics on the classifiers' prediction output vectors (ie, Diettrich's Kappa statistic) in training. Here is another good reference.  Hope that helps.";;;
6413;2;2015-07-09T18:15:42.020;;"It trains again based on what it learned the first time you did OP = train(OP,inputsVals,targetVals). More generally, train uses your network's weights, i.e. it does not initialize the weights. The weight initialization happens in feedforwardnet.Example:% To generate reproducible results % http://stackoverflow.com/a/7797635/395857rng(1234,'twister')% Prepare input and target vectors[x,t] = simplefit_dataset;% Create ANNnet = feedforwardnet(10);% Loop to see where train() initializes the weightsfor i = 1:10    % Learn    net.trainParam.epochs = 1;    net = train(net,x,t);    % Score    y = net(x);    perf = perform(net,y,t)endyieldsperf =    0.4825perf =    0.0093perf =    0.0034perf =    0.0034perf =    0.0034perf =    0.0034perf =    0.0034perf =    0.0034perf =    0.0034perf =    0.0028";;;
6414;1;2015-07-09T20:38:22.933;Scalable open source machine learning library written in python;I believe sci kit learn is written in python,however that not scalable.Spark mlib or ml is scalabale but written in scala.I am looking for an ongoing effort where a machine learning library is being built in python (available in github or so) so that I can contribute to that.Is anyone aware of such effort.;[education, open-source];70;2
6415;2;2015-07-09T20:44:47.437;;I am not sure I understood the problem,however if you are trying to predict sales amount my guess is ARIMA might not be the right choice as it will not consider external variables.My suggestion is to gather related features such as how did the other movie from same genre did at that time of the year in that region,weather,star presence,presence of other popular matches or tournament at that time and etc.Let me know your thoughts. ;;;
6416;2;2015-07-09T21:00:32.840;;Here is a nice link on that on stackexchange  http://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting ,however my general experience is the more depth the model has the more it tends to overfit.;;;
6417;1;2015-07-09T21:09:15.107;Adjusting Probabilities When Using Alternative Cutoffs For Classification;"I am reading Applied Predictive Modeling by Max Khun. I chapter 16 he discusses using alternate cutoffs as a remedy for class imbalance.Suppose our model predicts the most likely outcome of 2 events, e1 and e2. We have e1 occurring with a predicted probability 0.52 and e2 with a predicted probability 0.48. Using the standard 0.5 for e1 cutoff we would predict e1, but using an alternative cutoff of 0.56 for e1 we would predict e2 because we only predict e1 when p(e1) > 0.56.My question is, does it make sense to also readjust the probabilities when using alternate cutoffs. For example, in my previous example using 0.56 cutoff of e1.p(e1) = 0.52;p(e2) = 0.48Then we apply an adjustment of 0.56 - 0.5 = 0.06.So p_adj(e1) =  0.52 - 0.06 = 0.46;p_adj(e2) = 0.48 + 0.06 = 0.54Basically we shift the probabilities so that they predict e1 when p_adj(e1) > 0.5.I apologize if there is something obviously flawed with my logic but it feels intuitively wrong to me to predict e2 when p(e1) > p(e2). Which probabilities would be more in line with the real-world probabilities?";[education, open-source];43;1
6419;2;2015-07-09T21:23:51.790;;Actually deep learning can be run in spark using h2o sparkling water feature.Also you can use h2o.deeplearning to run deeplearning on your data in cluster or single node.Spark is good for munging the data in cluster as it does so distributedly but in memory,otherwise h2o has limited functions for data munging and that too it can't distribute data munging.;;;
6420;1;2015-07-09T21:29:17.050;Can theano work on mapreduce or on spark;I'm not sure whether the Theano library can be used to write parallelized code in map reduce or in spark.Any expert opinion is welcome.A discussion was on atTheano-dev;[education, open-source];61;
6421;2;2015-07-09T23:46:29.367;;Is there a specific reason beside the fact that you would like to contribute? I am asking because there is always pyspark that you can use, the Spark python API that exposes the Spark programming model to Python.For deep learning specifically, there are a lot of frameworks built on top of Theano -which is a python library for mathematical expressions involving multi-dimensional arrays-, like Lasagne, so they are able to use GPU for intense training. Getting an EC2 instance with GPU on AWS is always an option.;;;
6422;2;2015-07-10T00:23:22.263;;@xof6 is correct in the sense that the more depth the model has the more it tends to overfit, but I wanted to add some more parameters that might be useful to you. I do not know which package you are using with R and I am not familiar with R at all, but I think there must be counterparts of these parameters implemented there.Number of trees - The bigger this number, the less likely the forest is to overfit. This means that as each decision tree is learning some aspect of the training data, you are getting more options to choose from, so to speak.Number of features - This number constitutes how many features each individual tree learns. As this number grows, the trees get more and more complicated, hence they are learning patterns that might not be there in the test data. It will take some experimenting to find the right value, but such is machine learning.Experiment with the general depth as well, as we mentioned!;;;
6423;1;2015-07-10T03:47:16.397;Identifying repeating sequences of data in byte array;Given a sample of hexadecimal data, I would like to identify UNKNOWN sequences of bytes that are repeated throughout the sample. (Not searching for a known string or value) I am attempting to reverse engineer a network protocol, and I am working on determining data structures within the packet. As an example of what I'm trying to do (albeit on a smaller scale):(af:b6:ea:3d:83:02:00:00):{21:03:00:00}:[b3:49:96:23:01]{21:03:00:00}:(af:b6:ea:3d:83:02:00:00):01:42:00:00:00:00:01:57And(38:64:88:6e:83:02:00:00):{26:03:00:00}:[b3:49:96:23:01]{26:03:00:00}:(38:64:88:6e:83:02:00:00):01:42:00:00:00:00:00:01Obviously, these are easy to spot by eye, but patterns that are hundreds of chars into the data are not. I'm not expecting a magic bullet for the solution, just a nudge in the right direction, or even better, a premade tool.I'm currently needing this for a C# project, but I am open to any and all tools.;[education, open-source];31;
6424;2;2015-07-10T05:34:10.930;;"I believe the problem that you are referring to, is that of ""Motif Discovery in Time Series Data"". An appreciable amount of research literature already exists in this domain, so you can look through those. If the data that you handle is not very large, you can find some relatively easy to implement algorithms. If the data is large , you can look at more recent publications in this domain. As a starting point I would recommend taking a look at how Motif Discovery is done in SAX. SAX takes continuous signals as inputs and discretizes them. These discrete levels are then stored as alphabets. This resulting data looks very much like yours in my opinion. Take a look at what they do in ""Mining Motifs in Massive Time Series Databases"".";;;
6425;2;2015-07-10T10:47:50.590;;"It is very hard to evaluate the correctness of the results produced by an unsupervised algorithm. In many cases, such evaluation will be totally subjective, and it will require some knowledge about the domain of the problem.If we focus on clustering algorithms (as stated in a previous answer, PCA is not a clustering algorithm), many cluster validation measures may be applied, like the ones enumerated in the ""Evaluation and assessment"" section in the ""Cluster analysis"" Wikipedia page. These measures return a number that you can use to compare different clustering solutions, in terms of cluster compactness (how close to each other are the elements within each cluster) and separation (how far to each other are the elements from different clusters). You can, of course, use these measures in order to perform hyperparameter selection (lattice size, structure, learning rate, etc.) by means of cross validation.However, it must be noted that different cluster validation measures may produce different validation results, and as a consequence, your best clustering solution may vary depending on the chosen measure. Therefore, even the choice of a validation measure is subjective. Once again, knowledge about your data is very important to take this decision.";;;
6426;1;2015-07-10T13:09:57.497;How to pass data around an oozie workflow;I'm setting up a data processing stack, and figuring out how to co-ordinate jobs.My team has settled on Oozie for workflow management. There are a lot of examples of control flow, but I can't see anything about how to orchestrate the passage of data between tasks in the workflow.We're also planning on using spark as our main platform for writing the jobs.How do people do it? Do the tasks themselves need to know what files they should be waiting for? Are there abstractions for piping data between the tasks?;[education, open-source];13;
6427;1;2015-07-10T14:05:44.183;How to replace levels with certain value?;"This is a doubt on R. Sorry if I had posted under wrong tag. I have a categorical variable in my dataset. I want to replace the levels which are present in test dataset and not present in training set with a value called ""others""Here is how it looks:levels(training$var1) has levels as ""1""  ""2""  ""3""  ""Others""       levels(testing$var1) has levels as ""1""  ""2""  ""3""  ""4""  ""5""  ""6""  ""7""  ""8""  ""9""  ""10""  I want to replace all the levels in testing data which are not training data. To achieve this I take the difference between the levels first.   a <- setdiff(levels(levels(testing$var1),training$var1)).  and I get output as  levels(a) as ""4""  ""5""  ""6""  ""7""  ""8""  ""9""  ""10""  Now I need to replace all the above difference values with ""Others"". Kindly note that I do not want to drop out these levels but I want to replace with ""Others"".  For this I tried  testing$var1[testing$var1 == ""4"" <- 'Others""  testing$var1[testing$var1 == ""5"" <- 'Others"" ==> these works.  However I want to make it in run time, something like this:  testing$var1[testing$var1 == a[1,] <- 'Others""But this is not working.";[education, open-source];54;1
6428;2;2015-07-10T14:32:53.177;;"I could be able to achieve this with the following code:testing$var1 <- as.character(testing$var1)a <- data.frame(a)testing$var1 [testing$var1 %in% a[1,] <- ""Others""testing$var1 <- as.factor(testing$var1)In case if there is any other better/effective solution/function to achieve this, please let me know. Thanks all.";;;
6429;2;2015-07-10T15:16:06.270;;I believe you can try with range of cut offs to see which one gives highest accuracy or F-score in hold out set.I've seen winners in kaggle stating to do experiment with cut-offs to get best result.;;;
6430;2;2015-07-10T15:18:36.517;;"Relative to other models, Random Forests are less likely to overfit but it is still something that you want to make an explicit effort to avoid. Tuning model parameters is definitely one element of avoiding overfitting but it isn't the only one. In fact I would say that your training features are more likely to lead to overfitting than model parameters, especially with a Random Forests. So I think the key is really having a reliable method to evaluate your model to check for overfitting more than anything else, which brings us to your second question.As alluded to above, running cross validation will allow to you avoid overfitting. Choosing your best model based on CV results will lead to a model that hasn't overfit, which isn't necessarily the case for something like out of the bag error. The easiest way to run CV in R is with the caret package. A simple example is below:> library(caret)> > data(iris)> > tr <- trainControl(method = ""cv"", number = 5)> > train(Species ~ .,data=iris,method=""rf"",trControl= tr)Random Forest 150 samples  4 predictor  3 classes: 'setosa', 'versicolor', 'virginica' No pre-processingResampling: Cross-Validated (5 fold) Summary of sample sizes: 120, 120, 120, 120, 120 Resampling results across tuning parameters:  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD    2     0.96      0.94   0.04346135   0.06519202  3     0.96      0.94   0.04346135   0.06519202  4     0.96      0.94   0.04346135   0.06519202Accuracy was used to select the optimal model using  the largest value.The final value used for the model was mtry = 2. ";;;
6431;2;2015-07-10T18:33:46.307;;I have read @MaximHaytovich's answer and it is a good one. I would just like to suggest some further options, which were generalized in that answer as feature engineering. I would suggest trying to do the obvious first and analyze the data before transforming it to become ready for any machine learning algorithm. First look at the data and learn yourself and hypothesize about what patterns could be indicators of fraud. What is the nature of the data that you are 100% sure is reliable (it is mentioned in the question you posted that there is available data that is reliable) and how informative are they for the goal? The more familiar you are with the data, the better interpretation you can provide to any output you get from the methods you use. Try doing some outlier analysis on the incomes or any features that may be fraudulent that may make sense to do outlier detection on. Are there any suspicious missing values? Try doing clustering (one of the unsupervised algorithms you were talking about) to see if there are any in-betweens or irregularities. ;;;
6432;2;2015-07-10T22:42:22.630;;"First of all, you cannot always consider what a machine learning algorithm outputs as a ""probability"". Logistic regression outputs a sigmoid activation on a (0, 1) scale, but that doesn't magically make it so! We simply often scale things to a (0, 1) scale in ML as a measure of confidence. Also in your example, if the events are mutually exclusive (like classification), just think of them as ""event 1"" and ""NOT event 1"". Something like p(e1) + p(~e1) = 1. So when your book tells you to lower the threshold, it is simply saying that you require a smaller level of confidence to choose e1 over e2. This doesn't mean you are choosing one with smaller likelihood, you are simply making a conscious choice to adjust your precision-recall curve. There are other ways to combat class imbalance, but changing the threshold to be more sensitive to any indication of confidence of one class over another is certainly a way to do that. ";;;
6433;1;2015-07-11T01:55:26.303;Is it possible to connect three neural networks in Matlab?;If I have 3 separate feedforward neural networks in Matlab, is it possible to connect them so that, given input data and target data the 3 work in parallel to produce output? If so, how do I do this? ;[education, open-source];48;
6434;2;2015-07-11T17:12:17.687;;No, you are misinterpreting his comments.  If you have data that has some outliers in it then the outliers will extend beyond 3 standard deviations.  Then if you standardize the data some will extend beyond the [-3,3] region.  He is simply saying that you need to remove your outliers so the outliers don't reap havoc on your stochastic gradient descent algorithm. He is NOT saying that you need to use some weird scaling algorithm.You should standardize your data by subtracting the mean and dividing by the standard deviation, and then remove any points that extend beyond [-3,3], which are the outliers.In stochastic gradient descent, the presence of outliers could increase the instability of the minimization and make it thrash around excessively, so its best to remove them.  If the sparseness of the data prevents removal then... Do you need to use stochastic gradient descent, or can you just use gradient descent?  Gradient descent (GD) might help to alleviate some of the problems relating to convergence.  Finally, if GD is having trouble converging, you could always do an direct solve (e.g. direct matrix inversion) rather than an iterative solve.Hope this helps!;;;
6435;1;2015-07-11T18:04:05.710;Cooperative Reinforcement Learning;I already have a functioning $Q(\lambda)$ implementation for a single agent working on a dynamic pricing problem with the goal of maximizing revenue. The problem that I'm working with, however, involves several different products that are replacements for each other, so dynamically pricing them all with independent learners seems incorrect, because the price of one influences the reward of the other. The goal would be to dynamically price them all so as to maximize the sum of each individual revenue.I've been doing some research to try to find something that applies reinforcement learning in this way, but many multi-agent implementations I have found focus more on competitive games than cooperative, or they assume incomplete knowledge of other agents (I would have complete knowledge of each agent in this scenario). Are there any well-researched/documented applications of cooperative learning in this way?;[education, open-source];16;
6436;1;2015-07-11T20:30:43.377;Combinations of samples that satisfy many conditions;Given n-dimensional data consisting of over 20000 samples with 200 dimensions, using this as an example:      X      Y      Z      W      S     ...A     3.1    5.1    8.2    1.1    3.1 B     4.4    4.1    0.3    2.2    6.3C     9.5    4.6    1.0    8.5    7.4D     7.1    5.7    9.1    3.4    2.5E     0.0    7.4    0.2    2.3    5.5...How would you go about finding as many combinations of samples as possible, such that, the sum of each column satisfies certain conditions? For example:the sum of all values for column X is in [4, 4.5]the sum of all values for column Y is in [5.5, 6]the sum of all values for column Z is in [3, 4]...Only combinations of samples that satisfy all conditions at once are viable.Important: It is possible to apply multipliers to samples, such that the multiplier applies to all dimensions, e.g.:Match 1: 2.3 * A + 1.2 * B + 3 * CMatch 2: 1 * B + 8 * DI would appreciate a few pointers of where to look for similar work. Thanks!;[education, open-source];12;
6437;2;2015-07-11T21:31:45.743;;"If you want to combine the results from three different Neural Networks to ""boost"" the performance :) , you might want to look at the different Ensemble Learning Methods as I mentioned earlier. Which method you should use, depends on how you share or divide the training data between the three NNs. For example if the NNs are trained on same data but have different parameters, you can look at simple voting ( if you are doing a classification task) or averaging ( if you are using them for regression).The more advanced methods like AdaBoost divide the training data between the classifiers. You can read about it in Boosting Neural Networks";;;
6438;1;2015-07-11T23:08:14.300;Machine learning for state-based transforms?;"If I provide:A list of possible transforms, and,A list of input states, and,A corresponding list of output states for each input state, and,A fitness function to score each output stateWhich subset of machine learning can direct me towards an optimization algorithm that can map each input state to a dictionary of input states, and, failing to find a match, apply the necessary transforms to get me to the closest-related output state?An example involving polygon legalization:Any given ""window"" can contain N different polygons, where each polygon has lower-left and upper-right co-ordinates, as well as a polygon ""type"".The input state of the polygons may or may not be ""illegal"".A list of transforms includes: move, copy, rotate, resizeIf the input state maps directly to any output state, the input state is decided to be legal. Nothing more to be done; move on the next window.If the input state matches any previously seen input state, transform to the matching (known-legal) output state. Nothing more to be done; move on the next window.Attempt transforms in different sequences until a state is reached that satisfies a fitness function. Store this input:output state combination. Move on to the next window.Would this imply some combination of neural networking (for classification) and genetic/evolutionary algorithms? Or, does the presence of a fitness function negate the need to store combinations of input:output states?";[education, open-source];55;
6439;2;2015-07-12T15:57:38.307;;I think you are trying to see which original (not PCA) features contribute to which cluster users fall into, yes?First, be aware that you just reduced the dimensionality of a 5d dataset down to 2d. You need to be careful of how much variance of the data you just threw away by projecting it into 2d. You can easily calculate this. If your 2d features account for 95% of the variance of the data, then great! You've got some valid insights about which features are important. If lower, like say 40%, then not so much. (From a qualitative standpoint, just plot the clusters with different colors. That will give you some idea of how much variance you threw away - if the clusters look contiguous and not a lot of mixing, then qualitatively, you didn't throw away too much variance.)Second, realize that in order to intuit on which original feature values contribute to which clusters, you'll need to use the original features. The real answer to your question is that after that you should 1) cluster in 5d, 2) use a classifier, and 3) see the important features using 5d feature vectors. This is critical because in 2d, just because you might have kept most of the variance doesn't mean that last bit might have contained very discriminatory (read: important) information that might improve your underlying clustering error and give you a much more accurate answer. Most importantly, the feature importance will be computed on the actual features, not the PCA linear-combination features which don't translate directly to single site visit histories.Hope that helps.;;;
6440;1;2015-07-12T19:08:38.467;How to do multitask learning using Caffe?;"I wonder how to do multitask learning using Caffe. Should I simply use the output layer SigmoidCrossEntropyLoss or EuclideanLoss, and define more than one outputs?E.g. is the following architecture valid (3 outputs, i.e. 3 tasks concurrently learnt)?Corresponding prototxt file:name: ""IrisNet""layer {  name: ""iris""  type: ""HDF5Data""  top: ""data""  top: ""label""  include {    phase: TRAIN  }  hdf5_data_param {    source: ""iris_train_data.txt""    batch_size: 1  }}layer {  name: ""iris""  type: ""HDF5Data""  top: ""data""  top: ""label""  include {    phase: TEST  }  hdf5_data_param {    source: ""iris_test_data.txt""    batch_size: 1  }}layer {  name: ""ip1""  type: ""InnerProduct""  bottom: ""data""  top: ""ip1""  param {    lr_mult: 1  }  param {    lr_mult: 2  }  inner_product_param {    num_output: 50    weight_filler {      type: ""xavier""    }    bias_filler {      type: ""constant""    }  }}layer {  name: ""ip2""  type: ""InnerProduct""  bottom: ""ip1""  top: ""ip2""  param {    lr_mult: 1  }  param {    lr_mult: 2  }  inner_product_param {    num_output: 3    weight_filler {      type: ""xavier""    }    bias_filler {      type: ""constant""    }  }}layer {  name: ""loss""  type: ""SigmoidCrossEntropyLoss""   # type: ""EuclideanLoss""   # type: ""HingeLoss""    bottom: ""ip2""  bottom: ""label""  top: ""loss""}";[education, open-source];68;
6444;2;2015-07-13T03:02:48.130;;"I guess u are looking for feature which extract out new feature. A feature which best representthe dataset. If that is the case then we call such method ""feature extraction"".";;;
6446;1;2015-07-13T13:34:51.983;Errror in confusionMatrix in R;"While i am using the confusionMatrix in R, I am getting the following error:Error in binom.test(sum(diag(x)), sum(x)) :   'n' must be a positive integer >= 'x'The code i use as follows:rf.pred <- predict(rf, testing, type = ""class"")confusionMatrix(rf.pred, testing$target)";[education, open-source];43;
6447;1;2015-07-13T15:39:13.290;Problem while reusing model for new observations;"I want to  build a model using R, save it and reloading it when required. I am able to save the model and reload it without any problem, however when i use the model along with predict statement, I am getting errors.This is what i do: set.seed(345)  df <- data.frame(x = rnorm(20))  df <- transform(df, y = 5 + (2.3 * x) + rnorm(20)) model m1 <- lm(y ~ x, data = df) save this model save(m1, file = ""my_model1.rda"")a month later, new observations are available: newdf <- data.frame(x = rnorm(20)) load the model load(""my_model1.rda"")When i use the above model to predict for new values, I am getting output values as ""NA""   predict(m1, newdata = newdf)""NA""  ""NA""  ""NA""  ""NA""  ""NA""  ""NA"" ""NA""  ""NA""  ""NA""   I could able to see that the model has been loaded correctly by giving a print command print(m1)Call:lm(formula = y ~ x, data = df)Coefficients:(Intercept)            x      4.792        2.119  Is there anything that I miss here?";[education, open-source];35;
6448;2;2015-07-13T20:19:39.093;;I can't see any way to magically identify unreliable data in this context. I'm no expert but 2 ideas spring to mind:Domain knowledge: an expert on household income may be able to describe some general rules for identifying suspect data based on experience, and you could try to convert these into rulesBenford's Law - I don't know if it would be suitable for this scenario but Benfords Law has applications in detecting accounting fraud, based on the expected frequency of numbers appearing in numeric values. Probably of no use especially if incomes are reported in rounded figures (e.g. nearest 1000) but might be worth a look for inspiration.;;;
6449;1;2015-07-13T20:37:28.867;Learn and sample from a generative model with 2000 boolean features;My data is a set of about 1 million training examples, where each example is represented by about 2000 boolean features. Examples aren't labelled - it wouldn't really have any meaning in this domain. The features fully define the example. My goal is to learn a generative model that I can use to generate more examples. I would like to capture some of the dependencies between the features (e.g. if feature 10 is on, feature 14, 15, and 16 are usually on). I don't have much background in ML, so I'm not sure what a simple way to do this would be (both algorithms and software packages). My initial thought was to find a structure learning package to learn a Bayesian network and then sample from it, but I figured I'd ask here to see if anyone had any recommendations. Thanks;[education, open-source];47;
6450;2;2015-07-14T04:24:44.473;;In situations where a particular class is really a minority, I suggest using rare category detection. In this particular case of fraud/non-fraud, fraud is a rare category. Its an active field of research - Refere to Rare Category Detection;;;
6451;2;2015-07-14T06:09:27.803;;While checking on python xgboost I found the existence of this open source project that helps create scalable machine learning program.Should be worth exploring.;;;
6452;2;2015-07-14T08:18:37.257;;You may try an Apriori algorithm or more advanced FPGrowth algorithm. It will give you a set of association rules, which, I think, you required: e.g. if feature 10 is on, feature 14, 15, and 16 are usually on;;;
6453;1;2015-07-14T08:45:36.813;Find correlation in observed data;"I have a method that calculates a certain variable. This variable is biased by an error which should be related to the observables that were used to calculated the variable. I don't have an exact formula of the calculation and also no errors of the observables, therefore I can't just do error propagation.I do have training data (the true error). Now I want to use machine learning to find the correlations between the input observables and the error of the calculated output. For now I tried to gather as many observables and used their statistics (mean, variance etc.) as ""feature vectors"" for machine learning. So far I tried approaches like SVR, linear Regression, SGD Regression etc. but all showed the same bad result.I'm quite new in this field. So some things are still a little bit unclear to me. Do you have any ideas how to find the relations? Or do you think there is no relation at all? What if there are observables that are totally random; do they destroy the learning?Thanks in advance!";[education, open-source];40;
6454;2;2015-07-14T09:02:54.843;;I'm not sure it is possible to generalise as to how people may mis-state financial information they report to governments. This probably varies by country and hence culture and if the information is used directly to affect the welfare benefits received by that individual, it may not be. If there is data on household income from a more trusted source, tax receipts for instance, that might be used to normalise the self reported data.;;;
6455;2;2015-07-14T10:02:18.477;;You can also modify your cost function so that instances in the smaller class have more weight. This makes training computationally less demanding, when compared to your option 1.;;;
6456;1;2015-07-14T15:05:21.473;Is there a way to set the number of times a neural network encounters certain input in Matlab?;If I have input data and it's corresponding target data, and for each input there is a number associated with it, where this number represents the probability of the input occurring, is there a way to use this number to determine how many times a feedforward neural network in Matlab encounters the input. Currently what I'm doing to work around this is manually have the input present x amount of times where x corresponds to the probability associated with the input. So, for example say I have input: 1,2,1,0   1,4,1,00,1,1,01,4,2,1where each row corresponds to a specific item in the input data and for each row a number is associated with it which represents the probability of that item occurring: .2394.4958.0123.8721Is there a way to use these numbers to determine the amount of times a feedforward neural network encounters an item? ;[education, open-source];15;
6458;2;2015-07-14T21:30:00.377;;"If i get it correctly:You have an input polygonAs a first step you want to ""match"" that against a list of previously seen templates. If this is successful, you pick it's corresponding output and move on.If not, you wish to find some optimal transformation, in order for it to satisfy some constraints that you have (your ""objective function""). Then add the original+transformed shape to the templates list and move on.Is this correct? I'll risk an answer anyways:For the first part, I believe that there is a slew of literature out there. It's not my expertise, but first thing that comes to mind is measuring the distance in feature space between your shape and each template, and picking the closest one, it the distance is below a threshold that you set. ""Feature"" here would be either some low-level polygon property, e.g. x and y coordinates of vertices, or an abstraction, e.g. perimeter, area, no. vertices, mean side length/side length variance, etc.For the second part, it really depends on the nature of your constraints/objective functions. Are they convex? Uni- or multi-modal? Single or multi-objective? Do you want to incorporate some domain knowledge (i.e. knowledge about what ""good"" transformations would be?)? One can really not tell without further details. Evolutionary algorithms are quite versatile but expensive methods (although some argue on that). If you can spare the possibly large amount of function evaluations, you could try EAs as a first step, and then refine your approach.Finally, while not exactly related to what you describe in your process, I believe you may benefit by taking a look into auto-associative networks (and models in general); these are models that are able to perform constraint-satisfaction on their input, effectively enforcing learned relationships on input values. I could see this being used in your case by inputing a shape, and having a transformed shape as an output, which would be ""legal"", i.e. satisfying the constraints learned by the auto associative model. Thus, you would eliminate the need for a template matching + optimization altogether.";;;
6459;1;2015-07-14T22:39:37.943;What features from sound waves to use for an AI song composer?;I am planning on making an AI song composer that would take in a bunch of songs of one instrument, extract musical notes (like ABCDEFG) and certain features from the sound wave, preform machine learning (most likely through recurrent neural networks), and output a sequence of ABCDEFG notes (aka generate its own songs / music). I think that this would be an unsupervised learning problem, but I am not really sure.I figured that I would use recurrent neural networks, but I have a few questions on how to approach this:- What features from the sound wave I should extract so that the output music is melodious?- Is it possible, with recurrent neural networks, to output a vector of sequenced musical notes (ABCDEF)?- Any smart way I can feed in the features of the soundwaves as well as sequence of musical notes? ;[education, open-source];104;3
6460;1;2015-07-14T23:37:01.940;Fisher's Iris data set with Caffe;"I am trying to use Caffe on the usual Fisher's Iris data set (150 flowers, each having 4 features, and split into 3 classes):if a flower belong to class 1 (setosa), the network output should be [1, 0, 0]if a flower belong to class 2 (versicolor), the network output should be [0, 1, 0]if a flower belong to class 3 (virginica), the network output should be [0, 0, 1]I use the SigmoidCrossEntropyLoss as it used for predicting K independent probability values in [0,1]. Let ip3 be the layer connected to SigmoidCrossEntropyLoss (in addition to the label layer): looking at ip3's output, I interpret negative values as class absent (0), and positive values as class present (1). E.g. if ip3's output is [-0.3; 0.9; -0.4], then I interpret it as [0; 1; 0], i.e. class 2 is present but classes 1 and 3 are absent.The network does a good job classifying classes 1 and 3 (accuracy over 90%), but consistently fails to predict class 2: the network always predicts that class 2 is absent, i.e. the network's second output is always 0, i.e. that ip3's second output is always negative, no matter what the input is.Here is the architecture I use. Am I doing something wrong? Is the architecture not suited for the task?";[education, open-source];29;1
6461;2;2015-07-15T00:15:11.733;;"I believe the question is, you want to learn from musical pieces and try to generate a tune from the trained instance. Lets see if I can set up a simple model to do this, and then you can extrapolate from there. So, MFCC is a good feature when working with sound. You can use that to extract the features from lets say 1-2 second windows of your song. You now have a fingerprint for the audio file. Take a look at Conditional Restricted Boltzmann Machines. They are Neural Networks which use multiple binary states to encode time series information. As you can see in the webpage, they trained on human-gait data and can now generate their own human gait. This is essentially what you want but for music files. So you can train CRBMs on the Audio MFCC vectors that you have. After the training is done, to generate an audio file you can either ""seed"" the CRBM with a few seconds of some melody or just randomly initialize it. Then just allow the CRBM to go nuts and record whatever it produces. This is your new audio file. To produce another sample use a different seed.This solves the question of how you can implement a ""melody"" generation scheme. There are of course variations. You can add other features to you vector apart from MFCC. You can also use other time series predictors like LSTM or Markov models. All of this being said, the problem of generating music might be much more nuanced than it looks at first glance. Machine Learning algorithms just apply previously learned patterns in the data. How does that correspond to ""creating"" new music , is a philosophical question. If we analyze the aforementioned algorithm, essentially the CRBM will generate a next output based on the probability distribution that it has learnt. It would be very interesting to see what kind of output it generates when the said distribution is that of musical notes.  ";;;
6462;1;2015-07-15T01:17:20.583;Obtain Zip-Code Level Estimates from State Level Observed Data and Correlated Zip-Code Data;I'd like to use actual state estimates for X (financial variable in percentage format) and obtain zip-code estimates for X (same financial variable estimate in percentage format). I have zip-code level data for variables (let's call them A,B,C, etc) that most likely would help describe X at the zip-code level, but again I have no real observations of X at the zip code level. My first attempt was to aggregated zip code level data for (A, B, C, etc) to the state level, then regress against X at the state level. I then took the regression result and used it to predict X at the zip code level. To check the results, I then aggregated and averaged the zip code estimates for X. They aren't ideally close to the real observed state level X variable. I'm wondering if there's a better way to do this?;[education, open-source];19;
6464;1;2015-07-15T14:15:17.773;Spam detection in social media;I want to train a binary classification algorithm for spam detection using labeled data set of mentions from social media. These mentions have the following features: URL, text message, posting date, is deleted, authorwhere the author has its own fields: URL, nick, type (person or community), number of subscribers, registration date, last activity dateI have $\approx 13.5k$ observations with $\approx 450$ spam messages among them. My suggestions: transform data features in real numbersbinarize URLs by media sources (e.g. facebook, instagram, etc.)use SVM with Gaussian kernel for all dataQuestions: Am I on the right way? What else can I do? How can I process text data?Thanks in advance for any suggestions!;[education, open-source];47;
6466;2;2015-07-15T17:56:33.763;;The approach seems correct, don't worry. But I'd suggest to use Naive Bayes Classifier instead of SVM - it perfectly works for this task (and actually popular email services use this algorithm for spam detection).;;;
6467;1;2015-07-15T17:59:31.063;Deriving Confidences from Distribution of Class Probabilities for a Prediction;I run into this problem from time to time and have always felt like there should be an obvious answer.I have probabilities for potential classes (from some classifier).  I will offer the prediction of the class with the highest probability, however, I would also like to attach a confidence for that prediction.Example: If I have Classes [C1, C2, C3, C4, C5] and my Probabilities are {C1: 50, C2: 12, C3: 13, C4: 12, C5:13} my confidence in predicting C1 should be higher than if I had Probabilities {C1: 50, C2: 45, C3: 2, C4: 1, C5: 2}.Reporting that I predict class C1 with 60% probability isn't the whole story.  I should be able to derive a confidence from the distribution of probabilities as well.  I am certain there is a known method for solving this but I do not know what it is.EDIT: Taking this to the extreme for clarification: If I had a class C1 with 100% probability (and assuming the classifier had an accurate representation of each class) then I would be extremely confident that C1 was the correct classification.  On the other hand if all 5 classes had almost equal probability (Say they are all roughly 20%) than I would be very uncertain claiming that any one was the correct classification. These two extreme cases are more obvious, the challenge is derive a confidence for intermediate examples like the one above.Any suggestions or references would be of great help.Thanks in advance.;[education, open-source];50;1
6468;2;2015-07-15T18:11:33.163;;" If I have Classes [C1, C2, C3, C4, C5] and my Probabilities are {C1:  50, C2: 12, C3: 13, C4: 12, C5:13} my confidence in predicting C1  should be higher than if I had Probabilities {C1: 50, C2: 45, C3: 2,  C4: 1, C5: 2}.Assuming that those probabilities are accurate, this isn't true. In your second case you can be a lot more confident that the ground truth is one of C1 or C2, but in terms of absolute confidence about C1 the probability is the same across both examples. To illustrate this with a more clear example, if you had a 100 sided die that had 50 sides labeled with ""C1"" then the labels on the the other 50 sides are irrelevant to the likelihood that you would roll a ""C1"".Now with that said, your probabilities from your model are most certainly not perfect so there may be a way to use the intra-class correlations to improve them. Can you provide some more details about your specific problem and modeling workflow that you have used to get your probabilities?";;;
6470;2;2015-07-15T18:47:02.090;;"Yes, you are essentially on the right track. You can normalize the data , if your text samples have varied length. Also, you can use categorical data but not in the same feature. For example, having one feature which is labelled ""1"" for facebook and ""2"" for instagram is not good. Have separate features for these kind of categorical data with {0,1} labels. There are two additional inputs that I would like to give here. First, use a systematic grid search instead of just guessing the parameters. This is a very good practice which I picked up very late. If you are using a soft margin SVM ( which you should) with the gaussian kernel, you will have just two parameters to search over, C and gamma. This will make sure that you are extracting the best parameters from your classifier. Second, you have 13.5K samples and only 450 are spam. Which means you have 13 K desired mails. So you have 450 positive samples and 13K negative samples. If this is indeed the data distribution, you have an unbalanced data. You will have to weight the data according to the proportion otherwise SVM will end up classifying most of the samples as negative. To read about this in more detail look at Section 7 of this document . Weighted SVMs are implemented in LibSVM, so you can directly use that. ";;;
6471;2;2015-07-15T19:58:35.510;;As @David says, in your initial example, your confidence about C1 is the same in both cases. In your second example, you most certainly are less confident about the most-probable class in the second case, since the most-probable class is far less probable!You may have to unpack what you're getting at when you say 'confidence' then, since here you're not using it as a term of art but an English word.I suspect you may be looking for the idea of entropy, or uncertainty present in the distribution of all class probabilities. In your first example, it is indeed lower in the second case than the first. I don't think what you're getting at is just a function of the most-probable class, that is.;;;
6472;1;2015-07-15T21:06:00.360;IPython notebook shortcut to run;I am currious if there is a shortcut for runing the  selected cell in IPython notebook such as alt+F5 or ctrl+f5 or something simmiliar  that is present in Visual Studio (I don't recall exactly now ) , cause I found it annoing to go with  the mouse on  the run  button every  time I need to run a cell?;[education, open-source];54;
6473;2;2015-07-16T03:12:41.317;;IPython notebook Keyboard shortcuts![Option the options [2:4] i think is the answer to your question.] ;;;
6476;1;2015-07-16T08:08:45.270;Finding Episodes in event sequence;"In the paper ""Discovery of frequent episodes in event sequences"" by Mannila et al., a method for finding frequent episodes in an event sequence if a class of episodes and a sequence of events are given. It is not clear that how to find episodes on the first hand. I would like to know if there is any tool or algorithm or published work that describes a method to find episodes in a given stream.";[education, open-source];42;
6477;1;2015-07-16T10:03:38.970;What data format is required for RegressionDataset in lib shark?;"The format required for ClassificationDataset is straightforwardly a sequence of variable followed by a lable,but this form can't be applied for RegressionDataset and it throws an exception like this: ""[importCSVReaderSingleValues] problems parsing file (2)""The following is the code to parse a string in the data fileinline std::vector<std::vector<double> > importCSVReaderSingleValues(   std::string const& contents,    char separator,    char comment = '#') {    std::string::const_iterator first = contents.begin();    std::string::const_iterator last = contents.end();    using namespace boost::spirit::qi;    std::vector<std::vector<double> >  fileContents;    double qnan = std::numeric_limits<double>::quiet_NaN();    if(std::isspace(separator)){        separator = 0;    }    bool r;    if( separator == 0){        r = phrase_parse(            first, last,            (                +(double_ | ('?' >>  attr(qnan) ))            ) % eol >> -eol,            (space-eol) | (comment >> *(char_ - eol) >> (eol| eoi)), fileContents        );    }    else{        r = phrase_parse(            first, last,            (                (double_ | ((lit('?')| &lit(separator)) >>  attr(qnan))) % separator           ) % eol >> -eol,           (space-eol)| (comment >> *(char_ - eol) >> (eol| eoi)) , fileContents       );   }   if(!r || first != last){       throw SHARKEXCEPTION(""[importCSVReaderSingleValues] problems parsing file (2)"");   }   return fileContents;}";[education, open-source];9;
6478;1;2015-07-16T11:06:49.757;How to extract information from plot images?;Are there any free tools or libraries that can understand a plot image file automatically? Things like type detection (line, bar, scatter), as well as label and axis scaling detection.In the tools I have found so far, one has to do manual configuration. I'm asking if it is possible in our AI driven world to let a machine do this.If there are none, I will start a project for myself as it seems like a challenging but instructive AI task. Is this effort doomed from start? Are there any helpful libraries in C++ or Java and theoretical algorithms one could consider?Thanks,Tomas;[education, open-source];62;1
6479;1;2015-07-16T12:32:00.630;Where can I get a list of ATM identifiers that I can map to Geographic Location?;Most bank statements (at least mine here in the UK) contain coded information for cash withdrawls such as:CASH RB SCOT JAN11 TESCO STREAT@08:56CASH SAINSBY JUN23 STREATHAM C @10:46I'm fairly sure this can be broken down into the following tuple (tran_type, operator, date, location, @time)What I'd like to do is translate from (operator,location) to GPS coordinates, a postcode, or some other geographic identifier.Is anyone aware of where I might find such an information source? Is it commercial, or open-source?;[education, open-source];25;
6480;1;2015-07-16T13:08:07.277;Implementing GLM for a poisson model using matlab function fminunc;"I am trying to simulate a linear non-linear poisson model. The problem is that as far as I know, the log likelihood function in this case should be a convex one, but that does not seem to be true, because I receive different function values for the same initial guess using fminunc.BTW, I calculated the likelihood function below, let me know if there is anything wrong with it: Leaving that aside, the algorithm does not seem to work, since the weights(B) are very far from what they should be (all should be near -1). Since some of coefficients are positive, the mean becomes large and thus the counts are also very different from what I expect them to be.I tried to manually force the program to return only all negative component B for me, but it doesn't converge after giving it ample time.What should I do?            %Description: This program implements a simple generalized linear model        %approch to neural encoding. We assume that the stimulus is a binary on off        %4 pixel.We also assume that the training data comes from experimentally        %recording the response of a single neuron to 10 different 4pixel stimulus.        %we deploy the canonical link function and use maximum likelihood method to        %estimate the weights that predicts the number of spikes given a stimulus.        %Date:7/5/2015 Author: Mani Sotoodeh        clc;clear;close;        [B,fval]=Weightestimate(); % creating trainging data and finding optimum weights        b0=B(1);b1=B(2);b2=B(3);b3=B(4);b4=B(5);        % pre allocating to create test data        X=zeros(100,4);        Mu=zeros(100,1);% process parameter        diff=zeros(100,1);% difference of predicted and actual count for each training data        Mu2=zeros(100,1);% predicted process parameter        T=zeros(100,1);% spike times        T2=zeros(100,1);%predicted spike times        Y=zeros(100,1);%number of spikes        Y2=zeros(100,1);%predicted number of spikes        Eta=zeros(100,1);% linear predictor        Eta2=zeros(100,1);% linear predictor        for i=1:100 % on trials            for j=1:4 % on pixels                X(i,j)=randi(2,1)-1;            end            Eta(i)=-1+-1*X(i,1)+-1*X(i,2)+-1*X(i,3)+-1*X(i,4);            Mu(i)=exp(Eta(i));            [T,Y(i)]=ppsample(Mu(i),20);            Eta2(i)=b0+b1*X(i,1)+b2*X(i,2)+b3*X(i,3)+b4*X(i,4);            Mu2(i)=exp(Eta2(i));            [T2,Y2(i)]=ppsample(Mu2(i),20);            diff(i)=abs(Y(i)-Y2(i));         end        difference=sum(diff);Here's weightestimate function:        function [B,fval] = Weightestimate(~)        %Description: this function initialized a trianing data and returns the        %appropriate weights using the maximum likelihood method using fminunc        clc;clear;close all;        % pre allocating        x=zeros(10,4);        mu=zeros(10,1);% process parameter        T=zeros(10,1);% spike times        y=zeros(10,1);%number of spikes        eta=zeros(10,1);% linear predictor        for i=1:10 % on trials            for j=1:4 % on pixels                x(i,j)=randi(2,1)-0.8;            end            eta(i)=-0.8+-0.8*x(i,1)+-0.8*x(i,2)+-0.8*x(i,3)+-0.8*x(i,4);            mu(i)=exp(eta(i));            [~,y(i)]=ppsample(mu(i),20);            syms b0;syms b1;syms b2;syms b3;syms b4;syms objfu;syms b            o(i)=(((b0+b1*x(i,1)+b2*x(i,2)+b3*x(i,3)+b4*x(i,4))*y(i))-exp(b0+b1*x(i,1)+b2*x(i,2)+b3*x(i,3)+b4*x(i,4)));        end        %%finding optimum weights        objfu=-1*sum(o);        objfu=matlabFunction(objfu);        b = zeros(1,5);        function f = ofun(b)        b0=b(1);b1=b(2);b2=b(3);b3=b(4);b4=b(5);        f = objfu(b0,b1,b2,b3,b4);           end        [B,fval] = fminunc(@ofun,b);        C=B<0;        k=0;        %ensuring an all negative coefficients        while (sum(C==logical([1,1,1,1,1]))~=5)        b=rand(1)*zeros(1,5);        [B,fval] = fminunc(@ofun,b);        k=k+1;        end        B        fval        k        endI don't think it has anything to do with ppsample, but just for reference:            %Description: This function implements a simple poisoon process with a given rate in a specified time period and shows the result using stem        % the function gets tmax and rate of process (landa) and creates and shows        % a poisson process with given parameters.        function [T,nzT]=ppsample(landa,Tmax)        clc;            T(1)=random('exponential',1/landa);            i=1;            while T(i)< Tmax                T(i+1)=T(i)+random('exponential',1/landa);                i=i+1;            end            ST=numel(T);% number of spikes             for j=1:ST % zeroing arrival times that has surpassed tmax            if T(j)>Tmax                T(j)=0;            end                end            nzT=max(size(nonzeros(T)));            if isempty(T)==0            one=0.1*ones(1,nzT);            else                 one=0.1*ones(1,0);            end            T=T(1:nzT);        %     stem(T(1:nzT),one)        %     axis([0 Tmax 0 .11]);        %     axis normal        %     tit=strcat('a possible outcome of poisson process with parameters landa=',num2str(landa),'and Tmax=',num2str(Tmax));        %     title(tit,'color','r')        %     xlabel('Time')        end";[education, open-source];19;
6481;2;2015-07-16T13:52:17.240;;In the general sense this is most definitely a non-trivial problem and any full featured solution will require a considerable degree of work.Success will probably be dependent on how much you constrain your problem.Monochrome will be easier than color.Limited font, font size, line and marker styles will help.The more constrained, the more chance of success.For line detection, such as axes, you might want to consider the Hough and Radon transforms.Here are some stackexchange questions covering some of the topics you might want to address, sadly no bargraphs, some have a manual component which you might think how to automate. They are a good starting point to review some of the difficulties involved in the problem.Selecting a curve:http://mathematica.stackexchange.com/questions/44355/how-to-make-a-curve-selectable-from-a-scaned-image-and-convert-it-to-a-list-of-cDatapoints and Axes:http://mathematica.stackexchange.com/questions/1524/recovering-data-points-from-an-imageScatter:http://mathematica.stackexchange.com/questions/26356/how-do-i-find-the-coordinates-of-points-in-this-imageDatapoints:http://mathematica.stackexchange.com/questions/3831/how-can-i-extract-data-points-from-a-black-and-white-imagehttp://mathematica.stackexchange.com/questions/14444/obtain-data-points-from-a-graph-on-an-image-without-axes;;;
6482;2;2015-07-16T14:50:34.690;;IPython notebook Keyboard shortcuts:Shift-Enter: run cellCtrl-Enter: run cell in-placeAlt-Enter: run cell, insert belowCtrl-m: This is the prefix for all other shortcuts, which consist of Ctrl-m followed by a single letter or character. For example, if you type Ctrl-m h (that is, the sole letter h after Ctrl-m), IPython will show you all the available keyboard shortcuts.;;;
6484;2;2015-07-16T23:06:47.217;;The paper that you cited references two other papers that explain the method you are looking for here and here.  The second one is behind a pay wall.But... these look like deterministic methods to me, and I suggest you use a machine learning method instead.  You basically need to separate events from each other using some sort of clustering algorithm.  I suggest you use DBSCAN and adjust the parameters based on whether you want every event to be assigned to an episode or not.Another algorithm that might work well is simple k-means clustering with the addition of an elbow method or silhouette score to determine the optimal number of clusters. ;;;
6485;2;2015-07-16T23:55:59.507;;"First off, ignore the haters.  I started working on ML in Music a long time ago and got several degrees using that work. When I started I was asking people the same kind of questions you are. It is a fascinating field and there is always room for someone new. We all have to start somewhere.The areas of study you are inquiring about are Music Information Retrieval (Wiki Link) and Computer Music (Wiki Link) .  You have made a good choice in narrowing your problem to a single instrument (monophonic music) as polyphonic music increases the difficulty greatly.  You're trying to solve two problems really:1) Automatic Transcription of Monophonic Music (More Readings) which is the problem of extracting the notes from a single instrument musical piece.2) Algorithmic Composition (More Readings) which is the problem of generating new music using a corpus of transcribed music.To answer your questions directly: I think that this would be an unsupervised learning problem, but I am  not really sure.Since there are two learning problems here there are two answers. For the Automatic Transcription you will probably want to follow a supervised learning approach, where your classification are the notes you are trying to extract.  For the Algorithmic Composition problem it can actually go either way. Some reading in both areas will clear this up a lot. What features from the sound wave I should extract so that the output music is melodious?There are a lot of features used commonly in MIR. @abhnj listed MFCC's in his answer but there are a lot more.  Feature analysis in MIR takes place in several domains and there are features for each.  Some Domains are:The Frequency Domain (these are the values we hear played through a speaker)The Spectral Domain (This domain is calculated via the Fourier function (Read about the Fast Fourier Transform) and can be transformed using several functions (Magnitude, Power, Log Magnitude, Log Power)The Peak Domain (A domain of amplitude and spectral peaks over the spectral domain)The Harmonic DomainOne of the first problems you will face is how to segment or ""cut up"" your music signal so that you can extract features.  This is the problem of Segmentation (Some Readings) which is complex in itself.  Once you have cut your sound source up you can apply various functions to your segments before extracting features from them. Some of these functions (called window functions) are the: Rectangular, Hamming, Hann, Bartlett, Triangular, Bartlett_hann, Blackman, and Blackman_harris.Once you have your segments cut from your domain you can then extract features to represent those segments.  Some of these will depend on the domain you selected.  A few example of features are: Your normal statistical features (Mean, Variance, Skewness, etc.), ZCR, RMS, Spectral Centroid, Spectral Irregularity, Spectral Flatness, Spectral Tonality, Spectral Crest, Spectral Slope, Spectral Rolloff, Spectral Loudness, Spectral Pitch, Harmonic Odd Even Ratio, MFCC's and Bark Scale. There are many more but these are some good basics. Is it possible, with recurrent neural networks, to output a vector of sequenced musical notes (ABCDEF)?Yes it is. There have been several works to do this already. (Here are several readings) Any smart way I can feed in the features of the soundwaves as well as sequence of musical notes?The standard method is to use the explanation I made above (Domain, Segment, Feature Extract) etc.  To save yourself some work I highly recommend starting with a MIR framework such as MARSYAS (Marsyas).  They will provide you with all the basics of feature extraction.  There are many frameworks so just find one that uses a language you are comfortable in.";;;
6486;1;2015-07-17T08:58:09.017;Data.frame vs Data.table in R?;Data.table package is claimed to be faster than Data.frame. what are the implementation changes that made this possible? How can one leverage the power of this package for data analysis?;[education, open-source];68;1
6487;2;2015-07-17T10:13:29.843;;You have a nice free course about it on DataCamp Data Analysis in R, the data.table WayThe author of the course is Matt Dowle, the guy that actually wrote the data.table package.Hope that helps.;;;
6490;1;2015-07-17T14:17:23.207;Parameter estimation: reduce time;"Sorry if this is a duplicate.I have a two-class prediction model; it has n configurable (numeric) parameters. The model can work pretty well if you tune those parameters properly, but the specific values for those parameters are hard to find. I used grid search for that (providing, say, m values for each parameter). This yields m ^ n times to learn, and it is very time-consuming even when run in parallel on a machine with 24 cores.I tried fixing all parameters but one and changing this only one parameter (which yields m × n times), but it's not obvious for me what to do with the results I got. This is a sample plot of precision (triangles) and recall (dots) for negative (red) and positive (blue) samples:Simply taking the ""winner"" values for each parameter obtained this way and combining them doesn't lead to best (or even good) prediction results. I thought about building regression on parameter sets with precision/recall as dependent variable, but I don't think that regression with more than 5 independent variables will be much faster than grid search scenario. What would you propose to find good parameter values, but with reasonable estimation time?  ";[education, open-source];21;1
6491;2;2015-07-17T17:39:28.300;;If an exhaustive nonlinear scan is too expensive and a linear scan doesn't yield the best results then I suggest you try a stochastic nonlinear search i.e. a random search for hyperparameter optimization.  Scikit learn has a user friendly description in its user guide.Here is a paper on random search for hyperparameter optimization.;;;
6492;1;2015-07-17T18:50:39.330;How to calculate most frequent value combinations;"I have the following CSV data:shot_id,round_id,hole,shotType,clubType,desiredShape,lineDirection,shotQuality,note48,2,1,tee,driver,straight,straight,good,49,2,1,approach,iron,straight,right,bad,50,2,1,approach,wedge,straight,straight,bad,51,2,1,approach,wedge,straight,straight,bad,52,2,1,putt,putter,straight,straight,good,53,2,1,putt,putter,straight,straight,good,54,2,2,tee,driver,draw,straight,good,55,2,2,approach,iron,draw,straight,good,56,2,2,putt,putter,straight,straight,good,57,2,2,putt,putter,straight,straight,good,58,2,3,tee,driver,draw,straight,good,59,2,3,approach,iron,straight,right,good,60,2,3,chip,wedge,straight,straight,good,61,2,3,putt,putter,straight,straight,good,62,2,4,tee,iron,straight,straight,good,63,2,4,putt,putter,straight,straight,good,64,2,4,putt,putter,straight,straight,good,65,2,5,tee,driver,straight,left,good,66,2,5,approach,wedge,straight,straight,good,67,2,5,putt,putter,straight,straight,bad,68,2,5,putt,putter,straight,straight,good,69,2,6,tee,driver,draw,straight,bad,70,2,6,approach,hybrid,draw,straight,good,71,2,6,putt,putter,straight,straight,good,72,2,6,putt,putter,straight,straight,good,73,2,7,tee,driver,straight,straight,good,74,2,7,approach,wood,fade,straight,good,75,2,7,approach,wedge,straight,straight,bad,long76,2,7,putt,putter,straight,straight,good,77,2,7,putt,putter,straight,straight,good,78,2,8,tee,iron,straight,right,bad,79,2,8,approach,wedge,straight,straight,good,80,2,8,putt,putter,straight,straight,bad,81,2,9,tee,driver,straight,straight,good,82,2,9,approach,iron,straight,straight,good,83,2,9,approach,wedge,straight,straight,bad,84,2,9,putt,putter,straight,straight,good,85,2,9,putt,putter,straight,straight,good,86,2,10,tee,driver,straight,left,good,87,2,10,approach,iron,straight,left,good,88,2,10,chip,wedge,straight,straight,good,89,2,10,putt,putter,straight,straight,good,90,2,10,putt,putter,straight,straight,good,91,2,11,tee,driver,draw,straight,good,92,2,11,approach,iron,draw,straight,good,93,2,11,putt,putter,straight,straight,good,94,2,11,putt,putter,straight,straight,good,95,2,12,tee,iron,draw,straight,good,96,2,12,putt,putter,straight,straight,good,97,2,12,putt,putter,straight,straight,good,98,2,13,tee,driver,draw,straight,good,99,2,13,approach,wood,straight,straight,bad,topped100,2,13,putt,putter,straight,straight,good,101,2,13,putt,putter,straight,straight,good,102,2,14,tee,driver,draw,straight,good,103,2,14,approach,wood,straight,straight,bad,104,2,14,approach,iron,draw,straight,good,105,2,14,approach,wedge,straight,straight,bad,106,2,14,putt,putter,straight,straight,bad,107,2,14,putt,putter,straight,straight,good,108,2,15,tee,iron,draw,right,bad,109,2,15,approach,wedge,straight,straight,good,110,2,15,putt,putter,straight,straight,good,111,2,15,putt,putter,straight,straight,good,112,2,16,tee,driver,draw,right,good,113,2,16,approach,iron,straight,left,bad,114,2,16,approach,wedge,straight,left,bad,115,2,16,putt,putter,straight,straight,good,116,2,17,tee,driver,straight,straight,good,117,2,17,approach,wood,straight,right,bad,118,2,17,approach,wedge,straight,straight,good,119,2,17,putt,putter,straight,straight,good,120,2,17,putt,putter,straight,straight,good,121,2,18,tee,driver,fade,right,bad,122,2,18,approach,wedge,straight,straight,good,123,2,18,approach,wedge,straight,straight,good,124,2,18,putt,putter,straight,straight,good,125,2,18,putt,putter,straight,straight,good,And I would like to be able to identify which combinations of values are the most frequently occurring.club types: driver, wood, iron, wedge, putterShot types: tee, approach, chip, puttline directions: left, center, rightshot qualities: good, bad, neutralWhere ideally I'd be able to identify a sweet spot (no pun intended) combination: ""driver"" + ""tee"" + ""straight"" + ""good""I intend only to measure this for a static dataset, not for any future values or prediction. So, my thought is that this is probably a clustering / k-means problem. Is that correct?If so, how would I begin doing a K-Mean analysis with these types of values in R?If it isn't a kmeans problem, then what is it?";[education, open-source];49;
6493;2;2015-07-17T19:30:02.820;;"If I understand your question you want to know which combination is most frequent or how frequent a combination is relative to others. This is a static method that will determine the unique combinations in total (i.e., combinations of all five columns). The plyr package has a nifty utility for grouping unique combinations of columns in a data.frame. We can specify the names of the columns we want to group by, and then specify a function to perform for each of those combinations. In this case, we specify the columns associated with your golf shot qualities and use the function nrow which will count the number of rows in every subset of the large data.frame for which the columns are the identical.# You need this library for the ddply() functionrequire(plyr)# These are the columns that determine a unique situation (change this if you need)qualities <- c(""shotType"",""clubType"",""desiredShape"",""lineDirection"",""shotQuality"")# The call to ddply() actually gives us what we want, which is the number # of times that combination is present in the datasetcountedCombos <- ddply(golf,qualities,nrow)# To be nice, let's give that newly added column a meaningful namenames(countedCombos) <- c(qualities,""count"")# Finally, you probably want to order it (decreasing, in this case)countedCombos <- countedCombos[with(countedCombos, order(-count)),]Now check out your product. The final column has the count associated with each unique combination of columns you provided to ddply:head(countedCombos)   shotType clubType desiredShape lineDirection shotQuality count16     putt   putter     straight      straight        good    3010 approach    wedge     straight      straight        good     69  approach    wedge     straight      straight         bad     519      tee   driver         draw      straight        good     522      tee   driver     straight      straight        good     42  approach     iron         draw      straight        good     3To see the results for a particular cross-section (say, for example, the driver clubType):countedCombos[which(countedCombos$clubType==""driver""),]   shotType clubType desiredShape lineDirection shotQuality count19      tee   driver         draw      straight        good     522      tee   driver     straight      straight        good     421      tee   driver     straight          left        good     217      tee   driver         draw         right        good     118      tee   driver         draw      straight         bad     120      tee   driver         fade         right         bad     1As a bonus, you can dig into these results with ddply again. For example, if you wanted to look at the ratio of ""good"" to ""bad"" shotQuality based on shotType and clubType:shotPerformance <- ddply(countedCombos,c(""shotType"",""clubType""),    function(x){        total<- length(x$shotQuality)            good <- length(which(x$shotQuality==""good""))        bad <- length(which(x$shotQuality==""bad""))        c(total,good,bad,good/(good+bad))    } )names(shotPerformance)<-c(""count"",""shotType"",""clubType"",""good"",""bad"",""goodPct"")This gives you a new breakdown of some math performed on the counts of a character field (shotQuality) and shows you how you can build custom functions for ddply. Of course, you can still order these whichever way you want, too.head(shotPerformance)  shotType clubType total good bad   goodPct1 approach   hybrid    1  1   0 1.00000002 approach     iron    6  4   2 0.66666673 approach    wedge    3  1   2 0.33333334 approach     wood    3  1   2 0.33333335     chip    wedge    1  1   0 1.00000006     putt   putter    2  1   1 0.5000000";;;
6494;1;2015-07-17T21:04:15.433;Markov Chains: How much steps to conclude a Transition Matrix;I have just learned Markov Chains which I am using to model a real world problem. The model comprises 3 states [a b c]. For now I am collection data and calculating transitional probabilities:-T[a][b] = #transitions from a to b / #total transitions to aHowever I am stuck at determining the correct Transition Matrix. As I am getting more data, the matrix is changing drastically. So when do I finalize Transition Matrix? Does that mean that my data is too random and cannot be modelled or I am doing some mistake here?;[education, open-source];43;
6495;1;2015-07-18T04:15:53.060;How do I put plus or minus sign as an x-axis label on a Matlab plot?;"For Matlab plots, how do I place plus or minus sign as X tick labels in a Matlab plot? Currently, I'm using these lines of code but I would like to obtain plus-minus sign (±) values preceding a numerical value on the x tick labels.x_values = ['+/- 1';'+/- 2'];set(gca,'XTickLabel',x_values)";[education, open-source];16;
6496;2;2015-07-18T06:29:29.183;;I expect you have, or can make, a matrix of transition counts. Consider the data in each row to be draws from a multinomial distribution. Then you should be able to use sample size calculations for the multinomial to get off the ground. It is also possible that your data is not well described by a simple Markov chain. There are some available techniques for this, e.g. multistate modelling, but which may or may not fit your particular problem. ;;;
6497;2;2015-07-18T16:58:37.283;;"In gereral, there are four ways one can ""connect"" neural networks (depending on you application at hand) as described in Combining Artificial Neural Networks, Sharkey et al.:In the cooperative mode, there are various ways in which one can combine the decisions made by different models. One common way is to take the average of the predictions. Other ways are taking the median, or some weighed average of them.In the competitive mode, it is winner-take-all, where the prediction from the best model for that test point is selected.In sequential mode, you classify on coarse classes (i.e. natural objects vs. man made objects), the prediction from this stage would be used to decide what to do next..(i.e. if the model predicts, man-made objects, one can have a classifier to do fine grained classification).Neural networks have lot of hyper-parameters to tune. In the supervisory mode, one can train an neuralnet just to select these hyper-parameters of a ""real"" neuralnet that is actually trained on your problem at hand.Please dig deeper into the above mentioned article for more details.";;;
6498;1;2015-07-18T19:39:18.780;Advantage of a treebank in XML format;Which treebanks are based on an XML format? What is the advantage of XML format for a treebank? I think it may have effects on annotation and querying the treebank. for example LASSY and Alpino or TIGER are in xml format.;[education, open-source];17;
6499;2;2015-07-18T22:10:05.050;;If you are using sklearn, there is a good function available called model.feature_importances_.Give it a try with your model/new feature and see if it helps. Also look here and here for examples.;;;
6500;1;2015-07-19T02:40:32.413;Sort a data frame when column name is assigned in a variable;"I am new to R and hence my question is likely to be basic. I tried researching the answer before posting here however didn't get the answer I was looking for.I am trying to order a data frame on the basis of certain columns. The column is dynamic and depends on a certain ""outcome"" which would be from a vector of ""validoutcomes"".validoutcomes <- c(""A"",""B"",""C"")if outcome is ""A"" then I am supposed to read data from column 11. For ""B"" its columns 17 and for ""C"" it is 23.I renamed the columns so that it is easier for my readability and alignment with  validoutcomes. ""input"" is my source data from the CSV.colnames(input)[11] <- validoutcomes[1] colnames(input)[17] <- validoutcomes[2] colnames(input)[23] <- validoutcomes[3]I then tried to sort usingstatelist <- statelist[order(statelist$outcome)]Any ideas?";[education, open-source];41;1
6501;1;2015-07-19T04:15:48.013;Conversion to POSIXct from integral number in R;"Sorry if this is not a good question. I am working with a time series and I want to convert the integral time format downloaded in a file like ""20150710"" to the POSIXct time format which is the absolute number of seconds from the origin ""1970-01-01"" to the time given. My plan is first convert this integral time to some conventional format in character like ""2015-07-10"", and then use R function as.POSIXct to get the final answer. Right now I have difficulty to convert it into character. Does anyone have any other solution or any idea about my solution? Many thanks, ";[education, open-source];4;
6503;2;2015-07-19T14:54:16.683;;"I haven't tried R (well, a bit, but not enough to make a good comparison). However, here are some of Pythons strengths:Very intuitive syntax: tuple unpacking, element in a_list, for element in sequence, matrix_a * matrix_b (for matrix multiplication), ...Many libraries:scipy: Scientific computations; many parts of it are only wrappers for pretty fast Fortran codetheano > Lasagne > nolearn: Libraries for neural networks - they can be trained on GPU (nvidia, CUDA is required) without any adjustmentsklearn: General learning algorithmsGood community:Python has 448,000+ questions on SO (R has 99,000+)Python has 103 questions on datascience.SE (R has 140)PEPs, 63,000+ packages on PyPIIPython notebooksMisc:0-indexed arrays ... I made that error all the time with R.Established package structuresGood support for testing your code";;;
6504;1;2015-07-19T19:08:05.297;How to efficiently categorise transactions;"I'm doing analysis of our family spending over the past couple of years. I have a dump of our credit card and bank account statements, which consists of: date, description, amount. What I want to do is assign a category to each transaction, and from there do analysis of how much we're spending on each category. I already have a good idea of the categories (Food, Travel, Petrol, Socialising, Bills, Kids, etc.). When I look through the data, I can manually assign a category to each transaction, by looking at the description.So I could do this manually, but there's 2000+ transactions to go through. I've found that certain keywords in the description always result in a particular category (e.g. Sainsburys -> food; Shell -> petrol, etc.) I've done a quick and dirty python script that helps with this a bit. But as I've been doing this, I have a feeling that I'm missing a trick. There's probably some tool that can do this much better than my dodgy python script.So please, if you have any suggestions to improve my analysis workflow, let me know. I'm open to tool recommendations, or just general ideas for improving this. Thanks in advance for anything you can offer.";[education, open-source];14;
6505;2;2015-07-19T21:10:09.263;;Ok, I have found the likely answer. I am not sure if this is the best answer but it worked for me. Since $ is used to call the colnames, R was looking for a column name as outcome. However all it needed was square brackets[]. I replaced it and so far seems to be working for me.statelist <- statelist[order(statelist[outcome])]I am still very interested in knowing if there is a better solution or if someone sees issue with my observation.;;;
6506;1;2015-07-20T04:48:17.547;Shall I use the Euclidean Distance or the Cosine Similarity to compute the semantic similarity of two words?;I want to compute the semantic similarity of two words using their vector representations (obtained using e.g. word2vec, GloVe, etc.). Shall I use the Euclidean Distance or the Cosine Similarity?The GloVe website mentions both measures without telling the pros and cons of each: The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.;[education, open-source];61;
6507;1;2015-07-20T06:15:34.527;how to calculate recency(RFM) from a given date attribute in R;custid  tot_prod       tot_rev          Last_tran   total_trans1   1002          13       1465.25         11/23/2011       42   1003          2         353.25          1/1/2011        13   1004          12        2466.5          11/25/2011      12Above is the Retail dataset of customer. I need to calculate recency(a new attribute when the customer is last seen) from last transaction(last_tran)  in terms of month.i.e. he purchased in last 3 months or last 6 months or may be one year in R. please suggest.    ;[education, open-source];17;
6508;1;2015-07-20T08:03:21.013;K-means incoherent behaviour choosing K with Elbow method, BIC, variance explained and silhouette;"I'm trying to cluster some vectors with 90 features with K-means. Since this algorithm asks me the number of clusters, I want to validate my choice with some nice math.I expect to have from 8 to 10 clusters. The features are Z-score scaled.Elbow method and variance explainedfrom scipy.spatial.distance import cdist, pdistfrom sklearn.cluster import KMeansK = range(1,50)KM = [KMeans(n_clusters=k).fit(dt_trans) for k in K]centroids = [k.cluster_centers_ for k in KM]D_k = [cdist(dt_trans, cent, 'euclidean') for cent in centroids]cIdx = [np.argmin(D,axis=1) for D in D_k]dist = [np.min(D,axis=1) for D in D_k]avgWithinSS = [sum(d)/dt_trans.shape[0] for d in dist]# Total with-in sum of squarewcss = [sum(d**2) for d in dist]tss = sum(pdist(dt_trans)**2)/dt_trans.shape[0]bss = tss-wcsskIdx = 10-1# elbow curvefig = plt.figure()ax = fig.add_subplot(111)ax.plot(K, avgWithinSS, 'b*-')ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')plt.grid(True)plt.xlabel('Number of clusters')plt.ylabel('Average within-cluster sum of squares')plt.title('Elbow for KMeans clustering')fig = plt.figure()ax = fig.add_subplot(111)ax.plot(K, bss/tss*100, 'b*-')plt.grid(True)plt.xlabel('Number of clusters')plt.ylabel('Percentage of variance explained')plt.title('Elbow for KMeans clustering')From these two pictures, it seems that the number of clusters never stops :D. Strange! Where is the elbow? How can I choose K?Bayesian information criterionThis methods comes directly from X-means and uses the BIC to choose the number of clusters. another ref    from sklearn.metrics import euclidean_distancesfrom sklearn.cluster import KMeansdef bic(clusters, centroids):    num_points = sum(len(cluster) for cluster in clusters)    num_dims = clusters[0][0].shape[0]    log_likelihood = _loglikelihood(num_points, num_dims, clusters, centroids)    num_params = _free_params(len(clusters), num_dims)    return log_likelihood - num_params / 2.0 * np.log(num_points)def _free_params(num_clusters, num_dims):    return num_clusters * (num_dims + 1)def _loglikelihood(num_points, num_dims, clusters, centroids):    ll = 0    for cluster in clusters:        fRn = len(cluster)        t1 = fRn * np.log(fRn)        t2 = fRn * np.log(num_points)        variance = _cluster_variance(num_points, clusters, centroids) or np.nextafter(0, 1)        t3 = ((fRn * num_dims) / 2.0) * np.log((2.0 * np.pi) * variance)        t4 = (fRn - 1.0) / 2.0        ll += t1 - t2 - t3 - t4    return lldef _cluster_variance(num_points, clusters, centroids):    s = 0    denom = float(num_points - len(centroids))    for cluster, centroid in zip(clusters, centroids):        distances = euclidean_distances(cluster, centroid)        s += (distances*distances).sum()    return s / denomfrom scipy.spatial import distancedef compute_bic(kmeans,X):    """"""    Computes the BIC metric for a given clusters    Parameters:    -----------------------------------------    kmeans:  List of clustering object from scikit learn    X     :  multidimension np array of data points    Returns:    -----------------------------------------    BIC value    """"""    # assign centers and labels    centers = [kmeans.cluster_centers_]    labels  = kmeans.labels_    #number of clusters    m = kmeans.n_clusters    # size of the clusters    n = np.bincount(labels)    #size of data set    N, d = X.shape    #compute variance for all clusters beforehand    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])    const_term = 0.5 * m * np.log(N) * (d+1)    BIC = np.sum([n[i] * np.log(n[i]) -               n[i] * np.log(N) -             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term    return(BIC)sns.set_style(""ticks"")sns.set_palette(sns.color_palette(""Blues_r""))bics = []for n_clusters in range(2,50):    kmeans = KMeans(n_clusters=n_clusters)    kmeans.fit(dt_trans)    labels = kmeans.labels_    centroids = kmeans.cluster_centers_    clusters = {}    for i,d in enumerate(kmeans.labels_):        if d not in clusters:            clusters[d] = []        clusters[d].append(dt_trans[i])    bics.append(compute_bic(kmeans,dt_trans))#-bic(clusters.values(), centroids))plt.plot(bics)plt.ylabel(""BIC score"")plt.xlabel(""k"")plt.title(""BIC scoring for K-means cell's behaviour"")sns.despine()#plt.savefig('figures/K-means-BIC.pdf', format='pdf', dpi=330,bbox_inches='tight')Same problem here... What is K?Silhouette    from sklearn.metrics import silhouette_scores = []for n_clusters in range(2,30):    kmeans = KMeans(n_clusters=n_clusters)    kmeans.fit(dt_trans)    labels = kmeans.labels_    centroids = kmeans.cluster_centers_    s.append(silhouette_score(dt_trans, labels, metric='euclidean'))plt.plot(s)plt.ylabel(""Silouette"")plt.xlabel(""k"")plt.title(""Silouette for K-means cell's behaviour"")sns.despine()Alleluja! Here it seems to make sense and this is what I expect. But why is this different from the others?";[education, open-source];38;
6509;1;2015-07-20T10:52:59.633;Techniques for dealing with unevenly spaced time series data that have missing time-stamps?;What are the techniques to infer missing time-stamps in the unevenly spaced time series data that has missing time-stamps?The frequency of data recording is about 1-3 times a day without any fixed time of measurement. In some cases the time stamp of the measurement is unavailable.It is possible to predict at which time $t_0$ the new sample $x_0$ was recorded knowing the history of previous measurements $(t_n,x_n)$?$$(t_{-n},x_{-n}), ... ,(t_{-3},x_{-3}), (t_{-2},x_{-2}), (t_{-1},x_{-1}), (t_0,x_0)$$Or predict multiple missing time-stamps e.g. $t_{-2},t_{-6},t_{-7}$ from recorded sequence $t_n,x_n$?;[education, open-source];20;
6510;1;2015-07-20T13:42:21.283;Does modeling with Random Forests requre cross-validation?;"As far as I've seen, opinions tend to differ about this. Best practice would certainly dictate using cross-validation (especially if comparing RFs with other algorithms on the same dataset). On the other hand, the original source states that the fact OOB error is calculated during model training is enough of an indicator of test set performance. Even Trevor Hastie, in a relatively recent talks says that ""Random Forests provide free cross-validation"". Intuitively, this makes sense to me, if training and trying to improve a RF-based model on one dataset.Can someone please lay out the arguments for and against the need for cross-validation with random forests?";[education, open-source];85;1
6511;1;2015-07-20T15:54:31.440;Are there any interesting application of linear regression;Linear regression is a widely used ML algorithm. So far I have only encountered 'boring' applications of it. (e.g predict sales for next quarter, predict housing prices for next year , predict population of a country by 2020 etc.)What are some interesting/cool application of linear regression? Finding cool applications greatly helps in motivating myself to learn, hence the quest.Cool as in: Application to stock trading, application to video games, application to astronomy, application to sports betting, application to airfare prediction;[education, open-source];46;
6512;2;2015-07-20T15:59:25.220;;I think those were the coolest.I recommend you that video alsohttps://www.khanacademy.org/math/probability/regression/regression-correlation/v/regression-line-example;;;
6513;1;2015-07-20T16:00:53.090;How to control false positives in sequential A/B testing while keeping a low sample size?;"I am working on a planned sequence of n independent A/B tests(=run a maximum of n tests or stop earlier if a good improvement is found) and in order to keep the significance level within an acceptable level(=0.05) I'm considering ways of controlling the false positives rate while keeping the sample size as low as I can(it's for a low traffic website)I'm aware of the Bonferroni, Benjiamimi-Hochberg and related methods designed to control the proportion of false positives in the multiple comparisons situation. However the problem of computing the sample size still remains, the simplest approach seems the one of computing the required sample size starting from alpha = 0.05/n as dictated by the Bonferroni correction and then perhaps use Benjiamimi-Hochberg for the actual testing. Is there a better way of computing the sample size when Benjiamimi-Hochberg is used?Further as the A/B tests are designed to test possible improvements I was thinking that if the sequential tests are based on independent samples then then probability of two consecutive false positives would be alpha^2 = 0.025. My understanding is that the sequential tests would be independent if the respective samples do not refer to the same users. These could for example be users that joined the website after the previous test.If the above thinking makes some sense, I could use a ""weak"" Bonferroni (i.e. alpha' = alpha/(n*0.2) )correction in order to run tests with a reasonable power and lower sample size and once a positive is found(the null hypothesis is rejected), I could repeat the test and if it's positive again then the result is positive with p < 0.025 which means that I would accept the result. This approach would allow for some control over the sample size but... is it sound?Any opinion would be greatly appreciated. Thanks in advance. AMAR";[education, open-source];21;
6514;2;2015-07-20T16:10:10.593;;"Try thisas.POSIXct(Sys.Date(), format= ""%Y-%m-%d"", tz=""UCT"") - as.POSIXct(""11/25/2011"", format = ""%m/%d/%Y"",tz=""UCT"")More vectorized operation in your situationas.POSIXct(Sys.Date(), format= ""%Y-%m-%d"", tz=""UCT"") - as.POSIXct(Last_tran, format = ""%m/%d/%Y"",tz=""UCT"")";;;
6515;1;2015-07-20T16:15:54.270;How to exctrat a column that has the highest value within row in HiVe?;I have a table more or less in this formatcol1 col2 col3 ... col100val1 val2 val3 ... val100Where val* are doubles. Is there a way to extract for each row in which column is the highest value within row in HiVe?For example for table likecol1 col2 col32     4   58     1   2I would getcol3col1;[education, open-source];9;
6517;2;2015-07-20T17:19:25.467;;"I can't test in hive, but a possible SQL query is as follows (greatest returns the maximum value from the list):select   case    when col1 = greatest(col1,col2,col3) then 'col1'    when col2 = greatest(col1,col2,col3) then 'col2'     when col3 = greatest(col1,col2,col3) then 'col3'      end as c1 from test;Additional note:you should check, how ties are to be handled, in my solution I simple take the first column.";;;
6518;2;2015-07-20T17:22:41.453;;"The way that you have phrased this question makes it tough for people to answer without first offering you some background on linear regression (LR).  Its great that you are interested in learning some ML and LR is a great place to start.Linear regression is really just finding a line (or plain or hyperplain) that maps a relationship between two or more variables or features.  The important requirement is that the target variable be a continuous numerical value.  So its less about finding a problem for which linear regression ""works"" and more about finding some data that interests you and playing with it using linear regression.I suggest you download some open data in the realms that you have mentioned.  There is plenty of open data in every topic that you mentioned that contain continuous numerical values that you can predict using other features of the data.stock tradingvideo gamesastronomysports bettingflight time predictionAlso think about taking some sort of online MOOC as this will help you gain some footing in the subject.  Andrew Ng's Coursera on Machine Learning is highly recommended as a starting point and include some linear regression during the first portion and scikit-learn is a great Python based library.";;;
6519;1;2015-07-20T20:13:53.770;Combining Datasets with Different Features;I have multiple datasets, with slightly differing features. What tools can I use to make this a homogeneous dataset?Dataset1:featureA,featureB,featureC1,7,34,8,4Dataset2:featureA,featureC,featureD,featureE3,4,5,69,8,4,6Homogeneous DatasetfeatureA,featureB,featureC,featureD,featureE1,7,3,,4,8,4,,3,,4,5,69,,8,4,6;[education, open-source];65;
6521;2;2015-07-20T20:41:25.427;;"First of all, if GloVe gives you normalized unit vectors, then the two calculations are equivalent. In general, I would use the cosine similarity since it removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure ""term frequency"" space using the Euclidean distance. They will be right on top of each other in cosine similarity.";;;
6522;2;2015-07-20T20:48:56.047;;"You can use R to do that. The smartbind function is the perfect way to combine datsets in the way you are asking for:library(gtools)d1<-as.data.frame(rbind(c(1,7,3),c(4,8,4))))names(d1)<-c(""featureA"",""featureB"",""featureC"")d2<-as.data.frame(rbind(c(3,4,5,6),c(9,8,4,6)))names(d2)<-c(""featureA"",""featureC"",""featureD"",""featureE"")d3<-smartbind(d1,d2)";;;
6523;2;2015-07-20T21:51:46.383;;"In anomaly detection (one sort of STL application), it's easier to see outliers if you can normalize the original time series to a residual series (called ""remainder"" in R's STL package). That involves identifying components that you can subtract from the original series. In STL, the components are the seasonal series and the trend.If there is a regular, recurring pattern in the data (i.e., a seasonality), then you don't want the expected ups/downs of that pattern to drive the anomaly detector. Instead you want to be able to say ""are we higher or lower than expected, factoring out the regular ups/downs that happen over the course of the day (or week, or month, etc.)"". Identifying and removing the seasonal series allows you to ask that question.The decomposition you generated does not look right: the seasonal pattern doesn't seem to have captured much structure, the trend overfits the data, and the residuals have a regular structure that the seasonal series ought to have captured. I don't know whether this is because the dataset itself doesn't lend itself to an STL decomposition (the series tail appears qualitatively different than the initial part) or if it's because the parameters are wrong. Figure 6.10 on this page shows a more prototypical case.";;;
6524;2;2015-07-21T03:22:58.723;;There are many pitfalls to not scaling your data and it is generally very advisable to scale it. It is so easy to do, it is reversible, and it useful in other operations like removing outliers.Upon further analysis of the specifics of Hidden Markov Models using Multivariate Gaussians the theoretical accuracy should not suffer as a result of drastic differences in the scales of your features.  But, an important operation involving multivariate Gaussian distribution is matrix inversion.Though the theoretical invertibility won't change, the practical numerical solution to your problem will likely suffer inaccuracy due to the difference in scales.  Iterative methods will have issues with convergence and direct solve methods will suffer from stiffness.  This is especially true when common complications like linear dependence are present.Here is a set of 3 lectures on HMM with some specifics on multivariate Gaussians (1-2-3)I know we've already discussed this in the comments, but I wanted to close out this question, so have added it as an answer.I hope this helps! ;;;
6525;1;2015-07-21T06:25:38.057;Frechet distance v/s correlation;I have a series of arrays that correspond to some data measured over a period of time. This data is supposed to stabilize with time. I want to find the time at which this happens. I am thinking of finding the similarity between the plots of these arrays over time (and the point at which the measure of similarity doesn't change significantly would be the point where the data stabilizes). I wish to know if it would be better to find the correlation coefficient or the Frechet distance for the same. ;[education, open-source];11;
6527;2;2015-07-21T07:26:51.743;;One key difference is that cross validation ensures all samples will appear in the training and test sets, so 100% of your data gets used at some point for training and for testing. Depending on the size of your dataset the bootstrapping , sampling with replacement, occurring in the random forest will not guarantee the splits the trees see will contain all instances. If you have enough trees in your forest the OOB estimate should asymptotically converge towards the best OOB estimate value. The accuracy for both methods will to some degree be data dependent so it may be prudent to compare both methods on the particular data you have in front of you and see if CV and RF OOB estimates give similar values. If they do not, then it would be worth exploring further estimates of the true error rate, perhaps by much higher vales of K in CV.;;;
6528;1;2015-07-21T09:39:32.693;Newton-Raphson or EM Algorithm in Python;Is there any implementation of Newton-Raphson or EM Algorithm? Can I get the source code of it?I tried googling, but didn't come across any. So asking here.Thanks!;[education, open-source];31;
6529;2;2015-07-21T09:59:49.177;;scikit learn has the EM algorithm here.Source code is available.And if you are an R fan the mclust package is available here.;;;
6530;1;2015-07-21T12:24:18.423;How to convert unstructured texts to structured data?;I would like to right an algorithm to convert unstructured texts (with contests descriptions) to structured data with the following fields:contest start date (optional)contest end datemain prizeadditional prizes (optional)I have hundreds of text examples, which could be used for model learning. How to approach this task? Just in case this is important - the preferable language is Python. But I never worked on such tasks before. ;[education, open-source];59;
6531;1;2015-07-21T12:44:33.683;K MEANS Big dataset;Can you please recommend me for a big dataset for k means?It would be cool if it will integrate easily with python, but any thing will be good.;[education, open-source];28;
6532;2;2015-07-21T13:52:56.563;;For the very first start I'd recoment synthetical data. Simple draw a K sets of random distributed numbers with different means and required dimensionality.The big advantage is that you know the result clustering and you can easily verify the result. Also scaling the data is not problem.;;;
6533;1;2015-07-21T15:52:30.010;How to store complex tables and structures?;"I'm not speaking about any specific software, just the idea, in general.  but it could be applied to Excel, R or whatever.Imagine I have a table like this:               A1 A2 A3 A4 A5 B1 B2 B3 B4 B5 C1 ....Z5 individuals    1             .  .  .  .  . .  .  .  .  . 2             .  .  .  .  . .  .  .  .  . 3             .  .  .  .  . .  .  .  .  . 4             .  .  .  .  . .  .  .  .  . 5             .  .  .  .  . .  .  .  .  . ...           .  .  .  .  . .  .  .  .  .I just show the row and column names (variables) but the table would be filled.or it could be even much larger, with A1 to A999, B1 to B999 colums...I can want to perform some calculation on all Ax columns, or on all x3 ones.It can be difficult to deal with so many columns, It could be better to write it as: i  subcase  A B C ... Z 1  1      .  .  .  .   1  2      .  .  .  .   1  3      .  .  .  .   1  4      .  .  .  .   1  5      .  .  .  .   1  6      .  .  .  .   1  7      .  .  .  .   1  8      .  .  .  .   1  9      .  .  .  .   2  1      .  .  .  .   2  2      .  .  .  .   ...       .  .  .  .  I think I've seen that kind of structure several times, maybe it's called long format.you can later perform some operations on all the rows meeting some condition on ""case"".I has some advantages but it could be confusing.You could average or crate the diffs of all the subcase=1 rows, or only from individual=1, or when individual=subcase,  or even add another level.But what would be the best way to store that resulting numbers?, columnwise or rowise?If I wanted to store the average of all subcase=1 rows  I would need to repeat the number on evert row meeting that condition.Another option would be to keep the wide format structure but having 2 rows for the headers, one for the letters and a second one for the numbers.  I think it could be a natural way but I've never seen it before. Maybe it's not a good idea to use rows both for both individuals and headers.What other structures would you recommend for this problem or more comeplex ones (such as trees)?Maybe using different tables for the results? The question is: Wide format vs Long Format vs Multidimensional Array?";[education, open-source];40;1
6534;2;2015-07-21T17:45:54.623;;"If I get you right, you would like to extract the mentioned entities from the unstructured text. This sounds as Named Entity Recognition (NER) problem for me. In your case I would define the following entities:START_DATEEND_DATEMAIN_PRIZEEXTRA_PRIZEYour text now can be tagged or annotated with those entities. You would like to build a machine which will automatically tag the encountered entities in the unstructured text (free text).I would suggest to take a look at Conditional Random Fields (CRF), a classic approach to NER. There are also Recurrent Neural Networks (RNN) which are state of the art for sequence modeling but I am not an expert with those methods.Let's begin the process of model building. You need to decide what will be a single training example. This can be a sentence and this can be also a paragraph.Since you did not provide any example, let me invent two:The contest started on <START_DATE>july 1st</START_DATE> and ended on <END_DATE>july 25</END_DATE>.The winner got away with <MAIN_PRIZE>1 million dollars</MAIN_PRIZE> and <EXTRA_PRIZE>bunch of flowers</EXTRA_PRIZE>.Notice the way how the text is structured. Every word now is either label (surrounded by tags) or not labeled at all (outside of any tag). This is one way how to structure the text. This would be also the desired output on some unstructured text.The machine does not understand the text and annotation. You need to come up with features that you can extract from each word. It can be the word itself, the previous word, the next word, left bigram, right bigram, suffix of length 1,2,3, prefixes of length 1,2,3 and so on. I suggest you to look at Stanford NER features.Once you got the features, you need to create a training set. The training set is your annotated text (see my example) converted to label + feature information. It may look like thisO   feature1 feature2 ...O   feature1 feature2 ...B-START_DATE   feature1 feature2 ...I-START_DATE   feature1 feature3 ......This basically says that the first two words are not labeled, but the last two are part of an entity START_DATE.Each line is essentially a word followed by features for it. The label ""O"" is the way to say that the word is not labeled, the labels B- and I- for DATE mean the first and second words of a named entity (e.g. ""july 1st"").The sentences can be separated by empty lines.Once you've got the training set file ready, you can do the model training. With a model in hand, you can apply it on any unstructured data (extracted features) to get the desired entities extracted.As a tool I suggest you CRFSuite (also available with Python bindings). I can detail more each step if you would like.";;;
6535;2;2015-07-21T19:03:19.130;;Look for NLTK, a Python package for analysing natural language data. With it, you can add some structure to text as written by humans. ;;;
6536;2;2015-07-21T19:20:19.363;;"It all depends on what you're trying to accomplish. For example, if you want to save space and you have sparse data, you want your table to grow ""long"" as you describe and let the absence of a row signify that you don't have data (rather than structuring your sparse field as a column, which would typically require space to store your empty data points). If you need a data structure that makes it easiest to ""process"" the data, it depends on what your process is. Sometimes, a complex, nested data structure is best (e.g., JSON), other times, a flat table where some fields have repeating values is best. ";;;
6537;2;2015-07-21T21:45:39.853;;I am first deriving the error for a convolutional layer below for simplicity for a one dimensional array (input) which can easily be transferred to a multidimensional then:  We assume here that the $y^{l-1}$ of length $N$ are the inputs of the $l-1$-th conv. layer, $m$ is the kernel-size of weights $w$ denoting each weight by $w_i$ and the output is $x^l$.Hence we can write (note the summation from zero): $$x_i^l = \sum\limits_{a=0}^{m-1} w_a y_{a+i}^{l-1}$$ where $y_i^l = f(x_i^l)$ and $f$ the activation function (e.g. sigmoidal).With this at hand we can now consider some error function $E$ and the error function at the convolutional layer (the one of your previous layer) given by $\partial E / \partial y_i^l $. We now want to find out the dependency of the error in one the weights in the previous layer(s): \begin{equation} \frac{\partial E}{\partial w_a} = \sum\limits_{a=0}^{N-m} \frac{\partial E}{\partial x_i^l} \frac{\partial x_i^l}{\partial w_a} = \sum\limits_{a=0}^{N-m}\frac{\partial E}{\partial w_a} y_{i+a}^{l-1} \end{equation}where we have the sum over all expression in which $w_a$ occurs, which are $N-m$. Note also that we know the last term arises from the fact that $\frac{\partial x_i^l}{\partial w_a}= y_{i+a}^{l-1}$ which you can see from the first equation.To compute the gradient we need to know the first term, which can be calculated by: $$ \frac{\partial E}{\partial x_i^l} = \frac{\partial E}{\partial y_i^l} \frac{\partial y_i^l}{\partial x_i^l} = \frac{\partial E}{\partial y_i^l} \frac{\partial}{\partial x_i^l} f(x_i^{l})$$ where again the first term is the error in the previous layer and $f$ the nonlinear activation function.Having all necessary entities we are now able to calculate the error and propagate it back efficiently to the precious layer:$$ \delta^{l-1}_a = \frac{\partial E}{\partial y_i^{l-1} } = \sum\limits_{a=0}^{m-1} \frac{\partial E}{\partial x_{i-a}^l} \frac{\partial x_{i-a}^l}{\partial y_i^{l-1}} = \sum\limits_{a=0}^{m-1} \frac{\partial E}{\partial x^l_{i-a}} w_a^{flipped}$$Note that the last step can be understood easy when writing down the $x_i^l$-s w.r.t. the $y_i^{l-1}$-s. The $flipped$ refers to a transposed weight maxtrix ($T$).Therefore you can just calculate the error in the next layer by (now in vector notation):  $$\delta^{l} = (w^{l})^{T} \delta^{l+1} f'(x^{l})$$which becomes for a convolutional and subsampling layer:$$\delta^{l} = upsample((w^{l})^{T} \delta^{l+1}) f'(x^{l})$$where the $upsample$ operation propagates the error through the max pooling layer.Please feel free to add or correct me!For references see:  http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/and for a C++ implementation (without requirement to install):https://github.com/nyanp/tiny-cnn#supported-networks;;;
6538;1;2015-07-22T00:28:50.967;Different estimation methods of Time series model;This question might be a huge topic, but let's talk about the simple case, say AR (auto-regressive) model. Usually people have various methods of estimating the AR model, like Burgs algorithm, Yule walker algorithm, or even OLS (ordinary least squares). Could some one tell me what is the motivation of using so many different skills to estimate the same model. Is there any literature of book discussing these topic? Thanks a lot, ;[education, open-source];26;1
6539;1;2015-07-22T04:08:25.287;How to Restore from sql Maintenance plan backup;I have a SQl 2005 Instance in my PROD SQL server. I have been given a task to migrate databases inside that instance to different SQL box ( SQl 2008 r2 in Win 2008 r2 box) Database. Current sql instance have maintenance plan set up which backs up 8 different database every night under single .bak file.Now how do I take that bak file and restore it into new SQl 2008 r2 instance. I have tried using SSMS in Sql 2008 r2 and set the restore from option to device and point it to that .bak file but what should I target restore to since I have multiple databases restored in this new instance.What is the best approach to migrate all databases in single shot to next instance without doing one at a time. Please advise..;[education, open-source];4;
6543;2;2015-07-22T06:22:54.263;;The simplest way to setup a user directory is to login to Hue as an admin and create a user account. This will create a directory in HDFS located at /user/yourusername with the correct permissions to work in.;;;
6544;1;2015-07-22T09:21:21.573;Normalized Euclidean Distance versus cross correlation?;Normalized Euclidean Distance and Normalized Cross - Correlation can both be used as a metric of distance between vectors.  What is the difference between these metrics?  It seems to me that they are the same, although I have not seen this explicitly stated in any textbook or literature.thank you.  ;[education, open-source];41;
6545;2;2015-07-22T11:27:06.997;;These two metrics are not the same.The normalized Euclidean distance is the distance between two normalized vectors that have been normalized to length one.  If the vectors are identical then the distance is 0, if the vectors point in opposite directions the distance is 2, and if the vectors are orthogonal (perpendicular) the distance is sqrt(2).  It is a positive definite scalar value between 0 and 2.The normalized cross-correlation is the dot product between the two normalized vectors.  If the vectors are identical, then the correlation is 1, if the vectors point in opposite directions the correlation is -1, and if the vectors are orthogonal (perpendicular) the correlation is 0.  It is a scalar value between -1 and 1.This all comes with the understanding that in time-series analysis the cross-correlation is a measure of similarity of two series as a function of the lag of one relative to the other.;;;
6546;1;2015-07-22T14:16:22.823;How to calculate the mean of a dataframe column and find the top 10%;"I am very new to Scala and Spark, and am working on some self-made exercises using baseball statistics. I am using a case class create a RDD and assign a schema to the data, and am then turning it into a DataFrame so I can use SparkSQL to select groups of players via their stats that meet certain criteria.Once I have the subset of players I am interested in looking at further, I would like to find the mean of a column; eg Batting Average or RBIs. From there I would like to break all the players into percentile groups based on their average performance compared to all players; the top 10%, bottom 10%, 40-50%I've been able to use the DataFrame.describe() function to return a summary of a desired column (mean, stddev, count, min, and max) all as strings though. Is there a better way to get just the mean and stddev as Doubles, and what is the best way of breaking the players into groups of 10-percentiles?So far my thoughts are to find the values that bookend the percentile ranges and writing a function that groups players via comparators, but that feels like it is bordering on reinventing the wheel.I have the following imports currently: import org.apache.spark.rdd.RDD  import org.apache.spark.sql.SQLContext  import org.apache.spark.{SparkConf, SparkContext}  import org.joda.time.format.DateTimeFormat  ";[education, open-source];104;
6547;1;2015-07-22T14:26:58.660;Open source Anomaly Detection in Python;"Problem Background:I am working on a project that involves log files similar to those found in the IT monitoring space (to my best understanding of IT space). These log files are time-series data, organized into hundreds/thousands of rows of various parameters. Each parameter is numeric (float) and there is a non-trivial/non-error value for each time point. My task is to monitor said log files for anomaly detection (spikes, falls, unusual patterns with some parameters being out of sync, strange 1st/2nd/etc. derivative behavior, etc.). On a similar assignment, I have tried Splunk with Prelert, but I am exploring open-source options at the moment.Constraints:I am limiting myself to Python because I know it well, and would like to delay the switch to R and the associated learning curve. Unless there seems to be overwhelming support for R (or other languages/software), I would like to stick to Python for this task.Also, I am working in a Windows environment for the moment. I would like to continue to sandbox in Windows on small-sized log files but can move to Linux environment if needed. Resources:I have checked out the following with dead-ends as results:Python or R for implementing machine learning algorithms for fraud detection. Some info here is helpful, but unfortunately, I am struggling to find the right package because:Twitter's ""AnomalyDetection"" is in R, and I want to stick to Python. Furthermore, the Python port pyculiarity seems to cause issues in implementing in Windows environment for me. Skyline, my next attempt, seems to have been pretty much discontinued (from github issues). I haven't dived deep into this, given how little support there seems to be online.  scikit-learn I am still exploring, but this seems to be much more manual. The down-in-the-weeds approach is OK by me, but my background in learning tools is weak, so would like something like a black box for the technical aspects like algorithms, similar to Splunk+Prelert.Problem Definition and Questions:I am looking for open-source software that can help me with automating the process of anomaly detection from time-series log files in Python via packages or libraries. Do such things exist to assist with my immediate task, or are they imaginary in my mind? Can anyone assist with concrete steps to help me to my goal, including background fundamentals or concepts? Is this the best StackExchange community to ask in, or is Stats, Math, or even Security or Stackoverflow the better options?EDIT [2015-07-23] Note that the latest update to pyculiarity seems to be fixed for the Windows environment! I have yet to confirm, but should be another useful tool for the community.";[education, open-source];197;2
6548;2;2015-07-22T18:14:36.907;;I am assuming that you have only 1 attribute (numeric).What you can do is modify the .arff file and set 0s to 1 and 1s to 0.Then you can again run j48 classifier and visualise the decision tree.It would give you inverted result.Also, try watching this video.WEKA on MOOC;;;
6549;2;2015-07-22T18:55:05.250;;"Anomaly Detection or Event Detection can be done in different ways:Basic WayDerivative! If the deviation of your signal from its past & future is high you most probably have an event. This can be extracted by finding large zero crossings in derivative of the signal.Statistical WayMean of anything is its usual, basic behavior. if something deviates from mean it means that it's an event. Please note that mean in time-series is not that trivial and is not a constant but changing according to changes in time-series so you need to see the ""moving average"" instead of average. It looks like this:The Moving Average code can be found here. In signal processing terminology you are applying a ""Low-Pass"" filter by applying the moving average.You can follow the code bellow:MOV = movingaverage(TimeSEries,5).tolist()STD = np.std(MOV)events= []ind = []for ii in range(len(TimeSEries)):    if TimeSEries[ii] > MOV[ii]+STD:        events.append(TimeSEries[ii])Probabilistic WayThey are more sophisticated specially for people new to Machine Learning. Kalman Filter is a great idea to find the anomalies. Simpler probabilistic approaches using ""Maximum-Likelihood Estimation"" also work well but my suggestion is to stay with moving average idea. It works in practice very well.I hope I could help :)Good Luck!";;;
6550;1;2015-07-22T18:58:53.040;What is a good non cryptographic Hash for string feature translation?;What would be a good non cryptographic Hash function to use for converting string features to a numerical representation for feeding into machine learning algorithms?To explain the scenario my feature set has both categorical data (e.g.: Country) and non categorical data (e.g.: IP Address, Email address). I have used MurMur3 Hash function so far, is there some better algorithm?;[education, open-source];27;
6551;2;2015-07-22T19:52:55.367;;Maybe  this helps  cause  you mentioned  about steady states:https://github.com/twitter/AnomalyDetectionhttps://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series;;;
6552;2;2015-07-22T21:42:19.010;;"h2o has an anomaly detection module and traditionally the code is available in R.However beyond version 3 it has similar module available in python as well,and since h2o is open source it might fit your bill.You can see an working example over hereimport syssys.path.insert(1,""../../../"")import h2odef anomaly(ip, port):    h2o.init(ip, port)    print ""Deep Learning Anomaly Detection MNIST""    train = h2o.import_frame(h2o.locate(""bigdata/laptop/mnist/train.csv.gz""))    test = h2o.import_frame(h2o.locate(""bigdata/laptop/mnist/test.csv.gz""))    predictors = range(0,784)    resp = 784    # unsupervised -> drop the response column (digit: 0-9)    train = train[predictors]    test = test[predictors]    # 1) LEARN WHAT'S NORMAL    # train unsupervised Deep Learning autoencoder model on train_hex    ae_model = h2o.deeplearning(x=train[predictors], training_frame=train, activation=""Tanh"", autoencoder=True,                                hidden=[50], l1=1e-5, ignore_const_cols=False, epochs=1)    # 2) DETECT OUTLIERS    # anomaly app computes the per-row reconstruction error for the test data set    # (passing it through the autoencoder model and computing mean square error (MSE) for each row)    test_rec_error = ae_model.anomaly(test)    # 3) VISUALIZE OUTLIERS    # Let's look at the test set points with low/median/high reconstruction errors.    # We will now visualize the original test set points and their reconstructions obtained    # by propagating them through the narrow neural net.    # Convert the test data into its autoencoded representation (pass through narrow neural net)    test_recon = ae_model.predict(test)    # In python, the visualization could be done with tools like numpy/matplotlib or numpy/PILif __name__ == '__main__':    h2o.run_test(sys.argv, anomaly)";;;
6553;1;2015-07-22T22:34:43.403;How much training data does word2vec need?;"I'd like to compare the difference among the same word mentioned in different sources. That is, how authors differ in their usage of ill-defined words, such as ""democracy"".A brief plan wasTake the books mentioning the term ""democracy"" as plain textIn each book, replace democracy with democracy_%AuthorName%Train a word2vec model on these booksCalculate the distance between democracy_AuthorA, democracy_AuthorB, and other relabeled mentions of ""democracy""So each author's ""democracy"" gets its own vector, which is used for comparison.But it seems that word2vec requires much more than several books (each relabeled word occurs only in a subset of books) to train reliable vectors. The official page recommends datasets including billions of words.I just wanted to ask how large should be the subset of one author's books to make such inference with word2vec or alternative tools, if available?";[education, open-source];59;1
6554;2;2015-07-22T23:27:40.203;;R's performance depends incredibly on how you write it.  For example, you mostly should never use for loops in R - they're horribly slow because they execute a function call with every iteration.  (One should vectorize and use the apply family of functions instead.  Weird, I know..)  Vectorization is king in R if you want fast code.  Assuming you vectorize both your R and Python code (and other factors), you should probably get the same order of magnitude in speed.  For data larger than memory (you can specify the limit), R starts to become a bad choice.  I don't know much about python's internals, so I can't speak on that.;;;
6555;1;2015-07-23T03:45:36.867;Issue with IPython/Jupyter on Spark (Unrecognized alias);"I am working on setting up a set of VMs to experiment with Spark before I spend go out and spend money on building up a cluster with some hardware. Quick note: I am an academic with a background in applied machine learning and work quit a bit in data science. I use the tools for computing, rarely would I need to set them up. I've created 3 VMs (1 master, 2 slaves) and installed Spark successfully. Everything appears to be working as it should. My problem lies in creating a Jupyter server that can be connected to from a browser not running on a machine on the cluster. I've installed Jupyter notebook successfully... and it runs. I've added a new IPython profile connecting to a remote server with Spark. now the problemThe command$ ipython --profile=pyspark runs fine and it connects to the spark cluster. However, $ ipython notebook --profile=pyspark[<stuff is here>] Unrecognized alias: ""profile=pyspark"", it will probably have no effect. defaults to the default profile not the pyspark profile. My notebook config for pyspark has: c = get_config()c.NotebookApp.ip = '*'c.NotebookApp.open_browser = Falsec.NotebookApp.port = 8880c.NotebookApp.server_extensions.append('ipyparallel.nbextension')c.NotebookApp.password = u'some password is here'";[education, open-source];131;
6556;2;2015-07-23T05:25:56.953;;"Stability-Plasticity Dilemma, Learning Rates, and Forgetting Algorithms:First, let me say that this is a really great question and is the type of thought provoking stuff that really improves one's understanding of ML algorithms.Does this ""problem"" have a name that it can be referred to?This is generally referred to as ""stability"".  What's funny is that stability is actually a useful concept in regular clustering i.e. not online. The ""stability"" of the algorithm is often chosen as a selection criterion for whether the right number of clusters have been selected.  More specifically, the online clustering stability issue that you have described is referred to as the stability-plasticity dilemma.Are there ""standard"" solutions to this and ...First, the big picture answer is that many online clustering algorithm are surprisingly stable when they have been well trained with a large cohort of initial data.  However, its still a problem if you want to really nail down the cluster identities of points while allowing the algorithm to react to new data.  The trickiness of you point is briefly addressed in Introduction to Machine Learning By Ethem Alpaydin.  On page 319 he derives the online k-means algorithm through the application of stochastic gradient descent, but mentions that the stability-plasticity dilemma arises when choosing a value for the learning rate.  A small learning rate results in stability, but the system looses adaptability where as a larger learning rate gains adaptability, but looses cluster stability.I believe the best path forward is to choose an implementation of online clustering which allows you to control the stochastic gradient descent algorithm and then choose the learning rate so that you maximize stability and adaptability as best as you can using a sound cross-validation procedure.Another method that I've seen employed is some sort of forgetting algorithm e.g. forgetting older points as the data stream matures.  This allows for a fairly stable system on fast time scales and allows for evolution on slower time scales.  Adaptive Resonance Theory was created to try to solve the stability-plasticity dilemma.  You might find this article interesting. I'm not well-versed enough in R to suggest an algorithm, but I suggest you look for a mini-batch k-means algorithm that allows you to control the learning rate in its stochastic gradient descent algorithm.I hope this helps!";;;
6557;2;2015-07-23T09:36:01.793;;By default random forest picks up 2/3rd data for training and rest for testing for regression and almost 70% data for training and rest for testing during classification.By principle since it randomizes the variable selection during each tree split it's not prone to overfit unlike other models.However if you want to use CV using nfolds in sklearn you can still use the concept of hold out set such as oob_score(out of bag)=True which shows model performance with or without using CV.So in a nutshell using oob_score=True with or without nfolds can itself tell whether using CV is good for your data.Generally if your target is following a certain distribution and you don't have much observation data with you then using CV will not give much improvement.;;;
6558;1;2015-07-23T17:15:10.287;semantic relation or semantic relatedness between terms or phrases;"I am looking for a method that can realize if there is any semantic relation between two terms using ontologies. For example given two terms ""kitchen"" and ""Chef"" it can return s.th like chef works in a kitchen and a correlation value (for example 0.7). What about proper nouns or noun phrases? The relatedness of two phrases such as ""the biggest defense against the US Navy's Laser Weapon System"" and ""China's crippling pollution problem"".I would be appreciated if you could give me some references.";[education, open-source];37;1
6559;2;2015-07-23T17:29:03.497;;"My recommendation would be to explore some statistical approach to represent the reviews/rating pair as significance. For example, to translate the [# reviews, rating] into some test statistic type model, such as Student's t for starters. Considering your example numbers, some approaches can be:1) >>> 5/(1/sqrt(10))15.811388300841896>>> 3/(1/sqrt(10))9.4868329805051381>>> >>> >>> 4/(1/sqrt(60))30.98386676965934>>> 5/(1/sqrt(10))15.8113883008418962) Or, diving deeper into stats,:>>> (5-3)/(sqrt(1/10+1/10))4.4721359549995796>>> (4-5)/(sqrt(1/60+1/10))-2.9277002188455996(you will need to do some work on the alpha level to get at the significance in ex 2 above)You can see how these work out for you; if too basic/too many inaccurate assumptions/some other filtering that you need more resolution in your model/etc., you can explore ways to better represent:the sample standard deviation (assumed to be 1 in my examples above), orthe distribution (assumed Gaussian in examples above), or the appropriate statistical test (plow through Wikipedia), or etcPoint is you can continuously refine your model, depending how much resolution you need. Only you can make that call. Hope this helps!";;;
6561;1;2015-07-23T20:52:46.680;Cheat Sheet of UNIX commands for Data Science;"I was looking for a cheat sheet of UNIX commands, which are specifically usable for data science. I mean an introduction to very basic commands (starting from cd, ls, pwd, to some still simple but usable for data - e.g. wc, a few simple things with pipes, ssh, maybe s3cmd, etc), with some minimalistic examples.I couldn't find one; or at least, nothing close to:Git Cheat SheetRegular Expressions Cheat Sheetseries of R Cheat SheetsI am pretty much aware of datascienceatthecommandline.com, but my goal would be to have a 1-2 A4 pages printout, with really simple stuff.If there is one (but I missed it), I would be grateful for posting it as an answer. If there isn't, then I am asking to post pieces of such list.(If Moderator permits, we may create one list with Community Wiki.)EDIT: The closest thing I've found is here: Useful Unix commands for data science (very nice, but missing the very introductory part).";[education, open-source];91;2
6562;2;2015-07-23T21:26:54.077;;I would recommend this one - http://www.cheat-sheets.org/saved-copy/ubunturef.pdfIts comprehensive, but not data science specific. There are a number of great cheat sheets here that may be what you are looking for:https://dzone.com/refcardzIf you are wanting to create your own, this is a very nice tool: http://www.cheatography.com/;;;
6563;2;2015-07-23T21:28:10.853;;"I did some tests on a data set of 50k rows, using sklearn.RandomForestRegressor. I get significantly different scores - I'm using a normalized gini for the metric - depending on whether I use rf.oob_prediction_ (0.2927) or a KFold CV (0.3258 for 7 folds and 0.3236 for 3 folds).With that, it appears your point about ""especially if comparing RFs with other algorithms on the same dataset"" is a strong consideration towards using manual CV rather than relying on the OOB prediction.";;;
6564;2;2015-07-23T21:30:47.970;;I recommend these videos if you are just beginning Scikit:https://github.com/justmarkham/scikit-learn-videos;;;
6565;2;2015-07-24T02:21:36.687;;This is a post that starts from data science and walks you through options for completing common tasks:  http://www.drbunsen.org/explorations-in-unix/.  ;;;
6566;2;2015-07-24T05:02:23.250;;I assume the feature you use to detect abnormality is one row of data in a logfile. If so, Sklearn is your good friend and you can use it as a blackbox. Check the tutorial of one-class SVM and Novelty detection. However, in case that your feature is an entire logfile, you need to first summarize it to some feature of same dimension, and then apply Novealty detection. ;;;
6567;2;2015-07-24T05:53:55.760;;"To be frank, I really donot understand why this is an issue for you. If you want to a generic solution, I will say save it to a csv file. A file with 30,000 rows and 1,000 cols is NOT that big. Even a shell pipe with simple one-liners can do tasks you mentioned. For example, the following one-liner compute the mean of cells located at Ax rows and x3 cols of data.csv (I generate this data matrix of size 30,000x1,000 by filling random integers )cat data.csv | perl -F',' -lane 'BEGIN{ $avg=0; $num=0; $row=0; } $row++; next if( $row%3!=0);foreach $i (0..$#F){ if($i%26 == 0){ $avg+=$F[$i]; $num++;} }END{ print ""avg="" . ($avg/$num); print ""num=$num""; }' avg=0.1998num=390000It takes about 3 seconds on my laptop. You may put whatever interested conditions to filter data, collect filtered ones and compute stats. Since for whatever complicate condition you AT MOST scan each element once, the complexity is thus at most linear. Even for a big data file, this will not be a big issue. By the way, adding extra auxiliary is simply to 'pre-compute' something in advance. ";;;
6569;2;2015-07-24T13:15:31.040;;You can refer to my response related to h2o R or Python anomaly detection method in stackexchange,since that is scalable too.;;;
6570;1;2015-07-24T13:21:32.583;Non-parametric approach to healthcare dataset?;I have a Healthcare dataset. I have been told to look at non-parametric approach to solve certain questions related to the dataset. I am little bit confused about non-parametric approach. Do they mean density plot based approach (such as looking at the histogram)? I know this is a vague question to ask here. However, I don't have access to anybody else whom I can ask and hence I am asking for some input from others in this forum.Any response/thought would be appreciated.Thanks and regards. ;[education, open-source];28;
6572;2;2015-07-24T14:56:44.693;;They are not specifically referring to a plot based approach.  They are referring to a class of methods that must be employed when the data is not normal enough or not well-powered enough to use regular statistics.Parametric and nonparametric are two broad classifications of statistical procedures with loose definitions separating them:Parametric tests usually assume that the data are approximately normally distributed.Nonparametric tests do not rely on a normally distributed data assumption.Using parametric statistics on non-normal data could lead to incorrect results.If you are not sure that your data is normal enough or that your sample size is big enough (n < 30), use nonparametric procedures rather than parametric procedures.Nonparametric procedures generally have less power for the same samplesize than the corresponding parametric procedure if the data truly are normal.Take a look at some examples of parametric and analogous nonparametric tests from Tanya Hoskin's Demystifying Summary:Here are some summary references:Another general table with some different informationNonparametric StatisticsAll of Nonparametric Statistics, by Larry WassermanR tutorialNonparametric Econometrics with Python;;;
6573;2;2015-07-24T15:48:54.190;;To be honest binary classification is the easiest type compared to multi-class classification as at times by error you can classify a wrong class to a right one.So if you have a dataset with multiclass you will need a good distribution among them,so the expectation is more sample will give better insight,i.e CV should be less.However in case of binary classification if your class distribution is balanced enough you can easily go fo CV=10 for 25k observations,however if the class distribution is skewed you better go with less CV.So in a nutshell in case of binary distribution CV value really depends on your class distribution and not much on number of observations.;;;
6574;2;2015-07-24T15:55:45.900;;You can also use Python to do this. If you're familiar using Pandas data frames:import pandas as pdd1 = pd.DataFrame({'A':[1,4], 'B':[7,8], 'C':[3,4]})d2 = pd.DataFrame({'A':[3,4], 'C':[4,8], 'D':[5,4], 'E':[6,6]})d1.append(d2)This will output what you're looking for, except the empty cells would have NaNs (not a number) in them.;;;
6575;1;2015-07-24T16:09:38.027;Business Intelligence solution with robust backward browser compatibility;For a while I was wondering where this question would suit best and I think that the DS is the best match but I presume that it may be migrated to programmers or somewhere else.I'm looking at designing a business intelligence solution with a high degree of browser compatibility due to heterogeneous nature of IT systems across a wide user base. For example, the easy solution would be to program things in R and deploy some interactive gadgets via Shiny or do some front-end in Spotfire/Tableau but this will fall  when folk will try to access stuff with archaic browsers, like IE9. I would like to know how folk address such challenges, is the only solution $AWS + Python$ or are there some other less resource intense ways of developing BI products that would be accessible to across wide range of heterogeneous system configurations?;[education, open-source];9;
6576;1;2015-07-24T20:10:07.617;I am a programmer, how do I get into field of Data Science?;First of all this term sounds so obscure.Anyways..I am a software programmer. One of the languages I can code is Python. Speaking of Data I can use SQL and can do Data Scraping. What I figured out so far after reading soo many articles that Data Science is all about good at:1- Stats2- Algebra3- Data Analysis4- Visualisation.5- Machine Learning.What I know so far:1- Python Programming2- Data scrapping in PythonCan you experts guide me or suggest a roadmap to brush up both theory and practical? I have given around 8 months of time frame to myself.;[education, open-source];361;8
6577;1;2015-07-24T20:39:06.317;how to perform Step calcuations in R;"I have a rate-step plan for consumption that I'm trying to total up costs with.In this case, I have a rate plant that looks like this:First 20 Kwh is Free; Second 5 Kwh is $.1    Next 100 Kwh is $.053 Next 875 Kwh is $.042     Over 1000Kwh is $.039It would be helpful to see a worked example, e.g. consumption = 625. I would like to code this in R to calculate the total cost.I was thinking of a loop, but if it's a lot of accounts, the loop will take too long.  I was hoping for a single formula that I can use to calculate and put out the result.I'm going to compare results against others.Any wisdom would be greatly appreciated.  Thank you.";[education, open-source];25;
6578;1;2015-07-24T21:06:16.493;Identify document's owner by content;"Please, suggest a ML algorithm, which would be possible to use to identify a document as belonging to a certain organization.There is somewhat large number of organizations - several thousand. Naturally, there is no strict form of a document; but most of organization's documents look almost, but not quite, the same. Also there are some organizations that follow more then a one template, again, in that ""almost but not quite"" manner.Documents are in PDF format, and are reports of some kind and can contain several rectangular regions of text and tables. Most of them are not more than five pages long.New organizations can eventually appear - how does interfere with algorithm?";[education, open-source];32;
6580;2;2015-07-24T23:00:01.503;;Try a simple equation like this:max(x-20,0)*.1 + max(0,x-25)*(-.047) + max (0,x-125)*(-.011) + max(0,x-1000)*(-.003)This should take the usage (x) and walk it through steps, initially charging .1 for each Kwh over 20: max(x-20,0)*.1Then if x is above 20, it takes away .047 (.1-.053) for every Kwh between 20 and 25: max(0, x-25)*(-.047)Then, if x is above 125, it takes away .011 (.053-.042) for every Kwh between 25 and 125: max(0, x-125)*(-.011)etc.;;;
6581;2;2015-07-25T04:27:41.210;;"I do think Leaning Hadoop framework (hard way) is not a requirement of being a Data Scientist. General knowledge on all big data platforms is essential. I will suggest to know concept on it and only part need from Hadoop is the MapReducehttp://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.htmlA Data Scientist does not build cluster, administer ... is just make ""magic"" with data and does not care where is coming from. The term ""Hadoop"" has come to refer not just to the base modules above, but also to the ""ecosystem"", or collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Spark, and others.Most important is the Programing language, math and statistics for working with data (you'll need to find a way to connect with data and move forward).I wish I had somebody to point me to the concept and do not spend weeks on learning framework and build from scratch nodes and clusters, because that part is Administrator role and not Data Engineer or Data Scientist.Also one thing: all are changing and evolving but math, programing, statistics are still the requirements.";;;
6582;2;2015-07-25T04:49:31.850;;I do like Berkeley course on Data Science, will give a good foundation and taste for Data Science, After moved to udacity and coursera and many more resources. So if you have Programming skills than will need math and stat and a lot of visualization. Also will be great to get used to IPython because is essential to see every step(visualize)how it perform instead writing a whole script and test after (anaconda is easy to install and work with). Course is listed bellow: bcourses.berkeley.edu/courses/1267848/wikialso the stat i find good free course from SAS: Statistics 1: Introduction to ANOVA, Regression, and Logistic Regression support.sas.com/edu/schedules.html?ctry=us&id=1979Starting with ML will recommend: www.kaggle.com/c/titanic/details/getting-started-with-pythonon left side is also for Excel using Pivot tables  and R. DataCamp has released the tutorial on how to use R. Once you complete this steps than more competitions in gaining experience are on kaggle (recently released one for San Francisco Crime Classification) and ultimately amazing video tutorials from www.dataschool.iohope it helps ...;;;
6583;2;2015-07-25T07:36:25.153;;Try Decision Trees,Support Vector Machines or Naive-Bayes method with TF-IDF weighting for creating document vectors and check the Precision/Recall/F Measure scores. But this will not deal with unknown/new organizations, they will get classified to any other one. One way would be remodel/train again when there are so many unknown/new organization and the Precision/Recall/F Measure score too low.;;;
6584;2;2015-07-25T13:27:49.453;;"simple... use advanced Text Editorlike...Sublime text ,Notepad  ,Vim or Atom..paste all Bunch of data on any Editior and just remove '\n and replaced it by """" ..use FIND AND REPLACE functionNotepad   ,sublime text,vim and atomuse Ctrl   Hfind box type ""\n"" and in replace box """" done :) ;)tnx";;;
6585;1;2015-07-25T18:50:36.500;Ontologies with user interface;I am working on the development of an ontology for my dissertation project. I have read plenty of resources and tutorials on how to develop ontologies, why are they useful and how to use them. I've been trying to find examples of ontologies that include user interface such as textboxes, buttons etc. to study them for the development of my ontology. I checked databases such as ontoligua for similar projects but I didn't find anything to relate to.I would be grateful anyone can help me with that. ;[education, open-source];24;1
6586;2;2015-07-25T21:48:43.100;;Focus less on gaining skills and more on gaining experience. Try to actually solve some problems and post your work on github. You'll learn more in the process and be able to demonstrate knowledge and experience to employers, which is much more valuable than having a supposedly deep understanding of a topic or theory. Data Science is a pretty loaded field these days so I'm not sure what kind of work you specifically want to do, but assuming that machine learning is a component of it then kaggle.com is a good place to start. In terms of goals, if you're able to work with the data in pandas/numpy/scipy, build models in sci-kit learn and make some pretty graphs in seaborn, ggplot or even matplotlib then you won't have a problem getting a job from a skills perspective -- especially if you have code samples and examples to demonstrate your abilities. If you get stuck then stackexchange will either have the answer or you can post a questions and you'll have an answer shortly. Once you're doing the work for a living then you'll learn even more, likely from a senior team member who mentors you.Best of luck.;;;
6587;1;2015-07-26T10:15:25.393;visualizing large number of points as a 3D density map;"The  result of my computational simulation is a (time-dependent) system of large number (~100k) of moving points in a confined space. Each point has its own Cartesian coordinates as well as a weight (w) in the form of $(x_i,y_i,z_i;w_i)$. I'm looking for a software/app/package to create a snapshot of the 3D spatial density map of these points. (something like this). Like you see in this figure, the points are not going to be displayed individually, but only a transparent cloud will be drawn whose local intensity is proportional with the local number of points. The final goal is to create a movie of the changing 3D spatial density with time.So far, I have tried R, Matlab, Origin, and ImageJ. But no success!Thank you for your help!";[education, open-source];13;
6588;1;2015-07-26T10:16:49.500;Suggestion Regarding Data Science;Am a young programmer who is interested in Data Science field. I'm familiar with some of the programming languages like C#, Python, ASP.NET, C++ and have good hands on technologies.Kindly, suggest me how can i Start my DATA Science(Big Data) journey.Thanks in Advance.;[education, open-source];52;
6590;1;2015-07-26T12:17:09.947;Feature selection for Support Vector Machines;"My question is three-foldIn the context of ""Kernelized"" support vector machinesIs variable/feature selection desirable - especially since we regularize the parameter C to prevent overfitting and the main motive behind introducing kernels to a SVM is to increase the dimensionality of the problem, in such a case reducing the dimensions by parameter reduction seems counter-intuitiveIf the answer to the 1st question is ""NO"", then, On what conditions would the answer change that one should keep in mind ?Are there any good methods that have been tried to bring about feature reduction for SVMs in scikit-learn library of python - I have tried the SelectFpr method and am looking for people with experiences with different methods.";[education, open-source];38;1
6591;1;2015-07-26T17:08:34.013;Train a classifier for a game with feedback on chosen move instead of true labels;I'm having some trouble describing in one line what I want, which is probably why I haven't had much luck with Google.Say I have a game like 2048 where the possible actions each step are fixed (and more than two). I want to train a neural network that chooses a move, so I have 4 neurons in output layer and I make the move with the highest prediction. The output vector is normalized (softmax layer).However, the training data I have is just the state, the move that was made, and whether that had good or bad results. If the chosen move is bad, I don't know which of the other ones was better (if any).How should I train this? My current thought is like this:Good move? -> chosen action gets positive error (so prediction goes up)Bad move? -> chosen step gets negative error (so prediction goes down)But I haven't found literature supporting this guess. There are alternatives:Maybe I should also update the options that weren't chosen (in the opposite direction)?Is it a good idea to set error directly instead of using goal predictions?The error for correct and incorrect could be different, maybe to preserve normalization?...(I'm doing 2048 and using neural networks, but I think this is not limited to this game or this method.);[education, open-source];28;1
6592;1;2015-07-27T01:12:06.980;Integer optimization with R;I'm trying to find a optimal solution for my portfolio with DEoptim.This is the objective function I'm trying to optimize:obj <- function(w) {  r <- sum(return_of_stock * price * w)  return(r)}return_of_stock is a vector of rates between 0 and infiniteprice is a vector of the current price of each stockw is a vector of integers between 0 and infiniteI have data regarding 100 stocks, and I want to find the optimal solution, regarding some constraints:sum(w*price) < value_todayprice > price_threshold # i don't want blue chips stocksI didn't implement these constraints yet, but here is what I have done:min_stocks <- list(rep(0, nrow(retorno)))max_stocks <- list(rep(99, nrow(retorno)))DEoptim(obj, min_stocks,max_stocks)But DEoptim returns:Error in DEoptim(obj, min_stocks, max_stocks) :   (list) object cannot be coerced to type 'double';[education, open-source];20;
6593;2;2015-07-27T02:12:12.203;;"Wow, this was bit challenging but I was able to make one of these plots in python.The two main components are:plotting multiple radial axes on a polar plotremapping radial axes for variables with reversed scalescode:import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns # improves plot aestheticsdef _invert(x, limits):    """"""inverts a value x on a scale from    limits[0] to limits[1]""""""    return limits[1] - (x - limits[0])def _scale_data(data, ranges):    """"""scales data[1:] to ranges[0],    inverts if the scale is reversed""""""    for d, (y1, y2) in zip(data[1:], ranges[1:]):        assert (y1 <= d <= y2) or (y2 <= d <= y1)    x1, x2 = ranges[0]    d = data[0]    if x1 > x2:        d = _invert(d, (x1, x2))        x1, x2 = x2, x1    sdata = [d]    for d, (y1, y2) in zip(data[1:], ranges[1:]):        if y1 > y2:            d = _invert(d, (y1, y2))            y1, y2 = y2, y1        sdata.append((d-y1) / (y2-y1)                      * (x2 - x1) + x1)    return sdataclass ComplexRadar():    def __init__(self, fig, variables, ranges,                 n_ordinate_levels=6):        angles = np.arange(0, 360, 360./len(variables))        axes = [fig1.add_axes([0.1,0.1,0.9,0.9],polar=True,                label = ""axes{}"".format(i))                 for i in range(len(variables))]        l, text = axes[0].set_thetagrids(angles,                                          labels=variables)        [txt.set_rotation(angle-90) for txt, angle              in zip(text, angles)]        for ax in axes[1:]:            ax.patch.set_visible(False)            ax.grid(""off"")            ax.xaxis.set_visible(False)        for i, ax in enumerate(axes):            grid = np.linspace(*ranges[i],                                num=n_ordinate_levels)            gridlabel = [""{}"".format(round(x,2))                          for x in grid]            if ranges[i][0] > ranges[i][1]:                grid = grid[::-1] # hack to invert grid                          # gridlabels aren't reversed            gridlabel[0] = """" # clean up origin            ax.set_rgrids(grid, labels=gridlabel,                         angle=angles[i])            #ax.spines[""polar""].set_visible(False)            ax.set_ylim(*ranges[i])        # variables for plotting        self.angle = np.deg2rad(np.r_[angles, angles[0]])        self.ranges = ranges        self.ax = axes[0]    def plot(self, data, *args, **kw):        sdata = _scale_data(data, self.ranges)        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)    def fill(self, data, *args, **kw):        sdata = _scale_data(data, self.ranges)        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)# example datavariables = (""Normal Scale"", ""Inverted Scale"", ""Inverted 2"",             ""Normal Scale 2"", ""Normal 3"", ""Normal 4 %"", ""Inverted 3 %"")data = (1.76, 1.1, 1.2,         4.4, 3.4, 86.8, 20)ranges = [(0.1, 2.3), (1.5, 0.3), (1.3, 0.5),         (1.7, 4.5), (1.5, 3.7), (70, 87), (100, 10)]            # plottingfig1 = plt.figure(figsize=(6, 6))radar = ComplexRadar(fig1, variables, ranges)radar.plot(data)radar.fill(data, alpha=0.2)plt.show()    ";;;
6594;1;2015-07-27T03:24:46.260;Algorithm selection - Improve initial assignments in combinatorial allocation;"I hope this is the right place to ask for help.I have a problem that approximates the following example: Imagine a data set of articles of clothing (T-shirts, sweaters and dressed) which come in a variety of colors (red, green, blue), sizes (Small, Medium, Large) and price ranges (0-9.99,10-19.99,20-29.99). Example below:Item Number Type    Colour  Size    Price Range1234    T-Shirt blue    Medium  0-9.991235    Sweater green   Small   10-19.991236    Dress   red Large   20-29.99…   …   …   …   …I have an initial solution that allocates all articles to two boxes, which will be sent to different stores. E.g. Item 1234 - Box 1, Item 1235 - Box 2, Item 1236 - Box 1 etc. The aim is to send a well balanced box to each store, so that they both get a similar distribution of goods with regard to all variables.However my initial solution may not spread the items between boxes 1 and 2 in a way that equally distributes each characteristic. For instance the initial solution may have the following allocations with regard to item type:Box Type    Count   Percent1   T-Shirt 23  57.5%1   Sweater 11  27.5%1   Dress   6   15.0%2   T-Shirt 27  67.5%2   Sweater 8   20.0%2   Dress   5   12.5%I want to use an algorithm that will improve the initial solution by moving items between Box 1 and Box 2 in such a way that the distribution of the each attribute of the variables type, colour, size and price range is as equally distributed between Box 1 and Box 2 as possible. The second criteria is to minimize the amount of swaps needed to get in balance, because time for this task requires labor and thus cost. It does not have to be a 1-1 swap, e.g. it may be allowed to swap 1 item for 2 items, as long as in the final solution the number of items per box is as balanced as possible.Some initial thoughts for me are that I may somehow be able to check which variables each box is ""too high"" and ""too low"" on and then try to identify candidate items to swap. I am wondering if some variation on a stable marriage algorithm could help me here? Maybe each box could somehow identify its least desired item. And then if Box 1 prefers Box 2's least desired item over its own least desired item, and Box 2 is at least indifferent to the swap, then the swap will occur. And this algorithm would continue until no more swaps are accepted in either direction. Is there any algorithm like that that I could look into? I have a feeling that I would somehow need a way to measure the utility of each item to each box though and I am really unsure how to approach this.Thanks in advance for any help you can give!";[education, open-source];17;
6595;1;2015-07-27T05:55:57.130;Identifying top predictors from a mix of categorical and ordinal data;I have a dataset with 261 predictors scraped from a larger set of survey questions. 224 have values which are in a range of scale (some 1-10, some 1-4, some simply binary, all using 0 where no value is given), and the rest are unordered categories.I'm trying to perform classification using these predictors and identify the top n predictors. Am thinking of the following approach:convert the 224 ordered predictors into numeric, centered and scaled.Run separate modeling (I use caret from R): one for using the numeric predictors, another using the  remaining 37 categorical predictors (both cross-validated within each modelling exercise)Choose the respective best-fitting models modelN and modelC for the numeric and categorical predictors.Choose top n (say 10) predictors from modelN and modelCCombine them in an ensemble model that can handle both numeric and categorical data (say, random forest)Choose top n predictors in the ensemble model.I am going through this roundabout way rather than directly fitting all predictors into an ensemble model to try reduce the complexity of the problem first (and because in R, I'm having problem with too many levels from the predictors).Would this be a valid approach to identifying the n most salient predictors? Any possible issues to mitigate?;[education, open-source];40;
6597;2;2015-07-27T11:46:01.080;;My team and I recently experienced a similar problem. We used a Random Forest to be able to predict between 10 authors with about an 82% accuracy, as the number of users went up the accuracy went down. We were then asked to try and identify a new author which we implemented by creating a dynamic threshold for votes from the individual trees in forest. With the new user identification and 10 authors we were at about 65% accurate. We were working with twitter data also, so our document sets were fairly small. I have seen documentation where other teams have simply used the count of stop words in documents and a random forest that was 80% accurate, they weren't trying to identify a new user however. In your situation, breaking up your problem into two different problems might yield better results. First identify if the new document belongs to any of your known users and if it does then try and decide which one.;;;
6598;2;2015-07-27T12:15:37.550;;Possible, yes, but why do you want to do this?Topic modelling is a method of unsupervised learning with all it weaknesses.With the type of data you sketch you can used methods of supervised learning that will be much more stable, less prone to overfitting, and more reliable in the long run.;;;
6599;2;2015-07-27T12:24:39.650;;"It is an old question, but I want to suggest a different approach than the other answers.Using a sentiment analysis on reviewsLet's say that the data we have are:5-stars rating (zero to five)N number of reviews for each entryWe use a sentiment analysis model to calculate the ""mood"" of each review. The result would be ""negative"", ""neutral"" and ""positive"", based on the content of the review.You can read more about sentiment analysis and find some APIs in this Quora's question.Then, you will use a multiplier for each mood.0: negative mood0.5: neutral mood1: positive moodAs a result, you can add the review score like multiplier*5 for each review in your dataset. For example in pseudocode,var current_5_star;for each review in this_entry:    multiplier = calc_sentiment(review)    current_5_star += multiplier * 5If you want to improve further the model, you can use the total number of reviews on your dataset to normalize the multiplier.To sum up, you do not only use the number of reviews per se, but also the content of them to evaluate the exact impact of it in the rating score.";;;
6600;2;2015-07-27T12:30:12.220;;Just some random thoughtsDo you have a mathematical model to base on? For example, you want to predict how pressure is changing against temperature. You won't drop any predictor that is related to 'temperature', no matter how remote it is. If so, that should govern your choice of predictors and you should start with that as that would lend more credibility to your final model.If not and you just want an algorithm to pick the best predictors, have you thought of running a regression model with L1 norm on? This will drive out insignificant factors and you can start with that set as a basis.;;;
6601;1;2015-07-27T17:27:24.527;Time series classifcation;I am looking at time series security attack data where a given IP can either be labeled as (1) attack or (0) no attack. In total we will have thousands of IPs and roughly an equal number of attacks and non attacks. The data is rather noisy and every time series sequence can have a different length. I am looking for advise on state of the art approaches to time series classification. I am past the stage of simple things like moving averages and I am looking for ways to improve my current methods or new things to try. I have currently implemented a few different techniques:K-nearest neighbor with DTW. I am successfully using http://www.cs.ucr.edu/~eamonn/UCRsuite.html which provides state of the art performance. Logical shapelets (http://www.cs.ucr.edu/~mueen/LogicalShapelet/). This seems promising but have not been able to get any existing code base to work.  Can anyone suggest different technique to try? I have seen papers about discords and motifs but still need to investigate if they are relevant for my problem. ;[education, open-source];34;1
6602;1;2015-07-27T21:15:03.970;What kind of RAM to choose for data analysis?;I'm planning to upgrade my computer and also want to buy some new DDR3 RAM modules. Making some research I realized that there are 2 main characteristics of RAMs (beside capacity): frequency and latency.I want to use my computer for data analysis as well (like Kaggle competitions). Usually I work in R and sometimes in Python.I understand that there is a trade-off between latency and frequency (that is also reflected in the price sometimes). My question is which one is more important for data analysis purposes? How can it effect real world performance? How much difference should I expect between choosing a model with overall better and lower parameter values?UPDATE: I don't ask which RAM is better in general. I need answer from users who know how memory management happens in R (or Python). How are the different ML algorithms managed in memory and which RAM characteristics are preferred to support it. I would also appreciate personal experiences on this field. ;[education, open-source];52;1
6603;2;2015-07-27T21:32:58.897;;e1071 is a highly popular and therefore safe solution for this purpose. But if data starts to get bigger I would go with classyfire (https://cran.r-project.org/web/packages/classyfire/classyfire.pdf). It's a quite fresh package built on e1071. It is highly optimized and have some other nice feature too. In my experience it produced similar results to e1071 but with a much faster speed as it was able to utilize all 4 core of my processor.;;;
6604;1;2015-07-28T08:53:18.033;What does the Ip mean in the Bayesian Ridge Regression formula?;From http://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression, they gave the bayesian ridge distribution as this:$p(w|\lambda) = \mathcal{N}(w|0,\lambda^{-1}{I_{p}})$And there is a variable $I_p$ but it's unexplained what does the $I_p$ refer to?Also, the variable $\mathcal{N}$ is unexplained but I'm not sure whether I've guessed correctly but is that the Gaussian prior as described in the Bayesian regression section above the Bayesian Ridge?;[education, open-source];22;
6605;2;2015-07-28T09:09:21.467;;N is  function  and represents the Normal, or Gaussian, distribution. ;;;
6606;2;2015-07-28T11:13:17.270;;$\mathcal{N}$ does indeed denote a (multivariate) normal / Gaussian distribution. $I_p$ is just an identity matrix of dimension $p$. So this a matrix with $\lambda^{-1}$ along the diagonal. Read this as the covariance matrix, so this is a spherical Gaussian (0 covariance between different dimensions) where each variable has variance $\lambda^{-1}$.;;;
6607;2;2015-07-28T12:39:20.250;;"It's not easy to compare two RAMs with different frequency and latency, since both of them affect your performance with not the same way.The short answer is:If you have two RAMs with same capacity and frequency, choose the one with lower latency.If you have two RAMs with same capacity and latency, choose the one with the higher frequency.From lifehacker.com Essentially, you have two things to worry about when it comes to RAM ""speed"": frequency, which deals with how much data can be transferred to the stick at one time, and latency, which is how quickly it responds to requests. In the current market, as you get to higher frequencies, latency tends to increase, so in many cases, they tend to balance each other out. Buying RAM with a higher speed doesn't matter a ton.The long answer:Don't worry about RAM. Yes, if you have a crappy RAM, you will also have a problem with your performance. But, the most important thing in Data Analysis is the performance of your code. An example from my personal experience:I had a text dataset, about 30GB. I needed to create a corpus for a simple TF-IDF. The brute-force code, spent about 15 hours, whereas a map-reduce algorithm spent about 1.5 minute for the same dataset.I work on a MacBook Pro with 16GB RAM. I didn't have any problem with performance. However, if you have huge datasets and even code improvements and RAM upgrades cannot help, you may reconsider to move to Hadoop or similar. ";;;
6608;1;2015-07-28T12:40:39.007;Accessing directory of small files as one file;I have a job that results in a directory of part- files. I'd like to read it as if it were one file. Specifically, I'd like to read it that way over a web interface.How can I do either one of these things? Is there a hadoop component which makes this easy?;[education, open-source];17;
6609;2;2015-07-28T12:44:44.997;;Ricky,Loose thoughts: Depending on the algorithm you intend to use, centering might not be a good idea (e.g. if you go for SVM, centering will destroy sparsity)I would suggest not to handle ordered / unordered separately, as you are likely to miss interactions that way. If the categorical ones don't have too many possible values, randomForest in R can handle factors.if that is an issue (as you seem to hint), I think you have two possibilities: binary indicators or response ratesif it's feasible in terms of computational cost, i would convert all factors to binaries (use sparse matrices if necessary) and then try a greedy feature selection. caret, if memory serves, has rfe or somesuch.if that's too much trouble, try calculating response rates / average values per factor level (I don't see any info whether your problem is classification or regression): you split your set into folds, and then for each fold fit a mixed effects model (e.g. via lme4) on the remainder, using the factor of interest as the main variable. It's a bit of a pain to setup all the cv correctly, but it's the only way to avoid leaking information.Hope this helps,K;;;
6610;1;2015-07-28T14:55:13.340;"Is our data ""Big Data"" (Startup)";"I worked at a startup/medium sized company and I am concerned that we may be over-engineering one of our products.In essence, we will be consuming real-time coordinates from vehicles and users and performing analytics and machine learning on this incoming data. This processing can be rather intensive as we try predict the ETAs of this entities matched to historical data and static paths. The approach they want to take is using the latest and most powerful technology stack, that being Hadoop, Storm etc to process these coordinates. Problem is that no-one in the team has implemented such a system and only has had the last month or so to skill up on it.My belief is that a safer approach would be to use NoSQL storage such as ""Azure Table Storage"" in an event based system to achieve the same result in less time. To me it's the agile approach, as this is a system that we are familiar with. Then if the demand warrants it, we can look at implementing Hadoop in the future.I haven't done a significant amount of research in this field, so would appreciate your input.Questions:How many tracking entities (sending coordinates every 10 seconds) would warrant Hadoop?Would it be easy to initially start off with a simpler approach such as ""Azure Table Storage"" then onto Hadoop at a later point?If you had to estimate, how long would you say a team of 3 developers would take to implement a basic Hadoop/Storm system?Is Hadoop necessary to invest from the get go as we will quickly incur major costs?I know these are vague questions, but I want to make sure we aren't going to invest unnecessary resources with a deadline coming up. ";[education, open-source];179;
6611;1;2015-07-28T16:15:43.783;Item-Item similarity based on text;We're build an item-item recommender based on the text descriptions of the items. Our initial approach was to calculate the TF-IDF vectors for each item. We used a hashing tf with 5000 possible hashes for the words. Then approximate all-pairs using a sampling technique (DIMSUM).We have an mxn matrix where m is the number of words, n is the number of items. The naive approach of calculating all column cosine similarities won't work here since we have n=10^7 m=10^4. Our first attempt was using DIMSUM http://stanford.edu/~rezab/papers/dimsum.pdf which is an all-pairs sampling technique. The problem with DIMSUM is it works for data where m >> n. Our matrix is short and wide n>m.My Question:What is a good approach for estimating all-pairs similarity of items based on words. Where number of items is 10^7, number of words is 10^4. We only want items pairs above a certain threshold of similarity.We don't have to do it this way, our task is to recommend a small set of items given one item, we have ways of doing this using collaborative filtering, but we want to also handle new items that we have no user data for so we're trying to find a way to use tf-idf vectors for that case.;[education, open-source];48;2
6612;1;2015-07-28T17:54:23.927;Reducing the dimensionality of word embeddings;I trained word embeddings with 300 dimensions. Now, I would like to have word embeddings with 50 dimensions: is it better to retrain the word embeddings with 50 dimensions, or can I use some dimensionality reduction method to scale the word embeddings with 300 dimensions down to 50 dimensions?;[education, open-source];20;
6615;2;2015-07-28T20:23:27.530;;"Yes, this is a how-long-is-a-piece-of-string question. I think it's good to beware of over-engineering, while also making sure you engineer for where you think you'll be in a year.First I'd suggest you distinguish between processing and storage. Storm is a (stream) processing framework; NoSQL databases are a storage paradigm. These are not alternatives. The Hadoop ecosystem has HBase for NoSQL; I suspect Azure has some kind of stream processing story.The bigger difference in your two alternatives is consuming a cloud provider's ecosystem vs Hadoop. The upside to Azure, or AWS, or GCE, is that these services optimize for integrating with each other, with billing, machine management, etc. The downside is being locked in to the cloud provider; you can't run Azure stuff anywhere but Azure. Hadoop takes more work to integrate since it's really a confederation of sometimes loosely-related projects. You're investing in both a distribution, and a place to run that distribution. But, you get a lot less lock-in, and probably more easy access to talent, and a broader choice of tools.The Azure road is also a ""big data"" solution in that it has a lot of the scalability properties you want for big data, and the complexity as well. It does not strike me as an easier route. Do you need to invest in distributed/cloud anything at this scale? given your IoT-themed use case, I believe you will need to soon, if not now, so yes. You're not talking about gigabytes, but many terabytes in just the first year.I'd give a fresh team 6-12 months to fully productionize something based on either of these platforms. That can certainly be staged as a POC, followed by more elaborate engineering.";;;
6616;1;2015-07-28T22:14:31.177;What makes a graph algorithm a good candidate for concurrency?;GraphX is the Apache Spark library for handling graph data. I was able to find a list of 'graph-parallel' algorithms on these slides (see slide 23). However, I am curious what characteristics of these algorithms make them parallelizable. ;[education, open-source];20;
6617;2;2015-07-29T00:51:18.110;;Two words: associative and commutativeIn other words, the operations that the algorithm does need to be independent of how you order or group your data...this minimizes the need for cross-talk in the algorithm and leads to more efficiency.;;;
6618;1;2015-07-29T09:08:30.853;Which algorithm is good for genetics duplicated data?;My question is more related to find the best algorithm for my data set.I have data which has three columns namely, individuals, and disease and test score (I have 50 test scores features but only one test score feature is mentioned here). I have 3000 individuals and possible values for disease feature is disA, disB and disC where as test score is a discrete variable. Disease feature is my class attribute.One individual can have up to three different diseases but only one test score value. My objective is to classify test scores on the basis of disease (which test scores are associated with which disease) But here problem is if one individual has three diseases then all of test scores will be repeated three times. For example, for individual aa (with all disA, disB and disC) test score is 12. And then analysis file will look like thatindividuals, Disease, Test Scoreaa,disA,12,...aa,disB,12,...aa,disC,12,...This will result into biased analysis. Is there any data mining algorithm or statistical test for such type of data? I cannot remove these patients because they are highest proportion of data set.One possible answser which I got is to arrange the file like that:individuals, DiseaseA, DiseaseB, DiseaseC, Test Scoreaa,true,true,true,12bb,true,false,false,10 But, I am bit afraid about this binary data arrangement. For example, if I have many individuals like aa (true for all diseases) then there will not be a significant difference between disease variables.   ;[education, open-source];26;
6619;2;2015-07-29T10:37:15.300;;Clarification: are we talking about multiclasshttps://en.wikipedia.org/wiki/Multiclass_classificationor multilabel?https://en.wikipedia.org/wiki/Multi-label_classification;;;
6620;1;2015-07-28T12:15:31.230;Which language best to use for Machine Learning library?;We have a body of theoretical work on nearest neighbors that we would like to implement and make the code publicly available. Question: what's the best language to use? We're considering java, python, c, c++. We would like the code to be fast (big data and all) and easy to use -- so that others can readily download and run it. Any thoughts and suggestions are welcome.;[education, open-source];66;
6621;1;2015-07-29T13:35:03.853;Source of aggressively toned texts for machine learning;"I was looking for a source where there would be an extended amount of texts with aggressive tone. It can be intertwined with non-aggressive texts, but it would be good if the ratio of aggressive texts to non-aggressive texts be no less then 1:5.I was thinking about discussion forums which would be discussing the immigrants issue with contributors of lesser mind who would be very hateful and xenofobic. Or supporters of extreme political parties. The content being spoken about is not really important, the more diverse it would be the better.In the past I was looking for a less specific sentiment like ""like/dislike"" something for which reviews of movies on IMDB were an ideal source and I had some good results. However it is very difficult to find a base of data with aggressive sentiment or at least some good internet sources from which the data could be gathered.Do you have any ideas on how I could gather up such data?";[education, open-source];13;
6622;2;2015-07-29T15:35:00.647;;Almost all machine learning library such as scikit learn,mlib,weka can either be used by a native or wrapper version of R and Python.However I prefer python as it's a full scale language compared to R and at least 2-50x faster than R in different situations.;;;
6623;1;2015-07-29T17:16:15.640;Sharing industry data with academic class contracts?;Could someone point me to a standard data sharing agreement? I have an interesting data set that I want to share with my local university for  their Data Mining class. The data set includes time-series and email statistics. All domains and IPs are anonymized before release. Main points:Use at your own riskProtect my company and client's privacyCite my companyDo not re-share without permissionAm I missing anything important here?;[education, open-source];20;
6624;1;2015-07-29T17:40:21.780;Adaptive learning of user's IoT setting preferences;We are working on a project to Predict the settings of the IoT devices(fan/light/AC) the user is using, based on his: location, outside temperature, humidity, time of the day etc...Training data is not available and the model should start building/adapting itself every time the user uses the IoT device, to predict his preferred settings in any new environment he goes. This model will be unique for each user. Possibly we can start off with a demo model but it should adapt itself as per each user's preferences.Which (machine?)learning algorithm can be useful here? Any links to the same and some tips on implementation would be appreciated.;[education, open-source];34;1
6626;1;2015-07-29T20:50:40.583;"""Recursive ConvNets for Dummies"" Library";"I've look at the questions on here regarding the different python libraries around for deep learning and neural nets. They include:KerasCaffeLasagnePyLearn2DeepyTheanoTorchMy understanding is that Keras and Lasagne require the user to have varying degrees of interaction with Theano. I'm looking for the one that would be the easiest to start testing an algorithm I have already developed an apply deep learning on it in a packaged way. I am not looking to extend, modify or tweak anything. I certainly won't be doing any research in the field with it. The only advanced feature I would need would be GPU support. In short, what would be the best ""dummy"" library to test recursive convolutional neural nets? ";[education, open-source];29;
6627;2;2015-07-30T01:36:23.517;;This sounds like a fun problem but it very open ended!  I will provide some links from the Scikit-Learn User Guide.  There are many more reading options, but the SKL Userguide has lots of examples and usually links to academic publications for more in-depth reading.  Another great resource is: Introduction to Statistical Learning, and if you are good at math: Elements of Statistical Learning Model SelectionThe first big question is whether the settings you are trying to predict are ordinal values or continuous values?  Continuous value will allow you to use regression methods, where as ordinal values will give you a choice between regression methods and classification schemes.  I would suggest employing a couple of different models to start and then selecting the best one moving forward.  Some possible candidates are:Linear regression (with nonlinear features as appropriate) because linear models are usually insightful, Support Vector Classification/Regression (SVC/SVR) as these are often very accurate classifiers. and either Naive Bayes or Random Forests as these often give good results where other models fail.Initial Training DataYou will need to prime your solution with some deterministic data or data from another dataset that you have adapted to fit your model.  I suggest priming the system with common sense values and then employing some sort of algorithm to forget those values as the system becomes well-trained.Moving ForwardYou will have to decide whether each user's model will use other user's data or not.  This is sometimes solved via clustering methods like k-means or DBSCAN), where by users are clustered into groups each cluster has a different model associated with it.  I would suggest retraining (fitting) the model on regular time frames initially, and then investigating the possibility of moving to an online learning system.I absolutely suggest using a well known library for your problem. Scikit-Learn is great for the reasons mentioned above and for prototyping, but doesn't scale particularly well. H20 and Mahout are high quality scalable libraries that I would recommend for a big data production system.  Mahout in Action is a great book to learn from, also.Hope this helps!;;;
6628;2;2015-07-30T04:21:27.090;;hadoop fs -getmerge <hdfs-output-directory> <local-file> This command can be used to concatenate the HDFS files into a single local file.Ref : http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#getmerge;;;
6629;1;2015-07-30T07:16:07.637;Anonymizing Datasets;I would like to know what are the best practices for anonymizing datasets? Ideally I should be able to get the original data back after performing analysis on the anonymized dataset. Should I be using some encryption functions? Hashing maybe? ;[education, open-source];70;
6630;1;2015-07-30T13:39:40.077;Estimating mean time given currently active processes;Suppose that there are $N$ processes, $\{p_i\}$, and that we wish to find that each process has a unique total time which it is active, $\{t_i\}$, such that we can calculate statistics of average, $\mu_t$, and standard deviation, $\sigma_t$, for the population times. We can assume that some processes in the global group have already finished.If the information of total active time is lost after each process is finished, how can we estimate these parameters, $\mu_t$ and $\sigma_t$, while the processes are active?If the current time a process has been running is denoted $T_i$, then we can easily get a lower bound on the average time by $\mu_t >= \mu(\{T_i\})$, but how can we improve this to get an estimate what the average time of life will be for the population?As an example, suppose that we never kept information about human deaths. How might one speculate what the average life expectancy is of of mankind based on the total ages of everyone on earth, assuming that people on earth have already died at sometime in the past.;[education, open-source];10;
6631;1;2015-07-30T14:07:31.163;Alignment of square nonorientable images/data;Another post where I don't know enough terminology to describe things efficiently. For the comments, please suggest some tags and keywords I can add to this post to make it better.Say I have a 2D data structure where 'orientation' doesn't matter. The examples I ran into:The state of a 2048 game. In terms of symmetry groups this would be D4 / D8, except that an operation doesn't yield an identical state, it just yields another state that has the same solution.Images of plankton or galaxies (without background). Somewhat similar to above except that any rotation (not just 90o) yields an equally valid image (and one might take scale into account, but let's forget about that).In both cases I've wanted to transform all these equivalent states/images to remove all but one of the equivalent images. To illustrate with two that worked:I can use image moments M10 and M01 to transform horizontally and vertically mirrored equivalent data. E.g. apply horizontal mirroring iff it makes M10 bigger. This would transform a 2048 state and it's horizontal mirror image to the same state.I can use the eigenvector of the covariance matrix which has the largest eigenvalue as the orientation. Then I can rotate the image to align this eigenvector with some predetermined axis (e.g. horizontally).That still leaves a lot of operations though (diagonal mirroring, rotations around the center, inversion). And these operations do not commute (D8 is non-Abelian). Is there any comprehensive approach?The reason I want to do this is to help machine learning methods by removing variance that isn't actually meaningful. Hopefully that makes sure they don't have to learn these equivalences, so possibly need less train data (and time).;[education, open-source];26;
6632;1;2015-07-30T14:15:58.803;Getting prob of class using naive bayes;"I am trying to classify input with two classes, here is the simple code for the same. dino and crypto are two classesfor w, cnt in list(counts.items()): #count is dict with word and it's count value    p_word = vocab[w] / sum(vocab.values())     p_w_given_dino = (word_counts[""dino""].get(w, 0.0) + 1) / (sum(word_counts[""dino""].values()) + v)     p_w_given_crypto = (word_counts[""crypto""].get(w, 0.0) + 1) / (sum(word_counts[""crypto""].values()) + v)    log_prob_dino += math.log(cnt * p_w_given_dino / p_word)    log_prob_crypto += math.log(cnt * p_w_given_crypto / p_word)print(""Score(dino)  :"", math.exp(log_prob_dino + math.log(prior_dino)))print(""Score(crypto):"", math.exp(log_prob_crypto + math.log(prior_crypto)))another approach isprior_dino = (priors[""dino""] / sum(priors.values()))prior_crypto = (priors[""crypto""] / sum(priors.values()))for w, cnt in list(counts.items()):    p_word = vocab[w] / sum(vocab.values())    p_w_given_dino = (word_counts[""dino""].get(w, 0.0) + 1) / (sum(word_counts[""dino""].values()) + v)     p_w_given_crypto = (word_counts[""crypto""].get(w, 0.0) + 1) / (sum(word_counts[""crypto""].values()) + v)    prob_dino *= p_w_given_dino    prob_crypto *= p_w_given_cryptot_prior_dino = prob_dino * prior_dinot_prior_crypto = prob_crypto * prior_cryptoActually on 2nd approach I am getting very very small values so I got first approach. Which one is correct, or both of them?";[education, open-source];20;
6633;1;2015-07-30T14:30:15.820;Issue with 1L format;When doing this:np.array(data.ix[:,i])).shapeI get (number,) on a machine  with Python 3, but on a Python 2  doing the same thing I  get the long format (numberL,). How can I make python 2 use the standard numeric type instead of the long type?;[education, open-source];22;
6634;1;2015-07-30T14:47:48.963;Sales Predictions Over Time;I'd like to model the evolution of the sales of a store.Here are the data I have :Customers are aggregated into monthly cohort depending on the date of the first purchaseeg : customers who did their 1st purchase in Jan 2015 are in the cohort 1, customers who did their 1st purchase in Fev 2015 are in the cohort 2.Sales : aggregated for each Month * Cohort (cohort 4 is not visible because there is no line for only 1 record)if we put all the cohorts at the same origin:Question: How to predict the spending of the next 2 months, ie :the spending of the cohort 1 in its aging 5 and 6the spending of the cohort 2 in its aging 4 and 5the spending of the cohort 3 in its aging 3 and 4the spending of the cohort 4 in its aging 2 and 3But also for the cohort who don't exist yet :the spending of the cohort 5 in its aging 1 and 2the spending of the cohort 6 in its aging 1 I have 2 methodologies in mind, but don't know if they're good:Polynomial regression by using 2 parameters:aging: because there is a clear evolution over the time, up then down.cohort number: because more recent cohort seems to spend less.Time series : I didn't perform time series recently I'm a bit rust so I would prefer to use something else but I think it could fit with this problem.Indeed it seems obvious that yt-1 t in yt-2... are good predictors of yt but how to predict spending of future cohort with no history ?;[education, open-source];43;
6635;1;2015-07-30T17:48:12.383;Program to fine-tune pre-trained word embeddings on my data set;I am looking for a program that would allow me to fine-tune pre-trained word embeddings on my data set. Ideally, open source and working on Linux or Windows.;[education, open-source];25;
6638;2;2015-07-30T21:14:40.950;;Fun with Group Theory!There are only 8 unique rotation-inversion operations for a square matrix.The four rotation operators are (0,90,180,270).  Further rotation or rotation in the reverse direction is the same as these four.  Two successive rotations just yields one of the rotation operators, so we will only consider these four rotations applied one time.The five inversion operators are (0,/,\,|,-).  Two successive inversions just yields a rotation, so we will only allow for a single inversion.We can thus derive all operators by combining these two vectors, which yields 4*5=20 possible states.(0,90,180,270,0/,90/,180/,270/,0\,90\,180\,270\,0|,90|,180|,270|,0-,90-,180-,270-)But there is still symmetry to be exploited in the inversion operators.  You can probably intuit that the 4x4 matrix only has 8 final states: inverted or not and rotated by (0,90,180,270).  It turns out that you can arrive at any possible state involving an inversion using any of the other inversion operators followed by one of the rotations.  So we only need to retain a single inversion operator!  So the final set of 8 orthogonal operations are:(0,90,180,270,0|,90|,180|,270|)If there is any symmetry in the matrix's members then some of the resulting states may be degenerate.In terms of mapping possible states into a ground state, it makes sense to apply a set of successive deterministic rules to determine the ground state orientation.  I suggest finding the largest corner square and locating it in the lower right corner. If there are multiple candidates with equally large values in the corner square then use the next closest square as a tie breaker.  There are 16 squares, so you can eventually break all ties or declare degeneracy. There is one remaining \ operation that you can decide to apply in order to locate the larger of the two squares adjacent to the lower right corner at the bottom.  Again, you can use squares adjacent to these as tie breakers. ;;;
6639;1;2015-07-31T00:25:36.347;Extra output layer in a neural network;This is an exercise question from the online book: http://neuralnetworksanddeeplearning.com/chap1.htmlI can understand that if the additional output layer is of 5 output neurons, I could probably set bias at 0.5 and weight of 0.5 each for the previous layer. But the question now ask for a new layer of four output neurons - which is more than enough to represent 10 possible outputs at 2**4. I would like to understand how to interpret it and deduce the answer. The exercise question:There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.;[education, open-source];58;
6640;2;2015-07-31T04:39:50.677;;"While I am not aware of software specifically for tuning trained word embeddings, perhaps the following open source software might be helpful, if you can figure out what parts can be modified for the fine-tuning part (just an idea off the top of my head - I'm not too familiar with the details):GloVe: Global Vectors for Word Representation (part of Stanford NLP Group software);SENNA (its use for word embeddings is mentioned in this blog post);Code on GitHub for Deep Learning-based word embeddings training;Neural Probabilistic Language Model Toolkit, mentioned above (also Deep Learning-based).";;;
6641;2;2015-07-31T06:44:49.997;;First, understand and solve the problem. On manageable data. Gather experience on how to organize the data, and where the difficulties are. Try to identify points where parallelism is possible.Second, parrallelize and scale up as necessary.Don't do it backwards, popular mistake. Solving the wrong problem with the wrong tools will fail big, with big data.;;;
6642;1;2015-07-31T07:08:07.880;Custom trained model in Azure ML;I have a predictive model which I trained on a training set. I have written it in R. Now I want to deploy it as a web service so anyone can just input the data into it and get the output from the predictive model.I wanted to use Azure ML for deploying. I wanted to know whether I can drag and drop my already created/custom trained model to Azure ML studio instead of re-training it there ? I know we can train and save models in AML Studio but I am not sure about adding already created models and using them in AML solution. Help regarding this will be appreciated.;[education, open-source];13;
6643;1;2015-07-31T08:21:49.083;Do I need to buy a NVIDIA graphic card to run deep learning algorithm?;I am new in Deep learning. I am running a MacBook Pro yosemite (upgraded from Snowleopard), I don't have a CUDA-enabled card GPU, and running the code on the CPU is extremely slow. I heart that I can buy some instances on AWS, but it seems that they don't support Mac OS.My question is, to continue with the Deep learning, do I need to purchase a graphic card? Or is there other solutions? I don't want to spend too much on this... Thank you very much!;[education, open-source];71;
6644;2;2015-07-31T09:26:34.927;;"The question is asking you to make the following mapping between old representation and new representation:Represent    Old                     New0            1 0 0 0 0 0 0 0 0 0     0 0 0 0 1            0 1 0 0 0 0 0 0 0 0     0 0 0 1 2            0 0 1 0 0 0 0 0 0 0     0 0 1 0 3            0 0 0 1 0 0 0 0 0 0     0 0 1 1 4            0 0 0 0 1 0 0 0 0 0     0 1 0 0 5            0 0 0 0 0 1 0 0 0 0     0 1 0 1 6            0 0 0 0 0 0 1 0 0 0     0 1 1 0 7            0 0 0 0 0 0 0 1 0 0     0 1 1 1 8            0 0 0 0 0 0 0 0 1 0     1 0 0 0 9            0 0 0 0 0 0 0 0 0 1     1 0 0 1Because the old output layer has a simple form, this is quite easy to achieve. Each output neuron should have a positive weight between itself and output neurons which should be on to represent it, and a negative weight between itself and output neurons that should be off. The values should combine to be large enough to cleanly switch on or off, so I would use largish weights, such as +10 and -10.If you have sigmoid activations here, the bias is not that relevant. You just want to simply saturate each neuron towards on or off. The question has allowed you to assume very clear signals in the old output layer.So taking example of representing a 3 and using zero-indexing for the neurons in the order I am showing them (these options are not set in the question), I might have weights going from activation of old output $i=3$, $A_3^{Old}$ to logit of new outputs $Z_j^{New}$, where $Z_j^{New} = \Sigma_{i=0}^{i=9} W_{ij} * A_i^{Old}$ as follows:$$W_{3,0} = -10$$$$W_{3,1} = -10$$$$W_{3,2} = +10$$$$W_{3,3} = +10$$This should clearly produce close to 0 0 1 1 output when only the old output layer's neuron representing a ""3"" is active. In the question, you can assume 0.99 activation of one neuron and <0.01 for competing ones in the old layer. So, if you use the same magnitude of weights throughout, then relatively small values coming from +-0.1 (0.01 * 10) from the other old layer activation values will not seriously affect the +-9.9 value, and the outputs in the new layer will be saturated at very close to either 0 or 1.";;;
6645;1;2015-07-31T10:54:37.760;Error::Type of predictors in new data do not match that of the training data;"I am building a classification model using randomForest. When trying to predict I get the below error Type of predictors in new data do not match that of the training dataI made sure that the testing and training data has same levels.I also included levels(test_var) <- levels(train_var)to make sure that the levels are matching.But still I end up getting this error, is there anything else that I should look for?**EDITED ON 3rd Aug 2015** Here is the structure of the training dataset Structure of the test dataset Sapply training data Sapply testing data In order to make sure that the levels are matching between the training and test datasets, I wrote this loop to see if there are any differences exist between the datasetsfor(i in 1:28)  {   if(is.factor(testing[,i]))    {    print(names((testing[i])))    difference_in_test = setdiff(levels(testing[,i]), levels(training_data[,i]))      print(difference_in_test)    } } Results of the above For loop which shows that there are no differences exist between the levels.[1] ""VAR1""character(0)[1] ""VAR2""character(0)[1] ""VAR3""character(0)[1] ""VAR4""character(0)[1] ""VAR5""character(0)[1] ""VAR7"" I still continue to get the error as mentioned below: Edit on 4th AugI started getting the above error only after including a filter in my dataset like this:training_data <- subset(training_data, gender !=""F"")";[education, open-source];72;1
6646;2;2015-07-31T11:59:35.597;;I don't think you can currently upload a trained model. Your options would be to either re-train the model in AzureML or expose them as a web-service using an Azure Virtual Machine running something like:Rook Shiny DeployR;;;
6647;2;2015-07-31T14:21:03.577;;"AWS does ""support"" Mac OS. You can use any SSH client from your Mac to access the GPU instance. ";;;
6648;1;2015-07-27T12:59:05.890;Contributions of each feature in classification?;I have some features and I am using Weka to classify my instancesFor example I have :Number of adj   number of adverb    number of punctuation in my feature setbut I want to know the contribution of each feature in the feature set so what metrics or parameters are helpful to get the contribution of features?;[education, open-source];55;
6649;1;2015-07-31T17:20:10.873;Testing bimodality of data;I am trying to see if my data is multimodal (in fact, I am more interested in bimodality of the data). I performed dip test and it does evidence against unmodal data. However, I want to see, in particular, if it is bimodal. I believe silver man's test can be used. However, I couldn't find the implementation of it in either r or in python. (The one in R is old and not working with the current version of R). Also, assuming that I have a bimodal data and that I am able to get the two components (using mixtools in R), how do I figure out how to find the point of intersection of the two components. For example, here is the histogram (overlaid with its density estimation) of the entire data. Here are the two components: I want to get the the x value where the curves intersected. I could have uploaded the data, but the length of the vector is rather long. Any general thought and idea is welcome including the R and/or Python packages are welcome.Thanks;[education, open-source];39;
6650;2;2015-07-31T18:58:25.117;;I would recommend familiarizing yourself with AWS spot instances. It's the most practical solution I can think of for your problem, and it works your computer too. So, no you don't have to buy an Nvidia card, but as of today you will want to use one since almost all the solutions rely on them.;;;
6651;2;2015-07-31T19:49:03.447;;This is called feature ranking, which is closely related to feature selection.feature ranking = determining the importance of any individual featurefeature selection = selecting a subset of relevant features for use in model construction. So if you are able to ranked features, you can use it to select features, and if you can select a subset of useful features, you've done at least a partial ranking by removing the useless ones. This Wikipedia page and this Quora post should give some ideas. The distinction filter methods vs. wrapper based methods vs. embedded methods is the most common one.One straightforward approximate way is to use feature importance with forests of trees:Other common ways:recursive feature elimination.stepwise regression (or LARS Lasso).If you use scikit-learn, check out module-sklearn.feature_selection. I'd guess Weka has some similar functions.;;;
6653;2;2015-07-31T21:03:23.593;;Normalise using your training data statistics. Save the values used (e.g. mean and sd per feature), treating them as part of your model. Once you have used these values to transform input, they become fixed translate/scale factors in the model. Use the same values to normalise test data or new inputs as required. You do not need to calculate new normalisation constants for new data. In fact doing so will most likely reduce the effectiveness of your model.The same principle applies to interpreting output values if you need to scale those into range that your model produces. Scale according to your training data.;;;
6654;2;2015-08-01T10:56:11.233;;Other-than Ontologies do check word2vec, this tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words and can be used to find the closest words for a user-specified word;;;
6655;2;2015-08-01T17:47:09.340;;You should make the difference between the following concepts:encryption = encoding information in such a way that only authorized parties can read it.data anonymization = removing personally identifiable information from data sets, so that the people whom the data describe remain anonymous.de-identification = preventing a person’s identity from being connected with information, while preserving identifying information which could only be re-linked by a trusted party in certain situations (unlike data anonymization which aims to be irreversible).Your choice depends on your use case.;;;
6656;1;2015-08-01T20:50:39.397;Online machine learning for fuzzy matching ofshort text snippets (preferably in R);"My Problem: I work with large volumes of healthcare data that needs to be standardized prior to analysis (I am also relatively unfamiliar with text processing, though I have a strong statistical background with a decent amount of experience using machine learning techniques) The majority of the data comes in with text mapped to a standardized code which makes the process pretty easy. Unfortunately, labs and medications typically only come across as free text and thus mapping these text fields to a standardized term is an incredibly labor-intensive process. Over time we have built up a collection of > 200k ""mappings"" (free text records matched to their standardized terms). If an exact match is found between a new record that is received it is mapped and removed from the data set. The remaining records go through a series of normalization steps, are tokenized and then are run through some R code that looking for the intersection between the tokenized new record and the normalized, tokenized standardized terms (length of intersection/length of new record). This helps quite a bit in matching terms, but doesn't do nearly enough. Various fuzzy match efforts have been pretty hit or miss. The volume of incoming data has reached the point where mapping the terms manually is unsustainable. My goal: I am looking to utilize some form of online machine learning to aid in matching new text records to the standardized text. My idea was to generate some sort of confidence score for the matches and over time, we could assume that matches with a confidence score >= x were accurate and we would only have to review those that matched with a lower confidence. Since we would be correcting the matches that failed, I'd like to leverage this effort to continually improve the model (hence the reason for seeking an online ML solution). Ideally I'd like to implement this in R since it is the language I use for the majority of my work, but am open to python as well. Thanks in advance for any guidance that can be provided. ";[education, open-source];25;
6657;1;2015-08-02T07:09:54.577;how to calculate RMSE in SPSS neural network from output reports like SSE or relative error?;i want to calculate Root Mean Square of Errors (RMSE) in SPSS neural network , for training and testing routines ;[education, open-source];19;
6659;1;2015-08-02T09:22:41.890;What proxies could be used to assess economic value of Stackoverflow for its users?;"I'm intrigued by the open data provided by StackExchange, and have been running some really interesting queries on the data.stackexchange.com page (using the Stackoverflow dataset).In particular, I would like to dive deep on this claim taken from the Stackoverflow Developer Survey 2015 summary page: In other words: what is the ""value"" of Stackoverflow? Not in terms of its own economic success or funding amount, rather how it affects its users, for example accelerating a pay increase for active users.My ideal solution would be to correlate the activity of its users to some economic independent variable such as salary. Although the data on activity exists on Stackoverflow servers, this Salary data is not available (the downloadable StackOverflow Developer Survey dataset is anonymized).What other proxies/datasets could be used to conduct such a study? I would use github commits but it is more oriented towards productivity, and there already is a great paper on this topic.";[education, open-source];31;2
6660;1;2015-08-02T11:48:27.367;User-based nearest neighbour implementation in R?;I am just starting to learn to use R and am not sure how to find the best packages yet. I am looking for a package that will allow me to calculate user-based nearest neighbours as an input for a recommendation.My original data is approximately as below:User    Item    RatingAnna    Apple   2Bob Apple   4Carlos  Apple   1Ethan   Apple   1Gene    Apple   2Anna    Banana  4Dana    Banana  3Ethan   Banana  5Frank   Banana  5Bob Carrot  1Ethan   Carrot  2Gene    Carrot  4I have used the package reshape2 and dcast method to generate a different datastructure where users are rows, items are columns and the rating is in the value section (similar to an Excel pivot).User    Apple   Banana  CarrotAnna    2   4   n/aBob 4   n/a 1Carlos  1   n/a n/aDana    n/a 3   n/aEthan   1   5   2Frank   n/a 5   n/aGene    2   n/a 4Is there any R package that will allow me to compute the similarity between users based on the user vectors? Preferably I would like to use the Jaccard coefficient as my similarity metric. I tried to google for R packages with user-based nearest neighbour implementations but came up empty. Any help of alternative suggestions will be much appreciated.Thanks!;[education, open-source];38;
6662;2;2015-08-02T16:49:51.077;;"Since this question does not ask for programming or code examples, I will give you some basic advice about the methods you can research and use to start.You have two main choices here. Using R you can try :Use regular expressions:For example, you want to match 2 strings, the first one is ""samsung galaxy s4 2go ram"" and the second one is ""samsung s4"".  you can set manually a common pattern that you know exists in the data and based on that, match all the strings having that. In this case it would be ""samsung"". you might need to provide an example of the data you are looking to match so I can give you a basic example of the use of theses functions in R.  The functions you can use are: grep,grepl,regexpr and regexecSecond choice is Fuzzy lookup:This technique is based on something called "" Levenshtein distance"" which  compute the degree of similarity or “distance” between two expressions by counting the number of single character edits (insertion, deletes, or substitutions) you need to do in order to have matching strings.functions available in R to do this are the adist and there is a whole package dedicated to this particular task which is the stringdist packageFor starters you can look into these functions and decide which one suits you the most, and then start thinking about making that automated with a scoring model or confidence interval...etc";;;
6663;2;2015-08-02T18:39:25.623;;There has been an answer on this question, please check it : Test for bimodal distribution;;;
6664;2;2015-08-03T06:27:50.220;;If you want to get but the sum and length of id with data.table, you can do it like this:dt[,.(total=sum(id), n=length(id)), by=cid]You could even write this in shorter format with the .N operator of data.table:dt[,.(total=sum(id), .N), by=cid];;;
6665;1;2015-08-03T11:45:11.440;Why do my Latent Dirichlet Allocation Topics mix words that never co-occurred?;I have one corpus of documents on diabetes, another on Leonardo da Vinci, and another on animation and computer graphics.  I combined all of these documents into a LDA and got a topic like the one below.  I'm listing the top 30 terms, in descending order.  TOPIC 83: ['produced', 'company', 'john', 'weight', 'motion', 'information', 'painting', 'original', 'training', 'people', 'health', 'life', 'jesus', 'feature', 'body', 'lucretia', 'computer', 'graphics', 'time', 'madonna', 'story', 'florence', 'type', 'animated', 'paul', 'diabetes', 'animation', 'exercise', 'peter', 'film']Many of these words do not co-occur with each other in any documents.  For instance, 'lucretia' does not co-occur with 'exercise'.  Yet how are these terms put together in the same topic, in the top 30 words? (30 out of 20K words or so)Perhaps it has to do with my particular implementation?  I'm using the gensim library for Python.  Or is it a flaw of LDA generally?;[education, open-source];57;
6666;2;2015-08-03T12:19:06.493;;"It is a feature (I won't call it flaw) of LDA generally. Usually, you get some very clearly defined topics and some very muddy topics (best termed ""Miscellaneous"" or so).I also advice not to overinterprete the topics from topic modelling. Just changing a little (e.g. the random seed, taking a few texts out, applying some ""small"" modifications to the corpus) changes a significant amount of the topics.";;;
6667;2;2015-08-03T13:31:35.363;;"It could be partially due to the number of topics you selected but the fact that two words rank high for a given topic doesn't necessarily mean that the two words will frequently occur in the same document. Consider a topic that generally corresponds to ""medicine"". You could have a number of medical documents associated with heart disease and a number of medical documents associated with injuries. They may all be linked by common terms such as ""diagnose"", ""treatment"", ""patient"", etc. But you wouldn't necessarily expect that words like ""heart"" and ""aorta"" would appear in documents containing words such as ""sprain"" and ""fracture"", even though they are all associated with the general topic of medicine. If one were to choose a larger number of topics, those words might be separated into more specific topics (e.g., ""heart disease"" and ""sports injuries"").";;;
6669;1;2015-08-03T20:11:16.967;Can't connect to mysql server from WEKA tool;I need to connect mysql server from my WEKA tool for analysis some of the data. How can i make this connection in Linux platform.I decide to use jdbc driver for this but setting up the environment is not easy for me! How do i do this?;[education, open-source];5;
6670;2;2015-08-03T22:08:42.203;;AWS GPU instances are an option, if you want to do CUDA development. If you don't want to leverage the cloud, you can look into the Nvidia Jetson TK1 development kits.  They are about 200 dollars (in July 2015), have 192 CUDA cores, a quad-core ARM processor, and 2GB of RAM.  Conversely, the same amount of money could buy you a 640 CUDA core GeForce GTX 750 Ti, or maybe a 1024 CUDA core GTX 960.  A GT 720 with 1GB RAM and 192 cores could be had for 45 dollars (in July 2015).You don't have to use NVidia GPUs with deep learning.  GPUs will increase the speed dramatically, though.   There is very little support for non-NVidia GPUs with common deep learning toolkits.  ;;;
6672;2;2015-08-04T01:14:53.830;;"It sounds like you are making this more complicated than it is with trying to predict spend at the cohort level. It is best to use cohorts for understanding ""why"" something occurred and time series analysis for understanding ""what"" will occur. The Forecast package in R seems to be solution to your problem. Just forecast Monthly spend and be done with it! ";;;
6673;2;2015-08-04T01:29:21.623;;"Disagree with David, a true data scientist is an applied statistician who codes and knows how to use machine learning algorithms for the right reasons. Statistics is the base of all data science. It is the ""cake"" per se. Everything else is just icing. The question is what kind of data scientist do you want to be? Do you want to be a master of the subject (knowledge of how, why, when and when not to apply an algorithm or technique) or a Kaggle Script Kiddie using Scipy and thinking that he is a Data Scientist? 1 - Stats2- Everything else";;;
6674;1;2015-08-04T03:18:22.520;How to calculate a specific time complexity of inverse calculation of matrix?;I am a green-hand in calculating the time complexity. Given a calculation as follows:\begin{equation}\mathbf{x}=\mathbf{A^T}(\mathbf{AA^T}+\lambda\mathbf{I}_n)^{-1}\mathbf{b}\end{equation}where $\mathbf{A}\in R^{n\times p}$, $\mathbf{I}_n\in R^{n\times n}$, $\mathbf{b}\in R^{n}$ and $\lambda\in R$.Could some one kindly give me the complexity of the above formula in details? Thanks a lot. ;[education, open-source];37;
6675;1;2015-08-04T06:57:33.607;identifying input sequence using neural network;I fount simple neural scenario sayinh : network having 3 layers, 3 inputs and 2 outputs. It should be trained to recognize a simple pattern - if inputs are correspondingly 6.0,7.0 and 8.0 then outputs should be 3.0 and 4.0, otherwise outputs should be 23.0What I understand is, hidden layer should Check input pattern in sequence with simple if else condition. if in1 == 6 and in2 == 7 and in3 ==8:    out1, out2 = 3, 4else    out = 5Do I understand the problem correctly or something I am missing?;[education, open-source];37;
6676;1;2015-08-04T08:11:30.990;Scikit-learn: Getting SGDClassifier to predict as well as a Logistic Regression;"A way to train a Logistic Regression is by using stochastic gradient descent, which scikit-learn offers an interface to.What I would like to do is take a scikit-learn's SGDClassifier and have it score the same as a Logistic Regression here. However, I must be missing some machine learning enhancements, since my scores are not equivalent.This is my current code. What am I missing on the SGDClassifier which would have it produce the same results as a Logistic Regression?from sklearn import datasetsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.linear_model import SGDClassifierimport numpy as npimport pandas as pdfrom sklearn.cross_validation import KFoldfrom sklearn.metrics import accuracy_score# Note that the iris dataset is available in sklearn by default.# This data is also conveniently preprocessed.iris = datasets.load_iris()X = iris[""data""]Y = iris[""target""]numFolds = 10kf = KFold(len(X), numFolds, shuffle=True)# These are ""Class objects"". For each Class, find the AUC through# 10 fold cross validation.Models = [LogisticRegression, SGDClassifier]params = [{}, {""loss"": ""log"", ""penalty"": ""l2""}]for param, Model in zip(params, Models):    total = 0    for train_indices, test_indices in kf:        train_X = X[train_indices, :]; train_Y = Y[train_indices]        test_X = X[test_indices, :]; test_Y = Y[test_indices]        reg = Model(**param)        reg.fit(train_X, train_Y)        predictions = reg.predict(test_X)        total += accuracy_score(test_Y, predictions)    accuracy = total / numFolds    print ""Accuracy score of {0}: {1}"".format(Model.__name__, accuracy)My output:Accuracy score of LogisticRegression: 0.946666666667Accuracy score of SGDClassifier: 0.76";[education, open-source];26;1
6677;1;2015-08-04T08:48:19.787;How do you make the x axis a log scale using metafor for a forest plot?;"I am using the metafor library for plotting a forest plot of odds ratios.  I want the scale to be in log format:The default code is as below:data<-read.csv('test.csv')# If OR's and 95% CI's, run these commented out linesdata$beta <- log(data$OR)data$se   <- (log(data$UCL)-log(data$LCL))/(2*1.96)# Assign values for plottinglabs <- data$group    yi   <- data$betasei  <- data$se# Combine data into summary estimateres  <- rma(yi=yi, sei=sei, method=""FE"", measure='OR')#summary(res)# Plot combined dataforest(res, atransf=log, refline=1, xlab=""Odds Ratio (95%CI)"", slab=labs,      mlab=""Summary Estimate"")The plot that comes out is very nice, but the scale is not correct despite changing the atransf or the method and measure options.Anyone with metafor experience who has managed to make a log scale on the x axis, please let me know how they did it.";[education, open-source];11;
6679;1;2015-08-04T10:48:20.727;R: Comparing dissimilarity between metabolic models with discrete wavelet transformation;I’m working on comparing bacteria metabolic models. Each model has a set of metabolites (around 2000) and their concentration for 200 time points. I’m in the process of comparing the models to cluster them based on their similarity. One method I followed is I did a pair wise comparison for each of the metabolite pairs in two models using Euclidean distance. Below is how my data look like. This is a sample data file.I computed pair wise Euclidean distance for Met1 from Model A and Met1 from Model B. Likewise computed the distances for all the common metabolites between the 2 models (Met4 in Model A and Met4 in Model B) and summed up the distances to get a distance (dissimilarity) between the two models. Similarly I computed the dissimilarity matrix for all the models and I used hierarchical clustering to cluster them.As mentioned above, now I want to compute the dissimilarity of the models using Discrete Wavelet Transformation as my distance measure. I would like to know how to use Discrete Wavelet Transformation to compute a dissimilarity distance between 2 time series and hence for my models.Previously I used DWT as a distance measure with DBSCAN for clustering metabolites in one model according to their behavior. It worked fine.;[education, open-source];24;
6680;1;2015-08-04T12:36:20.967;Sequence of numbers as single feature;Is it possible to use a sequence of numbers as one feature?For example, using libsvm data format:<label> <index1>:<value1> <index2>:<value2>+1 1:123.02 2:1.23 3:5.45,2.22,6.76+1 1:120.12 2:2.23 3:4.98,2.55,4.45-1 1:199.99 2:2.13 3:4.98,2.22,6.98...Is there any special machine learning algorithm for this kind of data? Thanks in advance:) ;[education, open-source];30;
6681;1;2015-08-04T13:44:22.333;Data driven approach to define a churn user;"I'm trying to define a churn prediction model for an on-line service (betting/gambling).A lot of papers talk about churn analysis/prediction for telco companies.In that industry, defining a churn user is straightforward: a churn user is a user who cancels the contract.But how to define a churn user for this industry where users don't cancel any contract/account?The general idea is clear to me: if a user who plays every day for a month stops playing for 2 days, this is an alarm; if a user only plays once a week has no activity for 2 days, this is not an alarm.What can I use? Survival analysis? Is it enough? Is it correct to use it??";[education, open-source];29;
6682;1;2015-08-04T16:11:02.237;Where can I find some publicly available dataset for retail store companies?;I am looking for some publicly available dataset for retail store companies which (preferably) includes data about there stores and operations. I tried to look around but couldn't find any dataset related to retail store companies. Does anyone know about such datasets that I can view and download(free or paid-any option)?;[education, open-source];13;
6683;1;2015-08-04T17:44:35.277;Feature selection using feature importances in random forests with scikit-learn;I have plotted the feature importances in random forests with scikit-learn. In order to improve the prediction using random forests, how can I use the plot information to remove features? I.e. how to spot whether a feature is useless or even worse decrease of the random forests performance, based on the plot information? The plot is based on the attribute feature_importances_ and I use the classifier sklearn.ensemble.RandomForestClassifier.I am aware that there exist other techniques for feature selection, but in this question I want to focus on how to use feature feature_importances_.Examples of such feature importance plots:;[education, open-source];30;
6684;2;2015-08-04T17:55:20.787;;"You can simply use the feature_importances_ attribute to select the features with the highest importance score. So for example you could use the following function to select the K best features according to importance.def selectKImportance(model, X, k=5):     return X[:,model.feature_importances_.argsort()[::-1][:k]]Or if you're using a pipeline the following classclass ImportanceSelect(BaseEstimator, TransformerMixin):    def __init__(self, model, n=1):         self.model = model         self.n = n    def fit(self, *args, **kwargs):         self.model.fit(*args, **kwargs)         return self    def transform(self, X):         return X[:,self.model.feature_importances_.argsort()[::-1][:self.n]]So for example:>>> from sklearn.datasets import load_iris>>> from sklearn.ensemble import RandomForestClassifier>>> iris = load_iris()>>> X = iris.data>>> y = iris.target>>> >>> model = RandomForestClassifier()>>> model.fit(X,y)RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',            max_depth=None, max_features='auto', max_leaf_nodes=None,            min_samples_leaf=1, min_samples_split=2,            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,            oob_score=False, random_state=None, verbose=0,            warm_start=False)>>> >>> newX = selectKImportance(model,X,2)>>> newX.shape(150, 2)>>> X.shape(150, 4)And clearly if you wanted to selected based on some other criteria than ""top k features"" then you can just adjust the functions accordingly.";;;
6685;2;2015-08-04T17:59:06.823;;2 solutions:You aggregate each sequence of numbers into a single number, which use as a feature. There exist plenty of aggregation functions, such as some derived from descriptive statistics root-mean-square, kurtosis, skewness, max, min, duration, standard deviation, crest factor, mean, or more specific aggregation such as fourier transforms or wavelet transforms.You use some model that accepts sequences as input: all such models I have in mind would require your sequences of numbers to be of same length though. Example of such model: recurrent neural networks, Dynamic Bayesian networks.;;;
6686;1;2015-08-04T18:13:03.290;Digit recognition with semi-supervised learning and few labelled data points;Several months ago I watched a talk about a very interesting ML approach that was able to achieve high accuracy in digit recognition with only a few dozen labelled data points, while using the rest of the dataset (probably MNIST) for unsupervised learning.The algorithm used convolutional NNs but instead of MAX pooling between the convolutional layers it, it somehow passed the information about location of the features in the image.The problem is I did not save the link and I am unable to find the relevant talk/paper anymore. The closest match I found was an interesting paper Pseudo-Label : The Simple and Efficient Semi-Supervised LearningMethod for Deep Neural Networks that achieved very good performance with 100 labelled data points. However traversing the citation graph from there didn't reveal my desired paper.Does anyone here recall a similar talk/paper and could point me where to find it? A link to any other interesting work on digit/image recognition with severely limited ( < 100) amount of labelled datapoints is also very welcome.Thanks!;[education, open-source];18;
6687;1;2015-08-04T19:25:34.437;Spatial clustering of data points on a grid to obtain variable resolution map with constant statistical confidence;I have a grey valued image which is calculated as the mean of a series of images. The value of each pixel is therefore associated to a standard error.The pixel values and the relative standard error (RSE) vary smoothly in pixel space, so that the pixels of a downscaled image resulting as the mean of the underlying downscaled images would exhibit a lower RSE. Meaning that a high resolution image is associated to high RSEs and a low resolution image to lower RSEs. However, the RSE vary in space.Effectively this behaves like the trade-off of an uncertainty relation: Resolution in space vs resolution in the mean value.For representing this I am looking for a way to divide the image into regions with constant RSEs, which would be larger in parts of the image with high RSEs and smaller in parts with low RSEs.The aim would be to obtain irregularly sized superpixels with regular RSEs as opposed to my current image which contains pixels on a regular grid but with irregular RSEs.This would be equivalent to downscaling the image but not on a regular grid. One approach I tried is agglomerative clustering of the pixels computing new distances in every step, but so far I couldn't find the right distance and objective functions to obtain a more or less regular map of superpixels (generally one big cluster would eat up its neighbours).Do you know of any existing approach for this or if not which would be a good clustering method to apply? Would quadtrees help me?Thanks a lot in advance!;[education, open-source];20;
6688;2;2015-08-04T20:22:00.210;;"A layer in a neural network does not process logic of the kind you present. It cannot ""check the input pattern with a simple if else pattern"". You could make a model that processed your example inputs to example outputs as you show, but it would not be a neural network.In machine-learning, neural networks have a formal, numeric structure. There are a few variations, but essentially in your diagram:Each circle is a ""neuron"" which may have a numeric output value, also called its activation.Each arrow is a connection between neurons that shows where values are passed between neurons. Each connection has a strength or weight associated with it.When you have many arrows going into a neuron, you can calculate its new activation starting with the sum of weights times the activation values of neurons that they are connected from. Call this sum $z$. Then the activation of the neuron you are looking at is $a = f(z)$ where f is an ""activation function"" (also called ""transfer function"").The above very roughly covers how many neural networks are constructed. As you can see though, there is no room in that for having your if/else statement.Instead, neural networks like the one you show (technically a feed forward network also called a multi-layer perceptron or MLP) are most often trained by example. This involves repeatedly giving the network inputs where you know the output you want. To start with the network will give completely incorrect outputs, and you then adjust the weights to try and make the result closer. This is repeated very many times until the network becomes the best model it can for your purpose. A very common algorithm used to train a neural network in this way is backpropagation and gradient descent.For your problem as described, a neural network is not a good model, and would not be used in the real world. That is because you already know the full logic of a simpler model that covers all possible inputs. At best with a neural network you will get an approximation to that already-perfect model.However, if this is for the purposes of understanding neural networks, your approach would be roughly this:Generate some training data with example inputs and outputs. Most example outputs will be the 23,23 default values, but when inputs are the special values 6,7,8 then the training outputs should be 3 and 4.Build an initial network. Normally you would use an appropriate library - it is important you learn how the library works and understand any limitations. E.g. many activation functions have a maximum output of 1, so cannot learn the values 3,4, or 23.Train the network to fit the example data. There are very many options there, depending on the library you are using. Usually it is just a matter of selecting parameters for the learning that help it progress smoothly. You may have to run training for a large number of repeats over the whole dataset (called epochs)Test that your network works as you expect. Even if you have trained it well, the response of the network is never as precise as your problem statement. If you gave it inputs of 5.8,7.2,8.1 (i.e. close to 6,7,8) then it would likely output something like 7.2,8.5 and not 23,23 .  . . it will interpolate between values.";;;
6689;2;2015-08-04T21:09:48.260;;You should first define what your churn event is which you have started. Is it global or individual? Is it has not gambled for 3 months or has changed his pattern? Global is better to model. You can use survival models for this. ;;;
6690;2;2015-08-04T21:39:31.060;;Maybe the best answer for Your question is recommenderlab package for R available from CRAN. Here's the link to vignette. It has CF algorithms implemented and Jaccard coef. for 0/1 user feedback case.  Only similarity(distance) computation in base stats R package there is dist function. But it's limited to few distance measures. I know that when I search for distance measures in R package called vegan claimed to have many methods implemented. But I have never used it. IMHO most basic recommender systems algorithms are really simple. It worth spending few minutes and implement it to get a better understanding and intuition how it works. Nevertheless, below my quick and dirty user-based kNN implementation for Your case. The code can be rewritten in better way (Adv.R is really good resource) - but this is only example for learning purpose and I do not take responsibilities for any damages :-)require(reshape2)ratings <- read.csv(con <- textConnection('User,Item,RatingAnna,Apple,2Bob,Apple,4Carlos,Apple,1Ethan,Apple,1Gene,Apple,2Anna,Banana,4Dana,Banana,3Ethan,Banana,5Frank,Banana,5Bob,Carrot,1Ethan,Carrot,2Gene,Carrot,4'), header = T)R <- ratings %>% dcast(Item ~ User)ProductNames <- R[,1]R <- R[,-1] # only ratings# tanimoto similarity (~Jaccard coefficiet)tanimoto <- function(v1,v2) length(intersect(v1,v2))/length(union(v1,v2))# return top-k similar usersgetKNN <- function(R, i, k, sim = tanimoto) {  similarity <- array(0,length(R))  for(j in 1:length(R)) {    if(i!=j) similarity[j] = sim(which(!is.na(R[,i])),which(!is.na(R[,j])))  }  idx <- order(similarity, decreasing = T)[1:k]  data.frame(idx = idx, similarity = similarity[idx])}kNNRecommender <- function(R, k) {  reco <- data.frame()  for(u in 1:length(R)) {    userItems <- which(!is.na(R[,u])) #items rated by user    nn <- getKNN(R,u, k) # get user neighbours    # rating prediction - wiegted by similarity    r <- array(0, c(dim(R)[1], k))    for(i in 1:k) {      r[,i] = R[,nn[i,]$idx] * nn[i,]$similarity    }    r[is.na(r)] <- 0    userReco <- rowSums(r) / sum(nn$similarity)    userRecoIdx <- order(userReco, decreasing = T)    # remove items already rated by user    userRecoIdx <- userRecoIdx[-userItems]    # add to recommendations result    reco <- rbind(reco, data.frame(      User=u,      Item=(if(length(userRecoIdx)==0) NA else userRecoIdx),      Prediction=(if(length(userRecoIdx)==0) NA else userReco[userRecoIdx])    ))  }  reco}kNNRecommender(R, 2);;;
6691;1;2015-08-04T22:15:52.833;Sentiment Analysis model for Spanish;I barely know about Data Analysis, tools and techniques, so bare with me if I'm asking something too trivial.I'm looking for a Sentiment Analysis tool to process comments in Spanish. I do know some options for Sentiment analysis but those all work for English. Is there a model/tool that already works with Spanish? I'm language agnostic so it does not matter if it's a Java, Python or even Go code. Thanks.;[education, open-source];30;
6692;1;2015-08-05T03:08:49.750;read .shp GIS on Java;I know that can sounds weird this thread. but if we are talking about a popular data base, we need to meet a way to upload .shp file to post gis using an API, batch files seems a terrible workaround. I tried Rjava but nothing, I tried GISserver, but his .war is not working. So, I don't know what I can do anymore. Any advice are very welcome.Ps.: I'm using Java/Spring, if anyone knows an efficient solution to deal with these architecture are very welcome too.Thanks in advance.Cheers;[education, open-source];5;
6693;1;2015-08-05T05:18:55.957;Limitations on the number of items to use in apriori algorthim?;Is there any limitations on the number of items to use in a transaction for applying apriori algorrithm.I have a dataset with just 20 records, but the number of items extend upto 900. I am getting memory issue when I apply the apriori algorithm upon the dataset.Thank you;[education, open-source];17;
6694;1;2015-08-05T05:58:44.543;Books on Reinforcement Learning;I have been trying to understand reinforcement learning for quite sometime, but somehow I am not able to visualize how to write a program for reinforcement learning to solve a grid world problem. Can you suggest me some text books which would help me build a clear conception of Reinforcement Learning? ;[education, open-source];68;
6696;2;2015-08-05T06:48:49.087;;There is a free online course on Reinforcement Learning by Udacity. Check : Machine Learning: Reinforcement Learning ;;;
6698;2;2015-08-05T07:01:10.340;;As correctly pointed out above MinHash and SimHash both belong to Locality Sensitive Hashing. Reference: https://en.wikipedia.org/wiki/Locality-sensitive_hashingThe main difference between the two is the way collision is handled,SimHash, uses cosine similarityMinHash, uses Jaccard Index.Answers to your questions:No. They uses different collision handling techniques to validate similarity. Also there is a variant on single Hash Function for Min Hash but it works differently. For more details, check the following reference out, https://en.wikipedia.org/wiki/MinHash (Variant with a single hash function) Yes, https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.pyI think the complexity can be reduced to $O(n \log n)$ with modified form of binary search while clustering.;;;
6699;2;2015-08-05T07:17:04.600;;check the following links for Spanish sentiment analysis related links :Working Model :http://dtminredis.housing.salle.url.edu:8080/EmoLib/es/Data : http://www.daedalus.es/TASS2015/tass2015.phpApi : https://www.mashape.com/molinodeideas/sentiment-analysis-spanish;;;
6700;1;2015-08-05T10:27:51.370;When to stop calculating values of each cell in the grid in Reinforcement Learning(dynamic programming) applied on gridworld;Considering application of Reinforcement learning(dynamic programming method performing value iteration) on grid world, in each of the iteration, i go through each of the cell of the grid and update its value depending on its present value and the present value of the taking action from that state. Now 1. How long do I keep updating value of each cell? Shall I keep updating unless the change in the previous and the present value function is the least? I am not able to understand how to implement the stopping mechanism in the grid-world scenario(discount not considered)2. Is the value function the values of all the grids in the grid world?;[education, open-source];41;
6701;1;2015-08-05T10:53:17.293;How to keep a subsetted value for calculating mean;I am currently learning R and I have to solve an Issue were I have to extract values from a data set which are from a specific month and from this values I should calculate the mean Temp. I did it like that:data[data$X..Month.. == 6,]   mean(data$X..Temp.., na.rm=TRUE)It gave me the mean value but without taking my first statement into consideration. What do I need to do that both statements are taken into consideration?;[education, open-source];23;
6702;2;2015-08-05T11:24:34.843;;You can answer this using tapply, which will give you the mean for all your subsets, e.g.dummy datadf = data.frame(mon=rep(month.abb, times=10)calculate mean for all monthstapply(df$temp, df$mon, mean, na.rm=T);;;
6703;2;2015-08-05T11:28:25.060;;"To just get the mean for month 6:mean(df$temp[df$mon==""Jun""], na.rm=T)You were nearly there but didn't assign your subset to a value, if you had:x = data[data$X..Month.. == 6,]mean(x$X..Temp.., na.rm=TRUE)that should work.";;;
6704;2;2015-08-05T14:35:57.897;;"Take a look at the TSclust package. Here how you would apply it to your sample data. require(TSclust)#read in the datamodel_a <- read.csv(""~/Desktop/Model A.csv"", header = TRUE, stringsAsFactors = FALSE)model_b <- read.csv(""~/Desktop/Model B.csv"", header = TRUE, stringsAsFactors = FALSE)#data must be in rows rather than columnsmodel_a <- as.data.frame(t(model_a))model_b <- as.data.frame(t(model_b))#calculate dissimlarities between metabolites in models 1 and 2met1_DWT.diss <- as.numeric(diss.DWT(rbind(model_a['Met1', ], model_b['Met1', ])))met1_DWT.diss[1] 90.80332met2_DWT.diss <- as.numeric(diss.DWT(rbind(model_a['Met2', ], model_b['Met2', ])))met2_DWT.diss[1] 1.499241";;;
6705;2;2015-08-05T16:22:23.003;;I really enjoyed Reinforcement Leraning: An introduction by Richard Sutton. It provides a very nice unifying view on RL, although it does not mention the newest approaches (it's from 1998).;;;
6707;2;2015-08-06T01:25:40.610;;1- You should set a threshold (a hyper-param) that will allow you to quit the loop.Let V the values for all state s and V' the new values after value iteration.if $\sum_s|V(s) - V’(s)| \le threshold$, quit2 - V is a function for every cell in the grid yes because you need to update every cell. Hope it helps.;;;
6709;1;2015-08-06T06:03:20.613;Confusion in Policy Iteration and Value iteration in Reinforcement learning in Dynamic Programming;So what i understood for value iteration while coding is that, we need to have a policy fixed... according to that policy the value function of each state will be calculated right? But in policy iteration the policy will change from time to time.. Am I right?;[education, open-source];20;
6710;2;2015-08-06T09:09:00.657;;If you need complexity of this calculation in big O notation - it is: $$O(n^3)$$Why? Because matrix inverse needs $$O(n^3)$$ operations, and it is biggest complexity here.Multiplication matrix by its transpose is $$O(n^2p)$$ (Because for computing every value in the resulting matrix of size NxN you have to compute p multiplications).Matrix transpose is $$O(np)$$ But you can ignore any complexities lesser than $$O(n^3)$$ (like matrix sum, transpose, or multiplying by constant) because their order of growth much lesser than order of growth of inverse operation.Computational_complexity_of_mathematical_operations;;;
6711;2;2015-08-06T09:25:34.667;;"In value iteration, you implicitly solve for the state values under an ideal policy. There is no need to define an actual policy during the iterations, you can derive it at the end from the utilities that you calculate. You could if you wish, after any iteration, use the state values to determine what ""current"" policy is predicted.In policy iteration, you define a starting policy and iterate towards the best one. So the policy is more explicitly tracked and re-calculated on each step.";;;
6714;2;2015-08-06T16:32:40.243;;@Olologin is correct for very basic direct solves with no optimization, but there are optimizations that exist, so I wanted to add a few points...A great many mathematicians spend their entire careers on speeding up and optimizing this problem.  So, the solution depends a lot on the composition of the matrix (sparsity), whether the problem is convex,  whether it is being solved iteratively or directly, and what algorithm is being used.For lots of data science problems $O(n^2)$ and better solves are readily achieved.Some common libraries include:Basic Linear Algebra Subroutines (BLAS)LINPACKLAPACKHere are some surveys of the subject:General surveySurvey of Iterative MethodsLU decompositionConjugate GradientDirect Methods for Solving Linear Systems;;;
6715;1;2015-08-06T20:58:57.380;Is it necessary to standardize your data before clustering?;Is it necessary to standardize your data before cluster?  In the example from scikit learn about DBSCAN, here they do this in the line:X = StandardScaler().fit_transform(X)But I do not understand why it is necessary.  After all, clustering does not assume any particular distribution of data - it is an unsupervised learning method so its objective is to explore the data.  Why would it be necessary to transform the data?  ;[education, open-source];126;2
6716;2;2015-08-06T21:38:01.087;;Standardizing data is recommended because otherwise the range of values in each feature will act as a weight when determining how to cluster data, which is typically undesired.For example consider the standard metric for most clustering algorithms (including DBSCAN in sci-kit learn) -- euclidean, otherwise known as the L2 norm. If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature. To illustrate this look at the simple example below:>>> import numpy as np>>> from sklearn.preprocessing import StandardScaler>>> from sklearn.metrics.pairwise import euclidean_distances>>> >>> X = np.array([[1,2,100],[4,3,50],[1,1,75]])>>> >>> print X[[  1   2 100] [  4   3  50] [  1   1  75]]>>> >>> print euclidean_distances(X)[[  0.          50.0999002   25.01999201] [ 50.0999002    0.          25.25866188] [ 25.01999201  25.25866188   0.        ]]>>> print euclidean_distances(StandardScaler().fit_transform(X))[[ 0.          3.46410162  1.73205081] [ 3.46410162  0.          3.46410162] [ 1.73205081  3.46410162  0.        ]]From this you should see that the euclidean distances between the non-standardized versions are dominated by the third column because its range of values is much larger than the other two. However when the data is standardized this no longer becomes an issue and weights each feature as being equal when calculating the distance between each data point.;;;
6717;2;2015-08-07T00:20:03.757;;It isn't strictly necessary to standardise, whether it is required or not may depend on the distance metric you choose. For instance if you opt for the Mahalanobis distance then separation will be based upon the number of standard deviations separating points and not the absolute distance between them, as such it is a scale invariant metric.As with many things in machine learning there is no hard and fast answer and the only way to know for sure is to apply a variety of techniques see which gives the most appropriate results for your data.;;;
6718;2;2015-08-07T00:22:14.480;;"Normalization is not always required, but it rarely hurts. Some examples:K-means: K-means clustering is ""isotropic"" in all directions of space and  therefore tends to produce more or less round (rather than elongated)  clusters. In this situation leaving variances unequal is equivalent to  putting more weight on variables with smaller variance.Example in Matlab:X = [randn(100,2)+ones(100,2);...     randn(100,2)-ones(100,2)];% Introduce denormalization% X(:, 2) = X(:, 2) * 1000 + 500;opts = statset('Display','final');[idx,ctrs] = kmeans(X,2,...                    'Distance','city',...                    'Replicates',5,...                    'Options',opts);plot(X(idx==1,1),X(idx==1,2),'r.','MarkerSize',12)hold onplot(X(idx==2,1),X(idx==2,2),'b.','MarkerSize',12)plot(ctrs(:,1),ctrs(:,2),'kx',...     'MarkerSize',12,'LineWidth',2)plot(ctrs(:,1),ctrs(:,2),'ko',...     'MarkerSize',12,'LineWidth',2)legend('Cluster 1','Cluster 2','Centroids',...       'Location','NW')title('K-means with normalization')(FYI: How can I detect if my dataset is clustered or unclustered (i.e. forming one single cluster)Distributed clustering: The comparative analysis shows that the distributed clustering results  depend on the type of normalization procedure.Artificial neural network (inputs): If the input variables are combined linearly, as in an MLP, then it is  rarely strictly necessary to standardize the inputs, at least in  theory. The reason is that any rescaling of an input vector can be  effectively undone by changing the corresponding weights and biases,  leaving you with the exact same outputs as you had before. However,  there are a variety of practical reasons why standardizing the inputs  can make training faster and reduce the chances of getting stuck in  local optima. Also, weight decay and Bayesian estimation can be done  more conveniently with standardized inputs.Artificial neural network (inputs/outputs) Should you do any of these things to your data? The answer is, it  depends. Standardizing either input or target variables tends to make the training  process better behaved by improving the numerical condition (see   ftp://ftp.sas.com/pub/neural/illcond/illcond.html) of the optimization  problem and ensuring that various default values involved in  initialization and termination are appropriate. Standardizing targets  can also affect the objective function.  Standardization of cases should be approached with caution because it  discards information. If that information is irrelevant, then  standardizing cases can be quite helpful. If that information is  important, then standardizing cases can be disastrous.Interestingly, changing the measurement units may even lead one to see a very different clustering structure: Kaufman, Leonard, and Peter J. Rousseeuw.. ""Finding groups in data: An introduction to cluster analysis."" (2005). In some applications, changing the measurement units may even lead one  to see a very different clustering structure. For example, the age (in  years) and height (in centimeters) of four imaginary people are given  in Table 3 and plotted in Figure 3. It appears that {A, B ) and { C,  0) are two well-separated clusters. On the other hand, when height is  expressed in feet one obtains Table 4 and Figure 4, where the obvious  clusters are now {A, C} and { B, D}. This partition is completely  different from the first because each subject has received another  companion. (Figure 4 would have been flattened even more if age had  been measured in days.) To avoid this dependence on the choice of measurement units, one has  the option of  standardizing the data. This converts the original  measurements to unitless variables.Kaufman et al. continues with some interesting considerations (page 11): From a philosophical point of view, standardization does not really  solve the problem. Indeed, the choice of measurement units gives rise  to relative weights of the variables. Expressing a variable in smaller  units will lead to a larger range for that variable, which will then  have a large effect on the resulting structure. On the other hand, by  standardizing one attempts to give all variables an equal weight, in  the hope of achieving objectivity. As such, it may be used by a  practitioner who possesses no prior knowledge. However, it may well be  that some variables are intrinsically more important than others in a  particular application, and then the assignment of weights should be  based on subject-matter knowledge (see, e.g., Abrahamowicz, 1985). On  the other hand, there have been attempts to devise clustering  techniques that are independent of the scale of the variables  (Friedman and Rubin, 1967). The proposal of Hardy and Rasson (1982) is  to search for a partition that minimizes the total volume of the  convex hulls of the clusters. In principle such a method is invariant  with respect to linear transformations of the data, but unfortunately  no algorithm exists for its implementation (except for an  approximation that is restricted to two dimensions). Therefore, the  dilemma of standardization appears unavoidable at present and the  programs described in this book leave the choice up to the user.";;;
6719;1;2015-08-07T04:31:06.123;Value Updation Dynamic Programming Reinforcement learning;Regarding Value Iteration of Dynamic Programming(reinforcement learning) in grid world, the value updation of each state is given by: Now Suppose i am in say box (3,2). I can go to (4,2)(up) (3,3)(right) and (1,3)(left) and none of these are my final state so i get a reward of -0.1 for going in each of the states. The present value of all states are 0. The probability of going north is 0.8, and going left/right is 0.1 each. So since going left/right gives me more reward(as reward*probability will be negative) i go left or right. Is this the mechanism. Am I correct? But In the formula there is a summation term given. So I basically cannot understand this formula. Can anyone explain me with an example?;[education, open-source];23;1
6720;2;2015-08-07T05:15:00.393;;The probabilities you describe refer only to the go-north action. It means that if you want to go north, you have 80% chance of actually going north and 20% of going left or right, making the problem for difficult (non-deterministic). This rule applies to every direction. Also, the formula does not tell which action to chose, just how to update the values. In order to select an action, assuming a greedy-policy, you'd select the one with the highest expected value ($V(s')$).The formula says to sum the values for all possible outcomes from the best action. So, supposing go-north is indeed the best action, you have: $$.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + 0) = -.1$$But let us suppose that you still don't know which is the best action and want to select one greedily. Then you must compute the sum for each possible action (north, south, east, west). Your example has all values set to 0 and the same reward and so is not very interesting. Let's say you have a +1 reward to east (-0.1 for the remaining directions) and that south already has V(s) = 0.5 (0 for the remaining states). Then you compute the value for each action (let $\gamma = 1$, since it is a user-adjusted parameter):North: $.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (1 + 0) = -.08 - .01 + .1 = .01$South: $.8 * (-.1 + .5) + .1 * (-.1 + 0) + .1 * (1 + 0) = 0.32 - .01 + .1 = .41$East: $.8 * (1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + .5) = .8 - .01 + .04 = .83$West: $.8 * (-.1 + 0) + .1 * (-.1 + 0) + .1 * (-.1 + .5) = -.08 - .01 + .04 = -.05$So you would update your policy to go East from the current state, and update the current state value to 0.83.;;;
6721;1;2015-08-07T10:43:50.747;How to preprocess different kinds of data (continuous, discrete, categorical) before Decision Tree learning;I want to use some Decision Tree learning, such as the Random Forest classifier.I have data of different types: continuous, discrete and categorical. How do I have to preprocess data in order to have consistent results?;[education, open-source];35;1
6722;2;2015-08-07T11:02:11.387;;"I found in some cases useful to define a ""business evaluation"" function, defining the ""importance"" of the dimensions used for clustering. E.g. for greengrocer clustering the customers, if apples are twice as expensive as oranges, the number of apples will be doubled.";;;
6723;1;2015-08-07T11:16:57.770;How to choose GPS setup?;I some advice please on choosing a GPS setup to map estuarine habitats in the field?  Our SA estuaries are often very narrow (1 to 2 km wide) and habitats small.  We distinguish between supratidal salt marsh, intertidal salt marsh, reeds and sedges, among other habitats.  Reeds and sedges often form a very narrow band along an estuary.  Accuracy needs to be as close as possible to 1 m2, perhaps even better.Thanks in advance.Mawande ;[education, open-source];30;
6724;2;2015-08-07T21:37:39.207;;If you want to be a practical man with true knowledge, start with math(calculus, probability + stat, lelinear algebra). On every step try to implement everything with programing, python is nice for this. When u get good ground, play with real data and solve for problemsCourses.Linear algebra - edx Laff or coding the matrixStat - edx stat 2x BarkleyCalculus - read...its simple;;;
6725;2;2015-08-07T22:13:30.820;;One of the benefits of decision trees is that ordinal (continuous or discrete) input data does not require any significant preprocessing. In fact, the results should be consistent regardless of any scaling or translational normalization, since the trees can choose equivalent splitting points. The best preprocessing for decision trees is typically whatever is easiest or whatever is best for visualization, as long as it doesn't change the relative order of values within each data dimension.Categorical inputs, which have no sensible order, are a special case. If your random forest implementation doesn't have a built-in way to deal with categorical input, you should probably use a 1-hot encoding:If a categorical value has $n$ categories, you encode the value using $n$ dimensions, one corresponding to each category. For each data point, if it is in category $k$, the corresponding $k$th dimension is set to 1, while the rest are set to 0.This 1-hot encoding allows decision trees to perform category equality tests in one split since inequality splits on non-ordinal data doesn't make much sense.;;;
6726;1;2015-08-08T08:16:15.160;Random forest model gives same result for all test data, Next step?;I am teaching myself some data science and have started a Kaggle project. I have fitted a random forest classification model (using sci-kit learn) with a few millions rows of data. There are two possible outcomes for each row (0 or 1). When I run it against the test data, I get 0 for every row. This is a practical impossibility, but I am at a loss as to how to diagnose my model and how to move forward. Is this an example of extreme overfitting ? Is it more likely a problem with my training data (or test data) that i am not seeing ?Should I simply increase the # of estimators? Is it possible that I have misformatted the input files in a subtle way that doesn't cause an error ? I am at a loss as to how to move forward.;[education, open-source];40;
6727;1;2015-08-08T11:14:35.630;Merging repeating data cells in csv;I have a CSV file with around 1 Million rows. Let say its have details like Name      |   Age   | Salary name 1      52       10000name 2      55       10043 name 3      50       100054name 2      55       10023name 1      52       100322...and soon .but i need to merge the redundant details .and need a output like Name      |   Age   | Salary name 1      52       110322*name 2      55       20066 *name 3      50       100054 you might notice that the repeating Name 1 and Name 2 details are merged and the Salary values are added .So i'm looking for a way to apply this change to my original data set. so i need a python script to fix my problem . ;[education, open-source];44;
6728;1;2015-08-08T11:52:25.107;Validating hierarchical cluster solutioins in R;I have a data set which consists of dissimilarity distance among 55 bacteria models. I'm trying to cluster them into 4 groups. Here is the dissimilarity matrix.I'm looking for a method to evaluate the goodness of clusters produced by different agglomeration methods in hierarchical clustering  (e.g: single, average etc). My goal is to find find the most suitable agglomeration method. According to my understand Silhouette can't be used for hierarchical clustering. One method I tried was figuring out which models are clustered into which cluster segments for different agglomeration methods. Below is the matrix image. But it doesn't give me much information. Also I went thought an article which mentioned about using boxplots to plot cluster elevations. However it was not clear how to generate them and interpret them. Any help is appreciated.;[education, open-source];24;
6729;2;2015-08-08T13:22:01.227;;"My solution may not be the best, but I would opt for this one since it is the simplest.Create an empty dictionaryIterate on the CSV. If the name is already in the dictionary, sum up the salaries. If not, then create a new key with the salary.After that, iterate again on the dictionary to write a new CSV with the new values.If you are not familiar with Python code, just ask it and I will write it for you :)Edit with a code sample:import csv## open CSV file and rea itmyfile  = open('test.csv', ""rb"")reader = csv.reader(myfile)## create an empty dictionarymydictionary = {}rownum = 0for row in reader:    ## check if it is the header    if rownum == 0:        pass    else:        ## split the line of CSV in elements..Use the name for the key in dictionary and the other two in a list        line = row.split("","")        key = line[0]        age = line[1]        salary = line[2]        if key in mydictionary:            mydictionary[key][1] += salary        else:            mydictionary[key] = [age,salary]    rownum += 1ifile.close()## create a new list of lists with the data from the dictionarynewcsvfile = [""name"",""age"",""salary""]for i in mydictionary:    newcsvfile.append(i,mydictionary[i][0],mydictionary[i][1])## write the new list of lists in a new CSV filewith open(""output.csv"", ""wb"") as f:    writer = csv.writer(f)    writer.writerows(newcsvfile)";;;
6730;1;2015-08-08T17:47:46.043;How to land job for Information retrieval and/or related to data?;I don't know if this is right place to postquestion. If not please ignore and/or ask me to delete and I'll.I have done masters with focus on Info Retrieval,Machine Learning,Map Reduce. I'm currently working for 2 years in domain which is nothinglike that - (system level programming) I am trying hard to get jobon data science area, how should I convince others (believe me2 years is worst - cant ask for New Grad position and cant go forexperience one);[education, open-source];13;
6731;1;2015-08-09T03:02:22.893;How to incorporate indicator variables in sequential gaussian simulation?;I'd like to simulate the presence of snow in an aerial image using a Sequential Gaussian Simulation (SGS).  I understand that SGS simulates by determining the parameters of a gaussian distribution using Maximum Likelihood, and then randomly  choosing a value from that distribution.  I would like to use this simulation to predict the presence of snow, which is an indicator variable (either 0 or 1).  How do I incorporate this variable in my model?  Thanks.;[education, open-source];5;
6732;2;2015-08-09T03:06:58.420;;"Some of the possibilities include the following:1) The training data has class imbalance.Solution:Train the model using CV = 5 or 10;Do a log transformation to make the target distribution more normal in nature;Check if you can add a weight variable that can fix the distribution.2) Testing data is a small sub sample. Since it's Kaggle, I believe the testing data will be standard in nature, so the problem might be in the process that is generating the submission script.As a beginner, you should use any benchmark script available in the forum to start with, which shouldn't have such issues.";;;
6734;1;2015-08-09T10:48:34.667;Is the binary Time series form an abelian group?;Let A be the set of all binary time series. (for example, all spike trains between time t_1 and t_2)  I am looking for an operation , such that (A,) form a group. Is the set of all spike trains  form an abelian group? ;[education, open-source];14;
6735;1;2015-08-09T12:05:42.797;Can i use chi square test to remove a particular variable from the model?;I have 5 variables - Var1, var2, var3, var4 and var5. All are categorical variables.I conduct a chisquare test to understand the relationship  between these variables. I could see that var 3 and var 4 are having p-value as 0.  I understand that these two variables are dependent of each other. Should i remove one of the variable from further analysis.What if the same variable 3 and var 4 are indepenent of other variables like var1, var2 and var5?Thanks in advance,Arun;[education, open-source];31;
6736;1;2015-08-09T13:56:21.477;Do you have any real example of Data Science reports?;I recently found this use cases on Kaggle for Data Science and Data Analytics.Data Science Use CasesHowever, I am curious to find examples and case studies of real reports from other professionals in some of those use cases on the link. Including hypothesis, testing, reports, conclusions (and maybe also the datasets they have used)Do you have any kind of those reports to share?I am aware of the difficulty to share this kind of details and this is why it is not a Google-search-and-find answer. ;[education, open-source];35;1
6737;1;2015-08-09T14:27:33.883;Can natural language generation algorithms generate valid words too?; Natural Language Generation (NLG) is the natural language  processing task of generating natural language from a machine  representation system such as a knowledge base or a logical form. —   WikipediaIs NLG about building meaningful sentences, reports, etc.? Can NLG build valid dictionary words as well? For example, without consulting/reading from an English (or any language) dictionary, can an algorithm generate such words?;[education, open-source];36;0
6739;2;2015-08-09T18:49:55.170;;Outside of context of NLG (thus not a direct answer to your whole question, but an answer to your question's title): Generating words from a character-level model has been done using RNNs exposed to large corpora of text, such as Wikipedia content, and trained to predict text character-by-character.Used to generate content, the model is normally fed a few starting characters and asked to predict the next one. A choice is made from its most-likely predictions and fed back to it to continue the sequence. Here is a blog showing some examples trained on some Shakespear and Wikipedia.Such a network can and does generate nonsense words, although they are often fitting and might read like e.g. a noun or verb as you could expect depending on context. The sentence structure and grammar can come out sort of right, but the semantic content is usually complete gibberish.;;;
6740;1;2015-08-09T19:20:37.687;fit model with sd square;If I want to fit a nonlinear regression model with some parameters like $\sigma^2$(where $\sigma$ is the standard deviation, which is positive), how can I guarantee that $\hat{\sigma}$ is positive?I mean, if I use maximum likelihood(and the model only have square term of $\sigma$), how does the optimization method know $\sigma$ is positive? (well, it seems OK that the result estimation of $\sigma$ is negative, but it looks weird, and this is why I ask this question.);[education, open-source];18;
6741;2;2015-08-10T01:26:35.540;;I suspect you simply have a bug somewhere in the script. It's not possible for us to offer any advice because we don't know the data set and how you construct the random forest. In data science, we all have to learn how to debug the problem. Some suggestions:What happens when you print the tree? Does it look weird like predicting as 0 or 1?Construct a contingency table (or something like that) for your training set. Is the data balanced? If you see that you have no 0 or 1 class label in your training set, you'll find out why.;;;
6742;1;2015-08-10T05:36:02.730;Grid world Dynamic programming Value iteration related code;"Now I am trying to code for the gridworld of size(3 by 4) matrix, with (2,2) forbidden state, and (1,4) reward state with reward of +1 while (2,4) comes with a reward of -1, in Matlab, but i do not know the threshold of when to stop updating the values of each state. So I have given it a fixed number of iteration and its working. But I do not know whether the values I am getting are correct or not. Can someone please tell me whether I am doing the right process or not? The code is as follows:Main Function:clc;%setuprow=3;col=4;grid=zeros(row,col);grid1=zeros(row,col);gamma=0.15;epsilon=0.1;delta=0;inc=0.01;forbidx=2;forbidy=2;%description of the reward staterx1=1;ry1=4;rx2=2;ry2=4;threshold=epsilon*((1-gamma)/gamma);while(delta<threshold)for i=drange(1:row)for j=drange(1:col)    if i == rx1 && j == ry1        continue;    end    if i == rx2 && j == ry2        continue;    end    if i == forbidx && j == forbidy        continue;    end   p=max_action_value(i,j,grid,row,col,forbidx,forbidy,gamma);   grid1(i,j)=p;   if p~=0   end   if abs(p-grid(i,j))>delta       delta=abs(p-grid(i,j));       disp('entered');   end   %deltaendendfor i=1:rowfor j=1:col    grid(i,j)=grid1(i,j);endendendFinding Max action:function z = max_action_value(x,y,grid,row,col,forbidx,forbidy,gamma)E=0;W=0;N=0;S=0;%probabilities of going forward,left,rightpf=0.8;pl=0.1;pr=0.1;f=0;l=0;r=0;%action = WEST%forwardif (x == forbidx && y+1 == forbidy) || (y == col)     f = pf*(gamma*grid(x,y)+reward(x,y));else      f = pf*(gamma*grid(x,y+1)+reward(x,y+1));end%leftif (x-1 == forbidx && y == forbidy) || (x == 1)     l = pl*(gamma*grid(x,y)+reward(x,y));else     l = pl*(gamma*grid(x-1,y)+reward(x-1,y));end%rightif (x+1 == forbidx && y == forbidy) || (x == row)     r = pr*(gamma*grid(x,y)+reward(x,y));else     r = pr*(gamma*grid(x+1,y)+reward(x+1,y));endW=l+r+f;%action = EAST%forwardif (x == forbidx && y-1 == forbidy) || (y == 1)     f = pf*(gamma*grid(x,y)+reward(x,y));else      f = pf*(gamma*grid(x,y-1)+reward(x,y-1));end%leftif (x+1 == forbidx && y == forbidy) || (x == row)     l = pl*(gamma*grid(x,y)+reward(x,y));else     l = pl*(gamma*grid(x+1,y)+reward(x+1,y));end%rightif (x-1 == forbidx && y == forbidy) || (x == 1)     r = pr*(gamma*grid(x,y)+reward(x,y));else     r = pr*(gamma*grid(x-1,y)+reward(x-1,y));endE=l+r+f;%action = NORTHif (x-1 == forbidx && y == forbidy) || (x == 1)     f = pf*(gamma*grid(x,y)+reward(x,y));else      f = pf*(gamma*grid(x-1,y)+reward(x-1,y));end%leftif (x == forbidx && y-1 == forbidy) || (y == 1)     l = pl*(gamma*grid(x,y)+reward(x,y));else     l = pl*(gamma*grid(x,y-1)+reward(x,y-1));end%rightif (x == forbidx && y+1 == forbidy) || (y == col)     r = pr*(gamma*grid(x,y)+reward(x,y));else      r = pr*(gamma*grid(x,y+1)+reward(x,y+1));endN=l+r+f;%action = SOUTHif (x+1 == forbidx && y == forbidy) || (x == row)     f = pf*(gamma*grid(x,y)+reward(x,y));else      f = pf*(gamma*grid(x+1,y)+reward(x+1,y));end%leftif (x == forbidx && y+1 == forbidy) || (y == col)     l = pl*(gamma*grid(x,y)+reward(x,y));else     l = pl*(gamma*grid(x,y+1)+reward(x,y+1));end%rightif (x == forbidx && y-1 == forbidy) || (y == 1)     r = pr*(gamma*grid(x,y)+reward(x,y));else     r = pr*(gamma*grid(x,y-1)+reward(x,y-1));endS=l+r+f;%finding maxif E>=W && E>=N && E>=Sz = E;disp('east');elseif W>=E && W>=N && W>=Sz = W; disp('west');elseif N>=W && N>=E && N>=Sz = N;disp('north');elseif S>=W && S>=N && S>=Ez = S;disp('south');endendGetting the reward: function f = reward(x,y)if x == 1 && y == 4f = 1;elseif x == 2 && y == 4f= -1;else f=-0.1; end";[education, open-source];21;
6743;2;2015-08-10T08:53:30.417;;I would suggest parametrizing with a logarithm of volatility so you don't have to care about positivity, run the estimation and then invert back to original scale. Alternatively, you can consider constrained optimization routine. Without knowing more about the problem (at least the language you're using), that's about it.;;;
6744;1;2015-08-10T12:48:17.207;Prepare data for SVM, Is it valid to normalise the data before and after PCA dimension reduction;"Is it valid to normalise a dataset, reduce dimensionality with PCA and then to normalise the reduced dimension data.    Assuming this is performed on training data, should the same PCA coefficients be used to reduce the dimension of the test data.  Should the same max and min normalisation values be used for the test and training data. I have included a simplified example of the code I am using which may describe I said better. Thanks in advance. %% Prepare Training Data% Normalise training datamindata=min(TRAINDATA); maxdata=max(TRAINDATA);TRAINDATA = ((TRAINDATA-repmat(mindata,[size(TRAINDATA,1),1]))./(repmat(maxdata,[size(TRAINDATA,1),1])-repmat(mindata,[size(TRAINDATA,1),1])) - 0.5 ) *2;% Perform PCAmTRAINDATA = mean(mean(TRAINDATA));TRAINDATA = TRAINDATA - mTRAINDATA;[Cpca,~,~,~,~]=princomp(TRAINDATA,'econ');EigenRange = 1:2;Cpca = Cpca(:,EigenRange);TRAINDATA = TRAINDATA*Cpca;TRAINDATA = TRAINDATA + mTRAINDATA;% Normalise training data second timemindata2=min(TRAINDATA); maxdata2=max(TRAINDATA);TRAINDATA = ((TRAINDATA-repmat(mindata2,[size(TRAINDATA,1),1]))./(repmat(maxdata2,[size(TRAINDATA,1),1])-repmat(mindata2,[size(TRAINDATA,1),1])) - 0.5 ) *2;%% Prepare Test Data% Normalise using first normalisation values from training dataTESTDATA = ((TESTDATA-repmat(mindata,[size(TESTDATA,1),1]))./(repmat(maxdata,[size(TESTDATA,1),1])-repmat(mindata,[size(TESTDATA,1),1])) - 0.5 ) *2;% Perform PCAmTESTDATA = mean(mean(TESTDATA));TESTDATA = TESTDATA - mTESTDATA;TESTDATA = TESTDATA*Cpca;TESTDATA = TESTDATA + mTRAINDATA;% Normalise using second normalisation values from training dataTESTDATA = ((TESTDATA-repmat(mindata2,[size(TESTDATA,1),1]))./(repmat(maxdata2,[size(TESTDATA,1),1])-repmat(mindata2,[size(TESTDATA,1),1])) - 0.5 ) *2;";[education, open-source];18;
6745;2;2015-08-10T15:24:05.837;;"This isn't a python program so strictly it isn't a solution to your problem, technically it isn't even a program - just a command line, but it does illustrate that are many ways to skin a cat, some of them quite elegantly.Dictionaries are a lovely feature of the modern language python but much earlier languages such as AWK, or it's linux equivalent GAWK, have had them for just short of 40 years, though in those days they were called associative arrays. So here is a one line command using cat and gawk, with its ability to accept code as an argument, to accomplish your task, demonstrating that the design principle of linux to be a framework for collaborating utilities is still alive and well. $> cat in.dat | gawk '{if(NR==1)hdr=$0; else{emp[$1"" ""$2"" ""$3]=emp[$1"" ""$2"" ""$3]+$4}} END{print hdr; for(i in emp) print i,emp[i];}'  > out.datI've taken your file formats literally, if you do actually require true CSVs then a small mod will accomplish that.cat in.csv | awk -F"","" '{if(NR==1)hdr=$0; else{emp[$1"",""$2]=emp[$1"",""$2]+$3}} END{OFS="","";print hdr; for(i in emp) print i,emp[i];}' > out.csv Name,Age,Salary  name 2,55,20066 name 1,52,110322 name 3,50,100054Designed as a text manipulation language by some very clever computer scientists, including Brian Kernighan of C fame, AWK achieves this compactness by automatically parsing any input it is given into fields ( \$1, \$2, ... ) and keeping track of the number of lines processed. This allows a lot of the I/O management you might need to do in another language to be skipped over as it already exists.";;;
6746;2;2015-08-10T15:38:47.583;;TM7 of The Netherlands has an advanced NLP engine named CARP that could do this for you. Their examples are largely in Dutch and English language, but they have established ontologies for Spanish for a number of their functional modules, including sentiment analysis. Find the public 'playground' at http://www.tm7.nl . Select 'Carp Language Technologie' and follow the link in the blue box to the Playground.Information about available APIs using SOAP services is available as well under the link 'SOAP Webservices' in English;;;
6747;2;2015-08-10T18:25:21.680;;Pandas is a python library that you will find very useful for these types of tasks.Here is a stack overflow post that tells you how to do what you want to accomplish.It boils down to three very pythonic lines with a groupby and transformation followed by a drop_duplicates:import pandasdf = pandas.read_csv('csvfile.csv', header = 0)df['Total'] = df.groupby(['Name', 'Age'])['Salary'].transform('sum')df.drop_duplicates(take_last=True);;;
6748;1;2015-08-10T18:45:08.413;Time threshold maps plotting in R;I am trying to plot time threshold maps, proposed by Fryzlewicz. The relevant paper is: http://stats.lse.ac.uk/fryzlewicz/allth/allth.pdf.Questions: Is the top plot in Figure 1 a heat map? How can I plot this in R?;[education, open-source];23;
6749;1;2015-08-10T20:09:46.923;plotting graph in R: show 3 values in one graph;I have a function: m=a*n+2the range of a is 1 to 100 and the range of a is 1 to 5My question: is it possible to plot a figure with x-axis is n, and y-axis is a and also shows the value of m ?;[education, open-source];17;
6750;1;2015-08-11T02:15:24.130;How to explain decision tree algortihm in layman's terms?;I have a task at hand, where I have to explain decision tree algorithm to a person who has not much understanding of machine learning. I have been looking around, but find it difficult to explain the algorithm in layman's terms, so that a person will understand what is happening in the process.What is the best way to describe the algorithm, possibly using some very simple basic example, so that the whole process of the algorithm is broken down to simple steps for understanding?;[education, open-source];84;2
6751;1;2015-08-11T05:04:52.300;How word2vec can handle unseen / new words to bypass this for new classifications?;"In simple terms, if my classification is based on word2vec as features, what I am supposed to do, if a new word comes, which doesn't have a word2vec.I am trying to used word2vec or word vectors for classification based on entity.For example ,I have to classify the following words in a sentence as :"" Google gives information about Nigeria "" Here , I want to classify Nigeria as location.Suppose I am having good word2vec vectors for each of the word, based on some readings I came to know that, Recurrent Neural Network can be used for this. So, word2vec will capture most locations with a kind of similar word vectors.But my questions are:a.) Suppose a new location is there. lets say, Russia . So, do I need to assign a new word vector for this location ?b.) If my input for training does not have grammatical sense. For example,"" Google information Nigeria "" . Everything else Nigeria is associated with a non-location label. Will this condition work for find new location in non-grammatical sentences.Please help .";[education, open-source];25;
6752;2;2015-08-11T06:40:58.583;;"First of all let me tell you that I'm not a Spark expert; I've been using it quite a lot in the last few months, and I believe I now understand it, but I may be wrong.So, answering your questions:a.) they are equivalent, but not in the way you're seeing it; Spark will not optimize the graph if you are wondering, but the customMapper will still be executed twice in both cases; this is due to the fact that for spark, rdd1 and rdd2 are two completely different RDDs, and it will build the transformation graph bottom-up starting from the leafs; so Option 1 will translate to:rdd0 = rddIn.map(lambda x: customMapper(x)).filter(lambda x: x[0]==0).cache()rdd1 = rddIn.map(lambda x: customMapper(x)).filter(lambda x: x[0]==1).cache()As you said, customMapper is executed twice (moreover, also rddIn will be read twice, which means that if it comes from a database, it may be even slower).b.) there is a way, you just have to move cache() in the right place:mappedRdd = rddIn.map(lambda x: customMapper(x)).cache()rdd0 = mappedRdd.filter(lambda x: x[0]==0)rdd1 = mappedRdd.filter(lambda x: x[0]==1)By doing this, we are telling spark that it can store the partial results of mappedRdd; it will then use these partial results both for rdd1 and rdd2. From the spark point of view this is equivalent to:mappedRdd = rddIn.map(lambda x: customMapper(x)).saveAsObjectFile('..')# forget about everythingrdd0 = sc.objectFile('..').filter(lambda x: x[0]==0)rdd1 = sc.objectFile('..').filter(lambda x: x[0]==1)";;;
6753;1;2015-08-11T07:25:23.167;Building a PST in R;"I am trying to build a pst of a csv file in R, but facing with an error:""Error in names(StCol) <- A ""After reading the file:try1 <- read.csv(""C:/Users/.../Desktop/sonuclar/2merCsv/Arteanuine B_2mer.csv"") data(try1) Warning message: In data(try1) : data set ‘try1’ not foundThis is the first error before using pstree function. How can I handle with them ?The code I m trying to run is:try1 <- read.csv(""C:/Users/.../Desktop/sonuclar/2merCsv/Arteanuine B_2mer.csv"") data(try1) try1.seq <- seqdef(try1) try1.seq Try1 <- pstree(tyr1.seq, L = 3) print(Try1, digits = 3) Try1";[education, open-source];20;
6754;2;2015-08-11T09:20:53.177;;"A decision tree is a graph that uses a branching method to illustrate every possible outcome of a decision. Here each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represents classification rules. This can be represented as graph. Now you can parse through the possible nodes and edges as per the condition rules (if condition1 and condition2 and condition3 then outcome.)Wikipedia link itself has some great examples :https://en.wikipedia.org/wiki/Decision_treeDo check these links also : http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/http://whatis.techtarget.com/definition/decision-tree";;;
6755;2;2015-08-11T09:25:24.340;;"I think you can better go through various university thesis reports and data science related journal papers to get more details on ""Including hypothesis, testing, reports, conclusions"" of the above mentioned Data science related problems.Check these links from Stanford university :http://cs229.stanford.edu/projects2014.htmlhttp://cs229.stanford.edu/projects2013.htmlhttp://cs229.stanford.edu/projects2012.html";;;
6756;2;2015-08-11T10:19:02.817;;Maybe there's another way to go. The idea would be to generate the dataset you will be processing with your algorithms. You define the models of behaviour of events (those you're looking for and those into which they are hiden). Then generate the data, then analyse.This approach has the benefit to let you control exactly what is inside the processed data. And make sure your algorithm identifies exactly what it is supposed to identify, no more, no less.With GEDIS Studio we model events behaviours with activity profiles and the generator produces those events. We have implemented generators for telecom CDR, credit card usages, smart metering, etc.They are freely available online from the evaluation account on http://www.data-generator.comCheck the detailed CDR use case at http://www.gedis-studio.com/online-call-detail-records-cdr-generator.htmlRegards ;;;
6757;2;2015-08-11T11:01:44.517;;Is it possible that adding generated data to your data set will decrease the fraud/non fraud ration and make your dataset more representative / usable ?At GenieLog, we are producing test data for designing and testing fraud detection tools. We our generator GEDIS Studio we can define regular profiles and fraudster profiles, instantiating each category to a customizable ratio (for ex. 2 % of customer will have fraudulent usage of generated events.)We did it successfully for telecom CDR (http://www.gedis-studio.com/online-call-detail-records-cdr-generator.html) and Credit Card usages. There's a freely available access to the online generator on http://www.data-generator.com I'm pretty sure that even if the tool is not matching your needs at least the approach can be valuable. Otherwise I would be interested to read any objection :)Regards;;;
6758;2;2015-08-11T12:14:32.297;; Suppose a new location is there. lets say, Russia . So, do I need to  assign a new word vector for this location ?Define an Unknown Word Vector that is going to represent every word that is not in your list.;;;
6759;2;2015-08-11T12:22:01.997;;You are looking for Recurrent Neural Networks for character-level language models. Have a look at this. https://github.com/karpathy/char-rnn;;;
6760;1;2015-08-11T09:26:26.340;How to discard trash topics from topic models?;I am undertaking text analysis of some twitter data. In the end I want to have a data that is interpretable. And so in the end I would like to reduce the data to relevant unit of analysis. Topic models seem a nice fit since they reduce the noise significantly, but still at the end there are quite a lot of trash topic lists which do not mean anything.What would be the best way to remove these topics. Is there a working example of this? I am using LDA. Probably I will use Gensim. Its going to be unsupervised and its a simple twitter data. So the whole corpora will be 1500 tweets of 160 characters. ;[education, open-source];35;
6761;2;2015-08-11T12:27:50.903;;David has a good point , I would suggest you focus on whatever it is that drives your interest more. It's the only way to succeed in every kind of effort. If you want to build something cool start with it. If you want to read a book thats good too. The starting point doesn't matter. A few days ahead you will have a better understanding on what you want and should do next.;;;
6764;2;2015-08-11T17:10:07.043;;"I had to do something similar for Regression Tree. I came up with as very simple complete example; I wrote out tables of dependent and independent variables on the board. The trickiest part is to explain how the first split happens, (that it minimizes MSE of dependent variable within the sub-samples), being able to point to the table helps. And after that we do the same thing just for a sub problem.";;;
6765;1;2015-08-11T19:28:29.793;Advise on making predictions given collection of dimensions and corresponding probabilities;I am a CS graduate but am very new to data science. I could use some expert advise/insight on a problem I am trying to solve. I've been through the titanic tutorial on gaggle.com which I think was helpful but my problem is a bit different.I am trying to predict diabetes risk based upon Age, Sex...and other factors given this data: http://www.healthindicators.gov/Indicators/Diabetes-new-cases-per-1000_555/Profile/ClassicDataThe data gives new cases people per 1,000 people for each dimension (Age, Sex...etc). What I would like to do is devise a way to predict, given a list of dimensions (Age, Sex...etc) a probability factor for a new diagnosis.So far my strategy is to load this data into R and use some package to create a decision tree, similar to what I saw in the titanic example on kaggle.com, then feed in a dimension list. However, I am a bit overwhelmed. Any direction on what I should be studying, packages/methods/examples would be helpful.;[education, open-source];56;
6766;1;2015-08-11T20:39:08.153;Machine learning algorithm to predict product service requests;"I've attached a picture of a sample of my data set, below.  I work for an appliance repair company, and I am looking to write a machine learning algorithm that utilizes text classification to identify ""Servicer Action"" and ""Servicer Part"" based on the customer comment and customer issue that are inputted by our folks who work the phones at my call center.I'm looking to use R to write the program, and I have read a few overviews of text classifications.  My biggest hurdle is the syntax, and how to have my code go through each individual comment, and train it based on the corresponding servicer action and servicer part.  That's why I came here to see if anyone can help me get started.  I know that I want to set up a document-term matrix that contains the ""Customer Comment"" and the ""Customer Issue"".I can then create a container to feed the machine learning algorithms?  So, take about 75% of the comments as the training set and then the remaining 25% as the test set?Then I can pass the container into the individual training models and classification models.  I'm just not sure how the syntax should work!Looking forward to starting the discussion! ";[education, open-source];33;
6767;2;2015-08-11T21:37:41.603;;Aggregate DataSince you're only given aggregate data, and not individual examples, machine learning techniques like decision trees won't really help you much. Those algorithms gain a lot of traction by looking at correlations within a single example. For instance, the increase in risk from being both obese and over 40 might be much higher than the sum of the individual risks of being obese or over 40 (i.e. the effect is greater than the sum of its parts). Aggregate data loses this information.The Bayesian ApproachOn the bright side, though, using aggregate data like this is fairly straightforward, but requires some probability theory. If $D$ is whether the person has diabetes and $F_1,\ldots,F_n$ are the factors from that link you provided, and if I'm doing my math correctly, we can use the formula:$$ \text{Prob}(D\ |\ F_1,\ldots,F_n) \propto \frac{\prod_{k=1}^n \text{Prob}(D\ |\ F_k)}{\text{Prob}(D)^{n-1}} $$(The proof for this is an extension of the one found here). This assumes that the factors $F_1,\ldots,F_n$ are conditionally independent given $D$, though that's usually reasonable. To calculate the probabilities, compute the outputs for $D=\text{Diabetes}$ and $\neg D=\text{No diabetes}$ and divide them both by their sum so that they add to 1.ExampleSuppose we had a married, 48-year-old male. Looking at the 2010-2012 data, 0.73% of all people get diabetes ($\text{Prob}(D) = 0.73\%$), 0.77% of married people get diabetes ($\text{Prob}(D\ |\ F_1)$$= 0.77\%$), 1.02% of people age 45-54 get diabetes ($\text{Prob}(D\ |\ F_2) = 1.02\%$), and 0.70% of males get diabetes ($\text{Prob}(D\ |\ F_3) = 0.70\%$). This gives us the unnormalized probabilities:$$ \begin{align*} P(D\ |\ F_1,F_2,F_3) &= \frac{(0.77\%)(1.02\%)(0.70\%)}{(0.73\%)^2} &= 0.0103 \\P(\neg D\ |\ F_1,F_2,F_3) &= \frac{(99.23\%)(98.98\%)(99.30\%)}{(99.27\%)^2} &= 0.9897 \end{align*}$$After normalizing these to add to one (which they already do in this case), we get a 1.03% chance of this person getting diabetes, and a 98.97% chance for them not getting diabetes.;;;
6768;1;2015-08-11T22:21:42.453;sklearn - overfitting problem;I'm looking for recommendations as to the best way forward for my current machine learning problemThe outline of the problem and what I've done is as follows:I have 900+ trials of EEG data, where each trial is 1 second long. The ground truth is known for each and classifies state 0 and state 1 (40-60% split)Each trial goes through preprocessing where I filter and extract power of certain frequency bands, and these make up a set of features (feature matrix: 913x32)Then I use sklearn to train the model. cross_validation is used where I use a test size of 0.2. Classifier is set to SVC with rbf kernel, C = 1, gamma = 1 (I've tried a number of different values)You can find a shortened version of the code here: http://pastebin.com/Xu13ciL4My issues:When I use the classifier to predict labels for my test set, every prediction is 0train accuracy is 1, while test set accuracy is around 0.56my learning curve plot looks like this:Now, this seems like a classic case of overfitting here. However, overfitting here is unlikely to be caused by a disproportionate number of features to samples (32 features, 900 samples). I've tried a number of things to alleviate this problem:I've tried using dimensionality reduction (PCA) in case it is because I have too many features for the number of samples, but accuracy scores and learning curve plot looks the same as above. Unless I set the number of components to below 10, at which point train accuracy begins to drop, but is this not somewhat expected given you're beginning to lose information?I have tried normalizing and standardizing the data. Standardizing (SD = 1) does nothing to change train or accuracy scores. Normalizing (0-1) drops my training accuracy to 0.6.I've tried a variety of C and gamma settings for SVC, but they don't change either scoreTried using other estimators like GaussianNB, even ensemble methods like adaboost. No changeTried explcitly setting a regularization method using linearSVC but didn't improve the situationI tried running the same features through a neural net using theano and my train accuracy is around 0.6, test is around 0.5I'm happy to keep thinking about the problem but at this point I'm looking for a nudge in the right direction. Where might my problem be and what could I do to solve it?It's entirely possible that my set of features just don't distinguish between the 2 categories, but I'd like to try some other options before jumping to this conclusion. Furthermore, if my features don't distinguish then that would explain the low test set scores, but how do you get a perfect training set score in that case? Is that possible?;[education, open-source];62;1
6769;2;2015-08-11T22:34:51.927;;R also has an htmlwidgets package that incorporated D3 javascript visualizations at: http://www.htmlwidgets.org/ ;;;
6770;2;2015-08-12T03:54:32.253;;I learn well by examples, so perhaps this will be helpful:Lets say you are a resort considering your next advertizing campaign. You will be sending out 1,000 vouchers that give two free days at the resort, with the hope that the customer will decide to extend it to a full week (who want's a two day vacation?!). You have a rather extensive database of customer data, including where they are from, income range, duration of stay(s) and money spent at each stay, number of children, marital status, and employment status (working, unemployed, retired). You would like to create a decision tree to help select the customers to send the vouchers to (specifically, those who will spend at least $200 per day and stay at least one week.To create the tree, a computer will perform the following steps:From the above list of customer data, find the property that best separates the customers into two groups.Repeat step one for each of the above groups using the remaining properties.In the end, you will have a tree where at each point, you can make one of two decisions. Following a path leads to a decision. The split points will be chosen to maximize the probability of a correct classification.;;;
6771;2;2015-08-12T09:56:15.270;;What I am ending up using is a sort of hybrid solution:backup of the raw datagit of the workflowmanual snapshots of workflow + processed data, that are of relevance, e.g.:standard preprocessingreally time-consumingfor publicationI believe it is seldom sensible to have a full revision history of large amount of binary data, because the time required to review the changes will eventually be so overwhelming that it will not pay off in the long run.Maybe a semi-automatic snapshot procedure (eventually to save some disk-space, by not replicating the unchanged data across different snapshots).;;;
6773;1;2015-08-12T14:01:23.257;How to count observations per ID in R?;I have a large amount of Data where I have to count meassurments per one ID. What I already did was creating a Data Frame over all Files and I omited the NAs. This part works properly. I was wondering if the nrow-function is the right function to solve this but I figured out that this will not lead me to the target as it returns a single number as output. What I am looking for is if you have entries like that:1155 2010-05-02  2.7200    11156 2010-05-05  2.6000    31157 2010-05-08  2.6700    11158 2010-05-11  3.5700    2That I get a list:ID          Number of observations1           22           13           1;[education, open-source];35;
6774;1;2015-08-12T14:16:42.160;emission probability using hmmlearn package in python;I am learning hmm and try to implement it in  Python hmmlearn package(http://hmmlearn.github.io/hmmlearn/hmm.html#building-hmm-and-generating-samples). However I am not quite understand what the documentation says:Classes in this module include MultinomialHMM, GaussianHMM, and GMMHMM. They implement HMM with emission probabilities determined by multimomial distributions, Gaussian distributions and mixtures of Gaussian distributions.Does this means when we try to estimate the model given an observation sequence, the  emission probability can only be these three kind? what if I want to type in the emission probability by meself?;[education, open-source];11;
6775;1;2015-08-12T15:11:36.953;algorithmic difference between image analysis and video analysis;"Is there algorithmic difference between analyzing video and an image, say for example,if I want object recognition? Or do I just have to analyze every frame of the the video just as an image?Example, detecting an object in a single image is easy compared to video becuase time dimension is added to video. In addition, on video, in every frame, the object is most probably moving, which makes the frames in motion... So how can u handle the time factor and the ""in motion"" part in the video. Those are the problem I imagine in video, it would be nice if u add ur own thoughts on it. Thanks";[education, open-source];27;
6776;1;2015-08-12T16:17:14.967;Any idea about application of deep dream?;Recently Google publicized interesting deep dream. Besides art generation such as http://deepdreamgenerator.com/, do you see any potential applications of deep dream in computer vision or machine learning?;[education, open-source];80;
6777;2;2015-08-12T19:58:32.517;;OK if I understood correctly you can do something like: df$observations <- rep(1, nrow(df))df <- df[ ,-file_name_column]new_data <- data.frame(aggregate(df, by= ID, FUN=sum))Caution: this might not work exactly since I am not sure what you data frame looks like. ;;;
6778;1;2015-08-12T23:14:17.480;Python library to compute some metrics for multioutput-multiclass classification task;Is there any Python library that provides ready-to-use metrics to analyze the performance of a classifier for a multioutput-multiclass classification task? scikit-learn doesn't have this option yet (as stated in the documentation and in the corresponding feature request on GitHub).;[education, open-source];7;
6781;2;2015-08-13T04:21:31.877;;aggregate() should work, as the previous answer suggests. Another option is with the plyr package:count(yourDF,c('id'))Using more columns in the vector with 'id' will subdivide the count. I believe ddply() (also part of plyr) has a summarize argument which can also do this, similar to aggregate().;;;
6782;2;2015-08-13T07:31:42.373;;To see if SVM can capture any signal at all, try to balance your data: create training and test sets that consist of exactly 50% positive and 50% negative samples (i.e., by subsampling randomly from whichever one is bigger). Also standardize the data (subtract the mean and divide by standard deviation).(For balancing, you might try changing the class_weight parameter in sklearn, but we found the manual method (subsampling) to work better.);;;
6783;1;2015-08-13T07:37:46.547;Multiclass Classification that includes a Geospatial Element;I am attempting to train a classifier to predict different prices for an item in different suburbs. I have several features, two of which are a latitude and longitude for the centroid of the suburb.I am attempting to train the model to classify the price of an item in a bin of $10 size. The geospatial element will definitely affect the price of the item, however the training data I have will have gaps in it (i.e. I don't have prices for all suburbs).What is the best way to engineer a feature that will include this geospatial information and be able to fill in the gaps in the training/test data?So far I have tried creating new features for the bearing and distance from the capital city which seemed to work okay, as well as binning the latitude and longitude which performs worse than the bearing/distance. I did consider using a geohash, however I think that this will be too complex a feature for a classifier to understand.;[education, open-source];11;1
6784;2;2015-08-13T07:45:45.927;;IPython has now moved to version 4.0, which means that if you are using it, it will be reading its configuration from ~/.jupyter, not ~/.ipython. You have to create a new configuration file withjupyter notebook --generate-configand then edit the resulting ~/.jupyter/jupyter_notebook_config.py file according to your needs.More installation instructions here.;;;
6785;1;2015-08-13T13:04:36.083;Deploying R on Work Server;We are looking to install R on our UNIX server at work, where we currently have SAS up and running. I know that R uses RAM to run, so are there any concerns with having R installed on an enterprise level on a UNIX server? Will it kill performance with multiple users running several instances at the same time. Is there an ideal way to deploy R in an environment like this so that there are performance issues for all other users using the server?Does RStudio server bypass these issues?;[education, open-source];3;
6786;1;2015-08-13T13:52:11.463;Weighted k nearest neighbor search;I've searched quite a bit and haven't landed on any useful results.The problem statement is:Given a set of vectors, I wish to find its approximate k-nearest neighbors.The caveat here is that each of my dimensions resemble a different entity and hence we cannot use the same weight for each dimension while computing the distance.Thus, solutions like kd-tree don't work as is.Is there any data-structure or any alternate algorithm that I can use to find such approximate weighted k-nearest neighbors.Note: Multiplying the initial input data with their weights so as to get a uniform weight is not an option.;[education, open-source];32;1
6787;1;2015-08-13T13:59:52.603;Is decision tree algorithm a linear or nonlinear algorithm?;Recently a friend of mine was asked whether  decision tree algorithm a linear or nonlinear algorithm in an interview. I tried to look for answers to this question but couldn't find any satisfactory explanation. Can anyone answer and explain the solution to this question? Also what are some other examples of nonlinear machine learning algorithms?;[education, open-source];45;
6788;2;2015-08-13T15:41:05.987;;It is impossible to prove a negative, but other than using the same pattern detection system in general to detect shapes/images and replace them with other similar images, possibly for use in automatic image correction or similar, I don't think it has real potential outside of modifying pictures.I may have to delete this answer if it is proven wrong.;;;
6789;1;2015-08-14T07:19:18.463;Neural network for MNIST: very low accuracy;"I am working on solving the handwritten digit recognition problem by implementing a neural network. But the accuracy of the network is coming out to be very low, around 11% for the train dataset. I am not sure what is wrong with my program. I tried changing the learning rate and the number of hidden units, but no luck. Could anyone please take a look and help me out with what I am missing? I am pasting my Julia code below:# installPkg.add(""MNIST"");using MNIST# training dataX,y = traindata(); m = size(X, 2);inputLayerSize = size(X,1); hiddenLayerSize = 300;outputLayerSize = 10;# representing each output as an array of size of the output layereyeY = eye(outputLayerSize);intY = [convert(Int64,i)+1 for i in y];Y = zeros(outputLayerSize, m);for i = 1:m    Y[:,i] = eyeY[:,intY[i],];end# weights with biasTheta1 = randn(inputLayerSize+1, hiddenLayerSize); Theta2 = randn(hiddenLayerSize+1, outputLayerSize); function sigmoid(z)    g = 1.0 ./ (1.0 + exp(-z));    return g;endfunction sigmoidGradient(z)  return sigmoid(z).*(1-sigmoid(z));end# learning ratealpha = 0.01;# number of iterationsepoch = 20;# cost per epochJ = zeros(epoch,1);# backpropagation algorithmfor i = 1:epoch    for j = 1:m # for each input        # Feedforward        # input layer        # add one bias element        x1 = [1, X[:,j]];        # hidden layer        z2 = Theta1'*x1;        x2 = sigmoid(z2);        # add one bias element        x2 = [1, x2];        # output layer        z3 = Theta2'*x2;        x3 = sigmoid(z3);        # Backpropagation process        # delta for output layer        delta3 = x3 - Y[:,j];        delta2 = (Theta2[2:end,:]*delta3).*sigmoidGradient(z2) ;        # update weights        Theta1 = Theta1 - alpha* x1*delta2';        Theta2 = Theta2 - alpha* x2*delta3';    endendfunction predict(Theta1, Theta2, X)    m = size(X, 2);     p = zeros(m, 1);    h1 = sigmoid(Theta1'*[ones(1,size(X,2)), X]);    h2 = sigmoid(Theta2'*[ones(1,size(h1,2)), h1]);    # 1 index is for 0, 2 for 1 ...so forth    for i=1:m        p[i,:] = indmax(h2[:,i])-1;    end    return p;endfunction accuracy(truth, prediction)    m = length(truth);    sum =0;    for i=1:m        if truth[i,:] == pred[i,:]            sum = sum +1;        end    end  return (sum/m)*100;endpred = predict(Theta1, Theta2, X);println(""train accuracy: "", accuracy(y, pred));";[education, open-source];38;
6790;2;2015-08-14T12:13:05.467;;A decision tree is a non-linear mapping of X to y. This is easy to see if you take an arbitrary function and create a tree to its maximum depth.For example:if x = 1, y = 1if x = 2, y = 15if x = 3, y = 3if x = 4, y = 27...Of course, this is a completely over-fit tree and won't generalize. But it demonstrates why a decision tree is a non-linear mapping.;;;
6791;1;2015-08-14T16:12:54.317;Amplifying the impact of outliers in a dataset;I have a conundrum.  I currently have 8 datasets that are forced ranked and then weighted to create an overall index.  The interesting thing about these datasets is that many of them have a high sigma and are highly right skewed when looking at their distribution.  So minimal change in the data results in a large rank change.Is there a method to use the distribution of the data, specifically the outliers, and amplify the ranking based on the distance the value is from the mean? ;[education, open-source];9;
6792;1;2015-08-14T16:43:19.007;Privacy through fake data?;With companies and governments hungry for all the data about people, I was wondering if it was possible to gain some privacy by drowning relevant information in a sea of random data. For example a browser extension which keeps searching for random words and expressions in the background. Maybe sending generated emails to generated addresses. Or producing made up location data on my phone.Would these measures help at all or the data miner AIs would just see right through them?;[education, open-source];21;
6794;1;2015-08-14T19:36:11.590;What is the standard for dealing with null values in graphing?;"There's really no ""standard,"" I assume, but how do you deal with null values when graphing data? An example of what I'm talking about is listed below:Suppose I'm graphing data on number of toys by color. Nearly half of these toys in my database have null values for color. Do you guys include the toys with null values in your visualization? How do you address that issue? Saying ""nearly half of the toys didn't have colors listed"" doesn't really add much to the conversation.";[education, open-source];14;
6795;2;2015-08-14T21:12:39.333;;"Imo, It depends on two major factors:What is the purpose of the graphDoes the null value indicate incompleteness (one of the values, but unknown which one), or does it indicate a last final option, (like: multiple colors)Imagine if the purpose of the graph is to compare which toy colors are most popular to a particular person/group.If null indicates multi-colored toys, it may be important to include the data but rename it to something more meaningful. (Multi-colored)If null indicates that we just don't know the color data for those toys, it would probably be best to exclude that data from the graph. However, with such a large percentage, it would be good to note that ""x%"" were excluded from your graph because they had no color information associated with them. Perhaps purple is in such a majority because this person/group really likes purple, or maybe its because purple toys almost never made it into the null category for whatever reason.The same line of thinking should be applied to whatever the purpose of the graph is. If the null values are relevant to the purpose, include them. If not, don't - and maybe take a note of how much data is excluded if it could influence the results substantially (such as this case)";;;
6796;2;2015-08-14T22:44:27.743;;"This was the idea behind paranoid linux which began as fiction and became a real project that did not reach fruition:""Paranoid Linux is an operating system that assumes that its operator is under assault from the government (it was intended for use by Chinese and Syrian dissidents), and it does everything it can to keep your communications and documents a secret. It even throws up a bunch of ""chaff"" communications that are supposed to disguise the fact that you're doing anything covert. So while you're receiving a political message one character at a time, ParanoidLinux is pretending to surf the Web and fill in questionnaires and flirt in chat-rooms. Meanwhile, one in every five hundred characters you receive is your real message, a needle buried in a huge haystack.~Cory Doctorow (Little Brother, 2008)When those words were written, ParanoidLinux was just a fiction. It is our goal to make this a reality. The project officially started on May 14th, and has been growing ever since. We welcome your ideas, contributions, designs, or code. You can find us on freenode's irc server in the #paranoidlinux channel. Hope to see you there!""I think the most modern equivalent is Pirate Linux along with the TOR Project.BTW, Cory Doctorow's book ""Little Brother"" is kind of a fun read if you are a geek.  If you are reading this then you probably ARE a geek :-)";;;
6797;1;2015-08-14T23:12:44.480;Tune hyperparameters for cost-sensitive classification;I have an unbalanced data set with about 8% of negative examples. The goal is to minimize false negatives given a cost matrix. It seems like SVM (with radial kernel) and random forest work best. How should I tune hyperparameters in this setting? My suggestion: separate data into train/validation set, use probabilistic output together with cost matrix to assign predicted classes, tune hyperparameters to maximize accuracy.How can I increase performance? Currently I use random forest with nodesize=1 and mtry=5 which gives about 97% accuracy.;[education, open-source];10;
6798;1;2015-08-15T01:08:48.490;List of NLP challenges;"Is there any comprehensive list of past, current and future NLP challenges?E.g. for NLP conferences, Joel Tetreault's unofficially official conference calendar and WikiCFP are pretty good.The ""Competitions and Challenges"" page on the ACL wiki quite incomplete.";[education, open-source];24;
6799;2;2015-08-15T10:14:33.270;;There are some C++ tools for statistics and data science like ROOT https://root.cern.ch/drupal/ , BAT https://www.mppmu.mpg.de/bat/ , boost  , or OpenCV ;;;
6800;2;2015-08-15T18:52:20.147;;I agree that the current trend is to use Python/R and to bind it to some C/C++ extensions for computationally expensive tasks.However, if you want to stay in C/C++, you might want to have a look at Dlib: Dlib is a general purpose cross-platform C++ library designed using contract programming and modern C++ techniques. It is open source software and licensed under the Boost Software License. ;;;
6801;2;2015-08-15T18:52:22.243;;There are methods that approximate Jaccard similarity using hash functions, one of them MinHash. They scale well with the number of dimensions and  evaluate similarity of two items. If you want to do nearest neighbor queries (finding similar items given one item) you should look at locality sensitive hashing (LSH), which essentially maps similar items to the same hash value. There is a nice implementation in scikit-learn called LSHForest. It approximates cosine distance and scales well to large number of items and dimensions.;;;
6802;2;2015-08-15T19:12:31.760;;Take the cluster centers and project them back to the original feature vector using the inverse transform of your PCA. That gives you an intuition of what the clusters represent. As a second step you can take all original feature vectors that were assigned to one cluster and calculate histograms on each feature dimension. This will give you a feeling of in-cluster variation and cluster boundaries along the original feature dimensions.;;;
6803;2;2015-08-15T20:46:37.993;;Especially when your main goal is learning, I would break it into several phases:Get familiar with pandas dataframes and visualization using matplotlib. Try loading subsets of your datasets using pandas and visualize them using matplotlib, for example plot the time-series, or word count histograms. This will come in handy lateron to be able to understand the predictions/clusters you will create using machine learning methods. Pandas also provides functions to clean, aggregate and resample the data, which will be useful to align multiple time-series (e.g. weather and time-series) to the same sampling rate.Familiarize with scikit-learn using the articles and examples in the excellent online documentation. Play with the feature extraction, dimensionality reduction, classification and regression methods relevant to your data, apply them on smaller subsets and visualize the results. Learn about cross-validation and find a good algorithm and parameters that work well on a subset of your data.One you have a pipeline setup and you want to run it on the full dataset, Spark can come in handy for feature extraction and cross-validation, as those tasks can be often run very well independently and in parallel. Install the Spark libraries on your machine, and reproduce the Pyspark examples (e.g. calculating Pi) in local mode. No need to setup a Hadoop cluster for that, Spark can make full use of your machine in local mode, reading from local files on disk. Once you have that running, try to express the expensive steps of your code from step 2 as Spark RDD operations. As Spark can make use of all cores of your machine, you should already see a speedup - debugging becomes much harder though compared to working with pure Python scripts.Play with advanced stuff like deep learning, using the word2vec feature extraction. Compare with traditional feature extraction (bag of words).(optional) If you want to experience the speedup you can get from Spark in a distributed setting, get an account in AWS, put your data on S3 and fire your Spark scripts against a multi-node cluster instantiated using Amazon EMR. Regarding data storage, from my experience CSV files or similar work best to load the data into both Pandas and Spark. There are Spark connectors for databases (e.g. Cassandra), but I am not sure about MySQL and Solr. For processed data and intermediate results (e.g. models) I would always use Python's serialization framework pickle, so you can directly get the object instances back into Python without the need to parse file formats and reinstantiate objects.;;;
