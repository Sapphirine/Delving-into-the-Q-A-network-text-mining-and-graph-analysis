<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="5" PostId="5" Score="9" Text="this is a super theoretical AI question. An interesting discussion! but out of place..." CreationDate="2014-05-14T00:23:15.437" UserId="34" />
  <row Id="6" PostId="7" Score="4" Text="List questions are usually not suited for Stack Exchange websites since there isn't an &quot;objective&quot; answer or a way to measure the usefulness of an answer. Having said that, one of my recommendations would be MacKay's &quot;Information Theory, Inference, and Learning Algorithms.&quot;" CreationDate="2014-05-14T00:38:19.510" UserId="51" />
  <row Id="9" PostId="7" Score="3" Text="This question appears to be off-topic because it is asks for a favorite resource.  On other SE sites, this would immediately be closed.  Since this is a new site, we still have to decide if this is a valid question here" CreationDate="2014-05-14T01:16:12.623" UserId="66" />
  <row Id="12" PostId="15" Score="3" Text="This question is far too broad. It may be salvaged by restricting the question to a particular use case." CreationDate="2014-05-14T02:00:22.797" UserId="51" />
  <row Id="13" PostId="10" Score="2" Text="Nice one, @Nicholas... Another book from Hastie and Tibshirani is [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), which is a bit gentler of an entry compared to ESL." CreationDate="2014-05-14T02:16:20.503" UserId="24" />
  <row Id="14" PostId="7" Score="0" Text="Fair enough regarding what constitutes a &quot;valid&quot; question, although on other SE sites this question would **not** be immediately closed as you've stated: e.g., [2495 votes](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books), [1440 votes](http://stackoverflow.com/questions/1711/what-is-the-single-most-influential-book-every-programmer-should-read), [168 votes](http://tex.stackexchange.com/questions/11/what-is-the-best-book-to-start-learning-latex), and so on. There's great interest for these kinds of questions, even if this isn't deemed the right place." CreationDate="2014-05-14T02:35:50.090" UserId="36" />
  <row Id="17" PostId="22" Score="3" Text="Yes, using 1-of-n encoding is valid too." CreationDate="2014-05-14T06:47:00.223" UserId="21" />
  <row Id="19" PostId="19" Score="5" Text="A nice article about when your data starts to be too big for normal usage chrisstucchio.com/blog/2013/hadoop_hatred.html" CreationDate="2014-05-14T07:48:10.370" UserId="115" />
  <row Id="20" PostId="14" Score="0" Text="As to the second part of your question, I have proposed a discussion in meta: http://meta.datascience.stackexchange.com/questions/5/should-we-adopt-a-self-study-tag-like-stats-se-in-scope-and-approach How that gets received could shape whether your proficiency concern is answerable or within scope." CreationDate="2014-05-14T08:00:35.420" UserId="53" />
  <row Id="21" PostId="31" Score="1" Text="What do you mean under &quot;interesting groups&quot;? Do you have some predefined important feature list?" CreationDate="2014-05-14T09:08:56.007" UserId="120" />
  <row Id="22" PostId="31" Score="0" Text="Interesting groups are any groups of size greater than some threshold an that are much bigger than other possible clusters." CreationDate="2014-05-14T09:11:09.143" UserId="118" />
  <row Id="24" PostId="31" Score="1" Text="It isn't clear how you will perform preparement steps of your data. But you should look at algorithms described at http://en.wikipedia.org/wiki/Anomaly_detection . If I were you, I've checked SVM method first" CreationDate="2014-05-14T09:31:01.977" UserId="120" />
  <row Id="26" PostId="37" Score="1" Text="+1 I pretty much appreciate the stress out on big data being not about *what is the size*, and rather about *what is the content (characteristics of)*." CreationDate="2014-05-14T10:49:41.527" UserId="84" />
  <row Id="29" PostId="51" Score="0" Text="Excellent! Anything similar on Windows? I suppose if I want to condition it on something like the size of the database I can have my R script check that at the beginning and either stop or continue generating the report. What if I add this to the beginning of the R script and then I want to run it every 15 minutes? Do I need 24x4 `crontab` commands? Or can I iterate it somehow?" CreationDate="2014-05-14T14:58:48.107" UserId="151" />
  <row Id="30" PostId="51" Score="1" Text="@rnorberg You can set a cron job to repeat in any periodic time pattern as long as the time period is constant, and greater than 1 minute. Read the man page for more details, or such google &quot;crontab&quot;. Lots of stuff available." CreationDate="2014-05-14T15:14:00.130" UserId="62" />
  <row Id="31" PostId="37" Score="1" Text="That is a very refreshing perspective. I have never heard this before, but it is very true. This suggests that SQL and NoSQL technologies are not competetive, but complementary." CreationDate="2014-05-14T15:16:37.650" UserId="157" />
  <row Id="32" PostId="46" Score="0" Text="Thanks for your answer. It looks like different from SVM. I'll survey it. :)" CreationDate="2014-05-14T15:32:16.760" UserId="63" />
  <row Id="35" PostId="31" Score="0" Text="I've read about SVM and I think its more about classification of newly created data after manual training on existing dataset - not about clustering existing data and finding abnormally big clusters. Am I right? If I am then this method isn't what I want." CreationDate="2014-05-14T16:15:01.610" UserId="118" />
  <row Id="37" PostId="46" Score="4" Text="Just a reminder that we don't encourage linking off-site to an answer because its easy for links to break, causing an otherwise useful community resource to instead turn into a dead end. It's always best to put the answer directly into your post." CreationDate="2014-05-14T17:30:36.967" UserId="41" />
  <row Id="38" PostId="41" Score="4" Text="In addition to the answers below a good thing to remember is the fact that most of the things you need from R regarding Big Data can be done with summary data sets that are very small in comparison to raw logs.  Sampling from the raw log also provides a seamless way to use R for analysis without the headache of parsing lines and lines of a raw log.  For example, for a common modelling task at work I routinely use map reduce to summarize 32 gbs of raw logs to 28mbs of user data for modelling." CreationDate="2014-05-14T17:45:41.430" UserId="92" />
  <row Id="40" PostId="24" Score="5" Text="If your scale your numeric features to the same range as the binarized categorical features then cosine similarity tends to yield very similar results to the Hamming approach above.  I don't have a robust way to validate that this works in all cases so when I have mixed cat and num data I always check the clustering on a sample with the simple cosine method I mentioned and the more complicated mix with Hamming.  If the difference is insignificant I prefer the simpler method." CreationDate="2014-05-14T17:53:54.897" UserId="92" />
  <row Id="41" PostId="57" Score="0" Text="hm - get data, format data(awk sed grep stuff), remove noise as first step, then go deeper. so is't *hard preprocessing* comes at first, if use your therms" CreationDate="2014-05-14T18:09:54.360" UserId="146" />
  <row Id="42" PostId="52" Score="1" Text="probably it's good idea to clear a bit, what you mean under *cleaning data* , looks a bit confusing for my opinion" CreationDate="2014-05-14T18:11:45.363" UserId="146" />
  <row Id="43" PostId="57" Score="0" Text="@MolbOrg Yes, that's what I meant. I called *hard preprocessing* the *scripting side*, and *soft preprocessing* the use of data mining algorithms that generally reduce the *size* of the problem (cleans up the database). I also noted that the *second part, hard preprocessing, actually comes prior to any other process*. If it's not very clear with such terms, I'd gladly consider any other suggestions to improve the answer." CreationDate="2014-05-14T18:20:28.510" UserId="84" />
  <row Id="44" PostId="57" Score="1" Text="ah yes, did not paid enough attention, *raw data preprocessing*. Tested atm - yes perl oneliner is 3times slower then grep ) for 3.5kk strings in 300MB, for perl it took 1.1 sec, for grep 0.31 sec . I saw article where points that perl regexp is slow, much slower then it may be in practice, (i suspect that is also for grep too) [http://swtch.com/~rsc/regexp/regexp1.html](http://swtch.com/~rsc/regexp/regexp1.html)" CreationDate="2014-05-14T18:34:15.603" UserId="146" />
  <row Id="46" PostId="57" Score="0" Text="@MolbOrg Nice reference! AFAIK, `grep` uses POSIX basic regex by default, and allows for extended POSIX regex when run as `grep -E`, and for PCRE when run as `grep -P`." CreationDate="2014-05-14T18:44:15.457" UserId="84" />
  <row Id="47" PostId="57" Score="0" Text="yes, I used both variants, default was 6 times faster(just two attempts because lost a window with results, and second pattern was not suitable for default grep), and in the link above states that grep uses different(rigth?) approach, did't mean it's new for me, but new angle on that stuff for sure, have to think about ))" CreationDate="2014-05-14T18:49:49.287" UserId="146" />
  <row Id="48" PostId="24" Score="1" Text="That sounds like a sensible approach, @cwharland.  On further consideration I also note that one of the advantages Huang gives for the k-modes approach over Ralambondrainy's -- that you don't have to introduce a separate feature for each value of your categorical variable -- really doesn't matter in the OP's case where he only has a single categorical variable with three values.  Better to go with the simplest approach that works." CreationDate="2014-05-14T19:54:45.137" UserId="14" />
  <row Id="49" PostId="69" Score="0" Text="Can you give an example of a workflow which is reproducible without being a replication?" CreationDate="2014-05-14T21:00:16.777" UserId="157" />
  <row Id="50" PostId="46" Score="1" Text="Agree with that. At this point, it barely exists as more than that link anyhow. I will add a link to the underlying project." CreationDate="2014-05-14T21:02:21.930" UserId="21" />
  <row Id="51" PostId="69" Score="0" Text="@JayGodse: Good question, first I'd suggest reading the update I posted to the question, which might make my use of the terms a bit clearer. To answer your question, **no reproduction of a workflow is a replication of the original workflow**; which is to say, I don't have an answer to your question, since it is unclear to me how to you execute a data science workflow that allows for reproducibility without simply being a replication (that is, an exact copy) of what has already been done?" CreationDate="2014-05-14T21:46:39.713" UserId="158" />
  <row Id="52" PostId="70" Score="0" Text="Right, I get that, so maybe my question is unclear. The intent of my question is how to build a workflow that allows for reproducibility. Meaning if replication requires a step-by-step guide for taking a given input and reaching a given output, how do you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?  Meaning you run the build, and it executes the code pulls the input creates the output AND generates an abstraction in text that would allow the build to be reproducible." CreationDate="2014-05-14T22:14:32.813" UserId="158" />
  <row Id="53" PostId="70" Score="0" Text="Think of it as a [reproducibility documentation generator](http://en.wikipedia.org/wiki/Comparison_of_documentation_generators)." CreationDate="2014-05-14T22:16:44.943" UserId="158" />
  <row Id="54" PostId="70" Score="0" Text="+1 So, I've thought about it, and unable to think of a way to make the question more clear without make your answer invalid, nor would it be fair for me to ask you delete your answer so I'm able to delete my question. So, just going to leave things be, and sorry my question was unclear. Cheers!" CreationDate="2014-05-14T22:32:23.857" UserId="158" />
  <row Id="55" PostId="71" Score="2" Text="Snarky answer: almost always. There's a huge incentive to create Type 1 errors (i.e., &quot;false alarms&quot;) when analysts examine data, so almost all p-values you'll encounter are &quot;too&quot; small." CreationDate="2014-05-14T23:07:08.427" UserId="36" />
  <row Id="56" PostId="71" Score="7" Text="Just throwing this out there, but wouldn't this sort of question best be posed on [Cross Validated](http://stats.stackexchange.com/)?" CreationDate="2014-05-14T23:47:53.803" UserId="24" />
  <row Id="57" PostId="58" Score="1" Text="Open source and some wiki. It looks good. Thanks for your suggestion. :)" CreationDate="2014-05-15T01:05:14.973" UserId="63" />
  <row Id="58" PostId="70" Score="0" Text="@blunders: please change the question.  I would rather have my answer be invalid and your question answered :)." CreationDate="2014-05-15T01:37:11.243" UserId="178" />
  <row Id="59" PostId="70" Score="0" Text="Done, thank you!" CreationDate="2014-05-15T02:02:21.317" UserId="158" />
  <row Id="60" PostId="76" Score="1" Text="About how much tweets a &quot;run&quot; are we talking?" CreationDate="2014-05-15T07:02:00.873" UserId="115" />
  <row Id="61" PostId="71" Score="1" Text="@buruzaemon: Maybe.  I did a search, this is the closest match: http://stats.stackexchange.com/questions/67320/hypothesis-testing-with-big-data  There don't seem to be more than a handful of questions that touch on this." CreationDate="2014-05-15T08:55:18.250" UserId="26" />
  <row Id="64" PostId="78" Score="0" Text="thank you for posting a link to that paper" CreationDate="2014-05-15T11:01:32.950" UserId="59" />
  <row Id="65" PostId="76" Score="0" Text="It's hard to know without a range of tweets, 1000, 100,000 full datahose etc." CreationDate="2014-05-15T13:01:58.240" UserId="59" />
  <row Id="66" PostId="76" Score="1" Text="@Johnny000: [500 million Tweets a day](https://blog.twitter.com/2013/new-tweets-per-second-record-and-how); my understanding is that Twitter limits streams to vendors based on trust/need, but to insure the solution covers the current daily averages, the solution should account what is the max, or you're able to reference as the max via a reliable source other than yourself." CreationDate="2014-05-15T13:24:51.407" UserId="158" />
  <row Id="68" PostId="85" Score="0" Text="I would add that FDR and FER are used when you have many hypotheses tested simultaneously." CreationDate="2014-05-15T14:19:14.513" UserId="178" />
  <row Id="69" PostId="93" Score="0" Text="The context would be info trackable within a single site with a 3rd party cookie, via an iframe. The site would be ecommerce. I find google analytics mostly looks at IP, sometimes at useragent, and I am able to get very similar numbers from looking only at IP in a timeframe. But google analytics is known to over-report by 30% ish, depending on context" CreationDate="2014-05-15T14:57:50.803" UserId="116" />
  <row Id="70" PostId="93" Score="0" Text="Looking at visited product pages doesn't help much either, as the structure of the shop is such that it leads users down predetermined paths, leading to very similar behaviour" CreationDate="2014-05-15T14:59:59.857" UserId="116" />
  <row Id="71" PostId="93" Score="1" Text="Also, I am aware that ML does not fit in the context of this question. Rather, hard coded algorithms are used by most tracking solutions that offer sensible results. The last few degrees of accuracy, that would be achievable with ML are of less relevance, since this info is rather used for observing trends." CreationDate="2014-05-15T15:03:21.477" UserId="116" />
  <row Id="78" PostId="76" Score="1" Text="Here's more information, appear the &quot;firehose&quot; data feed is pricy, so guessing it would be more relevant to limit input to the volume produce via the [Twitter Streaming APIs](https://dev.twitter.com/docs/api/streaming) via the Public Stream; [guide to processing the data is here](https://dev.twitter.com/docs/streaming-apis/processing#Scaling)." CreationDate="2014-05-15T20:48:23.770" UserId="158" />
  <row Id="79" PostId="76" Score="0" Text="Public Stream is a random sample of the &quot;firehose&quot; and appears to 1% of its total volume; meaning I estimate the feed to be 5 million tweets per day, and spikes might reach 1432 tweets per second; appears the spike must be account for, otherwise the feed gets discounted." CreationDate="2014-05-15T20:49:10.247" UserId="158" />
  <row Id="80" PostId="7" Score="1" Text="@statsRus: Try posting a question like that to SO, and it'll be closed; these questions exists because they have historical significance, but they are not considered good, on-topic questions for Stack Exchange sites, so **please do not use them as evidence that you can ask similar questions here.**" CreationDate="2014-05-15T21:08:13.933" UserId="158" />
  <row Id="82" PostId="91" Score="0" Text="I edited the question to add another query." CreationDate="2014-05-16T02:48:03.413" UserId="189" />
  <row Id="83" PostId="92" Score="0" Text="First, I edited the question to add another query. Also: I imagine even with a significant minority pre-computed, the rest of the query should still take long time to complete. Besides, when the process is delegated from one machine to 100 machines, isn't the latency actually increased (network latency between machines, and total latency is maximum of the latencies of all machines)?" CreationDate="2014-05-16T02:52:12.253" UserId="189" />
  <row Id="84" PostId="91" Score="0" Text="@namehere I tried to address your edit; hope it helps answering the question." CreationDate="2014-05-16T04:29:25.523" UserId="84" />
  <row Id="85" PostId="101" Score="0" Text="A very good answer! For those reading, I'd like to add that in the case of 3rd party cookies, many safari mobile versions will not take those by default, and other browsers have the same in their pipelines. Keep those in mind and treat them separately." CreationDate="2014-05-16T05:10:18.347" UserId="116" />
  <row Id="86" PostId="52" Score="1" Text="Explaining further what cleaning data means would be helpful. In the context where I work, cleaning has nothing to do with formatting - I'd just call that parsing/importing - But rather it would mean talking noisy user data and verifying it for coherence. The techniques use are dataset specific, from simple statistical rules, to fuzzy algorithms, especially when the data is sparse." CreationDate="2014-05-16T05:17:27.677" UserId="116" />
  <row Id="87" PostId="102" Score="1" Text="Alot of browsergames use a documentdatabase like Mongo DB or Couch DB" CreationDate="2014-05-16T06:15:20.240" UserId="115" />
  <row Id="88" PostId="102" Score="1" Text="As @Johnny000 mentioned, there are ones like MongoDB and CouchDB, that are widely used for that purpose.  I would add that you should consider developer time as well when making the decision." CreationDate="2014-05-16T12:35:21.483" UserId="178" />
  <row Id="89" PostId="92" Score="0" Text="I mean that answering the query &quot;spaghetti diamond&quot;, which is a weird rare query, might be sped up by precomputed results for &quot;spaghetti&quot; and &quot;diamond&quot; individually. Intra-DC connections are very fast and low latency. An extra hop or two inside is nothing compared to the ~20 hops between your computer and the DC. The dominating problem in distributing work is the straggler problem; you have to drop results from some subset if they don't respond in time. These are all gross generalizations but point in the right direction." CreationDate="2014-05-16T13:07:54.197" UserId="21" />
  <row Id="90" PostId="61" Score="1" Text="Is your question actually whether there is a case where it's impossible to overfit?" CreationDate="2014-05-16T13:09:01.880" UserId="21" />
  <row Id="91" PostId="61" Score="0" Text="@SeanOwen: No, how would it be impossible to overfit?" CreationDate="2014-05-16T13:13:46.987" UserId="158" />
  <row Id="92" PostId="61" Score="0" Text="Agree, just checking as you asked if overfitting caused models to become worse regardless of the data" CreationDate="2014-05-16T13:14:24.237" UserId="21" />
  <row Id="93" PostId="61" Score="0" Text="Just to be clear, to me, you asking if it's &quot;impossible to overfit&quot; and &quot;overfitting caused models to become worse regardless of the data&quot; are completely different in my opinion; that said, as far as I know, both are impossible." CreationDate="2014-05-16T13:49:13.207" UserId="158" />
  <row Id="94" PostId="101" Score="1" Text="Cookie churn is quite the problem for services that don't require log in. Many users simply don't understand cookies though so you are likely to have some cohort that you can follow for an appreciable amount of time." CreationDate="2014-05-16T14:44:37.800" UserId="92" />
  <row Id="95" PostId="103" Score="2" Text="I wonder why you emphasized that you do **not** have a distance. I'm not an expert here, but wonder whether it should not be possible to convert such a similarity into a distance, if required, basically by considering its inverse. Regardless of that, I doubt that there are clustering algorithms that are completely free of parameters, so some tuning will most likely be necessary in all cases. When you considered k-Means, can one assume that you have real-valued properties (particularly, that you *can* take the &quot;mean&quot; of several elements)?" CreationDate="2014-05-16T16:28:18.287" UserId="156" />
  <row Id="96" PostId="103" Score="4" Text="You don't need to know k to perform k means. You can cluster with varying k and check cluster variance to find the optimal.  Alternatively you might think about going for Gaussian mixture models or other restaraunt process like things to help you cluster." CreationDate="2014-05-17T00:12:00.940" UserId="92" />
  <row Id="97" PostId="102" Score="1" Text="You may want to consider a graph database that will help you manage relationships much faster. My favorite is OrientDB, which we use to manage a social network." CreationDate="2014-05-17T03:51:36.990" UserId="70" />
  <row Id="98" PostId="20" Score="2" Text="For anything social networking, I would **highly** recommend a graph database like [Neo4j](http://neo4j.org) or [OrientDB](http://orientechnologies.com)" CreationDate="2014-05-17T04:21:42.693" UserId="70" />
  <row Id="99" PostId="112" Score="0" Text="Thanks for the suggestion. I've looked at neo4j earlier but never at orientdb. Currently I can't envision a lot of benefit in modelling leadeboard data as graph but I will still look at streaming options in orientdb" CreationDate="2014-05-17T08:15:16.490" UserId="200" />
  <row Id="100" PostId="113" Score="1" Text="This page http://www.mongodb.com/nosql-explained provides some details about it" CreationDate="2014-05-17T08:29:53.633" UserId="211" />
  <row Id="102" PostId="115" Score="2" Text="I doubt there's any general API for this. You can try crawling various services likes Academia.edu, publishers' sites and so on. Nevertheless, it would be easier to build a local database of documents first, and then experiment with extracting the abstracts." CreationDate="2014-05-17T08:55:28.927" UserId="173" />
  <row Id="104" PostId="103" Score="0" Text="@Marco13 good point. Sometimes it works like that, but not in all cases, making sure that you keep the properties (e.g. triangle inequality). Regarding the mean of several elements, I can't think how I can do that, or if it is possible. Good point, too!" CreationDate="2014-05-17T09:01:14.093" UserId="113" />
  <row Id="105" PostId="103" Score="0" Text="@cwharland Veru useful comment. Doesn't the initial choice of k, though influence the result? I will look at the Gaussian mixture models. Thanks!" CreationDate="2014-05-17T09:02:59.783" UserId="113" />
  <row Id="106" PostId="115" Score="0" Text="Thanks for your answer! I have already built a local database for this. The problem of crawling from various services is that I have to make parse rules for each website." CreationDate="2014-05-17T09:05:02.373" UserId="212" />
  <row Id="107" PostId="115" Score="0" Text="So, how about converting PDFs to TXTs and then extracting the abstracts with regular expressions?" CreationDate="2014-05-17T09:35:04.733" UserId="173" />
  <row Id="108" PostId="115" Score="0" Text="thx! However, the contract states that massive download of papers are not allowed. This creates some headache." CreationDate="2014-05-17T11:39:36.970" UserId="212" />
  <row Id="109" PostId="115" Score="2" Text="I think this stack-overflow answer [link](http://stackoverflow.com/questions/14530019/avoiding-google-scholar-block-for-crawling) gives the best answer I can get. Maybe people who encounter this problem could also have a look at this page." CreationDate="2014-05-17T11:55:48.903" UserId="212" />
  <row Id="111" PostId="103" Score="2" Text="I asked the questions for a specific reason: **If** you could apply k-Means, but the only problem was finding the initial &quot;k&quot;, then you could consider a http://en.wikipedia.org/wiki/Self-organizing_map as an alternative. It has some nice properties, and basically behaves &quot;similar&quot; to k-Means, but does not require the initial &quot;k&quot; to be set. It's probably not an out-of-the-box solution, because it has additional tuning parameters (and the training may be computationally expensive), but worth a look nevertheless." CreationDate="2014-05-17T16:07:09.590" UserId="156" />
  <row Id="112" PostId="103" Score="2" Text="The initial choice of k does influence the clustering results but you can define a loss function or more likely an accuracy function that tells you for each value of k that you use to cluster, the relative similarity of all the subjects in that cluster.  You choose the k that minimizes variance in that similarity.  GMM and other dirichlet processes take care of the not-knowing-k problem quite well.  One of the best resources I've ever seen on this is [Edwin Chen's tutorial](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/)." CreationDate="2014-05-17T16:30:06.077" UserId="92" />
  <row Id="113" PostId="116" Score="1" Text="When you say &quot;many more features than samples&quot; I assume you mean the unique number of liked sites is &gt;&gt; num users.  Is that also the case for the root domain of the sites?  i.e. are they a number of youtube.com or cnn.com urls in the sites or are they already stemmed to domain?  I'm leaning towards dimensionality reduction by collapsing URLs to domain roots rather than specific pages if it's possible." CreationDate="2014-05-17T18:12:20.767" UserId="92" />
  <row Id="114" PostId="116" Score="0" Text="Thanks for answer. The number of features (unique liked sites) is 32k, while the number of samples (users) is 12k. The features are Facebook Pages, so there's no need to stem the URLs. A user may either like facebook.com/cnn or not. I like the idea of trying to estimate users' age based on the links they share, though :)" CreationDate="2014-05-17T18:29:41.743" UserId="173" />
  <row Id="115" PostId="116" Score="0" Text="Ahhh, I misread the liked sites description.  Thanks for the clarification." CreationDate="2014-05-17T18:47:06.270" UserId="92" />
  <row Id="116" PostId="103" Score="4" Text="Just a thought: If your similarity score is normalized to _1_, than `1-sim(ei, ej) = Distance`. With distance metric you may apply for example hierarchical clustering. Going down from the root you will see at what level of granularity clusters would make sense for your particular problem." CreationDate="2014-05-18T02:16:46.537" UserId="31" />
  <row Id="118" PostId="76" Score="1" Text="I'd suggest Apache Kafka as message store and any stream processing solution of your choice like Apache Camel or Twitter Storm" CreationDate="2014-05-18T11:31:19.250" UserId="118" />
  <row Id="119" PostId="121" Score="0" Text="Hi, your answer is quite similar to my actual strategy. I used `sklearn.neighbors.KNeighborsRegressor` with cosine metric on SVD-reduced space (after applying SVD the average estimation error went down from ~6 years to ~4).&#xA;&#xA;Users in my database are aged 18-65 (older users were filtered out), so there are 48 possible classes. I wonder whether that's not too many classes for kNN, and whether I should treat it as regression or a classification problem (I think both are applicable)." CreationDate="2014-05-18T12:09:17.057" UserId="173" />
  <row Id="120" PostId="76" Score="0" Text="+1 @KonstantinV.Salikhov: Please post your comment as an answer, thanks!" CreationDate="2014-05-18T13:09:02.180" UserId="158" />
  <row Id="122" PostId="113" Score="0" Text="It depends on what kind of performance you are interested in: being able to handle a very large number of concurrent requests, being able to retrieve a specific record among a very large amount of records, being able to compute complex summary values from the data, etc?" CreationDate="2014-05-18T13:43:52.580" UserId="172" />
  <row Id="123" PostId="134" Score="2" Text="http://blog.mongodb.org/post/57611443904/mongodb-connector-for-hadoop" CreationDate="2014-05-18T14:16:42.150" UserId="118" />
  <row Id="124" PostId="126" Score="0" Text="Sorry for duplicate question, I search with a phrase and don't see other question, excuse me. Thanks for you answer. And, do you have a use case, personal experience, etc?" CreationDate="2014-05-18T16:07:01.423" UserId="109" />
  <row Id="125" PostId="138" Score="1" Text="Can you clarify the exact question? Maybe some possible answers you have in mind can also help." CreationDate="2014-05-18T19:10:48.500" UserId="227" />
  <row Id="127" PostId="159" Score="0" Text="Although this is a very nice and interesting question, I guess it will raise rather primarily opinion-based answers." CreationDate="2014-05-18T19:51:54.947" UserId="84" />
  <row Id="128" PostId="159" Score="0" Text="Please define long-term, computer science itself really is not that old." CreationDate="2014-05-18T20:00:18.810" UserId="158" />
  <row Id="129" PostId="155" Score="9" Text="This question might be more appropriate on the dedicated [opendata.SE](http://opendata.stackexchange.com/). That said, I cross my fingers for [dat](http://usodi.org/2014/04/02/dat), which aspires to become a &quot;Git for data&quot;." CreationDate="2014-05-18T21:23:52.687" UserId="216" />
  <row Id="130" PostId="135" Score="3" Text="R is a pleasure to work with for data manipulation (`reshape2`, `plyr`, and now `dplyr`) and I don't think you can do better than `ggplot2`/`ggvis` for visualization" CreationDate="2014-05-18T21:52:42.273" UserId="236" />
  <row Id="131" PostId="161" Score="0" Text="Sorry for the misunderstanding. My intention was to bring up answers concerning the importance of having control over an application, and how this control is *loosened* by libraries. Of course you can assume things about them (people don't normally rewrite pthreads), but if the data changes (load, throughput, ...), you may need to access the lib source to grant performance. And yes, it is not necessarily C/C++ -- although they're usually the languages chosen for hpc. May I delete my question, or would you like to change it into something more specific? I accept any suggestions to improve it." CreationDate="2014-05-18T21:56:12.430" UserId="84" />
  <row Id="132" PostId="138" Score="0" Text="@AmirAliAkbari SeanOwen posted an answer and I noticed the lack of specificity in my question. I've added a comment to his post. Please, feel free to suggest any improvements on the post -- I'm planing to delete it, otherwise." CreationDate="2014-05-18T21:59:09.827" UserId="84" />
  <row Id="133" PostId="159" Score="0" Text="While the &quot;Data&quot; subject still alive in digital world, It's scientific improvements / researchs stay alive ;)" CreationDate="2014-05-18T23:38:07.320" UserId="229" />
  <row Id="134" PostId="131" Score="0" Text="thanks for your answer." CreationDate="2014-05-19T07:18:05.963" UserId="212" />
  <row Id="135" PostId="120" Score="0" Text="thanks a lot. However arXiv does provide the papers I need." CreationDate="2014-05-19T07:18:44.240" UserId="212" />
  <row Id="136" PostId="140" Score="0" Text="I've read some basics about Apache Storm, it looks like it's concerned about issues related to scalability/reliability of stream processing, leaving you to handle the actual algorithms. Esper on the other handle process data for you based on your queries" CreationDate="2014-05-19T07:36:33.670" UserId="200" />
  <row Id="137" PostId="135" Score="0" Text="@pearpies As said in the beginning of my answer, I admit the good libraries available for R, but as a whole, when considering all areas needed for big data (which I as said a few of them in the answer), R is no match for the mature and huge libraries available for Python." CreationDate="2014-05-19T08:08:53.207" UserId="227" />
  <row Id="138" PostId="155" Score="2" Text="@ojdo Thanks, I never heard of opendata.SE before, I also found [this](http://opendata.stackexchange.com/q/266/2872) interesting (and very similar) question there." CreationDate="2014-05-19T08:28:56.713" UserId="227" />
  <row Id="139" PostId="165" Score="0" Text="Thank you very much Rapaio :) The points you gave me are very useful and gets something clearer..Since I'm a .NET developer and curious one on plain C (i start to learn) and new, fast, reliable, scalable ancd of course fully controllable -in a short term : very excited- techniques..So i need to learn very much..To learn, i try to read so many documents but as you can guess i'm at the start-line..&#xA;I didn't know that BTree has advantages on disk (In .Net world, so many writers explain it like : A hierarchical data structure like Linked-List..No More!) Thank you very much again" CreationDate="2014-05-19T11:41:09.810" UserId="229" />
  <row Id="140" PostId="165" Score="0" Text="And if you permit me, until there is a higher quality explanation / answer than yours, i want to accept this as answer.. And BTW, Lucene.NET is a .NET implementation of Java's Lucene" CreationDate="2014-05-19T11:45:21.380" UserId="229" />
  <row Id="141" PostId="84" Score="1" Text="While Bonferroni is definitely old-school it is still pretty popular. Related to it is a method called Šidák correction ( https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction ). I am calling it out, because in a large scale targeting advertising system I worked on we were able to implement this approach as a UDF in Hive. However this only works better when you have independence between tests. If not you have to fall back to Bonferroni or another method." CreationDate="2014-05-19T23:38:38.263" UserId="249" />
  <row Id="142" PostId="174" Score="1" Text="With LR you would have to make multiple models for age bins i think. How would compare two models for different age bins that predict the same prob on inclusion for a user?" CreationDate="2014-05-20T15:18:13.717" UserId="92" />
  <row Id="143" PostId="174" Score="1" Text="Note that LR fails when there are more variables than observations and performs poorly if the assumptions of the model is not met.  To use it, dimensionality reduction must be a first step." CreationDate="2014-05-20T17:06:04.223" UserId="178" />
  <row Id="144" PostId="59" Score="0" Text="Also see [Is the R language suitable for Big Data](http://datascience.stackexchange.com/q/41/227)." CreationDate="2014-05-20T17:42:19.287" UserId="227" />
  <row Id="145" PostId="135" Score="1" Text="[Peter](http://continuum.io/our-team) from Continuum Analytics (one of the companies on the [DARPA project referenced above](http://www.computerworld.com/s/article/9236558/Python_gets_a_big_data_boost_from_DARPA)) is working on some very impressive [opensource code](http://continuum.io/developer-resources) for [data visualization that simply do things that other sets of code are not able to do](http://bokeh.pydata.org/)." CreationDate="2014-05-20T18:46:45.713" UserId="158" />
  <row Id="146" PostId="174" Score="1" Text="@cwharland you should not consider the response variable to be categorical as it is continuous by nature, and discretized by the problem definition. Considering it categorical would mean telling the algorithm that predicting age 16 when it actually is 17 is as a serious error as predicting 30 when it actually is 17. Considering it continuous ensures that small errors (16 vs 17) are considered small and large errors (30 vs 17) are considered large. The logistic regression is used in this case to predict the continuous value and not estimate posterior probabilities." CreationDate="2014-05-20T19:28:01.027" UserId="172" />
  <row Id="147" PostId="174" Score="0" Text="@ChristopherLouden You are right that the vanilla version of logistic regression is not suitable for the 'large p small n' case, I should have mentioned that regularization is important in the present case. I update my answer. But  L1-regularized LR is a sort of feature selection so I consider no need for a preliminary FS step." CreationDate="2014-05-20T19:33:50.253" UserId="172" />
  <row Id="148" PostId="174" Score="0" Text="@damienfrancois: Agreed, thank you." CreationDate="2014-05-20T19:48:42.710" UserId="178" />
  <row Id="149" PostId="175" Score="1" Text="It may be easier to help if you show some two or three entries of your input file." CreationDate="2014-05-20T22:38:19.070" UserId="84" />
  <row Id="151" PostId="179" Score="1" Text="Hi, thanks for this tip, but I would prefer something that's easily publicable on the Web in a dynamic form. Also, I prefer free solutions, while Tableau - correct me if I am wrong - is only available as a trial version." CreationDate="2014-05-21T12:08:21.993" UserId="173" />
  <row Id="152" PostId="183" Score="0" Text="Thanks, I like this Python/networkx/matplotlib solution since it's my default working environment, and it's easy to make a gif out of this code. Still, something that looks nicer on the Web would beat this solution :)" CreationDate="2014-05-21T12:10:22.683" UserId="173" />
  <row Id="153" PostId="179" Score="0" Text="It also has &quot;Public&quot; edition, which means you have to store/share your results in the web, and can't save it locally." CreationDate="2014-05-21T12:10:59.243" UserId="97" />
  <row Id="154" PostId="187" Score="1" Text="Thanks, I misunderstood it. Now it is clear." CreationDate="2014-05-21T17:39:11.487" UserId="133" />
  <row Id="155" PostId="174" Score="0" Text="@damienfrancois: I definitely agree.  I'm just a little concerned that in this case LR will penalize intermediate values too harshly.  There's seem to be no motivation to map to a sigmoidal like curve given that you are not particularly interested in extreme age values.  Perhaps I'm misinterpreting the use though." CreationDate="2014-05-22T20:32:52.293" UserId="92" />
  <row Id="156" PostId="121" Score="0" Text="I can say, anecdotally, that I have use per class Random Forests to fit a number of classes individually then combined the results of each of those models in various ways.  In this case you might even think about assigning prior probabilities to each user's age with the kNN, then run through each class based model, use those scores to update the prior probabilities for each class and choose the most probable class from those posteriors.  It sounds like over complicating a bit but at worst you would have the kNN accuracy." CreationDate="2014-05-22T20:36:20.637" UserId="92" />
  <row Id="158" PostId="204" Score="2" Text="One interesting aspect of using the top 5000 sites is the fact that they may not be good at segmenting users on age.  The top sites, by construction, are ones that everyone visits.  They therefore are not very good at segmenting your users since all possible classifications (ages) have engaged with those sites.  This is a similar notion to the idf part of tf-idf.  idf helps filter out the &quot;everyone has this feature&quot; noise.  How do the most visited sites rank as features in your variable importance plots with your RF?" CreationDate="2014-05-24T04:59:04.420" UserId="92" />
  <row Id="159" PostId="209" Score="0" Text="There's no problem that -1 doesn't mean dislike. It's simply a way to differentiate that someone saw the item. In that sense it carries more info than a missing value. It may actually increase the accuracy of your recommendation. Depending on your distance metric in recommending you may consider changing it from a -1 to a slight metrics value so it doesn't influence the distance as much." CreationDate="2014-05-25T15:10:34.063" UserId="92" />
  <row Id="160" PostId="155" Score="2" Text="See http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public." CreationDate="2014-05-26T17:08:57.360" UserId="289" />
  <row Id="161" PostId="209" Score="0" Text="The canonical paper for implicit feedback is [Hu, Koren, and Volinsky](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.167.5120). Lots of good recommendations in there, including estimating your confidence in which -1 indicates a dislike or merely a &quot;didn't see.&quot;" CreationDate="2014-05-27T00:00:55.560" UserId="159" />
  <row Id="162" PostId="213" Score="0" Text="I see. I did study the README file, however I still can't figure out how the data it self can be read. For example, the train/X_train.txt'file represents training set (the sample data that I showed in post comes from this file)." CreationDate="2014-05-27T12:30:31.233" UserId="295" />
  <row Id="163" PostId="204" Score="1" Text="Good point. An easy fix for this would be to stratify the training dataset into J age bins (e.g., 13-16, 17-20, 21-24, etc.) and take the top (K/J) pages for each group. That would ensure you have significant representation for each group. There will certainly be some overlap across groups, so if you were really picky you might want to take the top (K/J) unique pages for each group, but I think that might be overkill." CreationDate="2014-05-27T13:37:09.423" UserId="250" />
  <row Id="164" PostId="213" Score="0" Text="It seems to me that the data set is rather wide, there are 561 variables per row which are listed inside of the features.txt file. I believe that is what you are referring to." CreationDate="2014-05-27T13:38:22.867" UserId="59" />
  <row Id="165" PostId="197" Score="0" Text="I had experimented with C45/J48 in a previous project. I did not realize there were rules I could retrieve from it. I'll also check out RIPPER. Thanks!" CreationDate="2014-05-27T13:53:22.987" UserId="275" />
  <row Id="166" PostId="213" Score="0" Text="So each of the variable from 'features' file corresponds to each column in e.g 'x-test.txt' file, or Am I wrong?" CreationDate="2014-05-27T14:40:54.283" UserId="295" />
  <row Id="167" PostId="213" Score="0" Text="That is how I am taking it. From what I could gather from the README is that is what the file contains." CreationDate="2014-05-27T14:53:03.520" UserId="59" />
  <row Id="168" PostId="213" Score="0" Text="@Jakubee Yes. There are 561 rows/variable names in the `features.txt` file, and 561 columns in the `X_train.txt` file, one for each variable." CreationDate="2014-05-27T14:53:30.540" UserId="156" />
  <row Id="169" PostId="213" Score="0" Text="Ok. I do get it now! Thanks!! But what about the data itself. What kind of analytics I can process on this? (That might be stupid question) You think I can apply this for future analysis with python, using e.g hadoop?" CreationDate="2014-05-27T15:08:46.697" UserId="295" />
  <row Id="170" PostId="213" Score="0" Text="You can definitely use Python, I don't know if Hadoop will be necessary for files of this size, you could also use R and install the package Rattle with the following command `install.packages(&quot;rattle&quot;)` it is a good data mining package for R. In the future if you are going to be using massive data sets then for sure Hadoop will be good to know and use" CreationDate="2014-05-27T15:12:05.937" UserId="59" />
  <row Id="171" PostId="212" Score="2" Text="I think imputing slight negative values for items that have been seen many times but never chosen is reasonable. The OP doesn't indicate they have access to data that qualifies these negative imputations but I wouldn't rule that tactic out entirely. The optimal magnitude of the negative value can be determined from the data.  I've had small gains from doing this in recsys scenarios. In any case...would you suggest other ways of differentiating between item seen once and not chosen vs seen N times and never chosen besides negative imputing?" CreationDate="2014-05-28T02:37:14.990" UserId="92" />
  <row Id="172" PostId="216" Score="1" Text="+1 automation doesn't necessarily make things better, only more consistently and often faster!" CreationDate="2014-05-29T13:52:18.137" UserId="297" />
  <row Id="173" PostId="161" Score="1" Text="No it's a fine question, you can reflect your comments here in edits to the question if you like." CreationDate="2014-05-29T13:57:43.450" UserId="21" />
  <row Id="174" PostId="161" Score="0" Text="Please, check if the question makes sense now. I've added a small case in order to make it more straightforward. In case you want to add some consideration in the question, please, feel free to edit it." CreationDate="2014-05-29T15:04:26.473" UserId="84" />
  <row Id="175" PostId="175" Score="0" Text="For next time, use [ASCII Delimited Text](https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/) if you are unsure of which delimiter to select." CreationDate="2014-05-30T06:16:20.733" UserId="227" />
  <row Id="176" PostId="179" Score="2" Text="@WojciechWalczak Maybe [gephi](http://gephi.org/) can be used instead of Tableau." CreationDate="2014-05-30T06:31:29.873" UserId="227" />
  <row Id="177" PostId="179" Score="0" Text="@Amir Ali Akbari, looks great thanks" CreationDate="2014-05-30T06:49:36.470" UserId="97" />
  <row Id="178" PostId="220" Score="0" Text="Thanks for your answer mate. unfortunately, if I transpose the matrix it will make the false calculation." CreationDate="2014-05-30T11:46:02.917" UserId="273" />
  <row Id="179" PostId="223" Score="0" Text="One of SGML's major purposes (the same holds for its offspring, XML) was to provide the means for tagging text documents (POS **and** semantic tags)." CreationDate="2014-05-30T20:47:32.157" UserId="318" />
  <row Id="180" PostId="175" Score="0" Text="@Rubens yes I will mock up some entries and edit this weekend." CreationDate="2014-05-31T06:56:25.183" UserId="249" />
  <row Id="181" PostId="175" Score="0" Text="@AmirAliAkbari unfortunately I cannot get the source system to change the format otherwise I wouldn't have the issue, and SASB on AIX has limited options, but thank you for the link." CreationDate="2014-05-31T07:00:48.387" UserId="249" />
  <row Id="182" PostId="182" Score="0" Text="Besides the metadata used to declare the hive table there is also information on the original informat from the SAS environment. Even if the case is more obscure than int, string, int I am hoping that that additional information could be used to get there if the table metadata is insufficient due to potential ambiguity (e.g. string, string, string)" CreationDate="2014-05-31T07:03:29.753" UserId="249" />
  <row Id="184" PostId="229" Score="0" Text="I referred to the processing via MapReduce in Hadoop Echo system as simply Hadoop because that's the term commonly used (Though technically wrong and I have changed the question accordingly)." CreationDate="2014-06-01T21:50:16.317" UserId="339" />
  <row Id="185" PostId="229" Score="0" Text="May be I am wrong but I think there is more to that than to just have near-real-time processing. If there were no trade-offs between them, everyone would have like to do things in near-real-time. A hybrid approach allows for getting the best of both worlds (to some extent). That's why Summingbird was created." CreationDate="2014-06-01T21:55:28.310" UserId="339" />
  <row Id="186" PostId="223" Score="0" Text="Could be more specific/restrictive about what kind of metadata you want to add? With your two examples, I doubt that there is a less verbose way that has the same generic expressiveness as XML tags." CreationDate="2014-06-01T22:28:33.520" UserId="216" />
  <row Id="188" PostId="229" Score="0" Text="A major difference is that a stream processing system can just touch data once, and by itself has no long-term state. Some problems can't be solved this way. For problems for which this is OK, it's faster to use a system that does not require first persisting data into (re-readable) storage. MapReduce is not inherently slower than Storm; both are containers. They are different paradigms for different problems." CreationDate="2014-06-01T22:53:58.930" UserId="21" />
  <row Id="189" PostId="229" Score="0" Text="By not having long-term persistent state does it mean that such near-real-time systems can not accumulate input updates over a long duration? Can you refer me to any resources that discuss further on this?" CreationDate="2014-06-01T23:07:22.743" UserId="339" />
  <row Id="190" PostId="229" Score="0" Text="This is kind of the definition of a streaming system. If you imagine a system that can access long-term state at will, it's not really streaming." CreationDate="2014-06-02T00:39:08.850" UserId="21" />
  <row Id="191" PostId="223" Score="0" Text="@ojdo The most of the meta-data is either for disambiguation (like the relative times), or for specifying special entities (i.e. FKs)." CreationDate="2014-06-02T16:59:48.337" UserId="227" />
  <row Id="192" PostId="220" Score="1" Text="Can you explain why you have to calculate such a giant covariance matrix? Would be easier to find a workaround if you could shed light on the motivation." CreationDate="2014-06-03T15:01:16.907" UserId="250" />
  <row Id="193" PostId="197" Score="0" Text="Also check out the C50 package in R." CreationDate="2014-06-06T14:56:59.903" UserId="375" />
  <row Id="194" PostId="232" Score="1" Text="When you say, when you sum results you can not interpret totals, you mean that each classification can have a different weight and its contribute can be over/under estimated in the total? If I suppose to run, e.g., 4 independent tests, may I assume that each classification has the same weight and interpret (painlessly) the totals? Hope it is clear.." CreationDate="2014-06-08T16:56:17.433" UserId="133" />
  <row Id="1194" PostId="224" Score="1" Text="Seems to be just a text reformatting problem, have you written any code yet?" CreationDate="2014-06-09T10:43:11.727" UserId="227" />
  <row Id="1196" PostId="223" Score="1" Text="I have used http://brat.nlplab.org/ in the past.  There is a nice interface for many different types of annotations.  The annotations are stored in a separate .annot file which is a list of the words that are annotoated and their position in the document." CreationDate="2014-06-09T13:09:03.373" UserId="387" />
  <row Id="1197" PostId="224" Score="2" Text="what have you tried so far?" CreationDate="2014-06-09T19:54:01.820" UserId="59" />
  <row Id="1198" PostId="236" Score="0" Text="I would check out research by faculty and number of citations. Generally that is a good way to rank programs." CreationDate="2014-06-09T20:58:14.230" UserId="141" />
  <row Id="1199" PostId="208" Score="1" Text="I think you should note that for processes on a single machine you can memory map variables with joblib/Numpy. You lose that ability for processes on different machines." CreationDate="2014-06-09T21:55:20.937" UserId="403" />
  <row Id="1200" PostId="44" Score="0" Text="But does using R with Hadoop overcome this limitation (having to do computations in memory)?" CreationDate="2014-06-09T23:07:41.553" UserId="413" />
  <row Id="1201" PostId="188" Score="1" Text="That would still have mongo doing the processing, which I believe from the question is to be avoided in the final solution.  Giving you an upvote anyways for bringing up an important piece of knowledge." CreationDate="2014-06-10T04:15:12.013" UserId="434" />
  <row Id="1203" PostId="232" Score="1" Text="What I meant to convey is that we lose track of what the actual numbers mean. E.g., if I have 4 in a specific entry in run 1 and get 5 in that same entry on run 2, it's hard to say exactly what 4+5=9 means. I'd rather look at a distribution (%'s) or averages of where individuals fall across the matrix. It seems much more intuitive." CreationDate="2014-06-10T06:11:12.453" UserId="375" />
  <row Id="1204" PostId="251" Score="1" Text="+1 That may be my fault for being not very clear in my post, so others had not got it before. This is surely the kind of answer I was looking for. Thanks." CreationDate="2014-06-10T07:06:17.757" UserId="84" />
  <row Id="1205" PostId="218" Score="0" Text="Isn't this a programming question that should be on Stack OVerflow? Getting an error when computing a covariance is not data science." CreationDate="2014-06-10T08:15:35.330" UserId="471" />
  <row Id="1206" PostId="256" Score="1" Text="Thank you for the detailed answer. That is relaxing!" CreationDate="2014-06-10T08:29:43.067" UserId="456" />
  <row Id="1207" PostId="191" Score="0" Text="http://en.wikipedia.org/wiki/Winner-take-all" CreationDate="2014-06-10T08:45:00.260" UserId="11" />
  <row Id="1208" PostId="271" Score="0" Text="Interesting.  The reason I asked the question is that I wondered if something similar to the &quot;gambler's fallacy&quot; (or even gf itself).  I thought there might be a chance it had already been proven to be a fruitless venture.  Still - these other answers are intriguing." CreationDate="2014-06-10T11:56:17.943" UserId="434" />
  <row Id="1210" PostId="274" Score="5" Text="From personal experience I'd add that built-in documentation/label is huge. Now all my datasets can can be stored with explicit records of where they came from, sampling frequency, anomalies, etc. etc." CreationDate="2014-06-10T13:04:36.180" UserId="403" />
  <row Id="1212" PostId="272" Score="5" Text="+1 For Andrew Ng's course.  It is very well done." CreationDate="2014-06-10T15:12:40.480" UserId="533" />
  <row Id="1216" PostId="245" Score="0" Text="You are right, the data science PhD is not yet listed.  Machine learning has a lot math behind it, we can't just use an algorithm without proving the theory part why it converges, optimizes, etc..." CreationDate="2014-06-10T16:38:53.567" UserId="386" />
  <row Id="1217" PostId="241" Score="0" Text="As Andrew Gelman [mentions HERE](http://andrewgelman.com/2014/05/25/decided-physicist/), it's pointless to be a math major if you are the best.  I am not even that great at math, actually.  That's what led me to think of data science." CreationDate="2014-06-10T16:41:45.913" UserId="386" />
  <row Id="1218" PostId="291" Score="0" Text="Thanks, see my edit. I have a lot of coding experience and have taken MOOCs. I have a masters in Statistics and a minor in applied mathematics, I would consider math my biggest strength. I am really looking for things to put on a PhD application." CreationDate="2014-06-10T19:23:53.090" UserId="560" />
  <row Id="1219" PostId="248" Score="0" Text="You have very good points.  But, imagine you are Data Science PhD grad, which journal will you publish in?  Stat, CS, journal of machine learning?  Each one of those fields has its own experts." CreationDate="2014-06-10T19:31:58.813" UserId="386" />
  <row Id="1220" PostId="135" Score="3" Text="This answer seems to be wholly anecdotal and hardly shows anywhere where R is weak relative to Python." CreationDate="2014-06-10T20:31:38.787" UserId="598" />
  <row Id="1221" PostId="291" Score="2" Text="Then write some papers and get them published in a good conference: that's the best signal that you are fit for research--and a PhD program. Maybe you can use your economics background to write a paper on [multi-agent learning](http://mitpress.mit.edu/books/theory-learning-games). You don't have to stick to the same subject once you get accepted; it's just to demonstrate your ability." CreationDate="2014-06-10T20:59:51.017" UserId="381" />
  <row Id="1222" PostId="295" Score="0" Text="Thanks for the answer. I have a minor in applied mathematics and a masters in statistics. I have been taking graduate math courses for the last two years, as I did my masters in statistics. Are there any specific classes I should take? I have taken my calc sequence, linear algebra, differential equations, fourier analysis, stochastic processes, advanced probability, statstical inference, bayesian analysis, time series and a few others. Any others in particular" CreationDate="2014-06-10T21:42:57.950" UserId="560" />
  <row Id="1226" PostId="289" Score="2" Text="He said it specifically about research job though." CreationDate="2014-06-10T23:58:32.280" UserId="615" />
  <row Id="1228" PostId="295" Score="0" Text="Statistics MS/MA is offered everywhere these days, they don't help you get into a stat PhD. Stat PhD is looking for solid math undergrads: real analysis, optimization, numerical analysis. CS PhD is looking for cs and math undergrad. Why don't you continue on economics?" CreationDate="2014-06-11T00:12:25.993" UserId="386" />
  <row Id="1230" PostId="253" Score="1" Text="This class of questions is being discussed on meta. You can voice your opinion on this [meta post.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:19:34.887" UserId="62" />
  <row Id="1232" PostId="234" Score="0" Text="This class of questions is being discussed on meta. You may voice your opinion [here.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:22:39.393" UserId="62" />
  <row Id="1234" PostId="44" Score="0" Text="RHadoop does overcome this limitation.  The tutorial here: https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md spells it out clearly.  You need to shift into a mapreduce mindset, but it does provide the power of R to the hadoop environment." CreationDate="2014-06-11T06:34:50.310" UserId="434" />
  <row Id="1235" PostId="308" Score="1" Text="Thank you. This is very helpful. I know its a bug space and there is no one right answer. I am very interested to know how one selects big data tools and technology to suit their needs. I am not marking this as the right answer for now but it certainly deserve lot of UP votes. Cheers :)" CreationDate="2014-06-11T07:40:08.317" UserId="496" />
  <row Id="1236" PostId="306" Score="3" Text="I assume that the OP's reference to TBs means &quot;for data on the small end of what you might use Hadoop for.&quot;  If you have multiple petabytes or more, Redshift clearly isn't suitable. (I believe it's limited to a hundred 16TB nodes.)" CreationDate="2014-06-11T07:47:01.247" UserId="14" />
  <row Id="1237" PostId="309" Score="2" Text="`easier to develop because of Redshift's maturity` contradicts with `Redshift isn't that mature yet` so what is your verdict?" CreationDate="2014-06-11T08:41:41.310" UserId="646" />
  <row Id="1238" PostId="309" Score="0" Text="@M.Mimpen: Edited answer to be more specific" CreationDate="2014-06-11T08:56:18.213" UserId="638" />
  <row Id="1239" PostId="287" Score="0" Text="Thanks for your answer, I will read it carefully." CreationDate="2014-06-11T09:09:30.187" UserId="133" />
  <row Id="1240" PostId="287" Score="0" Text="What do you mean for misclassification ratio in the fourth paragraph?" CreationDate="2014-06-11T09:14:36.593" UserId="133" />
  <row Id="1241" PostId="287" Score="0" Text="misclassification ratio = (number of instances correctly classified)/(total number of instances); in that paragraph we have 0.33 = proportion of each class (let's name labels as c1, c2, c3); we have 0.33*1.0 (c1 are all correctly classified), + 0.33*0.5 (c2 are random classified as c2 or c3) + 0.33*0.5 (c3 are random classified as c2 or c3) = 0.33 + 0.166 + 0.166 = 0.66 (instances classified correctly/total number of instances)" CreationDate="2014-06-11T09:36:33.673" UserId="108" />
  <row Id="1242" PostId="19" Score="8" Text="&quot;Anything too big to load into Excel&quot; is the running joke." CreationDate="2014-06-11T12:07:51.477" UserId="471" />
  <row Id="1244" PostId="313" Score="0" Text="Asking about &quot;good&quot; books will attract opinion-based answers and so this is off-topic. Flagged." CreationDate="2014-06-11T14:32:37.843" UserId="471" />
  <row Id="1245" PostId="313" Score="2" Text="I've changed it so I am just looking for books. Nothing opinion-based." CreationDate="2014-06-11T14:34:15.243" UserId="663" />
  <row Id="1246" PostId="313" Score="0" Text="It's spelled S-t-a-t-i-s-t-i-c-s :)   Stick with something pragmatic that focuses on prediction rather than inference. Both _Elements of Statistical Learning_ and _An Introduction to Statistical Learning_ are on most people's lists." CreationDate="2014-06-11T15:17:28.750" UserId="515" />
  <row Id="1248" PostId="291" Score="0" Text="Thank you, that is the best advice I have received." CreationDate="2014-06-11T16:07:35.017" UserId="560" />
  <row Id="1249" PostId="295" Score="0" Text="When I left undergrad I was 12 credit hours short of a math major. After I finished my MS in stats I could have pursued a PhD where I got my MS(top 30 school), however I am more interested in ML. I really don't think my math background will be a problem, as I feel it is very strong. I left economics and went to pure statistics in graduate school because economics no longer interested me, so that is definitely out. So do you think I should try to finish a math undergrad? It would take less than two semesters" CreationDate="2014-06-11T16:15:30.020" UserId="560" />
  <row Id="1250" PostId="316" Score="1" Text="+1 Thank you for taking the time to improve upon the existing answer, and given in my opinion your answer is now the better answer, I've selected your answer as the answer. Cheers!" CreationDate="2014-06-11T17:35:10.527" UserId="158" />
  <row Id="1251" PostId="316" Score="0" Text="Glad to answer!" CreationDate="2014-06-11T19:01:09.080" UserId="514" />
  <row Id="1252" PostId="305" Score="0" Text="Do you mean Hadoop or do you mean a specific counterpart to Redshift, like Impala?" CreationDate="2014-06-11T19:17:18.213" UserId="21" />
  <row Id="1253" PostId="311" Score="1" Text="I've already implemented feature extraction, and a toolkit for it (publication awaits some bugchecking)." CreationDate="2014-06-11T19:18:26.090" UserId="555" />
  <row Id="1254" PostId="272" Score="1" Text="John Hopkins also has a data science certificate track (9 classes) that started last week at Coursera.  https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage - it's not all machine learning, but worth sharing.  Coursera is full of awesomeness (and Andrew Ng is a great lecturer)." CreationDate="2014-06-11T20:49:17.350" UserId="434" />
  <row Id="1257" PostId="324" Score="1" Text="RODBC is for connecting to relational databases - often ones that are on your premises. You question alludes to  connecting to public API - that is something else." CreationDate="2014-06-12T03:33:12.170" UserId="366" />
  <row Id="1258" PostId="303" Score="0" Text="actually the ppt and the SO site you've linked to have nothing to do with the phrase alignments. It only achieves the word alignments that I have showed in the original post. =(" CreationDate="2014-06-12T04:38:42.697" UserId="122" />
  <row Id="1259" PostId="324" Score="2" Text="You're asking two questions. Please post them separately. You'd better flesh out the part that comes before sentiment analysis too." CreationDate="2014-06-12T05:12:19.767" UserId="381" />
  <row Id="1265" PostId="339" Score="3" Text="R hardly beats python in visualization. I think it's rather the reverse; not only does python have [ggplot](https://github.com/yhat/ggplot/) (which I don't use myself, since there are more pythonic options, like [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)), it can even do interactive visualization in the browser with packages like [bokeh](http://bokeh.pydata.org)." CreationDate="2014-06-12T15:57:49.973" UserId="381" />
  <row Id="1266" PostId="343" Score="0" Text="Deep learning is about increasing the _number_ of hidden layers. Otherwise it would be called fat learning :)" CreationDate="2014-06-12T16:00:20.453" UserId="381" />
  <row Id="1267" PostId="24" Score="2" Text="Good answer. Potentially helpful: I have implemented Huang's k-modes and k-prototypes (and some variations) in Python: https://github.com/nicodv/kmodes" CreationDate="2014-06-12T16:08:03.140" UserId="554" />
  <row Id="1271" PostId="203" Score="1" Text="Thank you for your post, but can you include some of the essential information to answer the question here? The material you linked is a great way to support the information in your answer, but this site was created to compile a great collection of questions with answers. Posts that send users elsewhere to find that information are not really considered &quot;answers&quot; here. Thanks." CreationDate="2014-06-12T16:56:07.723" UserId="50" />
  <row Id="1272" PostId="320" Score="0" Text="Good answer. Realize that 2, 3, 4 can interact in complex ways, though. Debugging could be done by checking the activation values of the ANN, the magnitude of the weights of the ANN, keeping an eye on the in-sample and out-of-sample error and convergence of the optimizer, etc." CreationDate="2014-06-12T16:59:43.233" UserId="554" />
  <row Id="1274" PostId="339" Score="0" Text="ggplot was built in 2005 and it continues to be the favorite of many researchers due to its intuitive nature and well defined grammar of visualizations. Seaborn is built on top of matplotlib and it could be adopted by many in the coming years but ggplot still leads the charts in statistical data analysis visualizations. ggplot for python is so unpythonic in nature. The API is directly taken from the R implementation. Coming to interactive visualisations part, both python and R have d3js and d3py to take leverage of interactive visualisations." CreationDate="2014-06-12T17:12:15.527" UserId="514" />
  <row Id="1275" PostId="343" Score="0" Text="@Emre definitely meant that. Curse my punctuation!" CreationDate="2014-06-12T18:08:53.677" UserId="754" />
  <row Id="1276" PostId="341" Score="0" Text="Apparently most of the people are using Hadoop for analytics. What I am thinking is do I need something like that or knowledge about database, ML, statistics is enough?" CreationDate="2014-06-12T18:26:34.837" UserId="456" />
  <row Id="1278" PostId="295" Score="0" Text="No, you should not back dig for that math major, but take courses you need like real analysis, and optimization.  I know these courses sound irrelevant, but PhD programs want to see it, please them.  They want to know do you have the theories down.  They don't worry if you don't understand neural network well.  As Prof. LeCun said, take as many math courses as you can." CreationDate="2014-06-12T20:46:56.670" UserId="386" />
  <row Id="1280" PostId="339" Score="4" Text="Also R has the ability to interactive viz with Shiny." CreationDate="2014-06-12T22:04:23.300" UserId="598" />
  <row Id="1281" PostId="350" Score="0" Text="This is an example of a career question. Please join in on the [discussion here](http://meta.datascience.stackexchange.com/q/41/62) and voice your opinion if you feel this should be on-topic." CreationDate="2014-06-13T01:41:26.040" UserId="62" />
  <row Id="1282" PostId="334" Score="3" Text="This is too broad and primarily opinion based. Please take a look at http://datascience.stackexchange.com/help/dont-ask" CreationDate="2014-06-13T01:44:05.283" UserId="62" />
  <row Id="1283" PostId="334" Score="3" Text="@AsheeshR - We're averaging 2 questions a day and 2 answers per question.  At this point the focus needs to be on encouraging participation and increasing interest." CreationDate="2014-06-13T04:29:41.007" UserId="434" />
  <row Id="1284" PostId="334" Score="8" Text="Engagement at the expense of site quality is not the solution. Engagement is transient. Quality is much harder to alter later on." CreationDate="2014-06-13T05:05:00.603" UserId="62" />
  <row Id="1285" PostId="334" Score="0" Text="@AsheeshR I'll agree with you if you can point me to a single instance of a QA site that is considered authoritative on less than 10 questions a day." CreationDate="2014-06-13T05:11:38.660" UserId="434" />
  <row Id="1286" PostId="334" Score="4" Text="[bicycles.se], [workplace.se], [money.se], [skeptics.se], [gamedev.se] all launched with less than 10 questions per day. [Bicycles](http://area51.stackexchange.com/proposals/2305/bicycles) was launched with 4 per day because it was considered to be a high quality site." CreationDate="2014-06-13T05:18:36.150" UserId="62" />
  <row Id="1287" PostId="334" Score="3" Text="Well... I guess I have to declare you the winner at this point.  :)" CreationDate="2014-06-13T05:37:04.533" UserId="434" />
  <row Id="1288" PostId="339" Score="6" Text="Librariers - I do not agree at all with that. R is by far the richest tool set, and more than that it provides the information in a proper way, partly by inheriting S, partly by one of the largest community of reputed experts." CreationDate="2014-06-13T06:11:10.367" UserId="108" />
  <row Id="1289" PostId="349" Score="2" Text="Sometimes experience is hard to gain if your current job is not focused on data science but on some related field (in my case statistics). I use the courses to gain some knowledge and stay on topic, which I cannot do in my daytime job." CreationDate="2014-06-13T07:16:19.973" UserId="791" />
  <row Id="1290" PostId="235" Score="6" Text="Voted down as question does not show any research effort. Its not that hard." CreationDate="2014-06-13T07:26:14.520" UserId="471" />
  <row Id="1291" PostId="38" Score="3" Text="Voted down for lack of research effort. Hadoop and noSQL are well-defined elsewhere." CreationDate="2014-06-13T07:29:24.280" UserId="471" />
  <row Id="1293" PostId="354" Score="5" Text="Career-question, off-topic: http://meta.datascience.stackexchange.com/questions/41/is-a-question-about-future-career-paths-appropriate" CreationDate="2014-06-13T07:33:53.157" UserId="471" />
  <row Id="1294" PostId="354" Score="0" Text="Thank you, I was wondering if this was an feasible question, but did not see the meta thread. maybe adding an description to this site?" CreationDate="2014-06-13T07:34:51.107" UserId="791" />
  <row Id="1297" PostId="351" Score="2" Text="And what about stats.SE, datascience.SE profiles. Do you think they can say much about relevant level of knowledge?" CreationDate="2014-06-13T08:26:14.040" UserId="97" />
  <row Id="1299" PostId="327" Score="2" Text="But what about the libraries? There are advanced R packages (think Ranfom Forest or Caret) that would be utterly impractical to reimplement in a general purpose language such us C or Java" CreationDate="2014-06-13T09:35:46.040" UserId="457" />
  <row Id="1300" PostId="352" Score="0" Text="This is a good question.  The answer has been laid out in papers relating to various graphing libraries, but it will always depend on the assumptions and restrictions that you put in place.  Are the circles area appropriate? Will they always have at least X points of intersection?  Are you limited in size or quantity of categories?  More importantly is the question of whether or not a Euler diagram will be useful.  More than a dozen intersecting circles is hard to interpret, but if there's a mostly hierarchical relationship that you are depicting a much larger quantity can work." CreationDate="2014-06-13T09:48:54.630" UserId="434" />
  <row Id="1301" PostId="352" Score="0" Text="I could be wrong and there's an easier test, but with my experience with visualizations, the question has always been a practical one first, and honestly if I'm looking at intersecting categories I usually have a small collection of them." CreationDate="2014-06-13T09:53:37.330" UserId="434" />
  <row Id="1302" PostId="345" Score="1" Text="+1 for the bonus paper. Great read" CreationDate="2014-06-13T10:00:56.180" UserId="457" />
  <row Id="1303" PostId="350" Score="1" Text="There is no easy answer to this question, IMO.  Workloads and project goals vary significantly.  Think of immunology, for example.  An immunologist's tasks will vary throughout his career and at the senior level the focus will be very specialized.  Data science is like that, but even more broad because the technologies they work with and the data they work with can vary so much.  A data scientist at Facebook is going to have a very different workload and type of data to analyze than one at Baxter." CreationDate="2014-06-13T10:16:04.927" UserId="434" />
  <row Id="1304" PostId="327" Score="0" Text="mahout i.e. supports random forest for java" CreationDate="2014-06-13T10:17:04.727" UserId="115" />
  <row Id="1305" PostId="327" Score="0" Text="ok, maybe RF wasn't a good example, but you get my meaning: there are hundreds of statistical packages in R not implemented in other platforms" CreationDate="2014-06-13T10:21:46.037" UserId="457" />
  <row Id="1306" PostId="327" Score="0" Text="Yeah maybe, but R doesn't bring the performance at all that you need for proccessing big sets of data and most of the time you have really big datasets in industrial use." CreationDate="2014-06-13T10:41:21.297" UserId="115" />
  <row Id="1307" PostId="351" Score="0" Text="What do dropouts have to do with it? Presumably, certification is contingent upon completing the course, not merely registering…" CreationDate="2014-06-13T10:49:41.657" UserId="762" />
  <row Id="1309" PostId="355" Score="1" Text="I thinkt that's a good answer, thank you for the hints. &#xA;I especially like the idea to build up from a better fitting job - I had the impression that I had to know all, which is not the case.+" CreationDate="2014-06-13T11:15:12.490" UserId="791" />
  <row Id="1310" PostId="358" Score="1" Text="That's pretty close to my initial thought. (Which was Export, hadoop m/r the merge, import to temp, atomically replace table).  Export/Import/Indexing will still be very slow, but it will scale beyond 100M rows in 24hrs on everything but DB2.  DB2s indexing speed is a bear.  I'm wondering if there might be a high speed algorithm for finding a changed column.  Right now I'm thinking of hashing the row and hashing some row segments to at least narrow down changes without having to iterate each column for comparison.  I'm also wondering about in-memory caching of the production dataset." CreationDate="2014-06-13T12:46:46.617" UserId="434" />
  <row Id="1311" PostId="358" Score="0" Text="future questions for when I get moving on it, I suppose :)." CreationDate="2014-06-13T12:47:26.107" UserId="434" />
  <row Id="1312" PostId="358" Score="1" Text="Yeah. If you go down this route then pre-process step after exporting is probably where you wrangle it to a good state for some kind of comparison algorithm. Sounds like a really fun project tbh :)" CreationDate="2014-06-13T13:20:10.287" UserId="587" />
  <row Id="1313" PostId="349" Score="0" Text="I agree fully, the courses are very valuable for giving you a starting point, and some structure to gain that experience.  To get the most out of the Mooc I suggest taking a very specific example, lets say logistic regression, and really working through it with a different data set, double bonus if you do it in a language other than the one the course is taught in." CreationDate="2014-06-13T13:36:51.167" UserId="780" />
  <row Id="1314" PostId="349" Score="0" Text="That's a good idea. &#xA;What#s missing for statistics in general is a training website. E.g. a set of databases, along with goals and possible results at the end.&#xA;Something like khancademy, but more powerful ;)" CreationDate="2014-06-13T14:07:52.810" UserId="791" />
  <row Id="1315" PostId="339" Score="0" Text="I am not sure if the number of libraries for R is a purely good thing. There are too many ways to accomplish a goal, which confuses beginners and advanced students alike (ok, how can I summarize my data? summary() or describe() or...)&#xA;Also, often packages have theor own styles, which leads to headaches when you are working with multiple ones. Third, the naming is atrochious, a software engineer would be flogged for methods like read.csv2 oder cut2..." CreationDate="2014-06-13T14:10:40.397" UserId="791" />
  <row Id="1316" PostId="361" Score="0" Text="I guess you meant, &quot;*Logic often states that by (over)underfitting a model, it's capacity to generalize is increased.*&quot;" CreationDate="2014-06-13T16:51:41.793" UserId="84" />
  <row Id="1317" PostId="361" Score="0" Text="@Rubens: Correct, thanks, updated the text!" CreationDate="2014-06-13T16:56:24.207" UserId="158" />
  <row Id="1318" PostId="360" Score="0" Text="+1 Thanks, as a result of your answer, I've posted a followup to the question above, &quot;[When is a Model Underfitted?](http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)&quot;" CreationDate="2014-06-13T16:59:09.830" UserId="158" />
  <row Id="1319" PostId="351" Score="0" Text="There are many people who mention that they are undergoing certification by doing a course on these MOOCs. You need to be careful with that." CreationDate="2014-06-13T17:02:53.243" UserId="735" />
  <row Id="1320" PostId="352" Score="1" Text="I am not familiar with this topic, but I spent in the past a lot of time studying graphs. I think that the property that an Euler diagram could be drawn is related with the planarity of the graph where the sets are the nodes. This paper seems to shade some light on this relation: [Ensuring the Drawability of Extended Euler Diagrams for up to 8 Sets](http://www.google.ie/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CDMQFjAC&amp;url=http://www-rocq.inria.fr/~verroust/diagrams04.pdf&amp;ei=eJKaU9nPAYXH7Ab7g4HQBQ&amp;usg=AFQjCNF_H6ptg8ApWGmrWed4TN6FIH5mAA&amp;sig2=5ZDb8tEOrWG1xv-O7ckn7w)." CreationDate="2014-06-13T06:04:40.787" UserId="108" />
  <row Id="1321" PostId="362" Score="0" Text="This works well in Oracle, but not on SQL Server (at least some older versions).  For some reason I see bad hash comparisons (false positives, unflagged mismatches).  Pulling the data into an external environment should alleviate that." CreationDate="2014-06-13T18:32:29.797" UserId="434" />
  <row Id="1322" PostId="365" Score="0" Text="Part of the question is meant to gauge whether or not the community placed value on certification.  In some areas, certification is an absolute necessity.  In others, certification doesn't matter at all.  In still others, certifications by a particular company are held in high regard and competitive certifications are not.  The other part was meant to understand the difference in topical focus of the certifications that are out there.  Data Science is a broad term.  Certifications are normally more focused.  This is a bad question for QA format - it's more of a discussion, subject to opinion." CreationDate="2014-06-14T03:26:47.133" UserId="434" />
  <row Id="1323" PostId="365" Score="0" Text="My purpose in noting that I chose the answer by votes was to make it plain that all of the answers deserved reading.  Everybody makes good points, including you way down here at the bottom.  Somebody who is wondering about these things shouldn't limit themselves to the top one or two answers." CreationDate="2014-06-14T03:29:39.203" UserId="434" />
  <row Id="1324" PostId="365" Score="0" Text="Voting to find the right answer is a horrible idea.  It is the wrong way to approach math.  You clearly missed my point." CreationDate="2014-06-14T04:04:26.623" UserId="386" />
  <row Id="1325" PostId="223" Score="0" Text="@user1893354 Very helpful! Specially the &quot;[brat standoff format](http://brat.nlplab.org/standoff.html)&quot; used by it seems very suitable to my needs. I suggest posting an answer if you like." CreationDate="2014-06-14T08:09:09.150" UserId="227" />
  <row Id="1326" PostId="50" Score="2" Text="This is off-topic and has probably already been answered a hundred times on more general computing forums. Flagged." CreationDate="2014-06-14T09:31:19.403" UserId="471" />
  <row Id="1327" PostId="379" Score="1" Text="Thank you !!! This is helpful.." CreationDate="2014-06-15T05:30:48.820" UserId="496" />
  <row Id="1330" PostId="388" Score="1" Text="+1 Thanks for the references. Do you have any small example, or could show just an intuition behind one of the approaches? I'm not acquainted to neural networks, but I can check it out if the example requires such knowledge base." CreationDate="2014-06-15T15:25:05.307" UserId="84" />
  <row Id="1331" PostId="388" Score="0" Text="Could you be more specific about your use case?  Strategy can vary wildly depending on how you tend to implement the solution." CreationDate="2014-06-15T20:26:30.203" UserId="780" />
  <row Id="1332" PostId="388" Score="0" Text="I mean, I just would like to see what is the *idea* behind a different approach. For example, if I was to tell you what is done by using blacklists (which we know is not good), I could describe the *algorithm* as: scan the dataset looking for entries containing &quot;viagra&quot;; add such entries to the blacklist. I just would like to see a high-level description of a *methodology/algorithm*. Do they gather spam network usage information and put on a neural network classifier, or what do they do?" CreationDate="2014-06-15T20:35:17.443" UserId="84" />
  <row Id="1333" PostId="305" Score="0" Text="@SeanOwen in my question, I was referring to Apache Hadoop. Although it would be interesting to make the Impala comparison as well." CreationDate="2014-06-16T03:46:01.843" UserId="534" />
  <row Id="1334" PostId="395" Score="0" Text="good, thanks! Though not sure yet if I can use R package &quot;randomForest&quot; (http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) to generate ERF. Probably not." CreationDate="2014-06-16T08:05:55.243" UserId="97" />
  <row Id="1335" PostId="399" Score="0" Text="I agree that I'm still only scratching the surface. That's mainly the result of being in the early stages of my project-design. (I had initially ruled out R since I was more interested in using a general purpose language)" CreationDate="2014-06-16T09:57:46.783" UserId="872" />
  <row Id="1336" PostId="61" Score="1" Text="Overfitting is bad by definition. If it weren't it wouldn't be *over*-fitting." CreationDate="2014-06-16T10:51:52.677" UserId="762" />
  <row Id="1337" PostId="351" Score="0" Text="@Kunal It makes sense but your answer jumps from the “certification” to “dropouts” (who presumably *don't have a certification*). The key here is *undergoing*. It's a bit like being registered as a student or having a Kaggle account. None of this tells us whether you should value someone who did actually get a degree, complete a course or participate in a competition to the end." CreationDate="2014-06-16T11:02:19.743" UserId="762" />
  <row Id="1338" PostId="400" Score="1" Text="Your point 2 is very important. Juggling with different tools means importing data. And data importing is an enormously error prone step and feels extremely unproductive - so avoid it, whereever you can." CreationDate="2014-06-16T13:25:03.337" UserId="791" />
  <row Id="1339" PostId="61" Score="0" Text="-1 for not being stated clearly. I would propose a fix, but the only ones I can think of would fundamentally change the meaning of the question. I think blunders might be conflating &quot;overfitting&quot; with &quot;adding model complexity&quot;." CreationDate="2014-06-16T14:06:19.127" UserId="675" />
  <row Id="1340" PostId="398" Score="9" Text="All these questions have been beaten to death on StackOverflow.  What value is there in rehashing it here?" CreationDate="2014-06-16T14:21:06.007" UserId="515" />
  <row Id="1341" PostId="404" Score="0" Text="Being a web developer, JSON seems completely reasonable to me, but, can you elaborate on the exact format of mapping words to entities?" CreationDate="2014-06-16T15:49:04.183" UserId="227" />
  <row Id="1342" PostId="61" Score="0" Text="@NathanGould: Thanks for commenting, though appears you're both inferring a meaning that is simply not present in the question, and quoting text that is also not present; meaning no where in the text are the words &quot;adding model complexity.&quot;" CreationDate="2014-06-16T15:57:44.980" UserId="158" />
  <row Id="1343" PostId="404" Score="0" Text="@AmirAliAkbari Updated answer to include more details." CreationDate="2014-06-16T17:35:52.767" UserId="548" />
  <row Id="1344" PostId="408" Score="1" Text="Another very useful package for forecasting and time series analysis is [forecast](http://cran.r-project.org/web/packages/forecast/index.html) by Prof. Rob J. Hyndman." CreationDate="2014-06-16T18:05:32.453" UserId="554" />
  <row Id="1345" PostId="409" Score="0" Text="Yes, visualization is an essential first step in any analysis." CreationDate="2014-06-16T18:42:16.387" UserId="178" />
  <row Id="1346" PostId="398" Score="0" Text="@DirkEddelbuettel Because they are off-topic on SO, hence often closed?" CreationDate="2014-06-16T18:56:18.513" UserId="843" />
  <row Id="1348" PostId="411" Score="11" Text="There is no question here.  If you need to do basic research on programming language, you are better off reading Wikipedia than to wait for someone to pop up here to push his hobby-horse." CreationDate="2014-06-16T20:05:09.750" UserId="515" />
  <row Id="1349" PostId="412" Score="0" Text="I added some tags." CreationDate="2014-06-16T20:07:44.737" UserId="381" />
  <row Id="1350" PostId="411" Score="0" Text="@DirkEddelbuettel Very good point. Thought it was better to try producing content than refining it at this point in the Beta, but I don't know a huge amount about SE betas. Was that a good move on my part or not?" CreationDate="2014-06-16T20:11:36.353" UserId="548" />
  <row Id="1351" PostId="61" Score="1" Text="I didn't mean to quote you on &quot;adding model complexity&quot; -- I was just highlighting the phrase. Anyhow I guess my issue is basically the same as @GaLa, which is that overfitting means fitting too much. So it seems you are asking us to confirm a tautology. So, I would tend to think that you actually meant to ask a different question. E.g., does increasing model complexity cause models to become worse? Or, how does complexity of the data relate to the tendency of a model to overfit?" CreationDate="2014-06-16T20:19:32.450" UserId="675" />
  <row Id="1352" PostId="411" Score="1" Text="Look at [these](http://economics.sas.upenn.edu/~jesusfv/comparison_languages.pdf) numbers." CreationDate="2014-06-16T20:20:29.817" UserId="381" />
  <row Id="1353" PostId="132" Score="0" Text="Both of these fall into the category of feature engineering as they involve manually creating or selecting features. Dimensionality reduction typically involves a change of basis or some other mathematical re-representation of the data" CreationDate="2014-06-16T21:05:47.767" UserId="890" />
  <row Id="1354" PostId="411" Score="0" Text="@DirkEddelbuettel you're not wrong, but my hope was to foster a discussion about the useful characteristics and tools associated with various languages. The language you use is an important tool in data science, so my thinking was that people could discuss the tools they preferred and there objective benefits here, as a resource for those looking to attempt similar work." CreationDate="2014-06-16T21:08:55.900" UserId="890" />
  <row Id="1355" PostId="411" Score="0" Text="This area is subject to such rapid change that it's probably best to strictly limit the scope of this question to &quot;what qualities make for the best tool&quot; rather than &quot;which tools exist and what are their qualities&quot; - even as a community wiki, the latter would require an intimidating amount of continued maintenance in order to be net useful" CreationDate="2014-06-16T21:54:22.960" UserId="322" />
  <row Id="1357" PostId="408" Score="0" Text="Do you know if this is already implemented in any other language?  I'm not exactly a pro with R.  I will definitely read the paper at least." CreationDate="2014-06-17T01:18:21.087" UserId="886" />
  <row Id="1358" PostId="409" Score="0" Text="I did add the month, day of month, day of week, and year as features.  I even tried a linearly decreasing &quot;Recentness&quot; value.  I don't think I have tried OLS.  I'm observing a time frame that could range anywhere from a couple weeks to multiple years.  As far as visualizing it goes, I did try to do that.  The problem is, we want the software to be able to predict automatically, without human intervention, for different customers." CreationDate="2014-06-17T01:24:29.210" UserId="886" />
  <row Id="1359" PostId="426" Score="0" Text="I am not 100% convinced by the topics on that course. For example, are SVMs actually of practical use these days? You never see a winning Kaggle entry that used SVMs as the main part." CreationDate="2014-06-17T07:45:32.770" UserId="910" />
  <row Id="1360" PostId="411" Score="0" Text="You need to learn either R  or Python properly and also be able to understand the other.  R has an unbeatable number of stats packages and nothing will ever be able to compare to that." CreationDate="2014-06-17T07:48:23.087" UserId="910" />
  <row Id="1361" PostId="405" Score="2" Text="R is unbeatable in terms of number of stats packages. So if you go for python you still need to learn enough R to be able to use rpy2 to call the missing packages." CreationDate="2014-06-17T08:07:24.300" UserId="910" />
  <row Id="1362" PostId="426" Score="2" Text="I think OP's question is specifically about *online* techniques - i.e. where system is expected to learn at least partially &quot;on the job&quot;. Not *online tutorials*" CreationDate="2014-06-17T08:40:15.450" UserId="836" />
  <row Id="1363" PostId="421" Score="2" Text="Could you clarify key aspects of &quot;online&quot; that you are interested in? Do you have a specific form for the data, or any options to pre-train your algorithm before the online part?" CreationDate="2014-06-17T08:41:49.577" UserId="836" />
  <row Id="1364" PostId="426" Score="0" Text="I agree with @NeilSlater since the OP mentioned &quot;compared to normal machine learning methods&quot;." CreationDate="2014-06-17T09:45:23.663" UserId="343" />
  <row Id="1365" PostId="426" Score="4" Text="lol, &quot;online&quot; is ambiguous" CreationDate="2014-06-17T10:19:41.480" UserId="122" />
  <row Id="1366" PostId="430" Score="0" Text="Did you make any research about this? There are many youtube videos and slideshare presentations describing different architectures" CreationDate="2014-06-17T10:53:46.840" UserId="478" />
  <row Id="1367" PostId="430" Score="1" Text="Hey Stanpol, thanks for your response - I did some initial searches and didn't really find anything besides AWS and cloudera stuff - maybe if you can give me some search terms that a promising, I'll be happy to take it from there." CreationDate="2014-06-17T11:28:41.010" UserId="913" />
  <row Id="1368" PostId="421" Score="0" Text="do you mean to analyze datastreams?" CreationDate="2014-06-17T11:42:23.613" UserId="115" />
  <row Id="1369" PostId="424" Score="0" Text="My guess is that this is based on learning topics in a large corpus." CreationDate="2014-06-17T12:49:06.043" UserId="241" />
  <row Id="1370" PostId="408" Score="0" Text="I am not familiar with one.  If you would like to use python, you can use the [rpy2](http://rpy.sourceforge.net/) package to call the glarma function while doing most of the rest of the programming in python.  Most other languages have such a connector as well." CreationDate="2014-06-17T12:59:46.167" UserId="178" />
  <row Id="1371" PostId="433" Score="0" Text="That's pretty awesome, exactly what I was looking for! Thanks a lot :)" CreationDate="2014-06-17T13:35:17.570" UserId="913" />
  <row Id="1372" PostId="433" Score="0" Text="@chrshmmmr You're welcome. Don't forget to upvote/mark as accepted if this helped!" CreationDate="2014-06-17T13:37:02.053" UserId="241" />
  <row Id="1373" PostId="433" Score="3" Text="These links seem very useful indeed, but then again, they are links, and I guess we should strive to maintain the answers independent of the stability of outer sources. Thus, it'd be nice if you could take some two or three minutes to add, for example, the diagram from [this link](http://blog.twitch.tv/2014/04/twitch-data-analysis-part-1-the-twitch-statistics-pipeline/), posting it along with a quick description. Something in the lines of: &quot;For example, this is the workflow of a ... system. &lt;img&gt;. Further info may be found in &lt;link&gt;.&quot;" CreationDate="2014-06-17T13:40:48.977" UserId="84" />
  <row Id="1374" PostId="427" Score="3" Text="There is no question here." CreationDate="2014-06-17T13:41:37.477" UserId="515" />
  <row Id="1375" PostId="433" Score="1" Text="@Rubens I will propose an edit in a bit.&#xA;fgnu: Will do so, just need a bit more reputation to actually upvote answers, but I certainly will honor your contribution :)" CreationDate="2014-06-17T13:42:47.237" UserId="913" />
  <row Id="1376" PostId="433" Score="0" Text="@Rubens That would be no more than reproducing the information at the link. I would if there were something I felt would add to the explanation already given there." CreationDate="2014-06-17T13:53:34.867" UserId="241" />
  <row Id="1377" PostId="433" Score="0" Text="The point is that the information would be here, instead. Of course, I'm not asking you to deliberately copy the whole of the references, but it'd be nice to have a small example in the answer itself, and not just in a link. I see this [*rule of thumb*](http://meta.stackexchange.com/questions/8231/are-answers-that-just-contain-links-elsewhere-really-good-answers) to be very praised in [stackoverflow](http://meta.stackoverflow.com/questions/251006/flagging-link-only-answers), but then again, we're not stackoverflow... I'll add a discussion about this on meta, anyway." CreationDate="2014-06-17T14:01:49.917" UserId="84" />
  <row Id="1378" PostId="411" Score="0" Text="@Lembik I appreciate the evangelism, but there isn't a single piece of statistical functionality that `R` has that `Python` doesn't. There are on the other hand, a large number of scientific and machine learning functions that `R` lacks, which are preset in `python`. Good `opencv` bindings are a great example." CreationDate="2014-06-17T14:45:46.360" UserId="548" />
  <row Id="1379" PostId="433" Score="0" Text="I've opened a discussion [here](http://meta.datascience.stackexchange.com/questions/63/flagging-link-only-answers). You're very welcome to participate. Thanks for raising this topic." CreationDate="2014-06-17T14:45:59.170" UserId="84" />
  <row Id="1380" PostId="411" Score="0" Text="@indico I am not sure if  I understand your point. There are literally thousands of packages in CRAN which don't exist for python. See http://cran.r-project.org/web/packages/available_packages_by_name.html ." CreationDate="2014-06-17T14:54:54.170" UserId="910" />
  <row Id="1381" PostId="411" Score="0" Text="@Lembik packages yes, but I am referring to functionality. The vast majority of the statistical functionality present in those `R` packages is contained simply within `scipy`" CreationDate="2014-06-17T14:56:48.123" UserId="548" />
  <row Id="1382" PostId="411" Score="0" Text="@indico This simply isn't true I am afraid.  People who do PhDs in stats, for example, still routinely write an R package as part of that. These very very rarely get implemented in scipy. How many new stats functions does scipy add every year? Try just picking a few of the R packages at random to see what I mean." CreationDate="2014-06-17T14:58:16.727" UserId="910" />
  <row Id="1383" PostId="405" Score="1" Text="@Lembik I would disagree with both of those points actually. As someone who has very frequently used both `R` and `python`, I've never had the need to use `rpy2`, and even though `R` wins in pure number of packages, `python` has decidedly more functionality across data science." CreationDate="2014-06-17T14:58:30.103" UserId="548" />
  <row Id="1384" PostId="411" Score="0" Text="@Lembik I don't really want to argue on this one. Could you please provide an example? `scipy` adds a huge amount of statistical functions every year, not really keeping track I would guess a few hundred each year." CreationDate="2014-06-17T15:00:20.027" UserId="548" />
  <row Id="1385" PostId="411" Score="0" Text="@indico Try http://cran.r-project.org/web/packages/overlap/index.html which is just the first one I happened to pick at random.  But really, I have personally known many statisticians who have written R packages. Not one of them has yet written a python one.  To broaden the conversation a little, http://www.kdnuggets.com/2013/08/languages-for-analytics-data-mining-data-science.html is interesting." CreationDate="2014-06-17T15:08:22.267" UserId="910" />
  <row Id="1386" PostId="411" Score="0" Text="@Lembik and my rebuttal in the form of a blog post: http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/" CreationDate="2014-06-17T15:11:36.837" UserId="548" />
  <row Id="1387" PostId="431" Score="0" Text="I thank you very much for posting this reference, but I was expecting the answers here to point a publicly available dataset/API for social network, *andalso* describe what is provided by such source (either the download rate of posts, or what kind of information about users). As your answer is, I guess it would be very welcome to the list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets) we have." CreationDate="2014-06-17T15:12:51.027" UserId="84" />
  <row Id="1388" PostId="411" Score="0" Text="@indico Oh I read that before. If you are the author of that blog I greatly admire it. However it is in no way a rebuttal to the fact that there are thousands of R packages written by statisticians which have no python equivalent :)" CreationDate="2014-06-17T15:13:00.223" UserId="910" />
  <row Id="1389" PostId="411" Score="0" Text="@Lembik I'm not trying to argue about a direct 1-to-1 correlation here. I'm trying to argue that in terms of raw functionality, here referring to kernel density estimation, there is nothing fundamentally missing from `python`, if we're going down the missing functionality line, there are an entire order of magnitude more python packages, and bindings to amazing academic projects such as opencv, which do not exist in R" CreationDate="2014-06-17T15:15:23.390" UserId="548" />
  <row Id="1390" PostId="411" Score="0" Text="@indico  You are right in terms of non stats packages. My comment was only meant to refer to statistics.  You may also be right about kernel density estimation although it looked like the overlap package had more in it than your blog post referred to. It would perhaps be interesting for someone to go through the thousands of R packages and see what is missing from python but I still claim that the fact is that PhD students from the best stats departments write R packages for their work and put them on CRAN. So if you have an interest in cutting edge stats you need to use those packages." CreationDate="2014-06-17T15:21:57.660" UserId="910" />
  <row Id="1391" PostId="411" Score="0" Text="@indico Just picking some more at random which have the word &quot;kernel&quot; in them. See http://cran.r-project.org/web/packages/bark/index.html , http://cran.r-project.org/web/packages/bbefkr/index.html , http://cran.r-project.org/web/packages/bpkde/index.html , http://cran.r-project.org/web/packages/DBKGrad/DBKGrad.pdf" CreationDate="2014-06-17T15:25:56.273" UserId="910" />
  <row Id="1392" PostId="405" Score="0" Text="I think that may be right for data science in general although I would argue that for cutting edge stats you still need to use R packages." CreationDate="2014-06-17T15:27:51.723" UserId="910" />
  <row Id="1393" PostId="405" Score="3" Text="@indico then you must be using the definition of &quot;Data Science&quot; that excludes statistics." CreationDate="2014-06-17T16:15:13.170" UserId="539" />
  <row Id="1394" PostId="416" Score="0" Text="We do quite a bit of processing already to link records in this way. I made some edits to the question, hopefully making it more clear what I'm after." CreationDate="2014-06-17T17:08:46.563" UserId="322" />
  <row Id="1395" PostId="405" Score="0" Text="I've accepted this answer since it gives the most direct answer to my question; not because it is the one true answer (read: check the other answers as well)" CreationDate="2014-06-17T17:30:31.490" UserId="872" />
  <row Id="1399" PostId="439" Score="0" Text="What interests me about the paper I linked is that it claims to show a method for performing this sort of computation *without knowledge of both input strings*. In the paper, each actor has knowledge of *one* string, which isn't useful for my purposes; I would need one actor to be able to perform the calculation without knowledge of *either* string. Calculating them beforehand is only feasible for very small datasets or very limited products; a full cross product of integer distances on my dataset would take ~10 PB of storage." CreationDate="2014-06-17T19:44:59.190" UserId="322" />
  <row Id="1400" PostId="439" Score="0" Text="That's why I brought up the idea of a substitution cipher (ROT13) since it preserves the distance between strings; but it's not secure, and I suspect it may be impossible to securely encrypt the strings while preserving the edit distance. (Would love to be wrong!)" CreationDate="2014-06-17T19:48:14.150" UserId="322" />
  <row Id="1401" PostId="416" Score="0" Text="Appreciate the k-anonymity reference, and the bin suggestion - that gives me some new things to think about." CreationDate="2014-06-17T19:57:32.907" UserId="322" />
  <row Id="1402" PostId="439" Score="0" Text="Right, I would just filter the matrix to only include Levenshteins below a certain cutoff, so you are only populating where there is high likelihood of overlap.  Additionally, when it comes to PII I am of the mindset that if you include enough information to determine a relationship among disparate entities in your datasets, its very unlikely you are preserving the customers anonymity.  The point of anonymizing the data is to avoid potential PII related regulatory headaches down that line, (standards can always be tightened), so personally I wouldn't take the risk." CreationDate="2014-06-17T20:27:56.733" UserId="780" />
  <row Id="1403" PostId="303" Score="0" Text="Maybe so.  As I said, this isn't my area.  Did you try going through all the steps in the Tutorials and also the &quot;Training&quot; section?  It appears that they take you through all the steps, starting with source texts.  Best wishes." CreationDate="2014-06-17T22:14:55.200" UserId="609" />
  <row Id="1404" PostId="303" Score="0" Text="it's the steps to get word alignments not phrases..." CreationDate="2014-06-17T22:30:15.820" UserId="122" />
  <row Id="1405" PostId="303" Score="0" Text="My impression was that the Tutorials and Training sections take you through all these steps, including phrase alignment.  I suggest going through all of them and you might discover which stage and tool accomplishes this task." CreationDate="2014-06-17T22:42:58.523" UserId="609" />
  <row Id="1406" PostId="449" Score="0" Text="thanks for the clarification. Keeping data in memory sounds like it has some interesting implications -I'll read up on Spark's Resilient Distributed Dataset concept a bit more." CreationDate="2014-06-18T10:30:55.527" UserId="426" />
  <row Id="1407" PostId="445" Score="0" Text="This is a pretty good idea. The Masters is in Statistics." CreationDate="2014-06-18T12:30:41.317" UserId="839" />
  <row Id="1408" PostId="453" Score="1" Text="Can you link to/reference where you read that the unary method is not universal? Context may help." CreationDate="2014-06-18T16:06:03.520" UserId="322" />
  <row Id="1409" PostId="453" Score="2" Text="I... am not sure how this relates to data science. It seems off-topic for this stack exchange. Could you possibly relate this back to data science?" CreationDate="2014-06-18T16:14:01.900" UserId="869" />
  <row Id="1410" PostId="453" Score="0" Text="@SlaterTyranus I... am not sure too (and that made me think about some two other questions I posted). My idea was to add this question since compression methods are largely used in information retrieval (mainly during indexing). In general, I find this related to efficiency, and it may be put in the *hacking skills* area of this [Venn diagram](http://en.wikipedia.org/wiki/Data_science). Anyway, I guess it'd be nice to discuss whether this kind of question is on topic." CreationDate="2014-06-18T16:33:25.077" UserId="84" />
  <row Id="1411" PostId="455" Score="1" Text="I think more details about your problem are necessary before anyone could recommend a dataset." CreationDate="2014-06-18T16:40:58.183" UserId="836" />
  <row Id="1412" PostId="451" Score="1" Text="how about some compression on the messages?" CreationDate="2014-06-18T17:54:09.643" UserId="743" />
  <row Id="1413" PostId="451" Score="0" Text="@user798196 That's one of the answers I was expecting. I'm just not sure about why it came as just a comment :D" CreationDate="2014-06-18T17:55:37.270" UserId="84" />
  <row Id="1414" PostId="453" Score="0" Text="@Rubens That seems like a reasonable discussion, in my mind efficiency talk fits much more into something like theoretical CS than explicit *hacking skills*. In my mind, hacking skills are much more related to things like databases, deployment, and knowledge of tools." CreationDate="2014-06-18T18:08:48.673" UserId="869" />
  <row Id="1415" PostId="451" Score="0" Text="I would totally suggest using zmq, what are you using for your message passing layer right now?" CreationDate="2014-06-18T18:10:56.937" UserId="869" />
  <row Id="1416" PostId="451" Score="0" Text="@SlaterTyranus I'm using a platform built on OpenMPI, but it does not perform message compression. Does zeromq do so? In fact, my intention here was to bring up such concepts (caching, compression, aggregation), and I was expecting them to come as answers. I'm also adding this question to a meta post, to discuss whether it is on topic." CreationDate="2014-06-18T18:22:29.890" UserId="84" />
  <row Id="1417" PostId="451" Score="1" Text="@Rubens They considered adding it, but decided it didn't make sense for a couple reasons. Discussion here: https://github.com/JustinTulloss/zeromq.node/issues/285" CreationDate="2014-06-18T18:26:11.227" UserId="869" />
  <row Id="1418" PostId="451" Score="0" Text="Also, @indico totally beat me to it. OpenMPI is very good for parallelism, but generally not optimal for actual distributed systems. My impression is that message compression adds more latency than it prevents." CreationDate="2014-06-18T18:27:44.977" UserId="869" />
  <row Id="1419" PostId="455" Score="3" Text="For what purpose? Spam filtering? Sentiment analysis? Without a clear purpose it is *very* difficult to suggest a dataset." CreationDate="2014-06-18T18:37:12.887" UserId="553" />
  <row Id="1420" PostId="453" Score="0" Text="@SlaterTyranus Hope you join the discussion [here](http://meta.datascience.stackexchange.com/questions/65/are-hacking-skills-questions-off-topic). I've tried to point what I consider to be *hacking skills*. Although I understand theoretical CS to be &quot;the place for algorithms&quot;, there are some *pratical* concepts that may would be nice to be added to this site." CreationDate="2014-06-18T18:41:55.673" UserId="84" />
  <row Id="1421" PostId="457" Score="0" Text="That's a very nice suggestion, thanks. I'm actually using a framework implemented upon OpenMPI, and therefore I won't be able to make such changes -- although they seem to be a very promissing improvement. Btw, I very much enjoyed the citation :D" CreationDate="2014-06-18T18:50:20.903" UserId="84" />
  <row Id="1422" PostId="452" Score="0" Text="Could you elaborate on how you would use this feature matrix?  Are you trying to learn based on change of traffic across days?" CreationDate="2014-06-18T19:19:17.557" UserId="886" />
  <row Id="1423" PostId="352" Score="2" Text="I'm assuming that by &quot;simple&quot; you mean the diagram uses only circles and that by &quot;proportional&quot; you mean that the area of each section of the diagram is proportional to the population it represents from the total set. It might help to make these definitions explicit in the question." CreationDate="2014-06-18T23:04:18.013" UserId="322" />
  <row Id="1424" PostId="466" Score="2" Text="What do you mean, bad results? Describe your process, your results, and how they differ from what you expected, instead of only linking to the git repository. Otherwise this discussion will be of no use to anyone." CreationDate="2014-06-18T23:08:41.183" UserId="322" />
  <row Id="1425" PostId="466" Score="0" Text="It's also true this :D. I added the description in the page &quot;The results has a 14.14 RMSE, so it can't predict so well the gas consumptions, consecutevely I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&quot;" CreationDate="2014-06-18T23:25:19.710" UserId="989" />
  <row Id="1426" PostId="466" Score="1" Text="@marcodena This is a QA site, and others need to know what you're trying to solve, so that they'll understand the answers, and will hopefully be able to use them in their own problems. That's what AirThomas meant, and is also why it'd be nice if you could describe what you're doing and what exactly you think is wrong. If the link to your git-hub page changes, the link here will be invalid, and others won't be able to understand what the problem is. Please, take a minute to make your question self-contained. Thanks." CreationDate="2014-06-19T00:35:40.470" UserId="84" />
  <row Id="1427" PostId="466" Score="0" Text="@Rubens you are right, but as you see in the github link, the description is more than 6 A4 pages long, so here it would be very difficult to explain everything. I'll try." CreationDate="2014-06-19T08:50:22.207" UserId="989" />
  <row Id="1429" PostId="477" Score="4" Text="This question appears to cross posted across multiple sites.  While it may be on topic here, it is also on topic (and has gained an answer) on Data Science.SE.  Questions should exist in one place only unless there are extenuating circumstances." CreationDate="2014-06-19T02:43:14.847" UserDisplayName="MichaelT" />
  <row Id="1430" PostId="477" Score="0" Text="As a computing problem it's hard to even make a start without some idea of scale and the success criteria. Is a cross-product possible? Should the answer be optimal, or just good enough? More info please." CreationDate="2014-06-19T06:30:51.003" UserDisplayName="david.pfx" />
  <row Id="1432" PostId="477" Score="0" Text="You need to define the question better, e.g., what is &quot;better suit their needs&quot;" CreationDate="2014-06-19T11:01:24.493" UserId="743" />
  <row Id="1434" PostId="61" Score="0" Text="@NathanGould: I think you're over thinking the question, or somehow expecting that I would ask a question that on a topic that I completely understood. Every single reference by me to complexity within the question related to the data being modeled, not the complexity of the model." CreationDate="2014-06-19T11:50:57.280" UserId="158" />
  <row Id="1437" PostId="468" Score="0" Text="I suggest you add the R tag as well." CreationDate="2014-06-19T13:28:55.910" UserId="178" />
  <row Id="1438" PostId="452" Score="0" Text="I edited the response to hopefully give more clarity." CreationDate="2014-06-19T14:32:56.907" UserId="403" />
  <row Id="1440" PostId="485" Score="0" Text="Your answer is very nice, and nice also would that be if you added a little example, so as to emphasize your point on data mining being more related to *detecting something new* rather than trying to *solve and reach results*." CreationDate="2014-06-19T16:54:42.093" UserId="84" />
  <row Id="1441" PostId="474" Score="1" Text="In what context did you see these terms? It seems that `p` and `k` simply show the size of the subgraph, not a special category of them." CreationDate="2014-06-19T17:07:13.287" UserId="227" />
  <row Id="1442" PostId="382" Score="0" Text="Great question! I think this is an important and non-trivial problem." CreationDate="2014-06-19T17:24:09.717" UserId="1011" />
  <row Id="1443" PostId="455" Score="0" Text="@lsdr Looking at the answers, it seems that the question does not necessarily need more details." CreationDate="2014-06-19T17:25:47.420" UserId="227" />
  <row Id="1445" PostId="455" Score="0" Text="@AmirAliAkbari I think they came after an edit. I retracted my close-vote, anyway." CreationDate="2014-06-19T17:30:36.410" UserId="84" />
  <row Id="1447" PostId="403" Score="0" Text="I don't know much about SAS, but your math is clearly right. Perhaps it's a bug in how you called the model predicted output?" CreationDate="2014-06-19T17:49:57.240" UserId="1011" />
  <row Id="1448" PostId="172" Score="0" Text="How large are your list of values ? Have you tried to pass it as a set ? For parallelism, you may be interested in Joblib. It is easy to use and can speed up computations. Use it with large chunks of data." CreationDate="2014-06-19T18:22:28.557" UserId="1023" />
  <row Id="1449" PostId="489" Score="0" Text="Many thanks for your answer." CreationDate="2014-06-19T18:22:56.117" UserId="1021" />
  <row Id="1451" PostId="61" Score="0" Text="@blunders Maybe the question should simply be “What is overfitting?” or “How can a model fitting the data more precisely be worse?”" CreationDate="2014-06-19T20:27:23.540" UserId="762" />
  <row Id="1452" PostId="492" Score="0" Text="Have you tried training the Stanford NER on your corpus? There's a tutorial [here](http://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials:automaticannotation:stanford_ner)." CreationDate="2014-06-19T20:58:53.030" UserId="381" />
  <row Id="1454" PostId="488" Score="1" Text="I think machine learning is usually an application of statistical modelling, so I'd say it is both." CreationDate="2014-06-19T21:08:37.097" UserId="922" />
  <row Id="1455" PostId="492" Score="0" Text="I have not -- should give that a go to see how it fares." CreationDate="2014-06-19T21:30:42.020" UserId="684" />
  <row Id="1456" PostId="475" Score="0" Text="Logistic regression can be used as a binary classifier, but it isn't inherently one.  You could be using it to estimate odds or determine the relationship of a predictor variable to the outcome." CreationDate="2014-06-19T21:32:33.223" UserId="953" />
  <row Id="1457" PostId="492" Score="0" Text="I'd like to use word2vec features or similar, though, since I have access to a relatively small labeled dataset and need to make the most of unlabeled data I have on hand." CreationDate="2014-06-19T21:37:43.327" UserId="684" />
  <row Id="1459" PostId="61" Score="0" Text="@GaLa: Thanks for the feedback, though I wanted to know why overfitting is bad, not what overfitting is, or how fitting the model to the data more precisely is bad. Beyond that, at this point, given there are 3 answers, and 40+ votes, I do not feel that it would either be fair or a good idea to change the meaning and/or request made by the question. If you believe that the question might be better expressed, my suggestion would be just to post a question yourself; please feel free to link to it in the comments here. Again, thanks!" CreationDate="2014-06-19T21:56:03.637" UserId="158" />
  <row Id="1460" PostId="466" Score="1" Text="When you find that your problem takes a very long time to explain, that is when it's *most important* to spend the time to explain your question to others, explicitly and with plenty of details and discussion of your research/attempts. Often during that process you will find some or all of the answers yourself. Not only is that a great feeling, if what you find is useful to others, you can still post that question you spend so much time on, *and* the answer(s) you came up with." CreationDate="2014-06-19T23:53:47.010" UserId="322" />
  <row Id="1461" PostId="474" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:27:44.880" UserId="957" />
  <row Id="1462" PostId="481" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:29:46.337" UserId="957" />
  <row Id="1463" PostId="495" Score="0" Text="Thanks for the input. I have taken a very similar approach. So, its good that I am proceeding in the right direction. I am going to keep the question open for a few more days to see if anyone comes up with other ideas or more complex analysis techniques." CreationDate="2014-06-20T02:56:48.260" UserId="1028" />
  <row Id="1464" PostId="495" Score="2" Text="That's very reasonable and I will be curious to read other peoples' approaches to this problem. I would just add that the goal of good data science should be to use data to answer the question at hand, not to concoct the most complex analysis technique." CreationDate="2014-06-20T03:56:30.570" UserId="1011" />
  <row Id="1465" PostId="202" Score="0" Text="We home/hope the same about tex support! :D" CreationDate="2014-06-20T06:15:57.220" UserId="84" />
  <row Id="1466" PostId="191" Score="0" Text="Please, add information/reference link on that *MNIST data*, so as to make your post self-contained. Thanks." CreationDate="2014-06-20T06:19:47.490" UserId="84" />
  <row Id="1467" PostId="500" Score="0" Text="Using that logic neural networks are statistical models since the architecture is decided in advance. I don't think attempts to define a clear cut between statistics and machine learning are possible nor necessary." CreationDate="2014-06-20T06:54:45.653" UserId="119" />
  <row Id="1468" PostId="415" Score="0" Text="To add to this answer: in terms of scalability, `Scala` and `Go` are worth mentioning." CreationDate="2014-06-20T07:17:43.250" UserId="119" />
  <row Id="1469" PostId="61" Score="0" Text="@blunders We are back to my original comment but it seems to me that if you know what overfitting is, you know why it's bad. Since you seemed to suggest earlier you didn't fully understand the topic (which is fine), I would think the first thing to ask is simply what overfitting is." CreationDate="2014-06-20T07:37:44.590" UserId="762" />
  <row Id="1470" PostId="61" Score="0" Text="Also, the answers seem quite messy, perhaps because the question was so confusing to begin with but some of them do seem to address it in the way I suggest." CreationDate="2014-06-20T07:39:37.753" UserId="762" />
  <row Id="1471" PostId="449" Score="2" Text="+1 for a really clear and useful answer for a lot of people who had this question, like me." CreationDate="2014-06-20T09:20:30.793" UserId="113" />
  <row Id="1473" PostId="500" Score="0" Text="This is exactly the reason why I have mentioned the word 'rarely' in machine learning paragraph. I haven't said that you absolutely don't! Well, to the people who start exploring these things, it's good to know the nuances between statistical learning and machine learning" CreationDate="2014-06-20T10:17:59.630" UserId="514" />
  <row Id="1474" PostId="61" Score="0" Text="@GaLa: If you want to ask your own question, as stated above, please do. If you have any issues with the answers, please address them within the comments to the answer. The top answer, which I selected when it had I believe one or two votes, now has 18+ votes, and the question received a number of valid answers; meaning nothing to see, moving on." CreationDate="2014-06-20T11:49:26.910" UserId="158" />
  <row Id="1475" PostId="481" Score="0" Text="an n-clique is something else altogether from a k-clique or a p-clique. The n-clique refers to a maximal distance between two subgroups, where a k-clique refers to a clique of size k, where k is some constant chosen, and a p-clique is simply a clique composed of p nodes again where p is a chosen constant" CreationDate="2014-06-20T11:50:47.167" UserId="59" />
  <row Id="1476" PostId="463" Score="0" Text="Wright. The main issue here is the number of dimensions, i.e the level of detail I need to use. Could you clarify to me  “IR approach”." CreationDate="2014-06-20T14:49:06.473" UserId="986" />
  <row Id="1478" PostId="502" Score="0" Text="Thank you very much for the answer and references." CreationDate="2014-06-20T15:02:47.023" UserId="1021" />
  <row Id="1479" PostId="500" Score="0" Text="Thanks for the further clarifications!" CreationDate="2014-06-20T15:03:33.823" UserId="1021" />
  <row Id="1481" PostId="442" Score="0" Text="Another vote for dimensionality reduction. Just some additions: `Principal Component Analysis` or `Non-Negative Matrix Factorization` will reduce number of variables, enrich sparse data, and transform all variables to quantitative. Moreover, evaluating quality of dimensionality reduction model, question author can estimate usefulness of textual variables." CreationDate="2014-06-20T16:13:35.127" UserId="941" />
  <row Id="1482" PostId="463" Score="1" Text="By IR, I meant Information Retrieval. You might think that the documents (sellers) in your collection and the query (a buyer) are all vectors embedded in a term (attribute) space. As I said, such an approach needs a preset number of dimensions to work with." CreationDate="2014-06-20T16:38:49.233" UserId="984" />
  <row Id="1483" PostId="510" Score="1" Text="Sheer awesomeness! I was actually expecting something like this dissolved into many answers, and you came carrying the whole :D Thanks for the answer. Nice job! :)" CreationDate="2014-06-20T18:08:24.743" UserId="84" />
  <row Id="1485" PostId="504" Score="0" Text="Can you clarify you question by defining the goals of this analysis?  What is the nature of the 10 to 15 categories?  Are these categories you define a priori or are they clusters suggested by the data itself?  I appears that your question is centered on the choosing a good data encoding/transformation process rather than on data analysis methods (e.g. discriminant analysis, classification)." CreationDate="2014-06-20T18:25:53.873" UserId="609" />
  <row Id="1486" PostId="511" Score="1" Text="I guess a prior question is: doesn't this selection depend on the algorithm (mainly its behavior) being evaluated?" CreationDate="2014-06-20T18:28:44.200" UserId="84" />
  <row Id="1487" PostId="504" Score="1" Text="If your documents range from single words to full page of text, and you aim to have any combination of document lengths/types in any category, then you'll need to use a very simple encoding method such as Bag of Words.  Anything more complicated (e.g. grammar style) won't scale across that range." CreationDate="2014-06-20T18:29:26.120" UserId="609" />
  <row Id="1488" PostId="511" Score="1" Text="@Rubens, I've updated the question: I'm intersted in RF and logistic regression" CreationDate="2014-06-20T18:34:39.000" UserId="97" />
  <row Id="1489" PostId="510" Score="1" Text="I left behind the scope LinkedIn, YouTube, Secret. Maybe other regional networks (QQ?). And would be glad to get any info about them." CreationDate="2014-06-20T18:36:03.220" UserId="941" />
  <row Id="1491" PostId="403" Score="0" Text="Ben, thanks for the reply. I called the Scoring node directly from SAS Enterprise Miner, not much options we can change there. I compared the scoring output with what R predicted, and they perfectly matched. Then used the estimates manually , R's estimates theoretically matched with the output. It's just the problem with what SAS is giving as output, I am unable to understand that." CreationDate="2014-06-20T18:45:56.357" UserId="880" />
  <row Id="1492" PostId="509" Score="0" Text="No, it has nothing to do with spatial data in particular. It's applicable to temporal, spatio-temporal, and other sorts of data too." CreationDate="2014-06-21T06:10:06.757" UserId="381" />
  <row Id="1493" PostId="516" Score="3" Text="What is this algorithm *fmincg*? Please, add some references to such algorithm in your question." CreationDate="2014-06-21T06:51:15.523" UserId="84" />
  <row Id="1494" PostId="61" Score="0" Text="@blunders I just think this question is a bit of a mess and since the comments seemed to confuse you, I tried to explain them a bit." CreationDate="2014-06-21T09:17:29.030" UserId="762" />
  <row Id="1495" PostId="464" Score="0" Text="Thanks :), I already visited it before but I found it's classifications are weak not abstract enough or it may be not related to my content" CreationDate="2014-06-21T10:25:19.050" UserId="960" />
  <row Id="1496" PostId="466" Score="1" Text="Just a clarification, when you mention that &quot;in some papers  they have errors like MSE = 0.01&quot;, do you refer to the same dataset you are using? Or is it a different dataset altogether?" CreationDate="2014-06-21T17:29:42.433" UserId="1085" />
  <row Id="1497" PostId="507" Score="0" Text="@1 for crazy marketing guys. Working in market research and the twisting done to poor statistics makes me sad..." CreationDate="2014-06-22T05:18:22.043" UserId="791" />
  <row Id="1498" PostId="389" Score="0" Text="Wouldn't that also mean that my *bug* is stable across subsamples?" CreationDate="2014-06-22T06:13:51.587" UserId="846" />
  <row Id="1499" PostId="516" Score="0" Text="What is your alternate cost function? The optimal optimization algorithm depends on it :)" CreationDate="2014-06-22T12:03:45.373" UserId="1061" />
  <row Id="1500" PostId="403" Score="0" Text="Well I'm glad you were able to get a reasonable result with R! Hopefully someone who knows why this issue exists in SAS can answer the question!" CreationDate="2014-06-22T13:04:26.187" UserId="1011" />
  <row Id="1501" PostId="518" Score="0" Text="Your question is very weak, since you are neither presenting any problem, nor trying to understand something specific. Asking for comparisons between [online/batch k-means](http://datascience.stackexchange.com/questions/458/k-means-vs-online-k-means), or for how to efficiently implement some part of the algorithm is ok; asking for simple descriptions is too broad -- a google search may be a better fit." CreationDate="2014-06-22T16:56:52.783" UserId="84" />
  <row Id="1502" PostId="518" Score="0" Text="@Rubens I'm asking for a recommendation so what's wrong about that ?, I already googled it but I couldn't find something useful." CreationDate="2014-06-22T17:27:57.603" UserId="960" />
  <row Id="1503" PostId="389" Score="0" Text="That is a possible outcome, but you'll only knew once you try. And if so, you could at least debug on smaller data sets." CreationDate="2014-06-22T18:15:10.680" UserId="515" />
  <row Id="1505" PostId="527" Score="2" Text="It might be easier to answer your question if you gave an example of a matrix that you would consider similar to the first, and explained what qualities you are looking for in terms of similarity. Or if there is a general goal here, what is the task you mean to accomplish?" CreationDate="2014-06-23T02:25:12.900" UserId="322" />
  <row Id="1506" PostId="466" Score="0" Text="@AirThomas that's why I created a repository with a full explanation, with graphs etc." CreationDate="2014-06-23T09:38:09.920" UserId="989" />
  <row Id="1507" PostId="466" Score="0" Text="@insys no.. different :)" CreationDate="2014-06-23T09:38:25.643" UserId="989" />
  <row Id="1508" PostId="415" Score="0" Text="I would add *clarity and brevity* (related to syntax and language architecture, but not only). Being able to write fast and read without pain makes a huge difference (as programmers time is more expensive than machine time)." CreationDate="2014-06-23T11:14:14.553" UserId="289" />
  <row Id="1509" PostId="536" Score="1" Text="The question you are asking is vague. Please correct me if I'm wrong but the question essentially sums up to &quot;what to do if I have a small amount of data entries that need processing?&quot;. There are about a gazillion different answers to that depending on an equally large number of factors. You might at least present an example or make your question more specific. Things to consider: - What kind of data/fixes/errors are we talking about? -What prevents you from using a script exactly? -What are the &quot;unintended consequences&quot; you mention?" CreationDate="2014-06-23T11:15:13.220" UserId="1085" />
  <row Id="1510" PostId="536" Score="0" Text="@insys I gave examples (with city names) and modified question to address your doubts." CreationDate="2014-06-23T11:21:30.430" UserId="289" />
  <row Id="1511" PostId="536" Score="1" Text="About your phrase &quot;is not 100% clean&quot;: get used to it - it will always be like that :) Other than that, you have multiple questions in one. For example, named entities (cities) disambiguation reserves to be a question on its own. Overall there is no best practice, there's a whole field called data cleaning..." CreationDate="2014-06-23T11:29:16.800" UserId="418" />
  <row Id="1513" PostId="536" Score="0" Text="@iliasfl It's not my first day (but where data is far from being clear manual fixes usually make no sense). Point of this question is not city disambiguation (which is a fascinating topic on its own, and depends on a lot of things). It is how to incorporate manual changes (of, say, a few entries) into the workflow." CreationDate="2014-06-23T12:36:51.537" UserId="289" />
  <row Id="1514" PostId="483" Score="0" Text="I was looking into Neural Networks as an option, but didn't know what kind of parameters I would use.  I'll have to give those a shot." CreationDate="2014-06-23T12:54:17.227" UserId="886" />
  <row Id="1516" PostId="408" Score="0" Text="I will look into this option. Thanks." CreationDate="2014-06-23T12:59:31.563" UserId="886" />
  <row Id="1520" PostId="533" Score="0" Text="Adjusting the threshold for logistic regression did the trick. Thanks for the list of sources." CreationDate="2014-06-23T15:13:12.467" UserId="793" />
  <row Id="1521" PostId="538" Score="3" Text="off topic - discussion, opinion, and fist-fights will result." CreationDate="2014-06-23T15:15:53.153" UserId="471" />
  <row Id="1522" PostId="538" Score="2" Text="['To prevent your question from being flagged and possibly removed, avoid asking subjective questions where every answer is equally valid: “What’s your favorite ______?”'](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-06-23T15:29:06.037" UserId="322" />
  <row Id="1524" PostId="540" Score="4" Text="This is too broad - please read [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask) in the help center." CreationDate="2014-06-23T16:58:38.603" UserId="322" />
  <row Id="1525" PostId="541" Score="0" Text="Thanks, but you answered a different question. Mine is on workflow, not - science. I mean, in this particular case my goal is not making numerical estimations (then removing 'exceptions' by hand, or any other form of censoring, needs serious justification; and even with it is is easy to delude oneself or others), but rather (say) modifying data for visualization so end-user don't see misspelled cities (or worse: data split between copies of the same city, one with misspelled name). In any case you are right that explicit documentation of changes is crucial." CreationDate="2014-06-23T17:15:46.740" UserId="289" />
  <row Id="1526" PostId="541" Score="0" Text="My point is that the same principle applies to *how* you change the data, even if only how it is displayed. You need to make clear to the end user that some data has been modified. If what you are asking is exactly how one should modify their data, there is no good answer that will apply to every circumstance; it is entirely dependent on your use case, your software, your data structure, etc. etc. etc." CreationDate="2014-06-23T17:25:05.013" UserId="322" />
  <row Id="1527" PostId="540" Score="0" Text="Depends what type of dataset it is and more importantly what do you want to do with it? Ask yourself these questions... i) what is my objective; ii) if i'm going to train a model, will it be supervised or unsupervised?; iii) if supervised, do i have a train-test split? iv) do i have the reference class labels? etc." CreationDate="2014-06-23T17:28:50.353" UserId="984" />
  <row Id="1530" PostId="497" Score="0" Text="There is a whole class of statistical analytics devoted to modeling longitudinal data. If you had repeated measures on the same subjects then mixed models are used often as state of the art in social sciences to determine if there is impact of an intervention. If you have a time series only something like an Arima can be used." CreationDate="2014-06-23T18:14:57.430" UserId="1138" />
  <row Id="1532" PostId="527" Score="0" Text="Yeah, I'd like to see an example of what a 1 would look like and what a 0 would look like." CreationDate="2014-06-23T18:58:14.737" UserId="1011" />
  <row Id="1533" PostId="546" Score="3" Text="And you can formalize this and make it &quot;prettier&quot; by constructing in your script a set of known cities and an `N:1` mapping of known misspellings." CreationDate="2014-06-23T19:11:23.767" UserId="322" />
  <row Id="1534" PostId="497" Score="0" Text="A RDD approach might also be useful for you: http://austinclemens.com/blog/2014/06/08/436/" CreationDate="2014-06-23T19:30:58.427" UserId="1138" />
  <row Id="1535" PostId="549" Score="0" Text="My concern with this answer is that PCA doesn't recognize the clear dependency between the series t and t+1." CreationDate="2014-06-24T01:50:57.443" UserId="1138" />
  <row Id="1541" PostId="559" Score="1" Text="Perhaps this question is better suited to the cross validation SE site, now that I think of it. The distinction is somewhat unclear to me..." CreationDate="2014-06-24T12:36:56.940" UserId="1147" />
  <row Id="1542" PostId="559" Score="4" Text="I think the question is very much fitting to this site, since it discusses a practical application of machine learning. btw, silly question, why so few photos of cats? Do they only come around for just five seconds?" CreationDate="2014-06-24T12:45:46.473" UserId="1085" />
  <row Id="1545" PostId="552" Score="0" Text="Thanks for the link. I had also considered using DTW and hierachical clustering. I have experimented with the R package for DWT. http://www.jstatsoft.org/v31/i07/paper" CreationDate="2014-06-24T13:58:49.467" UserId="1138" />
  <row Id="1546" PostId="552" Score="1" Text="I considered specifically creating n clusters and using the clustering membership as a feature." CreationDate="2014-06-24T13:59:40.057" UserId="1138" />
  <row Id="1547" PostId="550" Score="0" Text="Nice suggestions! Can you flesh out the use of derivatives more?" CreationDate="2014-06-24T14:04:44.013" UserId="1138" />
  <row Id="1548" PostId="550" Score="0" Text="I agree completely with your first statement. I would LOVE to see a box written which collected case studies on feature engineering / extraction. The adage is that feature creation is much more important than the latest greatest algorithm in predictive model performance." CreationDate="2014-06-24T14:07:06.180" UserId="1138" />
  <row Id="1550" PostId="523" Score="0" Text="I tried your FFT method but I really don't get how to set the frequency threshold and amplitude with my data. I'll keep looking, but if u could help me..." CreationDate="2014-06-24T15:28:00.807" UserId="989" />
  <row Id="1552" PostId="559" Score="0" Text="@insys, rumors about my vigilance with the soaker appears to have spread in the feline community. They tend not to linger like they used to. I guess that's a good thing w/r/t the actual objective of ridding my garden of cats, even though it complicates my preferred, more sophisticated solution." CreationDate="2014-06-24T19:54:37.957" UserId="1147" />
  <row Id="1553" PostId="561" Score="0" Text="This is a nice idea for a controlled environment but I'm not sure of it's applicability in this case, since we are dealing with the natural environment where there is continuous change, i.e. change in weather, position of sun, plants and trees because of wind, seasons etc. I believe the region of change as you describe would grow close to the size of the whole image in any case." CreationDate="2014-06-24T20:05:18.477" UserId="1085" />
  <row Id="1555" PostId="567" Score="0" Text="thanks but I'm aiming this at non technical business users, ive updated question. I think a colormap would go over their heads" CreationDate="2014-06-24T20:34:33.083" UserId="237" />
  <row Id="1556" PostId="567" Score="0" Text="@blue-sky Updated my answer. May not be what you're looking for, but that's all I've got =)" CreationDate="2014-06-24T20:37:47.817" UserId="1163" />
  <row Id="1557" PostId="561" Score="0" Text="@insys - I see your point but I disagree - I believe it makes the detector more resilient to change. The time difference between relative frames should be small (~ seconds to a minute) so sun, season, weather should be negligible. I agree that wind will cause plants to move but the classification step can avoid those since their size/shape/color are different than a cat. Plus, using two frames at similar times enables normalizing pixel intensities to better handle varying illumination conditions (e.g., a cat on a sunny vs. cloudy day)." CreationDate="2014-06-24T20:55:20.650" UserId="964" />
  <row Id="1558" PostId="559" Score="2" Text="Seems like the obvious next step (after you get the cat detection working) is a raspberry pi controlled super soaker :-)" CreationDate="2014-06-24T20:56:38.683" UserId="1167" />
  <row Id="1559" PostId="561" Score="0" Text="Actually, I am more confused about your answer now that I read through your comment :) Perhaps I misunderstood, but if you actually use the &quot;extracted change regions&quot; to form your positive samples, as mentioned in your question, how do you even make sure they are cats? They could be anything. As such, your classification step would fail to detect anything but what is taught to detect – that is, changes of any kind. So it is actually repeating the job of the &quot;change&quot; detector." CreationDate="2014-06-24T21:21:19.717" UserId="1085" />
  <row Id="1560" PostId="561" Score="0" Text="Furthermore, illumination conditions are definitely of concern, but, if I get your point right, it is unclear what two similar images, taken with a difference of 1 minute would offer towards normalising pixel intensities?" CreationDate="2014-06-24T21:22:00.297" UserId="1085" />
  <row Id="1561" PostId="561" Score="0" Text="The extracted regions can represent either positive or negative examples - they are what you would use to train the cat classifier. With regard to intensities, Suppose the classifier is trained from regions extracted primarily from sunny images. The classifier might then easily find cats with bright white fur but that won't work well later on a cloudy day (when the white fur isn't nearly as bright) or near dusk. Performing a normalization of the two images helps mitigate that problem (i.e., a pair of bright images and a pair of dim images would appear similar to the classifier)." CreationDate="2014-06-24T22:00:18.433" UserId="964" />
  <row Id="1562" PostId="446" Score="0" Text="Marking this as the accepted answer because out of the approaches suggested, it's the most promising for my particular use case." CreationDate="2014-06-24T22:00:36.210" UserId="322" />
  <row Id="1564" PostId="512" Score="0" Text="I don't think this suggestion addresses the problem as it's presented in the question. Where is the flexibility post-encryption? How do I refine your analysis without access to the original data?" CreationDate="2014-06-24T22:17:32.963" UserId="322" />
  <row Id="1565" PostId="568" Score="0" Text="would this work on a manchester encoded signal where the value is encoded in the state transitions?" CreationDate="2014-06-24T22:21:32.167" UserId="890" />
  <row Id="1566" PostId="559" Score="1" Text="@Kryten related: YouTube [Militarizing Your Backyard with Python: Computer Vision and the Squirrel Hordes](https://www.youtube.com/watch?v=QPgqfnKG_T4)" CreationDate="2014-06-24T22:26:40.920" UserId="1170" />
  <row Id="1570" PostId="512" Score="0" Text="@AirThomas I'm sorry but I don't understand your two questions.  What do you mean by &quot;flexibility post-encryption&quot;? I didn't see anything in your question/description like that.  What do you mean &quot;refine your analysis without access to the original data&quot;? I didn't see anything about &quot;refining&quot;." CreationDate="2014-06-25T02:53:14.440" UserId="609" />
  <row Id="1571" PostId="566" Score="0" Text="So can I say that your question is about &quot;Is combining two consecutive NNP Trees a good approach?&quot;" CreationDate="2014-06-25T03:43:55.783" UserId="1177" />
  <row Id="1572" PostId="512" Score="0" Text="I tried to identify the problem in the second paragraph of the **Motivation** section. Imagine, for example, that you want to release your data set to various researchers who want to do some modeling. There are any number of clever and effective methodologies that could be applied, and each researcher works a little differently. You can't disclose the names of private individuals in your data set. If you perform that portion of the analysis before releasing the data, it forces your choice of methodology on everyone." CreationDate="2014-06-25T03:49:58.180" UserId="322" />
  <row Id="1573" PostId="512" Score="0" Text="If you additionally provide hashes of the names, the benefit is that third parties can distinguish exact identity, but no more. So the question is, how might you provide more information about the data that you can't release? For example, is there a method that preserves in the hashing/encryption output the edit distance between arbitrary inputs? I have found at least one method that at least approximates that functionality (for more information, see my own answer). I hope that makes things more clear." CreationDate="2014-06-25T03:57:55.367" UserId="322" />
  <row Id="1574" PostId="47" Score="1" Text="Another package is distributedR which allows you to work with distributed files in RAM." CreationDate="2014-06-25T07:03:56.810" UserId="1155" />
  <row Id="1575" PostId="560" Score="0" Text="And what would happen if your block splits the cut into two or more slices? The blocking strategy is a very common approach, but when having a camera completely fixed to a certain position, motion detection is a better and less time-consuming approach, from my point of view." CreationDate="2014-06-25T07:08:37.443" UserId="1155" />
  <row Id="1576" PostId="568" Score="0" Text="It depends on the Manchester codification but I would say so. Nonetheless, previous to a HMM training, I'd suggest to use a zero-crossing algorithm to detect flanks of the signal. With this, you could detect the minimum time a change occurs which can give you a hint on the clock speed." CreationDate="2014-06-25T07:11:45.257" UserId="1155" />
  <row Id="1578" PostId="565" Score="1" Text="Any chance of a reproducible example with code and data?" CreationDate="2014-06-25T07:29:35.100" UserId="471" />
  <row Id="1579" PostId="560" Score="0" Text="@adesantos – What you say may well be true, and for prediction differentiating between moving and non-moving parts has it's advantages. But for training, the way it is described by bogatron, it is unclear what benefits it brings to the table. Overall, my opinion is it adds complexity, which lengthens the debugging time significantly. The advantage of moving window is in it's simplicity." CreationDate="2014-06-25T07:36:28.230" UserId="1085" />
  <row Id="1580" PostId="560" Score="0" Text="Btw, regarding the split that you mention, an obvious strategy is to let your windows overlap, so that split position does not affect your classifier." CreationDate="2014-06-25T07:36:50.183" UserId="1085" />
  <row Id="1581" PostId="570" Score="0" Text="please see question update, so for my use case instead of a movie display a user (that is similar) , the stars indicate the degree of similarity ?" CreationDate="2014-06-25T08:01:22.837" UserId="237" />
  <row Id="1584" PostId="560" Score="0" Text="I would add to my proposal (motion detection) the use of SIFT algorithm with a cat texture. The SIFT method can also be used with that strategy of blocks, but in that case you will compare more blocks than require.&#xA;&#xA;Notice that a cat moves, but a tree or a bush not that much." CreationDate="2014-06-25T09:06:43.937" UserId="1155" />
  <row Id="1585" PostId="570" Score="0" Text="Exactly - instead of a movie poster, you can display a user profile picture.  So several rows - Users similar to A, Users similar to B, etc. and across each row, user profile pictures.  Below each user profile picture a star rating.  You indicate that the similarity is stronger the closer to 0, so .2-.0 = 5 stars, .4-.2 = 4 stars, etc.  If there are no profile pictures available, a user name would do, but people will respond to photos better." CreationDate="2014-06-25T09:08:57.833" UserId="434" />
  <row Id="1586" PostId="221" Score="0" Text="I do not agree trees are of help here. It is a matter of filtering rules, and that can be achieved with the _arules_ package in R." CreationDate="2014-06-25T10:13:45.667" UserId="1155" />
  <row Id="1587" PostId="566" Score="0" Text="True that... good approach for Named Entity Recognition (NER) ?" CreationDate="2014-06-25T14:12:52.660" UserId="1165" />
  <row Id="1588" PostId="568" Score="0" Text="Why would I need clock speed? Manchester encoding is self clocking. Timing should be unimportant." CreationDate="2014-06-25T14:21:50.763" UserId="890" />
  <row Id="1590" PostId="435" Score="2" Text="What about rules mining? It is not clear to me what is your aim." CreationDate="2014-06-25T14:32:26.520" UserId="1155" />
  <row Id="1591" PostId="24" Score="1" Text="I do not recommend converting categorical attributes to numerical values. Imagine you have two city names: NY and LA. If you apply NY number 3 and LA number 8, the distance is 5, but that 5 has nothing to see with the difference among NY and LA." CreationDate="2014-06-25T14:38:17.337" UserId="1155" />
  <row Id="1592" PostId="568" Score="0" Text="I though it could be helpful to know the clock speed in order to know how fast are the transitions between low/high values." CreationDate="2014-06-25T14:39:49.943" UserId="1155" />
  <row Id="1594" PostId="559" Score="0" Text="@MichaelT Yeah, I've been looking at that to solve a pigeon problem at my house" CreationDate="2014-06-25T15:39:56.180" UserId="1167" />
  <row Id="1596" PostId="559" Score="0" Text="There is open-source software for this, known as &quot;motion&quot;: http://www.lavrsen.dk/foswiki/bin/view/Motion/WebHome" CreationDate="2014-06-25T15:54:51.600" UserId="924" />
  <row Id="1597" PostId="565" Score="2" Text="which playground competition is it?" CreationDate="2014-06-25T16:01:01.027" UserId="153" />
  <row Id="1598" PostId="264" Score="1" Text="Can you be more specific? EM refers to an optimization algorithm that can be used for clustering. There are many ways to do this and it is not obvious what you mean." CreationDate="2014-06-25T19:18:45.110" UserId="1193" />
  <row Id="1600" PostId="559" Score="0" Text="@Anony-Mousse I tried the motion software before posting the question here, but it failed miserably in detecting any kind of animals, instead reacting only to leaves/trees moving, sunlight intensity changes and such – _except_ in the sample image above where it actually detected a bee flying close to the camera. Might work better in other environments." CreationDate="2014-06-25T20:43:05.533" UserId="1147" />
  <row Id="1601" PostId="518" Score="0" Text="This question appears to be off-topic because Data Science Stack Exchange is not a link recommendation service. Please use Google for basic information and tutorials." CreationDate="2014-06-26T01:41:16.660" UserId="62" />
  <row Id="1603" PostId="586" Score="1" Text="This online book will be useful http://www.nltk.org/book/" CreationDate="2014-06-26T00:40:36.690" UserId="1196" />
  <row Id="1604" PostId="200" Score="0" Text="Thank you for your post, but can you include here an overall description of the solution linked? The link may have the answer to the question, but link-only answers are discouraged." CreationDate="2014-06-26T04:41:26.317" UserId="84" />
  <row Id="1607" PostId="555" Score="1" Text="+1 This is one of the best answers I've ever read." CreationDate="2014-06-26T09:14:03.430" UserId="1155" />
  <row Id="1609" PostId="595" Score="1" Text="What do you mean by 'restoring parameters'? Do you mean populating those users who didn't indicate gender/age and so forth?" CreationDate="2014-06-26T13:00:25.503" UserId="1155" />
  <row Id="1611" PostId="595" Score="0" Text="@adesantos Yes. Is it possible?" CreationDate="2014-06-26T13:09:04.630" UserId="1207" />
  <row Id="1612" PostId="598" Score="0" Text="Which classification algorithm do you recommend in my case? Will be Naive Bayes classifier good choice?" CreationDate="2014-06-26T14:23:49.747" UserId="1207" />
  <row Id="1613" PostId="598" Score="0" Text="I suggest, as I said, not to populate and keep a variable called unknown." CreationDate="2014-06-26T14:25:01.953" UserId="1155" />
  <row Id="1614" PostId="598" Score="0" Text="I have no choice. It's must be done. But haven't expirience enough." CreationDate="2014-06-26T14:32:36.413" UserId="1207" />
  <row Id="1615" PostId="595" Score="0" Text="You may wish to look up the term data imputation as well" CreationDate="2014-06-26T14:46:13.673" UserId="1085" />
  <row Id="1616" PostId="598" Score="0" Text="Then use the second approach I told you: Create a distribution. But if you want to keep it simple, populate with the most common value. That's the easiest option." CreationDate="2014-06-26T14:53:21.123" UserId="1155" />
  <row Id="1617" PostId="600" Score="3" Text="[Cross posting](http://stats.stackexchange.com/q/104868) seems in poor form. Why is this tagged SQL?" CreationDate="2014-06-26T15:34:29.440" UserId="322" />
  <row Id="1618" PostId="600" Score="0" Text="Solve the (approximate) travelling salesman problem (TSP)..." CreationDate="2014-06-26T16:08:01.983" UserId="984" />
  <row Id="1620" PostId="597" Score="0" Text="Well, *someone* knows for sure what Google uses, and there are plenty of Google employees with SE accounts. Perhaps one of them will come along and provide us all with some interesting (non-proprietary) information." CreationDate="2014-06-26T17:28:18.160" UserId="322" />
  <row Id="1621" PostId="596" Score="1" Text="I think you could improve this question by defining &quot;gold standard dataset&quot; in a more objective fashion. What makes it &quot;must-know&quot;? Should it be referenced in a number of textbooks? Used in a number of published models? Etc. Otherwise the answers will be subjective AND they will change as time passes. A bad combination here." CreationDate="2014-06-26T17:33:37.880" UserId="322" />
  <row Id="1623" PostId="605" Score="0" Text="Normalize which matrix? These similarity measures do not require arguments in [0,1], by the way." CreationDate="2014-06-26T22:33:11.973" UserId="21" />
  <row Id="1624" PostId="599" Score="0" Text="Thanks, but it's not exactly what I'm looking for. See update for more details." CreationDate="2014-06-27T05:23:06.967" UserId="941" />
  <row Id="1625" PostId="612" Score="0" Text="Yes I still intend to cross-validate to check for over-fitting, my question is more basic than that - I cannot find any references to using regularisation with a per-item weight adjustment in MLP at all, and am concerned there is a good reason for that - e.g. it does not work in that learning mode, or needs adjustment. The crossvalidated question *is* very similar though and gives me more confidence, thank you. The SGD algorithm page seems to have a different, stochastic method for introducing regularisation, which might be a bit advanced for me, but is exactly what I am looking for." CreationDate="2014-06-27T07:30:40.650" UserId="836" />
  <row Id="1626" PostId="612" Score="0" Text="Regularization is relevant in per-item learning as well. I would still suggest to start with a basic validation approach for finding out lambda. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method." CreationDate="2014-06-27T09:38:04.807" UserId="1085" />
  <row Id="1627" PostId="612" Score="0" Text="If your comment above was edited in and replaced the first sentence/question in your answer, then I think I could accept it." CreationDate="2014-06-27T09:40:46.120" UserId="836" />
  <row Id="1628" PostId="612" Score="0" Text="Thanks for pointing out, I agree. Edited it in. Hope it is more clear." CreationDate="2014-06-27T09:55:29.610" UserId="1085" />
  <row Id="1629" PostId="468" Score="1" Text="Given that this is a question about the statistical model, you may want to go to [CrossValidated](http://stats.stackexchange.com) website, but keep in mind that it is a terrible practice to cross-post the questions: you would either want to formulate it to highlight the methodological issues you are facing, or migrate the whole question." CreationDate="2014-06-27T13:23:38.907" UserId="1237" />
  <row Id="1633" PostId="468" Score="0" Text="Without really explaining why, [ISL](http://www-bcf.usc.edu/~gareth/ISL) notes (on p 137) that discriminant analysis (like LDA, QDA) is more often used than multiple class extensions of logistic regression. Packages like [penalizedLDA](http://cran.r-project.org/web/packages/penalizedLDA/index.html) may therefore be worth examining." CreationDate="2014-06-27T21:03:57.007" UserId="953" />
  <row Id="1636" PostId="607" Score="1" Text="First question you should ask here is do you need a virtual machine as an instrument for your data science practice? (Maybe even rephrase the original question.) And there are more than the two mentioned VM to choose from. Here is a link to a [post comparing four VMs for data science.](http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html)" CreationDate="2014-06-28T09:51:37.420" UserId="454" />
  <row Id="1637" PostId="619" Score="0" Text="Thank u for ur kind answer..." CreationDate="2014-06-28T16:20:27.867" UserId="1235" />
  <row Id="1638" PostId="619" Score="0" Text="Can we do Regresssion,clustering,classifications using Rmr...... If it is possible then we can do using R directly.. only because of Parallelism we are using Rmr.. If i am correct.. Is there any main Difference between Hadoop Mapreduce and R mapreduce (apart from Parallelism)..." CreationDate="2014-06-28T16:21:39.303" UserId="1235" />
  <row Id="1639" PostId="619" Score="0" Text="rmr is a framework for running R functions in MapReduce. That is the thing it lets you do that you could not do before. It is not a library of new statistical functions of course." CreationDate="2014-06-28T17:32:36.687" UserId="21" />
  <row Id="1640" PostId="607" Score="0" Text="This would even be sufficient as a reply. Fantastic. Even though the poster is the creator of the data science tool box so it is not an unbiased report, but it's the best thing I've seen - so thanks." CreationDate="2014-06-28T20:54:44.647" UserId="1223" />
  <row Id="1641" PostId="561" Score="0" Text="Have a look at background subtraction in opencv...it is doing anomaly detection on pixels, by basically calculating historical mean, variance (in fact gaussian mixture model)." CreationDate="2014-06-28T23:06:53.930" UserId="1256" />
  <row Id="1642" PostId="623" Score="0" Text="If you have different classifiers trained on different datasets, how can you compare them in a meaningful way? Apples and oranges, chalk and cheese come to mind. Also, if you have multiclass classifiers, how do you calculate precision and recall?  Even knowing N=1 is not necessarily helpful - if there is only one egg in the world, your egg classifier is fine." CreationDate="2014-06-29T06:58:18.617" UserId="1230" />
  <row Id="1643" PostId="623" Score="0" Text="They're different classifiers trained on the same datasets, e.g. we know we have a document that's about apples and oranges, so we run an apple classifier on it to determine  type of apple it's talking about, and an orange classifier to determine type of orange it talks about. If our documents are 99% about apples, 1% about oranges, and both classifiers have the same prec/rec (summing rows/cols over confusion matrix), is there any info we can present that takes into account the differences in quantities of each? (it might be that no, there isn't, which is an answer I'd be happy with)" CreationDate="2014-06-29T09:06:28.753" UserId="474" />
  <row Id="1644" PostId="619" Score="0" Text="Then any other  specific feature where hadoop mapreduce can't handle?....." CreationDate="2014-06-29T11:32:23.643" UserId="1235" />
  <row Id="1645" PostId="414" Score="0" Text="In practice, you will use a learning rate with adadelta. On some problems it does not work without." CreationDate="2014-06-29T18:18:55.930" UserId="1193" />
  <row Id="1646" PostId="535" Score="0" Text="This seems to work pretty well except the scaling of the numbers between 0 and 1. Not sure whether the python version is as intended?" CreationDate="2014-06-29T22:08:50.460" UserId="1107" />
  <row Id="1647" PostId="534" Score="0" Text="True, I'd forgotten about norms, I'll look into this thanks." CreationDate="2014-06-29T22:09:23.323" UserId="1107" />
  <row Id="1648" PostId="603" Score="0" Text="I agree with Steve K that the key to tackling this is to aim for approximately optimal or just good route strategies. Many times the difference between &quot;best&quot; and &quot;good enough&quot; isn't much." CreationDate="2014-06-30T03:55:22.573" UserId="609" />
  <row Id="1651" PostId="535" Score="0" Text="I simplified your python version. And what's wrong with scaling? Assuming, that all values in original matrix are between 0 and 1, result should also be of the same scale." CreationDate="2014-06-30T05:38:40.697" UserId="941" />
  <row Id="1652" PostId="586" Score="0" Text="@RajaPasupuleti Ah yes this is very useful. Thanks!" CreationDate="2014-06-30T07:38:22.640" UserId="1192" />
  <row Id="1653" PostId="203" Score="0" Text="Sorry, I take your point.  I thought &quot;Symphony / COIN-OR seem to be the dominant suggestions&quot; answered the question &quot;is there an open source [...] alternative [...]?&quot; - the links were for backup as to why. I did caveat that the OP's scenario is on a bigger scale than I'm used to - I cannot give any extra information from personal experience but as this is a new stackexchange site and no-one had jumped in, I thought I'd try to assist. Will delete if you think it's not helpful / an answer." CreationDate="2014-06-30T10:45:17.277" UserId="265" />
  <row Id="1654" PostId="535" Score="0" Text="Nothing is wrong with the scaling now... I must of had a bug in my code. Thanks for the help this works great on my dataset" CreationDate="2014-06-30T11:38:42.077" UserId="1107" />
  <row Id="1655" PostId="636" Score="0" Text="That is a lot of open ended questions to ask all at once. One of the goals of this site is that questions and their answers should be useful to future visitors. Can you be more specific? What looks bad in your linear model? When you tried the logistic model, how did it come out? Were there specific problems?" CreationDate="2014-06-30T15:52:28.900" UserId="322" />
  <row Id="1656" PostId="30" Score="1" Text="If the total amount of data in the world doubles every 40 months, then surely it *can* get bigger than that. ;p" CreationDate="2014-06-30T16:00:06.990" UserId="322" />
  <row Id="1660" PostId="636" Score="0" Text="Sorry,an eg:&#xA;&#xA;On trying logit in R with these values:&#xA; &#xA;day-28, hour-11,day of the week-7, state-New Mexico, OS-iOS, OS Version-7.0, browser family-Mobile Safari,  browser version-7.0, device manufacturer-apple, IP carrier- Comcast, user age-20, gender-male, click happened-yes, predicted probability from GLM in R-0.000000001.&#xA;&#xA;In another instance,day-28, hour-12th, day of the week-7, state-Connecticut, OS-iOS, OS version-7.0, browser-mobile safari, browser version-7.0, device manufacturer-apple, IP carrier-Comcast, user age-22, user gender-female,click happened-no, predicted probability-.046" CreationDate="2014-06-30T17:02:06.487" UserId="1273" />
  <row Id="1662" PostId="636" Score="0" Text="Thanks. There's an &quot;edit&quot; link at the bottom of your question so that you can update it with this additional information." CreationDate="2014-06-30T17:13:58.197" UserId="322" />
  <row Id="1663" PostId="636" Score="0" Text="That is what I meant by how the model does not look good. I think this is due to the sparsity of instances of clicks happening and random nature of event of clicks happening.&#xA;&#xA;So how can I go about sampling the data? Also how should I add weights to the parameters for improving the prediction? Sorry for adding more open-ended questions but encountering issues as they come, I also want to ask how to work with large datasets in R?" CreationDate="2014-06-30T17:44:17.230" UserId="1273" />
  <row Id="1664" PostId="641" Score="1" Text="Recommend asking on http://opendata.stackexchange.com/" CreationDate="2014-06-30T21:28:41.943" UserId="322" />
  <row Id="1665" PostId="520" Score="0" Text="I added some graphs, after having improved the model A LOT. In github there are the new steps. May I ask you how I can apply linear regression in a time series problem? :(" CreationDate="2014-06-30T22:59:25.760" UserId="989" />
  <row Id="1666" PostId="523" Score="0" Text="I added the sources also" CreationDate="2014-06-30T23:00:24.603" UserId="989" />
  <row Id="1667" PostId="519" Score="0" Text="I added some graphs and you can check also about the parameters now :)" CreationDate="2014-06-30T23:01:05.820" UserId="989" />
  <row Id="1668" PostId="466" Score="0" Text="Project updated!" CreationDate="2014-06-30T23:01:21.583" UserId="989" />
  <row Id="1669" PostId="603" Score="0" Text="Of course the optimum can be found, it might just take longer than the age of the universe to iterate over all the possibilities. Your answer fails to mention this." CreationDate="2014-07-01T13:23:16.427" UserId="471" />
  <row Id="1670" PostId="600" Score="0" Text="Beyond lat-long, what's the geography like? A gridded city? An almost tree-shaped suburb with smaller roads into cul-de-sacs? Has a MASSIVE influence." CreationDate="2014-07-01T13:24:47.337" UserId="471" />
  <row Id="1671" PostId="24" Score="0" Text="@adesantos Yes, that's a problem with representing multiple categories with a single numeric feature and using a Euclidean distance.  Using the Hamming distance is one approach; in that case the distance is 1 for each feature that differs (rather than the difference between the numeric values assigned to the categories).  Making each category its own feature is another approach (e.g., 0 or 1 for &quot;is it NY&quot;, and 0 or 1 for &quot;is it LA&quot;)." CreationDate="2014-07-01T14:36:16.047" UserId="14" />
  <row Id="1672" PostId="642" Score="0" Text="Is this the Gelman paper referenced: http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf?" CreationDate="2014-07-01T17:51:00.910" UserId="684" />
  <row Id="1673" PostId="641" Score="1" Text="Thanks for the recommendation!" CreationDate="2014-07-01T17:51:26.133" UserId="684" />
  <row Id="1674" PostId="644" Score="0" Text="I didn't quite get your question. Do you intend to compute pairwise cosine similarities between every pair of row vectors (observations)?" CreationDate="2014-07-01T18:37:05.693" UserId="984" />
  <row Id="1675" PostId="644" Score="0" Text="Moreover, I didn't quite get why are you taking the average?" CreationDate="2014-07-01T18:37:53.627" UserId="984" />
  <row Id="1677" PostId="642" Score="0" Text="That works, but I highly recommend his [book on multilevel modeling](http://www.stat.columbia.edu/~gelman/arm/)." CreationDate="2014-07-01T19:01:15.310" UserId="159" />
  <row Id="1678" PostId="642" Score="0" Text="Ah, awesome, thanks!" CreationDate="2014-07-01T19:14:50.183" UserId="684" />
  <row Id="1679" PostId="644" Score="0" Text="If your observations are just 0s and 1s, then I don't think cosine similarity will work. Perhaps you should consider using Tanimoto similarity, which considers similarity across bitmaps." CreationDate="2014-07-02T01:30:01.480" UserId="24" />
  <row Id="1680" PostId="644" Score="0" Text="@Debasis yes, I do plan to compute similarities between each row. And I'm only taking the average as an example strategy to show what my computations might involve." CreationDate="2014-07-02T03:30:08.680" UserId="754" />
  <row Id="1681" PostId="644" Score="0" Text="@buruzaemon my observations aren't actually 0s and 1s, though I do like the simplicity of those kind of similarities. I actually asked this question because I'm interested in what the right answer for cosine similarities is. But yeah, I do appreciate the advice." CreationDate="2014-07-02T03:35:10.417" UserId="754" />
  <row Id="1682" PostId="649" Score="1" Text="Thanks! I would upvote this, but I don't have the reputation necessary. Maybe someone else can do that for me?" CreationDate="2014-07-02T03:43:26.020" UserId="754" />
  <row Id="1683" PostId="555" Score="1" Text="+1, and you should always put the original data into some kind of source control. Very sound advice, @Steve" CreationDate="2014-07-02T04:48:19.950" UserId="24" />
  <row Id="1684" PostId="628" Score="0" Text="Did you mean to say &quot;increase reduce the model complexity&quot; on the last bullet point? I think just &quot;increase the model complexity&quot; . . . BTW good timing I am enrolled in that course and had only just watched the video you are referring to." CreationDate="2014-07-02T10:24:36.813" UserId="836" />
  <row Id="1685" PostId="654" Score="1" Text="So, kNN is not training but lazy learning, because it does not abstract and generalize but, in case of classification, for each new observation it learns from the nearest k-labeled observations. While NTP is kNN-1 between an observation and the signature (that can be considered a pseudo-observation). Am I right?" CreationDate="2014-07-02T14:57:33.457" UserId="133" />
  <row Id="1686" PostId="616" Score="0" Text="Note to readers: [This question is also under discussion on Stack Overflow, where a high-quality solution has been offered.](http://stackoverflow.com/q/24455620/2359271)" CreationDate="2014-07-02T15:23:50.970" UserId="322" />
  <row Id="1687" PostId="654" Score="2" Text="While the kNN classifier is referred to as a &quot;lazy learner&quot;, that term is somewhat misleading and it is more accurately described as a &quot;lazy classifier&quot;. With regard to your comment, it doesn't actually *learn* when you give it a new observation to classify - it simply calculates the appropriate class. If you give it that same observation later, it would perform the same computation the second time because it doesn't actually learn from the first observation. In short, kNN doesn't produce/learn a model - it simply stores training data, then uses those data when it is time to classify." CreationDate="2014-07-02T16:48:34.320" UserId="964" />
  <row Id="1688" PostId="628" Score="0" Text="@NeilSlater Thanks, good catch, there was indeed a typo :)" CreationDate="2014-07-02T16:56:18.073" UserId="843" />
  <row Id="1690" PostId="655" Score="0" Text="I'm not sure if you'll do better than predicting 1/f noise when using past values as indicators for future ones. http://www.scholarpedia.org/article/1/f_noise#Stock_markets_and_the_GNP - your results so far seem consistent with that. Probably you should look at other possible features that have some reason to correlate with future exchange rates. If this were easy, there would be more rich data scientists." CreationDate="2014-07-02T23:32:03.263" UserId="836" />
  <row Id="1691" PostId="636" Score="0" Text="The class imbalance is such that you don't really want to predict click vs no click.  Since almost no one clicks your best result is to simply predict they don't click. Instead, think about predicting for all the users that click...which ad will they click on. This is a much more useful problem." CreationDate="2014-07-03T03:49:44.793" UserId="92" />
  <row Id="1692" PostId="660" Score="1" Text="You need to find the log output from your R script, which would indicate the error. &quot;hadoop streaming failed with error code 1&quot; just means &quot;the script failed for some reason&quot;" CreationDate="2014-07-03T11:19:25.687" UserId="21" />
  <row Id="1694" PostId="660" Score="1" Text="Sometimes the folder where you write must be deleted before writing (if it exists). Check that out." CreationDate="2014-07-03T11:41:17.003" UserId="1155" />
  <row Id="1695" PostId="646" Score="1" Text="Shouldn't any old spatial index that supports containment queries work? i.e. an r-tree?" CreationDate="2014-07-03T12:40:20.623" UserId="1283" />
  <row Id="1696" PostId="660" Score="0" Text="thank u for ur answer ... but i have check all the possibilites what u have mentioned... i doubt that there is some problem with code itself...can someone please rectify..." CreationDate="2014-07-03T15:19:02.067" UserId="1235" />
  <row Id="1697" PostId="661" Score="2" Text="try `sudo hadoop fs -ls -l /` and see what comes back.  Usually a permissions error comes back explicitly, but lets have a gander at what the root path looks like.  I wonder if the file system was never formatted." CreationDate="2014-07-03T17:39:19.290" UserId="434" />
  <row Id="1698" PostId="661" Score="2" Text="This might be better at serverfault.com" CreationDate="2014-07-03T21:36:59.580" UserId="21" />
  <row Id="1699" PostId="667" Score="0" Text="Algorithm-wise: what would you recommend ?" CreationDate="2014-07-03T21:59:30.367" UserId="1315" />
  <row Id="1700" PostId="663" Score="0" Text="I would love to see your example." CreationDate="2014-07-03T22:03:13.677" UserId="1315" />
  <row Id="1701" PostId="663" Score="0" Text="Updated with quick example." CreationDate="2014-07-03T22:52:43.353" UserId="375" />
  <row Id="1702" PostId="653" Score="2" Text="Other keywords that might be useful for what you are looking are *clustering* and the more general *unsupervised learning*." CreationDate="2014-07-04T07:24:06.803" UserId="113" />
  <row Id="1703" PostId="667" Score="0" Text="you mean algorithm for computing the most similar resume vectors given a query job vector? you can use any standard algorithm such as BM25 or Language Model..." CreationDate="2014-07-04T11:35:38.297" UserId="984" />
  <row Id="1704" PostId="667" Score="0" Text="I have never heard of these algorithms at all. Are these NLP algorithms or ML algo ?" CreationDate="2014-07-04T13:32:43.553" UserId="1315" />
  <row Id="1705" PostId="667" Score="0" Text="these are standard retrieval models... a retrieval model defines how to compute the similarity between a document (resume in your case) and a query (job in your case)." CreationDate="2014-07-04T15:23:49.700" UserId="984" />
  <row Id="1706" PostId="667" Score="0" Text="I have no knowledge about information retrieval, do you think machine learning algorithms like clustering / nearest neighbour will also work in my case ?" CreationDate="2014-07-04T16:37:18.267" UserId="1315" />
  <row Id="1707" PostId="629" Score="0" Text="This. Is. Fantastic! Thanks @AsheeshR" CreationDate="2014-07-05T08:07:48.490" UserId="1223" />
  <row Id="1708" PostId="669" Score="0" Text="great thanks. I will evaluate robust regression and see how it goes." CreationDate="2014-07-05T11:04:54.390" UserId="1300" />
  <row Id="1709" PostId="655" Score="0" Text="yes, Maybe other variables are contributing to the next value more than the time series values it self.. I will experiment with that too. Thank you for the pointers." CreationDate="2014-07-05T11:08:20.760" UserId="1300" />
  <row Id="1710" PostId="667" Score="0" Text="yes, nearest neighbour is in fact very similar in objective to that of information retrieval... that is that of finding the most similar k points given a query point... the only problem is that when N (the number of data points) is too large, IR is more efficient" CreationDate="2014-07-05T12:00:14.867" UserId="984" />
  <row Id="1712" PostId="675" Score="0" Text="Say I am analyzing linkedin data, do you think it would be a good idea for me to merge the previous work experience, educations recommendations and skills of one profile into one text file and extract keywords from it ?" CreationDate="2014-07-05T14:46:08.110" UserId="1315" />
  <row Id="1713" PostId="675" Score="0" Text="LinkedIn now has skill tags that people assign themselves and other users can endorse, so basically there's no need to extract keywords manually. But in case of less structured data - yes, it may be helpful to merge everything and then retrieve keywords. However, remember main rule: **try it out**. Theory is good, but only practical experiments with different approaches will reveal best one." CreationDate="2014-07-05T20:33:05.633" UserId="1279" />
  <row Id="1714" PostId="675" Score="0" Text="true said. Thanks a lot" CreationDate="2014-07-05T23:33:49.547" UserId="1315" />
  <row Id="1715" PostId="679" Score="1" Text="Search queries are not noisier (there are very few words in a query not actually related to the search), but may contain misspellings, ambiguity, slang and other stuff that you have to deal with separately. Beyond these issues, queries and documents may be processed pretty much the same way." CreationDate="2014-07-06T00:13:02.033" UserId="1279" />
  <row Id="1716" PostId="677" Score="1" Text="Your second import is not correctly indented.  I would correct the code myself if the edit was long enough." CreationDate="2014-07-07T10:08:28.323" UserId="1367" />
  <row Id="1717" PostId="684" Score="0" Text="Thanks. I wouldn't have known what had caused it. I only know most of the time it's my work that's at fault :)" CreationDate="2014-07-07T12:56:31.427" UserId="974" />
  <row Id="1718" PostId="679" Score="0" Text="maybe you can extract keyword vectors from queries, and then compute the distance between those vectors, and how the similarity is defined, i think this is still an open question:)" CreationDate="2014-07-06T06:15:21.697" UserId="1006" />
  <row Id="1719" PostId="684" Score="0" Text="@elksie5000 : I have added how to debug the call. I hope the last call is what you would expect from a successful call to the function (?). Otherwise, it is always good to know how to step into the code with `pdb` :)" CreationDate="2014-07-07T13:44:45.997" UserId="1367" />
  <row Id="1720" PostId="684" Score="0" Text="I must admit pdb was something I was looking at again after working through the Python for Data Analysis book by Wes McKinney. I already work in IPython, but had been reasonably happy with print statements. Thank you again." CreationDate="2014-07-07T15:03:33.867" UserId="974" />
  <row Id="1721" PostId="684" Score="0" Text="As a side note, the debugger prompt says &quot;ipdb&quot; because it is the ipython debugger - this is an extra install in my setup. Under normal circumstances, it would be the regular pdb that is called. Just noticed this difference." CreationDate="2014-07-07T15:27:48.683" UserId="1367" />
  <row Id="1722" PostId="680" Score="1" Text="Note that when doing LSA, typically you use the cosine distance on the LSA projections of the original dataset. Just to clarify." CreationDate="2014-07-07T18:09:18.477" UserId="1301" />
  <row Id="1723" PostId="691" Score="0" Text="How exactly have you used LSA? It's worth noting that LDA is actually a pretty thin wrapper around LSA (it's pLSA with a dirichlet prior) that has been empirically shown to greatly increase generalization. You would almost certainly see better accuracies with LSA, but that's generally a result of overfitting, which is a very notable problem with LSA. Also, what exactly do you mean by scaling here? doc2vec does not actually require a new model for each document, and for computation there's no notable difference between LSA and LDA, both being very scalable." CreationDate="2014-07-07T19:36:26.987" UserId="869" />
  <row Id="1725" PostId="697" Score="2" Text="This is quite a broad question. You may be memory-bound (dataset doesn't fit into memory, and thus swap is used extensively, making it deadly slow) or CPU-bound (memory is ok, but operations just take too long). In latter case you can try to use vectorization more widely or write extensions in C directly. You can also try to compile your functions with `cmpfun()` or parallelize code to use several cores. Finally, you can rent Amazon web server to run your experiments, which will cost much less than buying hardware yourself. Anyway, try to determine your bottleneck first." CreationDate="2014-07-08T06:18:59.317" UserId="1279" />
  <row Id="1726" PostId="701" Score="0" Text="I just added an answer assuming that you want to cluster the samples `id_1`...`id_n` *based on their distances*. If you do want to cluster *the distances themselves*, you just need to use them as a 1-dimensional array." CreationDate="2014-07-08T09:28:49.813" UserId="1367" />
  <row Id="1729" PostId="679" Score="1" Text="Both of your questions are broad, subjective and will require significant maintenance to avoid becoming obsolete. Since the community appreciates that sort of question, keeping one of them might be reasonable - but certainly not both, when this discussion is a proper subset of the other. Please review [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-07-08T15:13:07.897" UserId="322" />
  <row Id="1731" PostId="693" Score="0" Text="Do you still use SVM when you have 3 or more classes ? And what features do you want to extract using a natural language parser? For what purpose ?" CreationDate="2014-07-08T15:40:48.113" UserId="1315" />
  <row Id="1733" PostId="694" Score="0" Text="See also http://stackoverflow.com/q/2276933/2359271" CreationDate="2014-07-08T20:14:34.267" UserId="322" />
  <row Id="1734" PostId="691" Score="0" Text="I have not observed over fitting with LSA, and like I said, I have met multiple other people who have seen better performance over LDA. Also, I have seen LSA used in many winning entries in semeval competitions, I have never seen LDA used in a winning entry. That is the academic conference for comparing semantic similarity between documents, so I assume they know what they are doing. Doc2vec, if you are referring to Mikolov's paragraph vector implementation, does SGD on each document separately. So it's very slow." CreationDate="2014-07-08T20:48:55.170" UserId="1301" />
  <row Id="1736" PostId="686" Score="0" Text="Note my comments below about paragraph vector scalability. This technique looks very promising, but is hard to implement, and does not scale well at all, as you are doing a separate SGD for each document, which is very costly, if I remember the paper correctly" CreationDate="2014-07-08T20:52:43.710" UserId="1301" />
  <row Id="1737" PostId="700" Score="0" Text="I can't fully answer your question, but I know of one way I have selected the k-value before.  You can look at minimizing a function of (Sum of squares within clusters)/(Sum of Squares between clusters), or maximizing the inverse." CreationDate="2014-07-08T23:15:07.113" UserId="375" />
  <row Id="1738" PostId="694" Score="1" Text="I saw thanks, but it is outdated, and closed." CreationDate="2014-07-09T00:06:23.383" UserId="989" />
  <row Id="1740" PostId="687" Score="0" Text="Thanks for your answer. Do I understand right: your starting point is to merge the 2 datasets ?" CreationDate="2014-07-09T14:31:34.480" UserId="906" />
  <row Id="1741" PostId="690" Score="0" Text="Thanks for your answer. It's a great example of what generative models can do that discriminative models cannot." CreationDate="2014-07-09T14:34:15.320" UserId="906" />
  <row Id="1743" PostId="712" Score="1" Text="I don't mean to be overly critical, but SVMs are NOT efficient. They have a cubic complexity in most cases, which is why there is a lot of phasing out happening." CreationDate="2014-07-09T19:05:12.953" UserId="548" />
  <row Id="1744" PostId="713" Score="1" Text="This is probably better for serverfault.com" CreationDate="2014-07-09T20:01:26.083" UserId="21" />
  <row Id="1745" PostId="712" Score="1" Text="yes, standard convergence methods takes O(n^3)... but i think i've seen somewhere (may be from the home page of T. Joachims) that it's been reduced to O(n^2)" CreationDate="2014-07-09T20:21:05.123" UserId="984" />
  <row Id="1746" PostId="713" Score="1" Text="@SeanOwen Yes, or [cloudera support](http://www.cloudera.com/content/cloudera/en/about/contact-form.html), even" CreationDate="2014-07-09T21:13:50.973" UserId="322" />
  <row Id="1747" PostId="705" Score="0" Text="okay, can you elaborate what does interaction mean? I added classes like wifi enabled, gps enabled. now lasso performed slightly better than ridge, I also added more days for computation. However, the AUC is now in range of .51 to .55 only. Is there anything else i can do? Will try adding quadratic features, and what else?" CreationDate="2014-07-10T05:34:55.647" UserId="1273" />
  <row Id="1748" PostId="705" Score="0" Text="For every feature, you can create quadratic features (x_i ^ 2) and interaction features (x_i * x_j). Also be aware that there are hyper-parameters for both methods of regularization that should be tuned rather than left at their defaults" CreationDate="2014-07-10T08:19:26.393" UserId="1399" />
  <row Id="1749" PostId="636" Score="0" Text="for selecting what users will click on, we will have to search our system for keywords etc as relevant. so if there is a way to reject a request without delaying time in keyword lookup, it will save a lot of time in processing for us" CreationDate="2014-07-10T11:03:43.300" UserId="1273" />
  <row Id="1750" PostId="716" Score="1" Text="A non-random correlation might be an indicator that the feature *is* useful. But I'm not so sure about pre-training tests that could rule ideas out. The paper you link makes it clear that non-linear correlations are not well detected by the available tests, but a neural net has a chance of finding and using them." CreationDate="2014-07-10T11:28:15.503" UserId="836" />
  <row Id="1751" PostId="711" Score="0" Text="See also http://stats.stackexchange.com/questions/tagged/svm" CreationDate="2014-07-10T11:55:01.303" UserId="1237" />
  <row Id="1752" PostId="705" Score="0" Text="i took alpha in between 0 and 1 as well for trying regression, is that what you mean?" CreationDate="2014-07-10T14:47:05.807" UserId="1273" />
  <row Id="1753" PostId="700" Score="0" Text="Look up the Chinese Restaurant Process to help deal with dynamic numbers of clusters." CreationDate="2014-07-10T15:49:54.777" UserId="684" />
  <row Id="1754" PostId="718" Score="0" Text="I always thought to compare the value to predict with the features, you are talking about correlation between features. Is your answer applicable also to my case? in theory I should add only new features that are correlated to the value to predict, right?" CreationDate="2014-07-10T19:06:57.880" UserId="989" />
  <row Id="1755" PostId="718" Score="0" Text="That's also a valuable metric -- just updated my answer  to address that as well." CreationDate="2014-07-10T19:18:34.210" UserId="684" />
  <row Id="1756" PostId="718" Score="0" Text="In short, strong correlations with the value to predict is a great sign, but weak correlation with the value to predict is not necessarily a bad sign." CreationDate="2014-07-10T19:19:17.200" UserId="684" />
  <row Id="1757" PostId="718" Score="0" Text="Thanks. I'm writing a report and I wanted to show the linear/non-linear correlations in order to justify the features (even before the results). Does it make any sense?&#xA;From your answer I could make a matrix of correlations but maybe it's nosense" CreationDate="2014-07-10T19:27:33.073" UserId="989" />
  <row Id="1758" PostId="718" Score="0" Text="A matrix of correlations would make more sense if you were performing linear regression, but it could also be a useful metric for neural nets.  Give it a go and see what you get!" CreationDate="2014-07-10T21:29:06.873" UserId="684" />
  <row Id="1759" PostId="718" Score="1" Text="I would use non-linear correlations, but ok thanks" CreationDate="2014-07-10T23:17:47.063" UserId="989" />
  <row Id="1760" PostId="687" Score="0" Text="@cafe876 That is certainly one way to start, and then trying to basically recreate a clustering that closely approximates the original." CreationDate="2014-07-11T00:36:54.230" UserId="548" />
  <row Id="1761" PostId="720" Score="0" Text="Thank you for your answer. I already know these tools. Unfortunatly none of them are neither able to summarize a collection of documents nor perform query-based summarization" CreationDate="2014-07-11T09:23:52.577" UserId="979" />
  <row Id="1765" PostId="713" Score="0" Text="Could you provide the list of misconfigurations (that are just below the yellow bar)?" CreationDate="2014-07-12T00:35:10.563" UserId="2460" />
  <row Id="1766" PostId="730" Score="0" Text="This post was researched and presented well. It makes a poor question for an SE network site but it would be a great topic to start on a discussion forum." CreationDate="2014-07-12T19:38:44.743" UserId="322" />
  <row Id="1767" PostId="730" Score="0" Text="@AirThomas Thanks for the warning. I tried to save the post by making a proper question out of it." CreationDate="2014-07-13T03:06:30.877" UserId="84" />
  <row Id="1768" PostId="705" Score="0" Text="I suspect by alpha you mean the step size? If so, this is not the parameter I'm referring to---there's another parameter that regulates how much the regularization term is multiplied by in the objective, which allows you to balance model fit (likelihood) with sparseness as measured by the regularizer. This needs tuning." CreationDate="2014-07-14T10:04:29.207" UserId="1399" />
  <row Id="1769" PostId="704" Score="1" Text="Ah, the joys of deployment ... enhanced by the joys of distributed systems :)" CreationDate="2014-07-14T10:44:38.130" UserId="1367" />
  <row Id="1770" PostId="658" Score="0" Text="Please clarify: you say you want to use Column 2 as a feature, but then you say you want to predict/classify Column 2. Also, you call this feature 'non-atomic' ... do you mean it is not categorical?" CreationDate="2014-07-14T13:12:22.047" UserId="1367" />
  <row Id="1771" PostId="729" Score="0" Text="I think your answer is useful to the problem.  Just some suggestions: I would move the data generation code to the bottom, or even to an external Gist, since it is not really part of the proposed solution.  And I would elaborate a bit more on the fact that you are using 4 standard deviations to detect resets: right now, it is just a comment lost in the code, and it is the core of your solution." CreationDate="2014-07-14T13:26:23.830" UserId="1367" />
  <row Id="1772" PostId="729" Score="0" Text="Good ideas.  Will do." CreationDate="2014-07-14T15:18:33.427" UserId="375" />
  <row Id="1773" PostId="737" Score="0" Text="Thanks for the reply! I have actually tried numerous other algorithms/kernels and still have the same type of problem. So I am looking for more of an approach like undersampling or some way to even out the classes." CreationDate="2014-07-14T16:40:26.047" UserId="802" />
  <row Id="1774" PostId="737" Score="0" Text="Ok, you might also want to try replicating rows for classes containing sparse data, although its useful only if the features of the sparse data are really good." CreationDate="2014-07-14T17:25:06.797" UserId="2485" />
  <row Id="1775" PostId="740" Score="0" Text="thanks for the advice, do you know if libsvm automatically does this or do I need to manually pass in the class weights?" CreationDate="2014-07-14T19:24:14.940" UserId="802" />
  <row Id="1776" PostId="740" Score="0" Text="You have to manually pass in the class weights. The way to do that is different based on the interface you are using (python, java, matlab, c). It is well documented in the read me files if you download the tool from http://www.csie.ntu.edu.tw/~cjlin/libsvm/.&#xA;Also your data size seems to be large and the default multi-class implementation of libsvm will use one-vs-one classification which may take too long to run. You can try training 50 one-vs-all binary classifiers specifying the weights appropriately." CreationDate="2014-07-14T19:57:27.733" UserId="1350" />
  <row Id="1777" PostId="734" Score="0" Text="Thanks very much! Your comments are very helpful and provide a basis on which I may be able to tease more out of the article. John" CreationDate="2014-07-14T20:24:37.367" UserId="2458" />
  <row Id="1778" PostId="301" Score="1" Text="As a side comment: I think spotting errenous data caused by some problem further up the pipeline is a gold skill. Many a time I have wondered why my analysis produced weird results and when I looked at the pipeline i found some kind of error . E.g: I wondered why all my data where heavily skewed towards high prices - WAY out of my mental model. When I asked around, I found out that some subcontractor misunderstood the briefing and delivered data for high income groups, while we whanted mixed data..." CreationDate="2014-07-15T09:33:38.703" UserId="791" />
  <row Id="1779" PostId="742" Score="1" Text="+1 You do need a lot of engineering experience to be an effective data science, but you don't get that at school. Use school for the theory and use jobs for engineering skill." CreationDate="2014-07-15T11:07:06.440" UserId="21" />
  <row Id="1780" PostId="742" Score="1" Text="+1 for the role definition and the overall good advice" CreationDate="2014-07-15T14:03:13.123" UserId="1367" />
  <row Id="1781" PostId="506" Score="0" Text="Thank you very much for such a thorough explanation! I really appreciate it. Between yourself and neone4373 I was able to solve the problem! This community rocks!&#xA;&#xA;Thanks!" CreationDate="2014-07-15T15:06:43.223" UserId="1047" />
  <row Id="1782" PostId="301" Score="0" Text="Yes! Data errors are frequently signs of process problems. Knowing where in the process the errors were introduced and also the mechanism, will greatly help with the cleaning process.  But better still is to fix the process problems so that they produce clean (or cleaner) data." CreationDate="2014-07-15T19:12:57.530" UserId="609" />
  <row Id="1783" PostId="679" Score="0" Text="Thanks, AirThomas!  ffriend's post certainly seems to indicate that this is clearly a duplicate.  I'll see what I can do about this." CreationDate="2014-07-16T00:14:25.850" UserId="1097" />
  <row Id="1784" PostId="746" Score="1" Text="My vote is also for R. As far as I know, there is no time series decomposition functions in `statsmodel` (Python). Though in this case decomposition could be crucial to improving prediction. I see notable seasonal peaks." CreationDate="2014-07-16T08:45:53.197" UserId="941" />
  <row Id="1786" PostId="746" Score="0" Text="@sobach: Thank you for R solidarity. In regard to the rest of your comment, similarly to now famous tweet, I can neither confirm, nor deny that :-). [Since it's beyond my current level of knowledge on the subject.]" CreationDate="2014-07-16T10:38:45.793" UserId="2452" />
  <row Id="1787" PostId="755" Score="0" Text="I belive you are referring to OpenCV nactive_vars parameter (not max_depth), which I set to default sqrt(N) value, that is nactive_vars=sqrt(16) for first dataset and sqrt(200) for other two. max_depth determines whether trees grow to full depth (25 is its maximum value) and balances between underfitting and overfitting, more about it here: &#xA;http://stats.stackexchange.com/questions/66209/opencv-parameters-of-random-trees&#xA;Not sure about min_sample_count but I tried various values and setting it to 1 worked best." CreationDate="2014-07-17T07:05:44.933" UserId="1387" />
  <row Id="1788" PostId="755" Score="0" Text="OpenCV documentation gives brief explanation of parameters:&#xA;http://docs.opencv.org/modules/ml/doc/random_trees.html#cvrtparams-cvrtparams&#xA;For now I would like to make random trees work reasonably well and keep things simple because I want to focus on working with a multiple classifier system." CreationDate="2014-07-17T07:06:32.730" UserId="1387" />
  <row Id="1789" PostId="755" Score="0" Text="About kNN - these are all really good suggestions, but what I meant to say is that kNN performed better than random trees classifier and I think there is lots of room for improvement with random trees." CreationDate="2014-07-17T07:15:51.477" UserId="1387" />
  <row Id="1790" PostId="732" Score="0" Text="Thank you! Your answer really helped me!" CreationDate="2014-07-17T08:43:42.207" UserId="2471" />
  <row Id="1791" PostId="755" Score="0" Text="yes, i'm not sure why random forest is not performing as well (or better) than the simplistic k-NN approach... it just might be the case that a kernel based approach where you directly try to estimate P(y|D) (output given data) such as in k-NN without estimating P(theta|D) (latent model given data) such as in the parametric models." CreationDate="2014-07-17T09:28:49.887" UserId="984" />
  <row Id="1792" PostId="760" Score="1" Text="Transform your data to be a list of number of month between events (in Matlab this would be `diff(find(V))` where `V` is your current time series vector. Then try to fit an exponential distribution to this by estimating the *rate* parameter. *rate*, would be a decent metric of the frequency of job changes. The exponential distribution should show how the probability will increase with time since the last event. You also might want to [test for a goodness of fit](http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution) after estimating *rate*:" CreationDate="2014-07-17T09:41:18.727" UserId="2532" />
  <row Id="1793" PostId="760" Score="0" Text="what is the equivalent of `diff(find(V))` in R ?" CreationDate="2014-07-17T10:22:02.283" UserId="1315" />
  <row Id="1794" PostId="760" Score="1" Text="I don't know much `R` but I'll explain the Matlab code: `find` return the element number of the `1`s, `diff` returns the difference between each consecutive number. Hence that line just returns a vector of the number of months between each job change." CreationDate="2014-07-17T10:46:10.830" UserId="2532" />
  <row Id="1797" PostId="760" Score="0" Text="I'll point back to this link again: http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution looks like r has a `fitdistr` function" CreationDate="2014-07-17T16:00:12.900" UserId="2532" />
  <row Id="1798" PostId="753" Score="0" Text="Answers that rely on external resources to be useful are strongly discouraged on Stack Exchange. If a link stops working, or a reference book is not available, these answers become useless. Please *do* explain with words, either by paraphrasing or quoting your sources directly, and upload images (with attribution) as necessary and appropriate, using external links and references only for citation and &quot;further reading.&quot;" CreationDate="2014-07-17T16:01:42.867" UserId="322" />
  <row Id="1799" PostId="766" Score="0" Text="Essentially what I could do is to set self.error = error at the end of _tsne(), in order to retrieve it from the TSNE instance afterwards. Yes, but that would mean changing sklearn.manifold code, and I was wondering if the developers thought of some other ways to get the information or if not why they didn't (i.e.: is 'error' considered useless by them?). Furthermore, if I changed that code I would need all the people running my code to have the same hack on their sklearn installations. Is that what you suggest, or did I get it wrong?" CreationDate="2014-07-17T16:07:30.343" UserId="131" />
  <row Id="1800" PostId="760" Score="0" Text="actually, i just manually computed the MLE of $\lambda$ of exponential distribution." CreationDate="2014-07-17T16:08:29.123" UserId="1315" />
  <row Id="1801" PostId="769" Score="0" Text="Yes, indeed error rates on training data set are around 0. Changing parameters to reduce overfitting didn't result in higher accuracy on test dataset in my case. I will look into techniques you mention as soon as possible and comment, thank you." CreationDate="2014-07-17T16:16:51.433" UserId="1387" />
  <row Id="1802" PostId="769" Score="0" Text="What are the relative proportions of training and test dataset btw? Something line 70:30, 60:40, or 50:50?" CreationDate="2014-07-17T16:38:07.737" UserId="2556" />
  <row Id="1803" PostId="766" Score="0" Text="Yes, that is what I suggested as a possible solution. Since scikit-learn is open source, you could also submit your solution as a pull request and see if the authors would include that in future releases. I can't speak to why they did or didn't include various things." CreationDate="2014-07-17T17:10:10.463" UserId="159" />
  <row Id="1804" PostId="768" Score="1" Text="I like the watermark magic. For those who are unaware, GitHub now offers up to 5 free private repositories for users associated with academic institutions." CreationDate="2014-07-17T17:22:06.340" UserId="964" />
  <row Id="1805" PostId="773" Score="1" Text="&quot;distance metric&quot; is commonly used as an opposite of &quot;similarity&quot; in literature: the larger distance, the smaller similarity, but basically they represent same idea." CreationDate="2014-07-17T20:52:20.197" UserId="1279" />
  <row Id="1806" PostId="757" Score="0" Text="Do you have any advice related to your first sentence?" CreationDate="2014-07-17T23:42:37.857" UserId="989" />
  <row Id="1807" PostId="769" Score="0" Text="First dataset - UCI letter recognition is set to 50:50 (10000:10000), Digits is about 51:49 (1893:1796) and MNIST is about 86:14 (60000:10000)." CreationDate="2014-07-18T01:35:18.207" UserId="1387" />
  <row Id="1808" PostId="751" Score="0" Text="Why the dot product alone (equivalent to not normalizing) *not* account for features' data and frequency? I don't know that this is the difference." CreationDate="2014-07-18T11:27:57.540" UserId="21" />
  <row Id="1809" PostId="744" Score="0" Text="Note that neither of these are proper distance metrics, even if you transform them to be a value that is small when points are &quot;similar&quot;. It may or may not matter for your use case." CreationDate="2014-07-18T11:34:09.417" UserId="21" />
  <row Id="1810" PostId="751" Score="0" Text="Perhaps, I wasn't clear. I was talking about data diversity. E.g., we have two pairs of documents. Within each pair docs are identical, but pair-1 documents are shorter, than pair-2 ones. And we computing similarity within each pair. Dot product would produce different numbers, though in both cases maximum similarity estimate is expected." CreationDate="2014-07-18T12:36:29.073" UserId="941" />
  <row Id="1811" PostId="757" Score="0" Text="Create a dataset for each day of the week and fit a model to all seven of them." CreationDate="2014-07-18T12:58:53.813" UserId="325" />
  <row Id="1812" PostId="765" Score="0" Text="What do people generally due with predicted negative values if they know that the true target function cannot output negative values?" CreationDate="2014-07-18T14:05:20.120" UserId="1162" />
  <row Id="1813" PostId="758" Score="2" Text="Where is the *question* in this question? Please take a moment to review [the Help Center guidelines](http://datascience.stackexchange.com/help/dont-ask), specifically: &quot;If your motivation for asking the question is 'I would like to participate in a discussion about ______', then you should not be asking here.&quot;" CreationDate="2014-07-18T15:05:56.783" UserId="322" />
  <row Id="1814" PostId="766" Score="2" Text="Thanks. If anybody else is interested in this, https://github.com/scikit-learn/scikit-learn/pull/3422." CreationDate="2014-07-18T15:44:03.873" UserId="131" />
  <row Id="1815" PostId="764" Score="1" Text="How are points close to each other on the wrap-around point handled?" CreationDate="2014-07-18T17:00:53.687" UserId="2587" />
  <row Id="1816" PostId="765" Score="1" Text="I don't think it's applicable to what we're talking about, but if you have a strictly positive target, for example, you'd probably want to model the log of the target.  Then you'd exponentiate the predictions." CreationDate="2014-07-18T19:15:21.783" UserId="2543" />
  <row Id="1817" PostId="753" Score="0" Text="@keisZn, the pdf was a good explanation but there is no algorithm to explain how he parse through the alignment matrix to get consistent phrases..." CreationDate="2014-07-18T21:22:46.960" UserId="122" />
  <row Id="1818" PostId="785" Score="2" Text="It's over 4 GB of data. I should plot it by reading from stdin or something similar. It's a bad idea to load everything to RAM. I'll take a look at what you said in a couple of days - and hopefully, any other ideas that may arise - and I'll let you know how it went, thanks!" CreationDate="2014-07-20T01:39:58.163" UserId="2604" />
  <row Id="1819" PostId="787" Score="0" Text="Are you using &quot;poor&quot; Apache Hadoop or some other distribution like [HDP](http://hortonworks.com/hdp/) or [CDH](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html). I would heavily recommend using automated tools like these two instead of messing up with native settings. In addition to easy installation, they provide tools for monitoring and managing your cluster later." CreationDate="2014-07-20T11:28:51.477" UserId="1279" />
  <row Id="1821" PostId="787" Score="0" Text="This is better at serverfault.com" CreationDate="2014-07-20T17:32:17.743" UserId="21" />
  <row Id="1822" PostId="784" Score="0" Text="https://www.otexts.org/fpp/6/5" CreationDate="2014-07-20T18:02:37.283" UserId="989" />
  <row Id="1823" PostId="784" Score="0" Text="I think your question would be better stated as, What is the importance of seasonality for forecasting. As it is, it seems someone told you to use &quot;STL&quot;, but you don't say who told you so, neither why (which is probably what you're trying to find out)." CreationDate="2014-07-20T19:37:34.577" UserId="84" />
  <row Id="1825" PostId="769" Score="0" Text="I experimented with PCA, still didn't get good results with random forrest, but boost and Bayes now give results similar to other classifiers. I found a discussion about random forrest here: &#xA;http://stats.stackexchange.com/questions/66543/random-forest-is-overfitting&#xA;It is possible I am actually not overfitting but couldn't find the out-of-bag (OOB) prediction error mentioned there. Running experiment now with a large number of trees to see if accuracy will improve." CreationDate="2014-07-21T16:04:50.647" UserId="1387" />
  <row Id="1826" PostId="782" Score="1" Text="Great answer, Aleksandr, very informative!" CreationDate="2014-07-21T16:18:43.513" UserId="2599" />
  <row Id="1827" PostId="769" Score="0" Text="Okay, sounds you are making a little bit of progress :) A trivial question, but have you standardized your features (z-score) so that they are centered around the mean with standard deviation=1?" CreationDate="2014-07-21T16:19:55.410" UserId="2556" />
  <row Id="1828" PostId="769" Score="0" Text="Actually no, I usually would scale features to range 0-1 but now I see I didn't even do that correctly before PCA. So that would not be the right thing to do anyway? After PCA mean = 0, std = 0.5754." CreationDate="2014-07-21T16:41:29.853" UserId="1387" />
  <row Id="1829" PostId="782" Score="0" Text="@DaveKay: Thank you for kind words, Dave! Glad to help." CreationDate="2014-07-21T16:46:43.350" UserId="2452" />
  <row Id="1830" PostId="795" Score="0" Text="I'm aware of edit distance like Levenshtein distance, but I'm looking for something like semantic similarity." CreationDate="2014-07-21T16:55:04.707" UserId="921" />
  <row Id="1831" PostId="795" Score="0" Text="That's significantly harder.  The only way I know to do something like this is to be able to access a dictionary.  Then you could look into text mining definitions of the words.  Try looking into accessing 'wordnet', maybe that could help. http://wordnet.princeton.edu/wordnet/" CreationDate="2014-07-21T17:16:23.593" UserId="375" />
  <row Id="1832" PostId="787" Score="0" Text="@ffriend I am using &quot;poor&quot; Hadoop. I actually didn't know HDP or CDH existed. Is HDP an add-on or would I have to reinstall Hadoop entirely?" CreationDate="2014-07-21T17:27:07.070" UserId="2614" />
  <row Id="1833" PostId="795" Score="1" Text="-0 for suggesting Levenshtein distance." CreationDate="2014-07-21T19:08:36.797" UserId="869" />
  <row Id="1834" PostId="787" Score="0" Text="@BigDataDude: You will have to reinstall it entirely, but unlike manual installation, automated way will take only 10-15 minutes even for large clusters (at least, this is true for CDH - I haven't used Hortonworks' manager). So unless you have already pushed too much unique data to existing HDFS, migrating to maintained cluster should be pretty easy and painless." CreationDate="2014-07-21T20:38:09.437" UserId="1279" />
  <row Id="1835" PostId="794" Score="1" Text="I'm was also a bit thrown by the YAML files at first, but have since come to love the clean separation between configuration and code.  You can choose to use Pylearn2 without YAML files, although this option is not well documented." CreationDate="2014-07-21T20:52:39.473" UserId="684" />
  <row Id="1836" PostId="794" Score="0" Text="In short, however, I wouldn't discard the library because of this simple design decision." CreationDate="2014-07-21T20:53:05.997" UserId="684" />
  <row Id="1837" PostId="758" Score="0" Text="&quot;You should only ask practical, answerable questions based on actual problems that you face.&quot;" CreationDate="2014-07-21T21:04:02.473" UserId="895" />
  <row Id="1838" PostId="758" Score="0" Text="This is practical, answerable and based on an actual problem in much the same way that &quot;Tell me how to perform data science&quot; is practical, answerable and based on an actual problem." CreationDate="2014-07-21T21:44:19.357" UserId="322" />
  <row Id="1839" PostId="799" Score="0" Text="This looks quite like what I am looking for. I am studying for finals now and am unable to take time to think about this again, but as soon as I can I'll let you know.&#xA;&quot;A 256 byte periodic pattern would have manifested as vertical lines.&quot; -- exactly what I was thinking of. I can also show an image where I put all 256 bytes in the same line, and that is already obvious in text. I'm quite curious about what will come out of it :)" CreationDate="2014-07-22T00:08:30.030" UserId="2604" />
  <row Id="1840" PostId="807" Score="0" Text="Could you add how AUC compares to an F1-score?" CreationDate="2014-07-22T07:00:40.660" UserId="2532" />
  <row Id="1841" PostId="798" Score="0" Text="But the problem is that 'senior' and 'primary' don't occur in one title. How can I even compare this two words using list of job titles ?" CreationDate="2014-07-22T08:57:25.677" UserId="921" />
  <row Id="1842" PostId="798" Score="0" Text="Yes, this might help you learn that &quot;senior&quot; and &quot;developer&quot; go together, but not that &quot;senior&quot; and &quot;lead&quot; have similar semantic content." CreationDate="2014-07-22T09:35:35.613" UserId="21" />
  <row Id="1843" PostId="791" Score="0" Text="I think you will need some extra information to learn this. For example, do you have salary info, industry, and number of direct reports? This defines when two roles should be considered similar. Then you can ask what terms seem to be synonymous among similar roles. But if you don't know anything about what makes things similar I'm not sure what you can do." CreationDate="2014-07-22T09:36:47.343" UserId="21" />
  <row Id="1844" PostId="793" Score="2" Text="The major use is storing data and retrieving data. In fact, that's about the only use for a NOSQL database, or any database. Want to make your question better?" CreationDate="2014-07-22T15:09:41.543" UserId="471" />
  <row Id="1845" PostId="811" Score="1" Text="_If you're trying to build a representative model -- one that describes the data rather than necessarily predicts_ ... who builds a model which doesn't predcit?? Didn't get you there..." CreationDate="2014-07-22T15:23:14.600" UserId="2661" />
  <row Id="1846" PostId="812" Score="0" Text="you can try Graphviz... not sure if it scales up to millions of vertices...." CreationDate="2014-07-22T15:29:54.013" UserId="984" />
  <row Id="1847" PostId="797" Score="0" Text="You are right. NOSQL databases are mainly used for storing unstructured or semi-structured data like json. Can you  explain some of the types of data analysis we can do with them. What are the tools built into mongodb that can used for data analysis?" CreationDate="2014-07-22T15:32:12.733" UserId="2643" />
  <row Id="1848" PostId="793" Score="0" Text="Yes, database is mainly used for storing and retrieveing data. How can they be used for data analysis? What are the tools  built into  NOSQL databases like mongodb  which makes data analysis easy and powerful?" CreationDate="2014-07-22T16:56:17.863" UserId="2643" />
  <row Id="1849" PostId="793" Score="1" Text="Improve your question by editing it, not adding to the comments." CreationDate="2014-07-22T16:57:58.290" UserId="471" />
  <row Id="1850" PostId="798" Score="0" Text="@Mher, They're not supposed to occur in the same title; the terms _following_ them are supposed to occur in both, e.g., senior *developer*, or primary *developer*." CreationDate="2014-07-22T17:12:30.653" UserId="381" />
  <row Id="1851" PostId="798" Score="0" Text="@SeanOwen, If the titles are semantically similar, you would expect their co-occurrence vectors to be similar too since they would be used interchangeably." CreationDate="2014-07-22T17:14:14.703" UserId="381" />
  <row Id="1852" PostId="797" Score="0" Text="@jithinjustin there aren't data analysis tools built into mongo, or really any database. Also, `json` is totally structured data. You can technically do any kind of data analysis on it, using a NOSQL database is actually not related. There are tools built *on top of* mongo, like analytica though." CreationDate="2014-07-22T17:39:42.677" UserId="548" />
  <row Id="1853" PostId="811" Score="3" Text="Unsupervised learning would be an example where you build a model that isn't necessarily geared to predict. In some instances you might want to explore or summarize your data." CreationDate="2014-07-22T18:07:02.207" UserId="2513" />
  <row Id="1854" PostId="801" Score="0" Text="Related methods like LDA may also be a good bet." CreationDate="2014-07-22T18:39:23.373" UserId="684" />
  <row Id="1855" PostId="807" Score="1" Text="@Dan- The biggest difference is that you don't have to set a decision threshold with AUC (it's essentially measuring the probability spam is ranked above non-spam). F1-score requires a decision threshold. Of course, you could always set the decision threshold as an operating parameter and plot F1-scores." CreationDate="2014-07-22T19:14:58.447" UserId="2513" />
  <row Id="1856" PostId="815" Score="1" Text="Just as a comment, it looks like you have a pretty strict schema for your data at this. Depending on how many new &quot;columns&quot; you expect to appear at a later time, SQL may actually the best solution for you. But again, I'm just speculating." CreationDate="2014-07-22T22:38:43.677" UserId="1163" />
  <row Id="1857" PostId="815" Score="2" Text="As someone who is a huge fan of NOSQL, SQL is probably the right choice for this project." CreationDate="2014-07-23T06:16:20.823" UserId="869" />
  <row Id="1858" PostId="819" Score="0" Text="thanks for this, it is really useful." CreationDate="2014-07-23T09:32:40.020" UserId="906" />
  <row Id="1859" PostId="798" Score="0" Text="@Emre yes but you also 'learn' that &quot;strategist&quot; and &quot;ballerina&quot; and &quot;chef&quot; are similar because they follow &quot;head&quot; or something. I am not sure if this is enough to learn on by itself?" CreationDate="2014-07-23T10:52:17.920" UserId="21" />
  <row Id="1860" PostId="802" Score="1" Text="I think the structure you want to implement is a &quot;trie&quot; - whether you can find a DB that efficiently works with that structure, or need to roll your own in RDBMS of your choice I cannot say." CreationDate="2014-07-23T13:43:04.077" UserId="836" />
  <row Id="1861" PostId="822" Score="0" Text="Thanks for the answer, it also makes me think about the ethics of when you do an experiment in order to see its influence such as the recent [fabeook](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840) issue, perhaps that itself would make a good question as to the moral implications." CreationDate="2014-07-23T13:55:09.943" UserId="95" />
  <row Id="1862" PostId="821" Score="2" Text="This was a generic question. I did not include code because I did not want to solve a specific problem. I never stated I'm a data scientist and... IMHO it is not necessary to be so &quot;rude&quot; :). Thank you anyway for the explanation." CreationDate="2014-07-23T13:56:06.313" UserId="989" />
  <row Id="1863" PostId="822" Score="0" Text="You're welcome.  Ethics is something we deal with a lot in Biostatistics due to the history of medical research.  As a statistician/data scientist, I would argue that it is ethical to give an accurate portrait of the data and to not torture it into confessing.  A good place to start for the ethics of trials, from the medical point of view is the [Nurenburg Code](http://en.wikipedia.org/wiki/Nuremberg_Code).  It certainly has application for what Facebook did." CreationDate="2014-07-23T13:59:51.797" UserId="178" />
  <row Id="1864" PostId="823" Score="0" Text="I don't have enough rep to add an ethics tag or perhaps social experimentation and facebook, if someone could oblige" CreationDate="2014-07-23T14:06:20.877" UserId="95" />
  <row Id="1865" PostId="798" Score="0" Text="@SeanOwen: perhaps some constraints/context should be specified in this case. For me &quot;head chief&quot; and &quot;head developer&quot; sound similar because they both lead their teammates. If you want to distinguish between professions only, you can use only keywords meaning profession titles. But question clearly refers to employees level as well as his profession." CreationDate="2014-07-23T15:27:10.410" UserId="1279" />
  <row Id="1866" PostId="798" Score="1" Text="Yeah it must be about level and role. Two &quot;head&quot;s are similar, but that's obvious because both have the word &quot;head&quot;. My point was that &quot;chef&quot; and &quot;ballerina&quot; aren't necessarily similar just because you see &quot;head chef&quot; and &quot;head ballerina&quot; which is how I understood the cooccurrence idea. How do you learn that &quot;lead developer&quot; and &quot;senior developer&quot; are similar but &quot;junior developer&quot; is not? I think some other data has to enter the picture to tell us that the first two are supposed to be similar, then we can figure out why terms explain it." CreationDate="2014-07-23T15:40:02.637" UserId="21" />
  <row Id="1867" PostId="769" Score="0" Text="It depends on your data whether you want to do a Min-max normalization to unit range (e.g., 0-1) or Z-score normalization/standardization to unit variance (variance=1, mean=0). Sorry, but I forgot that you are doing text classification. I think normalization after you stemmed the words and used a vectorizer function would not be necessary" CreationDate="2014-07-23T16:22:00.253" UserId="2556" />
  <row Id="1868" PostId="823" Score="0" Text="I'm a bit confused as to what you're actually asking here. The title of this question seems inconsistent with the body and how ethics &quot;should&quot; be applied is way beyond the scope of this site. Academic programs are pretty diverse, but a question about professional standards accountability seems like it would be both answerable and reasonably scoped. Can you clarify or refocus your question?" CreationDate="2014-07-23T19:40:09.743" UserId="322" />
  <row Id="1869" PostId="823" Score="1" Text="You may also find [this meta discussion](http://meta.datascience.stackexchange.com/q/7/322) interesting, if you haven't read it already." CreationDate="2014-07-23T19:40:25.260" UserId="322" />
  <row Id="1870" PostId="823" Score="0" Text="@AirThomas Thanks for that meta link, I wasn't sure if this would be n topic or not. I guess what I'd like to know is whether ethics are taught or imbued academically and in the professional workplace and whether a moral code of practice exists for data science, this is a naive and broad question I admit" CreationDate="2014-07-23T20:00:22.007" UserId="95" />
  <row Id="1871" PostId="808" Score="0" Text="possible duplicate of [Starting my career as Data Scientist, is Software Engineering experience required?](http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require)" CreationDate="2014-07-23T21:03:31.483" UserId="553" />
  <row Id="1872" PostId="798" Score="0" Text="@SeanOwen: My answer below should clarify the idea, but in short it's all about context. If &quot;head&quot; is the only common word in chef and ballerina profiles, their semantic vectors will still be quite far from each other. Two chefs will have many common words about cooking, and two _head_ chefs will additionally refer to same ratings/certificates/management techniques, etc. Same thing with developers: lead and senior devs frequently use words &quot;client&quot;, &quot;strategy&quot;, etc., while juniors normally mention few most commonly known technologies." CreationDate="2014-07-23T21:49:55.393" UserId="1279" />
  <row Id="1873" PostId="826" Score="0" Text="I don't mean to be negative, but regular expressions are an extremely poor choice for this problem. Between state-by-state variation, vanity plates, and differing formats, regular expressions are a poor choice." CreationDate="2014-07-24T02:07:45.667" UserId="548" />
  <row Id="1874" PostId="829" Score="0" Text="What error are you getting? It might actually help us to know..." CreationDate="2014-07-24T06:46:00.500" UserId="471" />
  <row Id="1875" PostId="825" Score="2" Text="Any particular country? Different countries have different license plate formats. Its a big world out there." CreationDate="2014-07-24T11:48:14.770" UserId="471" />
  <row Id="1876" PostId="831" Score="1" Text="It looks like there are no prerequisites for membership, without which I see no reason why membership should carry any meaning. Membership is supposed to be a &quot;Valuable credential to signal to clients and employers&quot;, but this will only be true if there is some sort of qualification requirement/entrance exam and also some industry experience requirements. With a free for all membership, being a member doesn't distinguish you from a non-member." CreationDate="2014-07-24T11:56:29.887" UserId="2532" />
  <row Id="1877" PostId="826" Score="0" Text="How many different variations are there? I'm from a different country." CreationDate="2014-07-24T12:53:56.637" UserId="325" />
  <row Id="1878" PostId="826" Score="1" Text="A conservative estimate would be 51, probably close to a few hundred." CreationDate="2014-07-24T16:18:06.910" UserId="548" />
  <row Id="1879" PostId="826" Score="0" Text="Ah right I see the problem. I was thinking that I could find all the plates registered here with 3 regular expressions." CreationDate="2014-07-24T16:55:11.487" UserId="325" />
  <row Id="1880" PostId="825" Score="0" Text="@Spacedman Any country is fine :)" CreationDate="2014-07-24T17:38:07.027" UserId="1192" />
  <row Id="1881" PostId="826" Score="0" Text="@germcd Thanks for the attempt though! Glad to hear I wasn't the only person to think of regex too." CreationDate="2014-07-24T17:42:29.130" UserId="1192" />
  <row Id="1882" PostId="827" Score="0" Text="Never seen brat before; thanks for the tip!" CreationDate="2014-07-24T17:52:43.600" UserId="1192" />
  <row Id="1883" PostId="832" Score="0" Text="Do you want to be able to print the plot?" CreationDate="2014-07-24T19:54:46.143" UserId="2575" />
  <row Id="1884" PostId="799" Score="0" Text="I can't seem to run this on Debian Linux. I installed the packages `python-scitools` and `ipython`. The error message is `ValueError: invalid literal for int() with base 10: '#'`. I'll see if I can make it work anyway..." CreationDate="2014-07-25T02:41:49.133" UserId="2604" />
  <row Id="1885" PostId="797" Score="0" Text="I don't know about all that. MongoDB can perform better than MySQL. You'd have a better argument if you said PostgreSQL (which, by the way can accept JSON). Either way, I wouldn't consider some arbitrary &quot;performance&quot; (we don't know what the use case is) to be a reason not to use NoSQL. Also don't discount using multiple databases. Remember, MongoDB has amazing aggregation features that SQL does not have." CreationDate="2014-07-25T02:56:24.790" UserId="2711" />
  <row Id="1886" PostId="799" Score="0" Text="I succeeded (by running the code directly inside `ipython`, and changing `map(int, line)` to `map(ord, line)`, and updated the question with the new picture." CreationDate="2014-07-25T03:30:23.273" UserId="2604" />
  <row Id="1887" PostId="783" Score="0" Text="An example/excerpt of the data (maybe only a few MB) could be interesting." CreationDate="2014-07-25T09:43:55.557" UserId="156" />
  <row Id="1889" PostId="835" Score="1" Text="I don't see anything in that link or here that proves you can't do sentiment analysis in an SQL database. the mongoDB examples benefit from Javascript in the DB, so you could use any embedded language in an SQL database. For example Postgres + R." CreationDate="2014-07-25T12:54:27.213" UserId="471" />
  <row Id="1890" PostId="785" Score="0" Text="Don't load it in and treat it like a dataframe, its not a dataframe, its a stream of bytes." CreationDate="2014-07-25T12:56:20.203" UserId="471" />
  <row Id="1891" PostId="835" Score="0" Text="Would love to see where you could execute code and map/reduce in those databases. In all seriousness (especially Postgres). ...and even if you could, that still doesn't make the answer any less valid by the way. One simply just might want to use NoSQL. It does work." CreationDate="2014-07-25T14:07:06.143" UserId="2711" />
  <row Id="1892" PostId="835" Score="1" Text="Postgres + C, Python, Perl, R, feed your Postgres DB into the latest machine learning algorithms. Easy: http://www.postgresql.org/docs/9.0/static/xplang.html" CreationDate="2014-07-25T15:00:05.993" UserId="471" />
  <row Id="1893" PostId="835" Score="0" Text="Nice. I'll have to try that out sometime. How about MySQL?" CreationDate="2014-07-25T15:16:52.210" UserId="2711" />
  <row Id="1894" PostId="783" Score="0" Text="If you're interested in the periodic nature of the data taking a look at the DFT of the data could be revealing." CreationDate="2014-07-25T17:44:44.230" UserId="2724" />
  <row Id="1895" PostId="840" Score="0" Text="Your code please." CreationDate="2014-07-25T17:52:56.020" UserId="381" />
  <row Id="1896" PostId="832" Score="1" Text="@dsign, If you mean do I need to print it on a physical medium--no. But I would like a graphical representation of the data." CreationDate="2014-07-25T18:28:05.447" UserId="2702" />
  <row Id="1897" PostId="833" Score="0" Text="Unfortunately this type of plot doesn't work because it doesn't scale to show how many actions are taking place at the same time. I would estimate that at any given time there could be 100 actions taking place over the whole data set there are millions of actions." CreationDate="2014-07-25T18:32:48.153" UserId="2702" />
  <row Id="1898" PostId="833" Score="0" Text="you can change the thickness of the lines e.g. `geom_line(size = 2)`" CreationDate="2014-07-25T18:56:38.250" UserId="325" />
  <row Id="1899" PostId="797" Score="0" Text="@Tom on performance, you'll find that the only task that mongo actually outperforms mysql on is inserts (http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/), which is a comparatively small part of data analysis. SQL's aggregation features are FAR more mature than Mongo's. As far as MYSQL versus Postgres, the numbers are very temporily skewed and both tend to offer similar performance. MYSQL is more common, which is why I mentioned that instead, but the two are quite similar." CreationDate="2014-07-25T21:59:38.130" UserId="548" />
  <row Id="1901" PostId="845" Score="0" Text="Thank you so much, that helped clear up a lot of things." CreationDate="2014-07-26T01:14:10.963" UserId="2726" />
  <row Id="1902" PostId="783" Score="0" Text="@mrmcgreg: I'll have to re-learn how the DFT works. I should've paid more attention to the signals &amp; systems classes :)" CreationDate="2014-07-26T03:10:46.017" UserId="2604" />
  <row Id="1903" PostId="797" Score="0" Text="I've always seen better performance on MongoDB when things fit into memory. I take benchmarks with a gain of salt because if you Google a bit you're gonna find a bunch of benchmarks showing MongoDB as faster. It truly depends on your needs. That said, to help answer the original question - I think there's plenty of uses for NoSQL in big data science and analytics." CreationDate="2014-07-26T03:29:46.807" UserId="2711" />
  <row Id="1904" PostId="797" Score="0" Text="@Tom If things fit into memory, one should take advantage of that and use something like `redis` or `memchached`. I'm not saying that benchmarks should be viewed as very authoritative, but I would certainly be interested in seeing benchmarks where mongo beats nosql in a read-heavy situation. I'm trying to be objective here, and I'm not saying that there's no place for NoSQL, I'm saying that SQL is an excellent, mature technology and there's no reason to move to NoSQL unless you have a specific use case that calls for a NoSQL database." CreationDate="2014-07-26T03:51:09.707" UserId="548" />
  <row Id="1906" PostId="797" Score="0" Text="ok, there's nosql. redis is key/value. i was just pointing out some actual use cases in my answer below and then all of a sudden i'm getting down voted by people who, i can only assume, have never used any other database other than MySQL and are afraid of doing so. a question like this wouldn't even be on normal stack exchange. it's just frustrating. though i do understand your point of view. don't get me wrong there." CreationDate="2014-07-26T04:22:40.623" UserId="2711" />
  <row Id="1907" PostId="797" Score="0" Text="http://stackoverflow.com/questions/9702643/mysql-vs-mongodb-1000-reads/9703513#9703513 is another great answer and thing to think about. it depends on how you're using these databases. if you're doing all these joins, then i just can't see how it's faster. mongodb of course can be faster than mysql and easily vica versa. and i always find mongodb useful when working with unpredictable and changing data." CreationDate="2014-07-26T04:40:30.683" UserId="2711" />
  <row Id="1908" PostId="797" Score="0" Text="Oh, another good one to look at is InfluxDB. It's newer. NoSQL technically, but has a SQL-like syntax. It's great for time series. I had a bit of fun with it." CreationDate="2014-07-26T05:29:29.660" UserId="2711" />
  <row Id="1909" PostId="841" Score="0" Text="Thanks, I was looking for something on these lines. It would be extremely helpful if you could give reference to a tutorial or ipython notebook discussing this." CreationDate="2014-07-26T06:41:43.930" UserId="1131" />
  <row Id="1910" PostId="844" Score="0" Text="See http://stackoverflow.com/questions/22592811/scala-spark-task-not-serializable-java-io-notserializableexceptionon-when/22594142#22594142" CreationDate="2014-07-26T09:58:00.053" UserId="2668" />
  <row Id="1911" PostId="753" Score="0" Text="does anyone have the algorithm to get the &quot;consistent phrases&quot;?" CreationDate="2014-07-26T10:48:48.253" UserId="122" />
  <row Id="1912" PostId="525" Score="0" Text="Thanks, this is awesome!" CreationDate="2014-07-27T07:08:12.487" UserId="913" />
  <row Id="1913" PostId="806" Score="1" Text="Consider a highly unbalanced problem. That is where ROC AUC is very popular, because the curve balances the class sizes. It's easy to achieve 99% accuracy on a data set where 99% of objects is in the same class." CreationDate="2014-07-27T10:26:44.380" UserId="924" />
  <row Id="1914" PostId="851" Score="4" Text="There are basically 2 possible ways to go. Simple one is to simply compare colour histograms. [This question](http://stackoverflow.com/questions/6499491/comparing-two-histograms) give pretty good description of several good measures/methods. But if you are going to use it in image search engine or something like that, it makes sense to also mimic human _perception_ of colours, which is much harder task. [This paper](http://ect.bell-labs.com/who/emina/papers/pfl.pdf) provides some cues for better human-aware comparison." CreationDate="2014-07-27T22:18:12.620" UserId="1279" />
  <row Id="1915" PostId="851" Score="1" Text="That's a perfectly adequate answer, @ffriend." CreationDate="2014-07-28T01:27:55.820" UserId="381" />
  <row Id="1916" PostId="851" Score="0" Text="This is a cross post from Cross Validated. Anyone know what to do?" CreationDate="2014-07-28T02:23:36.587" UserId="1241" />
  <row Id="1917" PostId="831" Score="0" Text="@AsheeshR, yes this question is a matter of opinion but I think that the community can really benefit from it. How do we proceed?" CreationDate="2014-07-28T04:53:52.953" UserId="366" />
  <row Id="1919" PostId="850" Score="0" Text="Yes this is what I was looking for. I would be interested to see if this reduces to something like a Welch t-test because it may illuminate some complications. Eq. 52 in the paper you gave is nearly exactly what I want except it is not just a multinomial but a multinomial called a number of times for each user. I think the problem boils down to a generalized Behrens–Fisher problem. I am however a little out of my depth. Do you think you could sketch out a solution for this particular case? I know there are others on **Cross Validated** interested in the solution." CreationDate="2014-07-28T11:12:26.290" UserId="2511" />
  <row Id="1921" PostId="764" Score="0" Text="You need to find an algorithm that takes a pre-computed distance matrix or allows you to supply a distance-function that it can call when it needs to compute distances. Otherwise it wont work." CreationDate="2014-07-28T13:45:35.470" UserId="471" />
  <row Id="1922" PostId="823" Score="0" Text="Please, re-edit your question. I feel you have more than one question in your mind. Can you appropriately list them (first paragraph for introducing what you have read, second paragraph for your opinion about it, and third paragraph for listing your questions)?" CreationDate="2014-07-28T17:12:41.130" UserDisplayName="user1361" />
  <row Id="1925" PostId="831" Score="0" Text="@power Please take a look at http://datascience.stackexchange.com/help/dont-ask. If you still disagree, please raise the issue on [meta]." CreationDate="2014-07-29T02:08:03.853" UserId="62" />
  <row Id="1926" PostId="831" Score="0" Text="@AsheeshR Okay, I will re-word the question shortly." CreationDate="2014-07-29T03:56:38.243" UserId="366" />
  <row Id="1931" PostId="823" Score="1" Text="I'd suggest removing the word &quot;moral&quot; from the title of the question. Ethics, by definition, implies moral aspect as foundational." CreationDate="2014-07-29T12:38:08.847" UserId="2452" />
  <row Id="1932" PostId="823" Score="1" Text="@AleksandrBlekh sure will do, thanks for feedback, I noted today that okcupid have just admitted experimenting on users." CreationDate="2014-07-29T12:39:48.540" UserId="95" />
  <row Id="1934" PostId="846" Score="0" Text="Getting the data is often one of the biggest challenge :)" CreationDate="2014-07-29T19:35:55.310" UserId="737" />
  <row Id="1935" PostId="865" Score="0" Text="This question is very difficult to understand. Can you please consider rewriting it with a better description of what you are looking for and what you have already tried." CreationDate="2014-07-30T12:22:03.380" UserId="802" />
  <row Id="1936" PostId="866" Score="0" Text="Can you provide some example data (in plain English, no codes)?" CreationDate="2014-07-30T13:49:20.293" UserId="1279" />
  <row Id="1937" PostId="866" Score="0" Text="I added some example data to my original post.  In this version, each condition is denoted by a three letter code." CreationDate="2014-07-30T13:59:47.707" UserId="2781" />
  <row Id="1941" PostId="866" Score="1" Text="R is cool, but not very human-readable. Could you please reformat sample of your data as a table (e.g. using CSV or TSV format; 5-6 columns is ok)? Also, some explanation of variables (what &quot;anx.any&quot;, &quot;flu.isbefore.ckd&quot;, etc. actually mean and what is to be predicted) will help a lot." CreationDate="2014-07-30T19:49:21.060" UserId="1279" />
  <row Id="1942" PostId="871" Score="1" Text="Let me know if you have further questions and I'll do my best to provide some more detail." CreationDate="2014-07-30T20:09:29.443" UserId="684" />
  <row Id="1943" PostId="870" Score="0" Text="Worth noting that this is not explicitly a problem in all of machine learning, but only a problem when it comes to generating feature vectors, which are not ubiquitous in machine learning." CreationDate="2014-07-30T20:12:37.283" UserId="869" />
  <row Id="1944" PostId="870" Score="0" Text="What kind of machine learning doesn't use features?" CreationDate="2014-07-30T20:22:26.113" UserId="381" />
  <row Id="1945" PostId="870" Score="0" Text="Random forest is a good example of something for which getting a feature vector of the sort you see in neural nets is not an issue. A lot of unsupervised methods also work on raw words rather than feature vectors. Note: I didn't say there are methods that don't use features, only that there are methods which do not rely on strictly structured vectors." CreationDate="2014-07-30T23:00:17.087" UserId="869" />
  <row Id="1946" PostId="870" Score="0" Text="I don't know what you mean by &quot;strictly structured&quot;." CreationDate="2014-07-30T23:05:35.013" UserId="381" />
  <row Id="1947" PostId="870" Score="0" Text="strictly structured is a 1d vector of unint8's as opposed to a list containing a dictionary, a weight matrix, and a series of strings" CreationDate="2014-07-30T23:48:32.817" UserId="869" />
  <row Id="1948" PostId="769" Score="0" Text="It took me a while to try everything out, I had an error earlier with PCA, now I see I just get much lower accuracy when using it. I reduce dimensions to 100, and that should be fine, but SVM gives me 4% error on MNIST (0.6% without PCA) and over 20% error on DIGITS (4% without PCA). Same for other classifiers. Earlier I somehow made the error of doing PCA on the whole dataset (train and test sets) which gave me too optimistic results." CreationDate="2014-07-31T05:29:44.877" UserId="1387" />
  <row Id="1949" PostId="875" Score="0" Text="Thanks, I'll try it" CreationDate="2014-07-31T08:13:05.463" UserId="988" />
  <row Id="1950" PostId="863" Score="0" Text="I'm not so sure about what you say on MapReduce, especially `So basically any problem that doesn't break data locality principle may be efficiently implemented using MapReduce`. From my understanding, you can only solve problems which can be expressed with the MapReduce pattern." CreationDate="2014-07-31T09:33:22.557" UserId="883" />
  <row Id="1951" PostId="863" Score="0" Text="@fxm: MapReduce framework may be used for pretty much different tasks. For example, Oozie - workflow scheduler for different Hadoop components - has so-called Java action, that simply creates one mapper and runs custom Java code in it. With this approach you can essentially run _any_ code. This, however, won't give you any MR advantages compared to simple Java app. The only way to gain these advantages is to run computations in parallel (using Map) on *nodes with data*, that is, locally. To summarize: you can run _any_ code with MR, but to obtain performance, you need to hold  data locality." CreationDate="2014-07-31T11:23:04.177" UserId="1279" />
  <row Id="1952" PostId="863" Score="0" Text="I agree, but what I meant is a developer should not worry about managing data locality (which is managed by hadoop), but rather about expressing the wanted algorithms with the MapReduce pattern." CreationDate="2014-07-31T12:00:27.257" UserId="883" />
  <row Id="1953" PostId="866" Score="0" Text="Understandable.  I was trying to provide a &quot;reproducible example.&quot;  I will update the question now.  Thanks." CreationDate="2014-07-31T12:11:59.803" UserId="2781" />
  <row Id="1954" PostId="863" Score="0" Text="@fxm: developer ensures locality by using mapper. Roughly speaking, &quot;map(f, data)&quot; means &quot;transfer f() to data nodes and run it locally&quot;.  If developer doesn't consider locality (e.g. puts all computations to reducer), he loses all advantages of MR. So expressing algo as (efficient) MR job implies utilization of data locality anyway. That said, I would still suggest using more flexible tools than pure MR such as Spark." CreationDate="2014-07-31T13:29:35.030" UserId="1279" />
  <row Id="1955" PostId="769" Score="0" Text="Which programming language are you using btw? If you are a Python guy, I'd have some examples here where I used PCA, maybe it helps: http://sebastianraschka.com/Articles/2014_about_feature_scaling.html http://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html http://sebastianraschka.com/Articles/2014_pca_step_by_step.html Usually I prefer LDA since I am mostly working with supervised datasets (class labels), a separate article (like the step by step PCA) is in the works ;)" CreationDate="2014-07-31T13:47:17.910" UserId="2556" />
  <row Id="1956" PostId="871" Score="0" Text="Thanks, that gives me some great terms to continue exploring with!" CreationDate="2014-07-31T15:21:58.400" UserId="2790" />
  <row Id="1957" PostId="769" Score="0" Text="I am using C++ for classification and Matlab to prepare datasets. I will check out your links and try LDA too." CreationDate="2014-07-31T16:04:48.890" UserId="1387" />
  <row Id="1958" PostId="876" Score="0" Text="Thanks, Andy.  Could you elaborate a little? Is it because the variables don't capture enough detail?" CreationDate="2014-07-31T16:42:18.460" UserId="2781" />
  <row Id="1959" PostId="876" Score="0" Text="I have no idea. I guess it depends on how different models work." CreationDate="2014-07-31T16:54:46.747" UserId="1241" />
  <row Id="1960" PostId="876" Score="0" Text="Could you suggest some of the solutions you tried or considered?" CreationDate="2014-07-31T17:25:51.437" UserId="2781" />
  <row Id="1961" PostId="876" Score="0" Text="So far I haven't done either, so no help there. Sorry." CreationDate="2014-07-31T17:38:05.417" UserId="1241" />
  <row Id="1962" PostId="641" Score="0" Text="@Madison May. Did you find a data set? I'm looking for something similar. Thanks." CreationDate="2014-07-31T22:49:26.447" UserId="2507" />
  <row Id="1963" PostId="641" Score="0" Text="I had to make do with the twitter ner corpus from U. Washington (linked to in original post)." CreationDate="2014-07-31T23:01:20.507" UserId="684" />
  <row Id="1964" PostId="865" Score="0" Text="Not a data science question, its a programming question. Go ask on StackOverflow" CreationDate="2014-08-01T09:08:50.180" UserId="471" />
  <row Id="1965" PostId="840" Score="0" Text="Have you checked the contents of `df`?" CreationDate="2014-08-01T12:17:52.767" UserId="172" />
  <row Id="1966" PostId="881" Score="0" Text="Thanks for this! It confirms some of the steps I have already taken (exploratory analysis, hypothesis testing, etc.)." CreationDate="2014-08-01T15:15:02.910" UserId="2781" />
  <row Id="1967" PostId="883" Score="4" Text="What class is the Date column? It looks like it might be sorting by character (1 comes before 9) rather than date value." CreationDate="2014-08-01T20:13:47.650" UserId="2802" />
  <row Id="1968" PostId="883" Score="0" Text="Base R read.csv() converts strings to factor. I agree with @user1683454; you will find that your dates are in alphabetical order." CreationDate="2014-08-02T04:59:48.967" UserId="2666" />
  <row Id="1969" PostId="722" Score="2" Text="I would also add `dplyr`, which is an optimized rephrasing of certain `plyr` tools, and `data.table` which is a completely different approach to manipulating data. Both by Hadley Wickham." CreationDate="2014-08-02T05:22:07.200" UserId="1156" />
  <row Id="1970" PostId="722" Score="0" Text="@ssdecontrol: I agree - updated the answer. Hadley is the author of the `dplyr` package, but not of the `data.table` one." CreationDate="2014-08-02T08:22:18.307" UserId="2452" />
  <row Id="1971" PostId="722" Score="1" Text="Funny, I just kind of assumed he was. Thanks." CreationDate="2014-08-02T11:46:36.047" UserId="1156" />
  <row Id="1972" PostId="890" Score="0" Text="The computations I perform are both cpu and disk intense, which may occur concurrently. Using gpu's would surely speedup the cpu step, but disk access would still limit the performance." CreationDate="2014-08-02T16:53:34.197" UserId="84" />
  <row Id="1973" PostId="823" Score="1" Text="http://www.datakind.org/blog/meetup-recap-untangling-ethical-questions-in-data-science/" CreationDate="2014-08-02T19:58:25.010" UserId="381" />
  <row Id="1974" PostId="890" Score="1" Text="@Rubens you've to have good reason to use GPU for your computation. It expects fine-grained matrix type data, and well written parallel works, kernel implementations. For your disk problem, you'ld stream your data and feed them into RAM so that both (especially) GPU and CPU will benefits. Then, you can use your data in any, arithmetic, manner e.g. you can use map+reduce approach on your stream. To note, CUDA has built-in support for Map+Reduce+filter operations. However, testing performance on GPU based calculation is tricky (You can use manual calculation putting watcher around your code)." CreationDate="2014-08-02T20:51:25.250" UserDisplayName="user1361" />
  <row Id="1975" PostId="890" Score="0" Text="@Rubens Some patterns in [Patterns for Parallel Programming book](http://www.amazon.com/Patterns-Parallel-Programming-paperback-Software/dp/0321940784) will probably solve your disk problem." CreationDate="2014-08-02T20:53:10.027" UserDisplayName="user1361" />
  <row Id="1976" PostId="739" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:00.257" UserId="62" />
  <row Id="1977" PostId="808" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:34.900" UserId="62" />
  <row Id="1978" PostId="769" Score="0" Text="I tried using LDA but can't get it working with my data. Matlab function classify should perform LDA but it works only up to 20 dimensions, at least on my data. Also I found that maximum dimensions given by LDA should be number_of_classes-1, which is too little." CreationDate="2014-08-03T09:20:08.827" UserId="1387" />
  <row Id="1981" PostId="893" Score="4" Text="I'd say [CV.SE](http://stats.stackexchange.com/) is a better place for questions about more theoretical statistics like this. If not, I'd say that the answer to your questions depend on context. Sometimes it makes sense to flatten multiple levels into dummy variables, other times it's worth to model your data according to multinomial distribution, etc." CreationDate="2014-08-03T14:00:11.460" UserId="1279" />
  <row Id="1985" PostId="876" Score="0" Text="I'm now on vacation for the next few weeks, but when I get back I'll look into it because it really has piqued my interest." CreationDate="2014-08-03T20:29:59.447" UserId="1241" />
  <row Id="1986" PostId="878" Score="0" Text="Thanks for the response, do you have some source explaining more thoroughly about what kind of data is collected and methods of user tracking?" CreationDate="2014-08-03T20:37:39.297" UserId="2798" />
  <row Id="1987" PostId="878" Score="0" Text="I believe tasks are too diverse to be described in a single source. For example, in Facebook you would be interested in what user likes, who he talks to most frequently, etc. In commercial website you'd like to know what actions led to conversion and what forced user to leave site. In entertainment software (like games or funny web pages) you would most likely want to optimize user experience and thus look for UI component usage, time spent on page, etc. Different tasks require different data and different methods. Just determine your use case and look for appropriate approach." CreationDate="2014-08-03T21:15:16.970" UserId="1279" />
  <row Id="1988" PostId="769" Score="0" Text="I just uploaded the LDA article, although I used Python for the step-wise implementation, the Intro might still be interesting and helpful: http://sebastianraschka.com/Articles/2014_python_lda.html" CreationDate="2014-08-03T21:40:36.477" UserId="2556" />
  <row Id="1989" PostId="883" Score="1" Text="You should probably ask this on stackoverflow, its a basic R programming question, not really data science." CreationDate="2014-08-03T22:15:35.913" UserId="471" />
  <row Id="1990" PostId="883" Score="0" Text="@Spacedman good point. I will put it there next time." CreationDate="2014-08-04T14:23:22.043" UserId="2614" />
  <row Id="1991" PostId="902" Score="3" Text="There's one way to get &quot;generalization&quot; that is good enough for any scenario - sample the entire population. In all other cases your best option is to select confidence level and take sample large enough to give reasonable confidence interval." CreationDate="2014-08-04T20:06:43.500" UserId="1279" />
  <row Id="1993" PostId="886" Score="1" Text="As an addendum, I'd recommend taking a look at the `lubridate` package, which can make certain date manipulation tasks simpler." CreationDate="2014-08-04T23:30:59.173" UserId="1156" />
  <row Id="1994" PostId="812" Score="0" Text="Hopefully an answer to this question can touch on how graphs like these were made: https://medium.com/i-data/israel-gaza-war-data-a54969aeb23e?_ga=1.106579909.790909978.1407183841" CreationDate="2014-08-04T23:39:21.430" UserId="1156" />
  <row Id="1995" PostId="821" Score="2" Text="I would downvote this answer if I could. Modeling a feature of the data for its own sake is absurd. Models exist to answer questions, and if seasonality does not affect your question or its answer, then it is not only allowable but desirable to ignore it. With that in mind, @marcodena, the only answer to your question is &quot;because sometimes ignoring it will bias your predictions.&quot; I think a better question would ask which times those are. I also disagree with implication here that data always precedes a model in statistics." CreationDate="2014-08-04T23:44:04.903" UserId="1156" />
  <row Id="1996" PostId="811" Score="1" Text="I'd say it's safer to balance your sample, but also collect sampling weights so you can later re-weight your data for representativeness if you need to. @pnp plenty of social scientists build non-predictive models, e.g. for confirming theories." CreationDate="2014-08-04T23:56:14.120" UserId="1156" />
  <row Id="1997" PostId="886" Score="0" Text="@ssdecontrol: Agree, thanks for the comment. Upvoted." CreationDate="2014-08-05T00:15:21.147" UserId="2452" />
  <row Id="1998" PostId="904" Score="0" Text="I suggest replacing `untagged` tag with `r`, `dashboards`, `reports` or similar." CreationDate="2014-08-05T07:22:27.870" UserId="2452" />
  <row Id="1999" PostId="769" Score="0" Text="Finally I found out what was going on... My function in Matlab that writes features to a file would add spaces sometimes and only on some datasets and then my reader function in C would apparently read wrong values... PCA actually helped, boost classifier is still bad but will try to play with parameters some more to make it work. Still didn't try LDA but will do that too." CreationDate="2014-08-05T08:49:26.577" UserId="1387" />
  <row Id="2000" PostId="907" Score="0" Text="I didn't know about sparkTable, looks like a great tool for the job." CreationDate="2014-08-05T12:16:28.987" UserId="1156" />
  <row Id="2003" PostId="57" Score="1" Text="This is a very good post. R is excellent for data *manipulation* but can be pretty cumbersome with data *cleaning* because of its verbose syntax for string manipulation and fairly rigid adherence to lists and matrices for data structures." CreationDate="2014-08-05T13:01:12.313" UserId="1156" />
  <row Id="2004" PostId="871" Score="0" Text="Incidentally, I can relate to &quot;feature hashing&quot; since that seems very similar to a [bloom filter](http://en.wikipedia.org/wiki/Bloom_filter), which I'm familiar with from working with cryptocurrency code. I wonder if it's more effective to have a hashing function relate an input feature to multiple index positions (bloom-filter-style) rather than need a second hash function to set the sign of an index..." CreationDate="2014-08-05T13:44:28.797" UserId="2790" />
  <row Id="2005" PostId="769" Score="0" Text="Nice! I am glad to here that it was &quot;just&quot; a technical problem :). For supervised training samples, LDA is often (but not always) a better choice than PCA. There is a research article where the authors discuss this point: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=908974" CreationDate="2014-08-05T15:04:04.100" UserId="2556" />
  <row Id="2006" PostId="907" Score="0" Text="@ssdecontrol: Until recently, I didn't know about it, too. Looks like a great tool, for sure. I look forward to trying it in my project, if I will have a need and opportunity." CreationDate="2014-08-05T17:50:51.177" UserId="2452" />
  <row Id="2007" PostId="913" Score="0" Text="Nice answer! Any thoughts on my related questions? 1) http://stats.stackexchange.com/questions/101251/determining-characteristics-of-sampling-sets-for-efa-cfa-sem; 2) http://stats.stackexchange.com/questions/90386/optimal-sampling-strategy-for-efa-cfa-and-sem." CreationDate="2014-08-05T17:55:47.023" UserId="2452" />
  <row Id="2008" PostId="909" Score="0" Text="Hi, Alexei! It seems that you're proficient in R, so I'm wondering, if you have any advice on the problem I'm currently stuck with: http://stackoverflow.com/questions/25101444/errors-related-to-data-frame-columns-during-merging. Beyond that, I'd be glad to connect with you (see aleksandrblekh.com for my profiles on professional social networks), as it seems that we have some common interests (including the native language :-)." CreationDate="2014-08-05T19:53:25.950" UserId="2452" />
  <row Id="2009" PostId="891" Score="0" Text="I've always wondered about the difference between measures and metrics. According the government (NIST): &quot;...We use measure for more concrete or objective attributes and metric for more abstract, higher-level, or somewhat subjective attributes. ... Robustness, quality (as in &quot;high quality&quot;), and effectiveness are important attributes that we have some consistent feel for, but are hard to define objectively. Thus these are metrics.&quot; But the context is software engineering, not mathematics. What's your take?" CreationDate="2014-08-05T20:55:59.590" UserId="2507" />
  <row Id="2010" PostId="891" Score="1" Text="Wikipedia was more helpful. distance(x,y) is must be non-negative; d(x,y)=0 only if x=y; d(x,y) = d(y,x); and satisfy triangle inequality- d(x, z) ≤ d(x, y) + d(y, z)" CreationDate="2014-08-05T21:03:19.577" UserId="2507" />
  <row Id="2011" PostId="915" Score="0" Text="This is a good question. A lot of statistics books talk about the theoretical aspects of high-dimensional data and not the computational aspects." CreationDate="2014-08-06T00:23:33.773" UserId="1156" />
  <row Id="2012" PostId="896" Score="0" Text="Can you add a full citation for the paper? The link doesn't work." CreationDate="2014-08-06T00:47:34.150" UserId="1156" />
  <row Id="2013" PostId="917" Score="2" Text="Typo in the title: spare =&gt; sparse." CreationDate="2014-08-06T02:52:16.923" UserId="2452" />
  <row Id="2014" PostId="891" Score="0" Text="That's pretty much it: a metric has to meet certain axioms and a measure is less strictly defined." CreationDate="2014-08-06T03:34:29.120" UserId="2809" />
  <row Id="2015" PostId="893" Score="0" Text="Are your categorical variables ordered ? If yes, this can influence the type of correlation you want to look for." CreationDate="2014-08-06T06:58:25.637" UserId="906" />
  <row Id="2016" PostId="918" Score="3" Text="Thank you for the great answer! I am hesitant to classify the problem as sparse regression since I am not really trying to model and predict but rather solve for a set of coefficients. The reason I am using Genetic Algorithms is because I can also employ constraints on the equation. If no other answers come through I will gladly accept this though." CreationDate="2014-08-06T12:27:51.970" UserId="802" />
  <row Id="2017" PostId="919" Score="2" Text="With k=5 you will get 20k observations in training set and 5k in testing set. With k=25 you'll get 24k for training and 1k for testing. If you believe that additional 4k records will affect generalization a lot, use larger k. If you think that even, say, 10k records already give good generalization, use smaller k. If you are unsure, just use standard 10-fold cross validation, which is a good compromise in most cases." CreationDate="2014-08-06T13:02:48.630" UserId="1279" />
  <row Id="2018" PostId="924" Score="0" Text="Thanks a lot. As I see, it's all about learning Caret package.." CreationDate="2014-08-06T13:14:47.953" UserId="97" />
  <row Id="2019" PostId="927" Score="0" Text="I split my test/train sets this way:  http://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas  Maybe this can help." CreationDate="2014-08-06T17:09:21.220" UserId="375" />
  <row Id="2020" PostId="918" Score="1" Text="@mike1886: My pleasure! I have updated my answer, based on your comment. Hope it helps." CreationDate="2014-08-06T21:34:14.897" UserId="2452" />
  <row Id="2021" PostId="926" Score="1" Text="Stephan, I appreciate your kind words! Upvoted your nice answer. You might be interested in the update I made to my answer, based on comment by the question's author." CreationDate="2014-08-06T21:44:31.100" UserId="2452" />
  <row Id="2022" PostId="896" Score="0" Text="@ssdecontrol the paper is called &quot;Class-Based n-gram Models of Natural Language&quot; by Brown et al. I have actually managed to resolve this. I will post details as an answer." CreationDate="2014-08-07T03:02:32.407" UserId="2817" />
  <row Id="2023" PostId="685" Score="0" Text="Instead of &quot;saving the coefficients&quot; you could save the whole model to a file, later load it again and use the predict() function. Should make the process a bit easier and less error prone." CreationDate="2014-08-07T08:41:22.423" UserId="676" />
  <row Id="2024" PostId="685" Score="0" Text="How much data did you use for training? Did you tune glmnet's lambda parameter and how? Cross validation?" CreationDate="2014-08-07T08:43:01.317" UserId="676" />
  <row Id="2025" PostId="933" Score="1" Text="Also, his other publications are at publications at http://research.microsoft.com/en-us/people/cyl/publication.aspx" CreationDate="2014-08-07T09:34:05.100" UserId="2861" />
  <row Id="2026" PostId="934" Score="1" Text="I asked a somewhat related question : http://datascience.stackexchange.com/q/810/2661" CreationDate="2014-08-07T12:24:39.557" UserId="2661" />
  <row Id="2027" PostId="934" Score="1" Text="Also, did you try the `BayesNet` (Bayesian Networks) algorithm in Weka and tried tuning the `MaxNrOfParents` argument in K2 search algorithm? I found it of good help in class imbalance problems." CreationDate="2014-08-07T12:28:24.147" UserId="2661" />
  <row Id="2028" PostId="930" Score="0" Text="the first suggestion will work but it is kind of &quot;manual&quot;.I was looking for train_test_split way to get dataframe. The second explanation does what i normally do but it returns nparray to which it is hard to impute values since there are no column names." CreationDate="2014-08-07T13:57:43.967" UserId="2854" />
  <row Id="2029" PostId="929" Score="0" Text="I can do that but it does not preserve any column names from the original dataframe. Then I need to impute values by indexes which can be very painful and can be a source of errors." CreationDate="2014-08-07T14:00:36.443" UserId="2854" />
  <row Id="2030" PostId="929" Score="0" Text="You can simply get the columns from the first dataframe in &#xA;cols = list(df_orig.columns.values)&#xA;and then set the new column names with this&#xA;new_df = pf.DataFrame(a,columns=cols)" CreationDate="2014-08-07T14:06:47.757" UserId="802" />
  <row Id="2031" PostId="936" Score="1" Text="Have you tried running that code in pure R, not Knitr? It seems to be more of an R problem and rather than a rendering to graphics problem." CreationDate="2014-08-07T15:14:37.097" UserId="802" />
  <row Id="2032" PostId="929" Score="0" Text="That really worked! Thanks a lot!" CreationDate="2014-08-07T15:27:42.750" UserId="2854" />
  <row Id="2033" PostId="936" Score="0" Text="The code produces no error when run in R." CreationDate="2014-08-07T15:40:41.360" UserId="2792" />
  <row Id="2034" PostId="936" Score="1" Text="As it stands, this example is not reproducible and therefore nobody can help you. Can you provide a minimal working example (MWE) of your .Rmd file that replicates the error? Chances are, constructing the MWE will also help you figure out what the problem is. However, my guess is that you aren't loading your data anywhere in the code. Knitr searches a new environment and not the current global environment, so you will need to re-load all packages and objects you plan to use." CreationDate="2014-08-07T16:45:11.010" UserId="1156" />
  <row Id="2035" PostId="922" Score="0" Text="Hi @Ali, it would help if you added some more detail. What do you mean by &quot;similarity words&quot;? are you talking about some preprocessing step? SVMs don't use a similarity measure." CreationDate="2014-08-07T23:39:38.887" UserId="21" />
  <row Id="2036" PostId="777" Score="0" Text="I think this might be more suitable for StackOverflow if you're looking for how to use the Github API? Maybe ServerFault?" CreationDate="2014-08-07T23:40:33.203" UserId="21" />
  <row Id="2037" PostId="910" Score="0" Text="This answer is exactly what I was hoping to get here. Thank you very much." CreationDate="2014-08-08T07:18:47.767" UserId="1192" />
  <row Id="2038" PostId="866" Score="0" Text="Can you provide more information on the parameters used in the data set so that we can understand if there are any correlations. Some of the abbreviations mentioned by you are not clear to me. It would be great if you could share your email-id for us to collaborate offline. Thanks!" CreationDate="2014-08-08T06:04:45.150" UserId="1094" />
  <row Id="2039" PostId="748" Score="0" Text="Most of your question here is describing a problem with your question. The text does not contain your question. Please in-line it and improve the text if needed. I don't think you need to summarize the history on the other site." CreationDate="2014-08-08T12:20:17.923" UserId="21" />
  <row Id="2040" PostId="812" Score="0" Text="Hello @Cici, usually questions about recommended tools are discouraged on this and other SE sites, as they just invite a lot of opinion." CreationDate="2014-08-08T12:21:22.990" UserId="21" />
  <row Id="2041" PostId="821" Score="2" Text="I know but I'm a student, so I'm learning. The question is made for this reason :)" CreationDate="2014-08-08T13:27:54.967" UserId="989" />
  <row Id="2042" PostId="942" Score="0" Text="Thanks, that's very helpful! If you do a literature review for various predictive modelling techniques, I'm sure it would get referenced a lot. It would be very helpful for people who want to differentiate between which algorithms to use in large n or large p cases, or for medium values of those for more precise calculations. Do you happen to know how some of the more obscure techniques scale? (Like Cox proportional hazard regression or confirmatory factor analysis)" CreationDate="2014-08-08T13:31:01.120" UserId="2841" />
  <row Id="2044" PostId="934" Score="0" Text="http://cs229.stanford.edu/proj2005/AltendorfBrendeDanielLessard-FraudDetectionForOnlineRetailUsingRandomForests.pdf&#xA;A good read that involves a similar 'rare-event' problem.  The authors use a random forest and optimize based on the ratio of class occurrence in the training set. (I'm not affiliated, but was just reading this a few days ago for a problem I'm working on)." CreationDate="2014-08-08T16:12:01.837" UserId="375" />
  <row Id="2045" PostId="947" Score="0" Text="That's really helpful. In our case we have currently hundreds of patterns so we'd end up having vectors with hundreds of features. Do these algorithms behave well in this situation? Other thing, each time a totally new exception comes up we'd need to classify it manually, possibly add new features and launch the train process to get a new model. Is this an appropriate approach?" CreationDate="2014-08-09T09:35:12.643" UserId="2878" />
  <row Id="2046" PostId="947" Score="0" Text="NB and SVM should work fine, decision trees over hundreds of variables may be hard to visualize and interpret, but I don't expect degradation in accuracy (though **random forests** are often suggested in such settings). As for new exceptions, standard way is to keep special `&lt;UNKOWN&gt;` variable, and really add new variables only when percentage of unknowns exceeds, say, 5%." CreationDate="2014-08-09T12:09:35.123" UserId="1279" />
  <row Id="2047" PostId="947" Score="0" Text="Great. Thanks!!" CreationDate="2014-08-09T13:50:19.260" UserId="2878" />
  <row Id="2048" PostId="937" Score="0" Text="I created my own class for that but very surprised that sklearn doesn't have that." CreationDate="2014-08-09T14:36:09.090" UserId="2854" />
  <row Id="2049" PostId="713" Score="0" Text="FWIW, I just started the 5.1 QuickStart VM and there was no such warning." CreationDate="2014-08-10T10:08:52.010" UserId="21" />
  <row Id="2052" PostId="715" Score="0" Text="There's no configuration that is required, or certainly there shouldn't be. The 5.1 VM does not exhibit this." CreationDate="2014-08-11T14:18:47.123" UserId="21" />
  <row Id="2053" PostId="954" Score="1" Text="possible duplicate of [Coreference Resolution for German Texts](http://datascience.stackexchange.com/questions/955/coreference-resolution-for-german-texts)" CreationDate="2014-08-11T15:19:58.320" UserId="1279" />
  <row Id="2054" PostId="757" Score="0" Text="Have you tried a model using temperatures?" CreationDate="2014-08-11T19:03:57.133" UserId="325" />
  <row Id="2055" PostId="959" Score="0" Text="Thanks for the helpful suggestions. I will definitely check out that book! Though I have access to lab values, the data is unreliable and sporadic, so i am trying to stick to data I can get from claims. The variable abbreviations are actually AHRQ Clinical Classification Software groupings of diagnosis codes." CreationDate="2014-08-11T22:01:36.267" UserId="2781" />
  <row Id="2056" PostId="955" Score="2" Text="Your question is very concise, yet it'd be nice to hear from you what you've tried so far, or even see some example of what you're trying to achieve. Would you mind adding some further information to your post?" CreationDate="2014-08-12T01:55:23.210" UserId="84" />
  <row Id="2057" PostId="955" Score="0" Text="possible duplicate of [OpenNLP Coreference Resolution (German)](http://datascience.stackexchange.com/questions/954/opennlp-coreference-resolution-german)" CreationDate="2014-08-12T08:00:42.737" UserId="2920" />
  <row Id="2058" PostId="954" Score="0" Text="It is not a duplicate. It is a question specific to OpenNLP and the other question is about coreference resolution tools in general" CreationDate="2014-08-12T10:57:39.147" UserId="979" />
  <row Id="2059" PostId="955" Score="0" Text="I already added additional information to the post" CreationDate="2014-08-12T11:00:45.350" UserId="979" />
  <row Id="2060" PostId="954" Score="0" Text="I can't see the difference between these questions. Both ask for a coreference tool besides OpenNLP, because OpenNLP doesn't support it." CreationDate="2014-08-12T11:20:12.657" UserId="21" />
  <row Id="2061" PostId="965" Score="1" Text="I think you'll probably need to be more specific about what your data is like, what your tool requirements are, what you've tried, in order to get more specific replies." CreationDate="2014-08-12T11:22:30.637" UserId="21" />
  <row Id="2062" PostId="967" Score="1" Text="Actually I disagree. Since the author likes them being in trouble, it is a positive sentiment there. It's a negative comment on the company, but nevertheless a positive sentiment by the author. In this simpler scenario (I'm not saying this is the complete goal), **predicting which emojis a user would add to his post** sounds like a reasonable task to me. In fact you can construct many cases where the emoji will be essential.. Consider &quot;Got f_cked :-)&quot; as opposed to &quot;Got f_cked. :-(&quot;" CreationDate="2014-08-12T13:32:24.477" UserId="2920" />
  <row Id="2063" PostId="963" Score="0" Text="Don't believe that something like this exists currently, but would love it if you put something together for this!" CreationDate="2014-08-12T13:57:53.260" UserId="548" />
  <row Id="2064" PostId="967" Score="0" Text="In case you try to estimate person's emotion as opposed to person's attitude to a subject, then yes, this example doesn't work. But there are many others. Sarcasm is common case. Consider sentence &quot;oh yeah, you are real 'master' ;)&quot;. Human can catch negative context, but positive emoticon will point to positive emotion. But I haven't really got it: do you want to extract subjective information from tweets or just predict possible emojis? Even though they sound similar, second task is not really about sentiment analysis. Not directly, at least." CreationDate="2014-08-12T14:10:36.103" UserId="1279" />
  <row Id="2065" PostId="954" Score="0" Text="This question asks about training methods to add the coref resolution functionality to OpenNLP and the other one asks for available tools for German coreference resolution. This is a big difference!" CreationDate="2014-08-12T15:11:32.730" UserId="979" />
  <row Id="2066" PostId="967" Score="0" Text="The &quot;wink&quot; smiley is usually not considered to be &quot;positive&quot;, but &quot;ironic&quot;... which is why a good dictionary such as SentiWordNet makes sense. If you look up funny in SentiWordNet, is has more than one meaning, too! http://sentiwordnet.isti.cnr.it/search.php?q=funny&#xA;(So it *is* not trivial to annotate them manually, because it's not as simple as positive/negative; but you should do the usual interrater-agreement validation etc.)" CreationDate="2014-08-12T16:03:50.307" UserId="2920" />
  <row Id="2067" PostId="967" Score="0" Text="Now I see your idea. But I don't really think it will work, just because (most) emojis don't really sound like a good predictors to me, and you explicitly don't want to use other features. Anyway, this is just an opinion based on my experience, only data can give real answers. Good luck!" CreationDate="2014-08-12T21:39:06.460" UserId="1279" />
  <row Id="2068" PostId="967" Score="0" Text="Who said I don't want to use other features? But for these I have seen databases..." CreationDate="2014-08-13T10:13:26.760" UserId="2920" />
  <row Id="2069" PostId="954" Score="1" Text="OK. I suggest rewording these to make them more distinct. I think many readers will not get the nuance of what you are asking in each case." CreationDate="2014-08-13T12:17:52.483" UserId="21" />
  <row Id="2071" PostId="973" Score="0" Text="Can you clarify? It sounds like you are looking for feature importance, but the link you give talks about writing weights as a linear combination of input, which is not the same thing at all." CreationDate="2014-08-14T10:22:23.980" UserId="21" />
  <row Id="2072" PostId="973" Score="0" Text="@SeanOwen, actually, what I wanted to say was since you can compute the weight vector using the dual variables and the input examples (in a binary SVM), and the actual weights corresponding to features tell us the relative importance of the features [Gene Selection for Cancer Classification using&#xA;Support Vector Machines&#xA;](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9598&amp;rep=rep1&amp;type=pdf)" CreationDate="2014-08-14T14:48:25.540" UserId="2949" />
  <row Id="2074" PostId="957" Score="0" Text="If this does not get answered here, consider migrating it to the statistics site (stats.stackexchange.com) where you will be more likely to find experts in [time series](http://stats.stackexchange.com/questions/tagged/time-series)." CreationDate="2014-08-14T17:08:25.320" UserId="1237" />
  <row Id="2075" PostId="976" Score="2" Text="Math formatting does not work on DataScience? Really? May be we should ask to get it." CreationDate="2014-08-14T17:29:51.060" UserId="1237" />
  <row Id="2077" PostId="976" Score="0" Text="Good point about numerical accuracy." CreationDate="2014-08-15T00:24:26.790" UserId="1156" />
  <row Id="2078" PostId="980" Score="1" Text="Thanks a lot for such detailed response. I will go through all your suggestions, a lot of work to try and test! Also I have found that carrot2 tool is really doing great job on unsupervised clustering of textual data. Posting link for future reference [http://project.carrot2.org/](http://project.carrot2.org/)" CreationDate="2014-08-15T19:20:33.777" UserId="2958" />
  <row Id="2079" PostId="980" Score="0" Text="@MaximGalushka: You're very welcome! I'm curious to learn about your findings and the progress that you will achieve eventually. Feel free to post here or connect directly with me." CreationDate="2014-08-16T01:25:37.757" UserId="2452" />
  <row Id="2080" PostId="977" Score="0" Text="Cross-post from stats http://stats.stackexchange.com/questions/111911/solutions-for-continuous-online-cluster-identification AND stackoverflow: http://stackoverflow.com/questions/24970702/algorithm-for-identifying-non-ambiguous-clusters" CreationDate="2014-08-16T10:33:48.610" UserId="924" />
  <row Id="2082" PostId="915" Score="0" Text="In many cases, the original literature will discuss complexity. But often theoretical complexity is useless. QuickSort has a worst-case of O(n^2), but often is the fastest - faster than HeapSort, which has worst case O(n log n).&#xA;If you do a little research, you will find out complexity results for many algorithms - if known. E.g. PCA being O(n d^3), k-means being O(n k i d) etc." CreationDate="2014-08-16T10:39:50.540" UserId="924" />
  <row Id="2083" PostId="984" Score="0" Text="I don't think it's possible for people to help you without the data set, and it sounds like that is intentionally not public. I am not sure it's a good thing to ask for it then, and questions asking for links or resources are not considered on-topic for StackExchange sites." CreationDate="2014-08-16T17:31:37.270" UserId="21" />
  <row Id="2084" PostId="985" Score="2" Text="Sure, you can. It's common to do dimensionality reduction as a preprocessing step." CreationDate="2014-08-16T17:39:49.320" UserId="381" />
  <row Id="2085" PostId="898" Score="1" Text="Thanks Alexey for the details. Based on more research i found about polyserial and polychloric correlation. How is your approach better than these? Please explain" CreationDate="2014-08-17T09:58:57.173" UserId="1151" />
  <row Id="2086" PostId="898" Score="0" Text="I'm not aware of these things, sorry." CreationDate="2014-08-17T14:04:27.123" UserId="816" />
  <row Id="2087" PostId="936" Score="0" Text="As @ssdecontrol suggests, you probably haven't created or loaded `sample` in the R in your document subsequent to this line." CreationDate="2014-08-17T14:21:23.310" UserId="2978" />
  <row Id="2088" PostId="961" Score="0" Text="Please share some info on the data you are looking at." CreationDate="2014-08-17T19:22:03.863" UserId="1193" />
  <row Id="2089" PostId="961" Score="0" Text="@bayer: please check out my update." CreationDate="2014-08-17T19:48:04.687" UserId="1279" />
  <row Id="2090" PostId="961" Score="0" Text="This looks like a poor training procedure. Can you add information on the number of CD steps, learning rate/momentum, batch size etc?" CreationDate="2014-08-18T11:34:15.643" UserId="1193" />
  <row Id="2091" PostId="961" Score="0" Text="@bayer: at the moment of these experiments I used CD-1, batch size of 10 images, learning rate of 0.01 (0.1 / batch_size) and no momentum at all. I also noticed that weight initialization has some impact: with weights initialized from N(0, 0.01) I have almost never seen described issue, but with weights from N(0, 0.001) I get the issue almost each time." CreationDate="2014-08-18T11:58:30.507" UserId="1279" />
  <row Id="2092" PostId="989" Score="0" Text="Could you provide the code? Also, does it training or testing takes so much time? How about smaller training/testing datasets?" CreationDate="2014-08-18T12:09:04.657" UserId="1279" />
  <row Id="2093" PostId="989" Score="0" Text="I am just reading data from a csv file into a pandas dataframe and passing it to the scikit learn function. That's all! Providing code wouldn't really help here" CreationDate="2014-08-18T12:49:25.843" UserId="793" />
  <row Id="2094" PostId="989" Score="2" Text="sklearn's SVM implementation implies at least 3 steps: 1) creating SVR object, 2) fitting a model, 3) predicting value. First step describes kernel in use, which helps to understand inner processes much better. Second and third steps are pretty different, and we need to know at least which of them takes that long. If it is training, then it may be ok, because learning is slow sometimes. If it is testing, then there's probably a bug, because testing in SVM is really fast. In addition, it may be CSV reading that takes that long and not SVM at all. So all these details may be important." CreationDate="2014-08-18T13:22:28.580" UserId="1279" />
  <row Id="2095" PostId="995" Score="0" Text="I just edited my question to add that the dependent variable is binary. Hence, a linear model isn't suitable." CreationDate="2014-08-18T17:36:56.743" UserId="1241" />
  <row Id="2096" PostId="995" Score="0" Text="&quot; you should not expect different models to perform nearly identically, unless they all provide the same predictive bias.&quot; I used MAE and the ratio of actual to predicted outcomes as validation measures and the ratios were very close." CreationDate="2014-08-18T17:40:01.437" UserId="1241" />
  <row Id="2097" PostId="995" Score="1" Text="Andy, I would include logistic regression (and linear SVM) as 'linear' model. They are all just separating the data by a weighted sum of the inputs." CreationDate="2014-08-18T17:59:17.967" UserId="1256" />
  <row Id="2098" PostId="995" Score="1" Text="@seanv507 Exactly - the decision boundary is still linear. The fact that binary classification is being performed doesn't change that." CreationDate="2014-08-18T18:07:36.887" UserId="964" />
  <row Id="2099" PostId="961" Score="0" Text="Do you randomly initialize your weights before training?" CreationDate="2014-08-18T18:18:08.873" UserId="403" />
  <row Id="2100" PostId="961" Score="0" Text="@gallamine: yes, I initialize them from `N(0, 0.01)` or `N(0, 0.001)` with later case producing smoother images but giving described issues more frequently." CreationDate="2014-08-18T19:07:31.317" UserId="1279" />
  <row Id="2101" PostId="961" Score="0" Text="I recommend playing around with the learning rate and adding momentum. Another thing you should check is if the features (i.e. $p(h|v)$) are saturating. That is the case if all the features are always close to 0 or 1--learning gets harder then." CreationDate="2014-08-18T19:43:41.783" UserId="1193" />
  <row Id="2102" PostId="997" Score="0" Text="Could you be more specific into what you're looking for? does it matter?" CreationDate="2014-08-19T13:48:19.933" UserId="2969" />
  <row Id="2103" PostId="997" Score="0" Text="I just want to have a look at the dataset such that I can perform data visualization , or even apply time series models to the dataset." CreationDate="2014-08-19T14:44:20.317" UserId="2972" />
  <row Id="2104" PostId="997" Score="0" Text="Would EEG data work for you? It's spatial (channel locations on the head) time series data." CreationDate="2014-08-19T14:47:49.047" UserId="2969" />
  <row Id="2105" PostId="997" Score="0" Text="Sure. Thanks a lot" CreationDate="2014-08-19T14:49:30.277" UserId="2972" />
  <row Id="2106" PostId="995" Score="0" Text="What about trees? They really don't seem linear to me." CreationDate="2014-08-19T14:52:54.113" UserId="1241" />
  <row Id="2107" PostId="995" Score="0" Text="Random forests can approximate a linear boundary pretty well. But there are no details in your question regarding characteristics of the data, model parameters, or model performance such that anyone can give a definitive answer as to why they all perform similarly." CreationDate="2014-08-19T15:10:19.713" UserId="964" />
  <row Id="2108" PostId="995" Score="0" Text="With regard to oberservations-to-variables ratio, consider a 2-variable problem where `x * y &gt; 0` is classified as `True` and `x * y &lt;= 0` is `False`. Even as sample size goes to infinity, a logistic regression model will, on average, have no better than accuracy of 0.5, whereas a 2x2x2 neural network will have accuracy approaching 1.0. One can easily construct such examples where sufficiently high ratio of sample size to number of variables will not guarantee similar performance of classifiers." CreationDate="2014-08-19T15:19:36.383" UserId="964" />
  <row Id="2109" PostId="942" Score="0" Text="Unfortunately no, but if I ever do that review I will try to be comprehensive. I'd hardly call Cox regression &quot;obscure,&quot; at least in my field." CreationDate="2014-08-19T16:31:00.380" UserId="1156" />
  <row Id="2110" PostId="995" Score="0" Text="bogatron- could you elaborate? I'm not quite sure what you're asking for." CreationDate="2014-08-19T18:37:12.850" UserId="1241" />
  <row Id="2111" PostId="999" Score="0" Text="I'll ask my boss if I can get the company to pay for it." CreationDate="2014-08-19T18:39:02.327" UserId="1241" />
  <row Id="2112" PostId="699" Score="0" Text="On a related note, I found [this relevant paper](http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf)." CreationDate="2014-08-19T19:11:38.010" UserId="1097" />
  <row Id="2113" PostId="995" Score="0" Text="It *may* be that for your data set, each of the models can accurately approximate the optimal decision boundary (see answer by @StasK), though I wouldn't say that for sure without looking at the model parameters and doing a comparative error analysis. My main point was simply that a high sample-to-variable ratio does not guarantee similar performance for a given set of classifiers because they each have different biases/limitations on the kind of solutions they can produce." CreationDate="2014-08-19T19:17:08.837" UserId="964" />
  <row Id="2114" PostId="991" Score="0" Text="Thank you for your answer! I have two follow-up questions :) 1) How are SVM (with a linear kernel) and Naive Bayes different in that they do not sum up their features and corresponding weights (i.e. what you call an &quot;additive model&quot;)? Both effectively create a separating hyperplane so isn't the result is always some kind of adding features multiplied by corresponding weights? 2) I'd like to try random forests, but unfortunately the feature space is too large to represent it in dense format (I'm using sklearn). Is there an implementation that can handle that?" CreationDate="2014-08-19T22:07:18.420" UserId="2979" />
  <row Id="2115" PostId="987" Score="1" Text="Some update: I was able to achieve acceptable results by l2-normalizing the additional dense vectors. I wrongfully assumed the sklearn StandardScaler would do that. I am still looking for more complex methods, though, that would allow me to model label dependencies or incorporate confidence of sub-classifiers." CreationDate="2014-08-19T22:11:42.240" UserId="2979" />
  <row Id="2116" PostId="961" Score="0" Text="@bayer: thanks, I'll try these parameters. I'm more interested, however, in _why_ this may happen. That is, I already can fix described effect by setting larger number of hidden units, but I'm curious why at all weights in random Markov field could tend to change synchronously." CreationDate="2014-08-19T23:37:53.633" UserId="1279" />
  <row Id="2117" PostId="993" Score="1" Text="Hm why is that? more observations seems to increase the chance that the decision boundary is more complex -- i.e. definitely not linear. And these models do different things in complex cases, and tend to do the same in simple ones." CreationDate="2014-08-20T09:53:57.613" UserId="21" />
  <row Id="2118" PostId="993" Score="0" Text="@SeanOwen: I think I'm not understanding your comment. What part of my answer does &quot;why is that&quot; refer to? The OP said nothing about using linear decision boundaries - after all, he might by transforming predictors in some way." CreationDate="2014-08-20T10:31:49.017" UserId="2853" />
  <row Id="2119" PostId="993" Score="0" Text="Why would more observations make different classifiers give more similar decisions? my intuition is the opposite. Yes, I'm not thinking of just linear decision boundaries. The more complex the optimal boundary the less likely they will all fit something similar to that boundary. And the boundary tends to be more complex with more observations." CreationDate="2014-08-20T10:51:28.617" UserId="21" />
  <row Id="2120" PostId="909" Score="0" Text="This is a great explanation, and is in fact the method that I ended up using. I like to think of this method as splitting the entire sample set into smaller sub-samples and using the means (average with CLT) of each sub-sample as the distribution of the data set. Thanks for the answer!" CreationDate="2014-08-20T11:00:40.643" UserId="2830" />
  <row Id="2121" PostId="999" Score="1" Text="ESL is 'free' as a pdf from their homepage...also worth downloading is ISL (by many of same authors) - more practical http://www-bcf.usc.edu/~gareth/ISL/" CreationDate="2014-08-20T12:02:47.173" UserId="1256" />
  <row Id="2122" PostId="961" Score="0" Text="If your learning rate is too high, the first sample (or the mean of the batch) will be what the RBM overfits to. If the &quot;neurons&quot; (i.e. p(h|v)) then saturate, learning stalls--the gradients of these neurons will be close to zero. This is one way of this happening." CreationDate="2014-08-20T15:11:44.423" UserId="1193" />
  <row Id="2123" PostId="991" Score="0" Text="1) In linear regression you are interested in points _on_ hyperplane, thus you add up weighted features to get predicted point. In SVM, on other hand, you are looking for points _on the sides_ of hyperplane. You do classification by simple checking on which side is your example, no summation involved during prediction. Naive Bayes may incorporate different kinds of models (e.g. binomial or multinomial), but basically you multiply probabilities, not add them." CreationDate="2014-08-20T15:33:08.117" UserId="1279" />
  <row Id="2124" PostId="991" Score="0" Text="2) I have seen some research in this topic, but never encountered implementation (probably googling will give some links here). However, you can always go another way - reduce dimensionality with, say, PCA and then run random forest based on reduced dataset." CreationDate="2014-08-20T15:35:33.363" UserId="1279" />
  <row Id="2125" PostId="1007" Score="3" Text="Why not point us to the page that you are trying to scrape? The &quot;best tool&quot; is hard to define otherwise." CreationDate="2014-08-20T15:55:06.153" UserId="471" />
  <row Id="2126" PostId="977" Score="0" Text="Is the problem that you are trying to maintain the identity of the clusters as much as possible at each time step? So that at N+1 you can say how a cluster has changed because there is some relation between clusters at N and those at N+1? And the tricky bit is what happens if clusters split and merge?" CreationDate="2014-08-20T16:01:32.980" UserId="471" />
  <row Id="2127" PostId="977" Score="0" Text="@Spacedman: BINGO :) http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/" CreationDate="2014-08-20T16:31:32.067" UserId="725" />
  <row Id="2128" PostId="1006" Score="1" Text="+1 for all the links too! I see my road is long!" CreationDate="2014-08-20T19:10:45.143" UserId="2861" />
  <row Id="2133" PostId="858" Score="0" Text="Hey, fist I want to thank you for your answer. I have one more question. Word vector that are returned from word2vec algorithm have float values, so words like big and bigger will have vectors that are close in vector space, but the values of vectors could be completely different. For example big = [0.1, 0.2, 0,3] and bigger = [0.11, 0.21, 0.31]. Isn't that a problem for CRF algorithm, because this algorithm would treat them as not simillar? Is there any addional processing that sould be done before using this word vectors in CRF? I hope my question is clear enough." CreationDate="2014-08-21T08:10:01.407" UserId="2750" />
  <row Id="2134" PostId="961" Score="0" Text="@bayer: now it makes sense to me, thanks!" CreationDate="2014-08-21T11:00:44.460" UserId="1279" />
  <row Id="2136" PostId="1017" Score="0" Text="This is a quite open-ended question. Can you be more specific about what problem you are considering, how RL applies, what you have done so far and what your specific confusion is?" CreationDate="2014-08-21T11:53:25.470" UserId="21" />
  <row Id="2138" PostId="1010" Score="0" Text="Some sites create their HTML from Javascript and then you need a Javascript-powered scraper (Selenium). Or you reverse engineer their JSON API and get the data directly..." CreationDate="2014-08-21T15:20:35.523" UserId="471" />
  <row Id="2139" PostId="1010" Score="0" Text="some sites do, but the majority do not. As OP didn't specify, one assumes it's a 'normal' site." CreationDate="2014-08-21T15:22:06.923" UserId="2861" />
  <row Id="2140" PostId="1007" Score="2" Text="On StackExchange sites, when you say &quot;doesn't work&quot; or &quot;not satisfied&quot;, you really *have* to say what that means. Was it too slow? expensive? feature missing? Otherwise, how do you expect a suggestion for something &quot;better&quot;?" CreationDate="2014-08-21T15:49:30.967" UserId="21" />
  <row Id="2145" PostId="1017" Score="0" Text="Please ask just one question per post, unless the questions are closely related.  Your questions are quite different." CreationDate="2014-08-22T15:25:42.997" UserId="609" />
  <row Id="2146" PostId="811" Score="0" Text="How would a balanced model compare to a representative model using weighted observations to mimic a balanced model?" CreationDate="2014-08-22T22:25:21.753" UserId="1241" />
  <row Id="2147" PostId="1022" Score="0" Text="This is more of a comment than answer." CreationDate="2014-08-23T09:22:35.743" UserId="21" />
  <row Id="2148" PostId="1021" Score="2" Text="PS I think the term you're looking for is &quot;canonical&quot;" CreationDate="2014-08-23T09:23:00.063" UserId="21" />
  <row Id="2149" PostId="1028" Score="1" Text="All algorithms will overfit to some degree. It's not about picking something that doesn't overfit, it's about carefully considering the amount of overfitting and the form of the problem you're solving to maximize more relevant metrics." CreationDate="2014-08-23T18:16:45.767" UserId="548" />
  <row Id="2150" PostId="1030" Score="0" Text="The reason I'm using regression is that I need the per year rate of change for reasons I'd rather not get into right now." CreationDate="2014-08-23T20:24:15.167" UserId="1241" />
  <row Id="2151" PostId="1030" Score="1" Text="@AndyBlankertz: I just updated my answer." CreationDate="2014-08-23T20:33:27.433" UserId="2452" />
  <row Id="2152" PostId="1030" Score="0" Text="Thanks. I'd love to delve into the resources, but I'm time limited- the report I'm working on is due on Friday. I also have some slack in statistical rigorousness, because the target audience is Management :) Hopefully next week." CreationDate="2014-08-23T20:44:59.527" UserId="1241" />
  <row Id="2153" PostId="1030" Score="1" Text="@AndyBlankertz: You're welcome. I understand, as I'm not a statistician myself :-). But I'm trying to learn wherever and whenever I can." CreationDate="2014-08-23T20:54:16.963" UserId="2452" />
  <row Id="2154" PostId="1029" Score="4" Text="This is probably not a great question for StackExchange. It's going to be mostly opinions and speculation." CreationDate="2014-08-24T09:24:29.747" UserId="21" />
  <row Id="2155" PostId="1022" Score="0" Text="Will move it to comments ASAP." CreationDate="2014-08-24T12:59:11.480" UserId="941" />
  <row Id="2156" PostId="1020" Score="0" Text="AFAIK you have to buy that data. The FB api is more for interacting with the platform not for retrieving personal info of users that use the service." CreationDate="2014-08-24T16:49:27.613" UserId="92" />
  <row Id="2157" PostId="1021" Score="0" Text="Is the &quot;most probable&quot;/&quot;most consensual&quot; string you are looking to idendify a regular expression?  Or one of the strings on the list?" CreationDate="2014-08-24T21:47:38.487" UserId="609" />
  <row Id="2158" PostId="1021" Score="0" Text="@MrMeritology I am not looking for a regular expression. I have shown a regular expression in my question just to illustrate how flexible I am in the kind of strings I would consider to be correct." CreationDate="2014-08-25T08:07:58.927" UserId="3047" />
  <row Id="2159" PostId="1037" Score="0" Text="Thanks!  I thought of #1, but maybe that there was a highly visited site that might be doing it.  And you're right about #2- heuristic optimization is not guaranteed to come up with a interpretable solution." CreationDate="2014-08-25T16:32:56.073" UserId="375" />
  <row Id="2160" PostId="1021" Score="0" Text="OK.  Then the answer I gave below should work for you." CreationDate="2014-08-25T18:53:19.470" UserId="609" />
  <row Id="2161" PostId="1030" Score="0" Text="This isn't time series analysis (unless you throw away loads of data). I think the data records are individual sales records with a year attached as a covariate. Time series analysis is used when the variable of interest (eg *total sales*) has a unique time point. You could compute total sales within years and do time series analysis, but that would mean losing all the other information from each sales record (eg item purchased, buyer age etc). Regression is the right thing here." CreationDate="2014-08-26T08:51:42.180" UserId="471" />
  <row Id="2162" PostId="1030" Score="0" Text="@Spacedman: The term I've emphasized in my answer is **time series regression**. Thus, in my view, it could be considered as a **special case** of either of the two approaches, depending on the **perspective**." CreationDate="2014-08-26T09:12:48.210" UserId="2452" />
  <row Id="2163" PostId="1030" Score="0" Text="All I'm saying is that individual sales records data are not time series data. So you can't treat them like time series data. So reading about fitting AR(1) models and time series regression approaches is a waste of the OP's time here when all they have to do is convert year to numeric and run the model again. My concern now is wondering exactly what the OP means by &quot;per-year rate of change&quot;, which may imply something more than a linear term in year is required (some kind of smoother or polynomial term perhaps)." CreationDate="2014-08-26T09:40:26.557" UserId="471" />
  <row Id="2164" PostId="1030" Score="0" Text="@Spacedman: I see. Thank you for the clarification. However, my initial impression was that for this particular task, the OP is only interested in future values of a **single aggregate** *outcome variable* (keeping the model's **full information** for *regression analysis*). That would be the case for *time series forecasting*, wouldn't it? Perhaps, I misunderstood the question." CreationDate="2014-08-26T10:01:11.867" UserId="2452" />
  <row Id="2165" PostId="1044" Score="5" Text="Asking for tool recommendations is usually considered off topic for StackExchange. I think it could be a better question if you could narrow down requirements and ask for how to accomplish this in the few tools you are considering using." CreationDate="2014-08-26T15:56:43.573" UserId="21" />
  <row Id="2166" PostId="1041" Score="0" Text="Hmm... it never occurred to be to make year a continuous variable. In retrospect it seems obvious." CreationDate="2014-08-26T18:51:31.157" UserId="1241" />
  <row Id="2167" PostId="729" Score="0" Text="Hi nfmcclure, I've applied your suggestion and updated the post. Please provide your comments." CreationDate="2014-08-27T07:16:43.810" UserId="870" />
  <row Id="2168" PostId="1045" Score="1" Text="Use SparkSQL, Shark is deprecated." CreationDate="2014-08-27T09:21:59.780" UserId="2668" />
  <row Id="2169" PostId="1044" Score="2" Text="The answer to your question is &quot;all of them&quot;. Depending on where the real detailed challenges to your problem are, one language might give you better support for what you want to do. But to answer that, the question needs a *lot* more detail." CreationDate="2014-08-27T10:29:04.177" UserId="836" />
  <row Id="2170" PostId="671" Score="1" Text="You have the right idea, except when plotting it you should start where the series starts every reset.  For estimating where it will hit, say 120, see my first edit in my answer." CreationDate="2014-08-27T16:13:41.443" UserId="375" />
  <row Id="2171" PostId="1028" Score="1" Text="ISTR that Breiman had a proof based on the Law of Large Numbers. Has someone discovered a flaw in that proof?" CreationDate="2014-08-28T01:18:43.393" UserId="1241" />
  <row Id="2172" PostId="1044" Score="0" Text="I think you need to decide how good is good enough." CreationDate="2014-08-28T16:36:20.693" UserId="1241" />
  <row Id="2173" PostId="1046" Score="1" Text="I've had no problem finding word2vec implementations, but I have been unable to find a working recursive net to use." CreationDate="2014-08-28T16:57:14.207" UserId="684" />
  <row Id="2174" PostId="1045" Score="0" Text="@samthebeast can you provide a reference?" CreationDate="2014-08-28T17:53:20.473" UserId="403" />
  <row Id="2175" PostId="1045" Score="1" Text="I tried Shark a few weeks ago. I was able to get it running using their documentation and a lot of sweat. I ultimately gave up on it as I was trying to pull/push data from s3 and I couldn't find a way to do that. We since switched to Impala." CreationDate="2014-08-28T17:54:23.883" UserId="403" />
  <row Id="2176" PostId="985" Score="1" Text="I don't get the point of the question. What would prevent you from doing such a thing? The data science police? (I don't mean to be disrespectful, I simply don't understand the reason for the question)." CreationDate="2014-08-28T18:20:24.147" UserId="1281" />
  <row Id="2177" PostId="1045" Score="0" Text="I got it! I'll try to make some tests with SparkSQL to see if I feel confortable with that." CreationDate="2014-08-28T20:30:35.243" UserId="3050" />
  <row Id="2179" PostId="1058" Score="1" Text="Although this is about science, and data, I'm not sure the 'data science' StackExchange is the best place for this. Can you elaborate why this is of interest?" CreationDate="2014-08-29T06:00:38.573" UserId="21" />
  <row Id="2180" PostId="1045" Score="0" Text="@gallamine http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html" CreationDate="2014-08-29T22:58:59.587" UserId="21" />
  <row Id="2181" PostId="1045" Score="0" Text="This doesn't make a great question. I'd rephrase to ask about specific issues you have in mind." CreationDate="2014-08-29T22:59:40.270" UserId="21" />
  <row Id="2182" PostId="1060" Score="0" Text="Thx a lot. I've seen [Breeze can read a CSV](http://www.scalanlp.org/api/breeze/#breeze.io.CSVReader$) and can calculate several statistics like [mean and variance](http://www.scalanlp.org/api/breeze/index.html#breeze.linalg.meanAndVariance$). The quartiles are missing, but this may be the best in the state of the art for Scala, a bit far from R." CreationDate="2014-08-30T02:14:50.483" UserId="1281" />
  <row Id="2183" PostId="1060" Score="0" Text="@Trylks R is a DSL for statistics, so out of box it will have a lot of helper functions for every day work for statiticians, but its totally inappropriate for building a product, doing big data or writting complicated machine laerning algorithms in a scalable way.  ScaLa is a Scalable Language and is designed to be expanded to whatever people want.  You can do just about anything in Scala, but it wont all be out-of-box. The payoff is massive though, once u climb a little way up the learning curve one realizes how much more powerful it is." CreationDate="2014-08-30T05:42:33.703" UserId="2668" />
  <row Id="2184" PostId="1061" Score="1" Text="I think recommendations, and for web-services, are mostly off-topic, even though this does concern hosting data." CreationDate="2014-08-30T16:13:10.113" UserId="21" />
  <row Id="2186" PostId="92" Score="0" Text="@Anony-Mousse That's a good topic for meta. I would like the site to be what it can be. I think I've contributed for my part as a user and moderator. I'm not sure what you suggest should be done better." CreationDate="2014-08-30T18:44:11.267" UserId="21" />
  <row Id="2187" PostId="1061" Score="0" Text="I was marginally aware of that though I find it perhaps a tad perverse. Where does one draw the line between the relevance of finding a suitable function, programming pattern, library, framework, or an app/web-service? Any of these may serve to solve data science issues, albeit at different resolutions." CreationDate="2014-08-30T20:12:14.117" UserId="3133" />
  <row Id="2188" PostId="1038" Score="1" Text="This isn't a great fit for the Data Science site. I'd suggest posting this in serverfault.com perhaps." CreationDate="2014-08-30T22:55:15.140" UserId="21" />
  <row Id="2189" PostId="1045" Score="0" Text="This is my issue! I nerd a technology to stage my resulting data from spark as I asked before. Thanks folks! I'm getting great results from it!" CreationDate="2014-08-30T23:54:34.620" UserId="3050" />
  <row Id="2190" PostId="1063" Score="0" Text="Looks cool, lots of stuff and I had no idea about that library. It seems a bit confusing, though, there is a textwriter but nothing named &quot;reader&quot;. In particular, I don't see anything about quartiles. In this case the problem is that mixing libraries is probably not a good idea, they will have different data formats and I will need to move the data from one to another back and forth, or something even worse." CreationDate="2014-08-31T01:58:32.983" UserId="1281" />
  <row Id="2191" PostId="1060" Score="0" Text="I know, that's why I'm asking for Scala libraries :) Making my own library would not be wise if there were good libraries out there, which is usually the case. I found some libraries like Breeze (and Breeze-Viz, and Breeze-Bokeh), Saddle, Spire, Spark, Scalding, etc. The problem is that I'm quite uncertain about which one to choose, to use or expand, the nicest ones in philosophy seem to be dead, the most efficient ones seem to be useless, and in general I found none that would suit this simple use case :/" CreationDate="2014-08-31T02:07:13.160" UserId="1281" />
  <row Id="2192" PostId="1053" Score="0" Text="I found [a thread](https://groups.google.com/forum/#!topic/cascading-user/YlCKDmjlkP0) discussing something similar for cascading, mahout, hadoop and some other technologies. I have to check it into detail, but I can't now, so I'll simply leave it here..." CreationDate="2014-08-31T02:13:36.947" UserId="1281" />
  <row Id="2194" PostId="1058" Score="0" Text="Try Fermi Estimation: http://en.wikipedia.org/wiki/Fermi_problem" CreationDate="2014-09-01T08:30:00.547" UserId="471" />
  <row Id="2195" PostId="1063" Score="0" Text="CSV data is usually read using `FileBasedDatabaseConnection` and `NumberVectorLabelParser`. Quantiles can be determined using `QuickSelect`.&#xA;If you don't want to mix libraries, then don't *ask* about libraries..." CreationDate="2014-09-01T08:58:01.877" UserId="924" />
  <row Id="2196" PostId="1061" Score="1" Text="The &quot;best&quot; depends very much on your application and your data. Without you expanding on that, this Q will just become a list of &quot;Howabout datafoo.io?&quot;. Even with that expansion, its still probably too much opinion." CreationDate="2014-09-01T12:14:02.260" UserId="471" />
  <row Id="2197" PostId="1058" Score="4" Text="As soon as you wrote a scientific article on this, you would be wrong." CreationDate="2014-09-01T12:21:10.427" UserId="471" />
  <row Id="2198" PostId="1063" Score="0" Text="1. I would have never guessed those names, thank you. 2. Spark (MLib) uses Breeze over Hadoop clusters, the combination is very natural for those libraries, AFAIK, because they are quite agnostic in the information representation. When moving to higher level (aka less raw) libraries decisions will be made and that means more compatibility problems, specially when that means decisions about information representation." CreationDate="2014-09-01T17:37:10.143" UserId="1281" />
  <row Id="2199" PostId="1066" Score="0" Text="Hi, thanks for your answer. &#xA;&#xA;I'm looking for an estimation of the amount of scientific knowledge. After that I want to study the distribution between different fields. Maybe if I use a wordcount and I don't take care about the pictures the problem is more easy to resolve." CreationDate="2014-09-02T00:23:28.073" UserId="3128" />
  <row Id="2200" PostId="1058" Score="0" Text="Why would be an error? &#xA;&#xA;I thing that to resolve this problem I need data science tools. After know the total amount I want to extract patters of activity in each field for every year and study the dynamics of science and how this would be under the influence of historical events" CreationDate="2014-09-02T00:26:47.553" UserId="3128" />
  <row Id="2201" PostId="1072" Score="0" Text="I am interested in analytics. I know HANA has a lot of transnational functionality but I am more interested what can be done in terms of machine learning, custom python reducers and hive interface. For example I know HANA has an library of machine learning algorithms built right in. Does Exasol?" CreationDate="2014-09-02T14:28:16.337" UserId="2511" />
  <row Id="2202" PostId="1072" Score="0" Text="EXASOL has a framework called &quot;EXAPowerlytics&quot; which uses protobuf and ZeroMQ to connect with Hadoop (for example). It also allows the ability to write User Defined Functions in Java, Lua, Python and R - for example importing any R machine-learning libraries you need without limit and running them in-memory and in parallel (where the algorithm is parallelisable). Definitely recommend signing-up at [link](www.exasol.com/portal) to get some more information." CreationDate="2014-09-02T14:57:12.140" UserId="3181" />
  <row Id="2203" PostId="1058" Score="0" Text="I'm not confident about the feasibility of an attempt to solve this problem. You can certainly come up with a very rough estimate (and I see such numbers from time to time), but the accuracy is pretty low, considering the diversity of the outlets human knowledge can found at, as well as frequency of appearing of new research studies and even research repositories." CreationDate="2014-09-02T15:22:54.523" UserId="2452" />
  <row Id="2204" PostId="1071" Score="1" Text="The variety of perspectives for this particular topic is not obvious for me. I'd say that: Data = symbols (of an alphabet). Information = data + syntax. Knowledge = information + semantics." CreationDate="2014-09-02T16:27:09.260" UserId="1281" />
  <row Id="2205" PostId="866" Score="0" Text="This is only a little bit related, but our most recent data science challenge concerned predicting claims from other claims. http://www.cloudera.com/content/cloudera/en/training/certification/ccp-ds/challenge/challenge2.html  When the solution is released it may contain a few interesting ideas." CreationDate="2014-09-03T14:20:04.223" UserId="21" />
  <row Id="2207" PostId="1075" Score="1" Text="How much memory one dataset takes? I.e. what is approximate size of one row?" CreationDate="2014-09-04T18:34:29.907" UserId="1279" />
  <row Id="2209" PostId="1076" Score="0" Text="Thanks for your comments. I am very aware and use scikit and caret regularly but I am looking for more of a state of the art online learning package, and neither scikit or caret are these. I am looking for people who are actually using something and can give their experience with a given package." CreationDate="2014-09-05T13:23:49.067" UserId="802" />
  <row Id="2210" PostId="1077" Score="2" Text="Few corrections. Firstly, Hadoop is a common name for a set of tools, file system is called HDFS. Secondly, data may be processed in parallel without distributing it on HDFS, e.g. sending data tuples to Storm or even RabbitMQ with a number of workers on a consumer side will do the trick as well. Thirdly, in this specific case you need to distribute not just 2 separate datasets, but instead all pairs from both (90 B tuples). Spark already has `.cartesian()` method which does exactly this, but custom lazy generator + RabbitMQ will work fine too." CreationDate="2014-09-05T23:44:41.593" UserId="1279" />
  <row Id="2211" PostId="1075" Score="0" Text="The size of rows differs, but on average about 30-50. Data sets are about 100Mb each." CreationDate="2014-09-06T12:44:19.140" UserId="3203" />
  <row Id="2212" PostId="1077" Score="0" Text="Splitting up using a queue and calculating in a distributed fashion is an option. just assume this will come with a price tag of handling a lot of complexity thats been solved in other distributed products. Spark seems like a good candidate on paper." CreationDate="2014-09-06T12:46:38.340" UserId="3203" />
  <row Id="2214" PostId="1084" Score="0" Text="Original Poster used the word &quot;classify&quot; but &quot;cluster&quot; is a more accurate description of his problem because he has no a priori definitions of categories. Therefore, this is not necessarily a supervised learning problem." CreationDate="2014-09-08T02:28:56.183" UserId="609" />
  <row Id="2215" PostId="1084" Score="0" Text="@MrMeritology: hmm, from context I would say that author is just not sure about concrete classes he's going to use, but still wants classification, not clustering. Anyway, he's the only person who knows the truth :)" CreationDate="2014-09-08T05:31:22.507" UserId="1279" />
  <row Id="2216" PostId="1084" Score="0" Text="Maybe i was not clear at the point. The categories are going to be selected in advice, so it is rather classification than clustering problem. The idea of creating a complex features vector seems to be quite reasonable - especially, that there are some particular tags, that are most likely probably to quickly classify some of samples. I'm not sure if SVM are going to fit the problem, as I predict high nonlinearities, but decision trees and Bayes seem to be applicable. I'm starting also to think about application of a hybrid algorithm (SVM based decision trees)." CreationDate="2014-09-08T05:46:48.257" UserId="3215" />
  <row Id="2217" PostId="1084" Score="0" Text="@GrzegorzE. -- If your categories are defined in advance, then please list these three categories in your question.  In my opinion, you are too focused on ML algorithms and not enough on the nature of your problem and the nature of your data. For example, you predict &quot;nonlinearies&quot; in features for web sites of unknown structure. Why? Also, you are mixing tags with web page text with who-knows-what-else, and they have different semantic significance." CreationDate="2014-09-08T06:07:25.307" UserId="609" />
  <row Id="2218" PostId="1084" Score="0" Text="@GrzegorzE. -- I strongly suggest that your classification method should be primarily driven by the nature of your a priori categories and the nature of the data. There are an infinite number of ways to categorize arbitrary web sites into 3 categories. Each way of categorizing will suggest salient features in the data or salient patterns. There's no substitute for manual analysis of individual data elements (web pages) and their context." CreationDate="2014-09-08T06:17:58.650" UserId="609" />
  <row Id="2219" PostId="1084" Score="0" Text="@MrMeritology -- I wan to classify if the particular page is a personal page (or social profile page). As I am a newbie in data science, my control engineering background drives my modelling and algorithmic focus in some particular areas. Similarly, the nonlinearities in feature vectors space and selection of appropriate algorithms have background in control..." CreationDate="2014-09-08T06:42:40.300" UserId="3215" />
  <row Id="2220" PostId="1086" Score="0" Text="From this description, I'm not at all clear what the ranking is supposed to be. There is no such thing as ordering by &quot;unique content&quot;. Clarify please what the ordering is supposed to be determined by?" CreationDate="2014-09-08T09:15:13.517" UserId="21" />
  <row Id="2221" PostId="1075" Score="0" Text="Meta-comment: N^2 algorithms are always bad news at scale. I would in any event investigate algorithms that do not require all-pairs computation. Cartesian joins destroy data locality optimization." CreationDate="2014-09-08T11:51:40.437" UserId="21" />
  <row Id="2222" PostId="1084" Score="0" Text="@GrzegorzE. - I'm familiar with Control Engineering as I have a EE background, so I can understand how you'd quickly focus on algorithms, nonlinearity, etc. I renew my suggestion to focus first on problem definition, understanding the data, and feature selection. Your problem is essentially a Social Science problem, not just a &quot;data classification problem.&quot; Insightful selection of features is worth more than 50% improvement in algorithm effectiveness." CreationDate="2014-09-08T18:17:17.203" UserId="609" />
  <row Id="2223" PostId="1084" Score="0" Text="@MrMeritology - great thanks for suggestions. Since yesterday I'm building a feature vector, focusing on text corpus, tags and tag arguments. I'll drop a line, with project status when get first results." CreationDate="2014-09-09T05:32:08.690" UserId="3215" />
  <row Id="2224" PostId="1075" Score="0" Text="You haven't specified the constraintd..eg response time, throughput etc" CreationDate="2014-09-09T06:32:03.607" UserId="1256" />
  <row Id="2225" PostId="1092" Score="0" Text="I would be interested to know where this sits, too, as currently I feel obliged to learn Python, R and Octave, just so I have access for tools for a hobby (whilst I know Ruby for professional reasons). I don't know enough about it to suggest an answer, but have known about http://sciruby.com/ for a while. My gut feel is that it is not ready yet" CreationDate="2014-09-09T09:29:27.130" UserId="836" />
  <row Id="2226" PostId="1092" Score="0" Text="Yeah, we took a look at sciruby, and while it looks nice, it seems limited to providing some data structures and linear algebra operations. If someone were to build a unified ML library for Ruby it would probably be a great basis for that." CreationDate="2014-09-09T16:59:37.457" UserId="2487" />
  <row Id="2227" PostId="1073" Score="0" Text="Dumb question, but do you mean `online` as in &quot;non-batch mode&quot;, or as in &quot;processed in the cloud&quot;?" CreationDate="2014-09-09T17:14:02.150" UserId="3248" />
  <row Id="2228" PostId="1081" Score="0" Text="I'm not sure data is a &quot;thing&quot;. If it is a thing, it should exist somewhere. Data doesn't exist anywhere. Take your own examples: (1) ethnicity: belonging to a social group - this quality does not &quot;exist&quot;, but is assigned to an individual and a group of individuals. What counts as an ethnicity is a social construction and may differ between individuals, societies, and eras; (2) housing prices do not have existence either, but are a manifestation of perceived property/material/social value. But anyway, the point is taken that data is a construction that is used to make meaningful inferences." CreationDate="2014-09-10T05:56:32.147" UserId="3178" />
  <row Id="2229" PostId="1086" Score="0" Text="@SeanOwen All documents come under the same theme for instance Tissue engineering for Bonemarrow related cancer. Within this theme we know that most research article will definetly speak about genes (ABC) so this becomes the predominant theme within those topics. Most of these articles dwell within these same set of genes. But very few articles start speak about latent themes which involves  genes (ADF) and (XYZ). The current thought process is we want to rank the articles based on the  content (XYZ),(FDA),(ABC). This is what I meant by unique content." CreationDate="2014-09-10T10:49:37.477" UserId="3232" />
  <row Id="2230" PostId="1089" Score="0" Text="Thanks! this is usefull" CreationDate="2014-09-10T10:56:00.230" UserId="3232" />
  <row Id="2231" PostId="1087" Score="0" Text="Thanks. Im looking at weighted TF-IDF in cases were the key-words are already known . any suggestions for that ?" CreationDate="2014-09-10T11:00:57.753" UserId="3232" />
  <row Id="2232" PostId="1096" Score="0" Text="I did look at GATE, i was not successful. I am not familiar with python. Any other tool.. I know spotfire but its way too expensive.." CreationDate="2014-09-10T16:33:45.287" UserId="3244" />
  <row Id="2233" PostId="1098" Score="0" Text="Thanks Harjeet, SKlearn is simple but it does not solve my problem. SKlearn is more of analytical algorithm approach which R also provides but I am looking for something like we do in Google, put in n-words and it gives all suitable hits.." CreationDate="2014-09-10T16:43:49.457" UserId="3244" />
  <row Id="2234" PostId="1101" Score="1" Text="Gini coefficient is not Gini impurity. See the links in the question" CreationDate="2014-09-10T19:15:20.863" UserId="21" />
  <row Id="2235" PostId="1073" Score="0" Text="Hi Mike, I mean non-batch mode. So soon after a prediction is made we learn the true label and then use that in the training. So think in terms of predicting a stock price, in a few minutes we would learn the true value, and then use that value in our training." CreationDate="2014-09-11T13:16:04.657" UserId="802" />
  <row Id="2236" PostId="1101" Score="0" Text="Wikipedia ist not always a reliable source of information :-)" CreationDate="2014-09-11T13:40:48.833" UserId="979" />
  <row Id="2237" PostId="1096" Score="0" Text="Sorry, I don't know of any easy-to-use free tools that do exactly what you want to do." CreationDate="2014-09-11T13:56:59.770" UserId="819" />
  <row Id="2238" PostId="1087" Score="0" Text="What are you hoping to achieve by boosting the TF-IDF scores of known keywords?" CreationDate="2014-09-11T13:58:46.060" UserId="819" />
  <row Id="2239" PostId="1101" Score="1" Text="Sure. Go look it up somewhere else: http://mathworld.wolfram.com/GiniCoefficient.html What makes you think Gini coefficient = Gini impurity?" CreationDate="2014-09-11T14:03:08.870" UserId="21" />
  <row Id="2240" PostId="1101" Score="0" Text="Look it up: http://books.google.de/books?id=DQXhYAgXRpEC&amp;pg=PA373&amp;dq=gini+coefficient+classification&amp;hl=de&amp;sa=X&amp;ei=Q7sRVOKDOcvAPLDugbAP&amp;ved=0CCkQ6AEwAQ#v=onepage&amp;q=gini%20coefficient%20classification&amp;f=false" CreationDate="2014-09-11T15:10:51.260" UserId="979" />
  <row Id="2241" PostId="1101" Score="0" Text="By the way: I have never seen a publication that cites mathworld.wolrfram.com !" CreationDate="2014-09-11T15:12:19.297" UserId="979" />
  <row Id="2242" PostId="1101" Score="0" Text="I'm sure you can find people that use coefficient and impurity interchangeably within the field of ML. Gini coefficient is not misclassification error, outside of ML. It has a meaning, whose interpretation when brought back into ML is something else. If the question is, are they the same thing, then I can't see how the answer is &quot;yes&quot;. Search for &quot;gini coefficient&quot; and tell me those are all about misclassification error?" CreationDate="2014-09-11T16:01:22.573" UserId="21" />
  <row Id="2243" PostId="1104" Score="0" Text="I'm sure this is tagged incorrectly and not enough info.  But i dont know what i dont know at this point" CreationDate="2014-09-12T04:23:19.613" UserId="3279" />
  <row Id="2244" PostId="1085" Score="0" Text="I do not understand the down vote.  This is a perfectly reasonable answer.  There are platforms other than Hadoop for computational grid computing." CreationDate="2014-09-12T12:57:55.210" UserId="961" />
  <row Id="2245" PostId="1106" Score="0" Text="Huge topic, have a look at [Into to IR](http://nlp.stanford.edu/IR-book/), this walks you from basic first principles how to build what you are asking about. Something to lookup is [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) then realise this doesn't solve everything and look at bayesian probability" CreationDate="2014-09-12T13:00:09.070" UserId="95" />
  <row Id="2246" PostId="1105" Score="0" Text="This looks more like a programming question or maybe a stats question. Its not a data science question. Try StackOverflow?" CreationDate="2014-09-12T13:10:54.800" UserId="471" />
  <row Id="2247" PostId="1105" Score="1" Text="I was not that clear about it either. I think it'll be quite hard for someone who's not into the topic to see the problem, but I may give it a try" CreationDate="2014-09-12T13:40:05.153" UserId="3283" />
  <row Id="2249" PostId="1100" Score="0" Text="Why aren't other classification methods appropriate here? Sounds like you are concerned with multiple cases leading to a reduced set of outputs. But this is basically every case of representation learning in classification. Neural nets will help you sort out what interaction effects are predictive when base features are not too predictive. Otherwise you could use other methods." CreationDate="2014-09-13T19:03:16.963" UserId="92" />
  <row Id="2250" PostId="1113" Score="0" Text="How do text responses look like? And what are you trying to achieve with the groping?" CreationDate="2014-09-13T21:19:46.030" UserId="1279" />
  <row Id="2251" PostId="1054" Score="0" Text="magicharp, thank you for your answer." CreationDate="2014-09-13T21:26:27.317" UserId="3068" />
  <row Id="2252" PostId="1113" Score="0" Text="The text responses are sentences or phrases of 50-200 characters. The goal is just to group the numeric variable (that is, create cut points) based on the text responses." CreationDate="2014-09-13T23:02:06.283" UserId="36" />
  <row Id="2253" PostId="1113" Score="1" Text="Then you can group them by first letter of texts. It will satisfy your current description, but most probably you want something different. So what you actually want to achieve? Do you want to split people in age categories based on words they use? Or you are looking for specific signs in responses? Even assuming you want clustering, there are literally dozens of features that may extracted from text and same number of distance measures to use. But in current wording I can't even be sure we are talking about clustering. So please provide *example* and/or *context* of what you want to achieve." CreationDate="2014-09-14T00:18:08.677" UserId="1279" />
  <row Id="2256" PostId="1105" Score="1" Text="That's why I suggested stats and programming. Those would be the people into the topic. All you've got here are data miners and hadoop word-counters." CreationDate="2014-09-14T08:52:44.690" UserId="471" />
  <row Id="2257" PostId="1100" Score="0" Text="I welcome any solution. Neural net was just the one I was attempting to utilize." CreationDate="2014-09-14T20:08:15.810" UserId="3263" />
  <row Id="2260" PostId="1106" Score="0" Text="Why do you consider &quot;repairs&quot; as relevant?" CreationDate="2014-09-15T11:13:30.170" UserId="1279" />
  <row Id="2261" PostId="1114" Score="0" Text="My understanding is that gradient boosting suffers from the same limitations as RF when dealing with imbalanced data: http://sci2s.ugr.es/keel/pdf/algorithm/articulo/2010-IEEE%20TSMCpartA-RUSBoost%20A%20Hybrid%20Approach%20to%20Alleviating%20Class%20Imbalance.pdf" CreationDate="2014-09-15T13:02:40.797" UserId="3294" />
  <row Id="2262" PostId="1114" Score="0" Text="Boosting is an additional step you take in building the forest that directly addresses imbalance. The paper you link notes this in the intro stating boosting helps even in cases where there is no imbalance. And that paper concludes boosting significantly helps. So not sure where the equivalence between RF and boosting is shown there?" CreationDate="2014-09-15T13:09:40.260" UserId="92" />
  <row Id="2263" PostId="1119" Score="0" Text="Not sure I understand the question completely, but features based on RFM type calculations are almost always some of the most powerful in a predictive model in the database marketing domain." CreationDate="2014-09-15T16:42:22.670" UserId="1138" />
  <row Id="2264" PostId="1119" Score="1" Text="Your question is too broad.  Prediction for what purpose? RFM models have been around for decades. Pre-internet, every direct marketing organization used RFM for promotional spending -- i.e. who to send catalogs and flyers to." CreationDate="2014-09-15T17:08:02.633" UserId="609" />
  <row Id="2265" PostId="1108" Score="1" Text="you might want to rephrase. the definition for kappa is easily available, but i believe - but don't know - that it has some limitations that are relevant here." CreationDate="2014-09-15T19:40:48.483" UserId="3294" />
  <row Id="2266" PostId="1119" Score="0" Text="@MrMeritology, thanks, I have made edits in original questions. I'd like to predict customers tendency to continue buying, spending more or the end of customer life cycle." CreationDate="2014-09-15T20:06:36.593" UserId="97" />
  <row Id="2267" PostId="1113" Score="0" Text="What do you mean by &quot;linguistic similarity&quot;?  Grammar/syntax? reading comprehension level? semantics (meaning)?" CreationDate="2014-09-15T22:25:56.510" UserId="609" />
  <row Id="2268" PostId="1106" Score="0" Text="Was just an example. Didn't think too much about it :). I guess it is not relevant?" CreationDate="2014-09-16T08:06:47.127" UserId="3284" />
  <row Id="2269" PostId="1126" Score="0" Text="I have read something about ensemble learning (see scientific paper http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1105916&amp;tag=1). However, it is not clear to me how to apply them. `SVM-text` is returning a probability of belonging to class `C`, and the same holds for `SVM-image`. Is it sufficient to average them? You talk about weighted average: how to select weights?" CreationDate="2014-09-16T13:48:52.677" UserId="3321" />
  <row Id="2270" PostId="1103" Score="0" Text="I think this would be more helpful to recommend particular models or tools in answer to this question." CreationDate="2014-09-16T15:54:26.090" UserId="21" />
  <row Id="2271" PostId="1104" Score="1" Text="Can you give more detail on the data? it's not clear what pipeline means here, what type of data is in question, or what the budget concern is." CreationDate="2014-09-16T15:55:55.150" UserId="21" />
  <row Id="2272" PostId="1128" Score="1" Text="sounds more like something for stats or even maths stack exchange sites..." CreationDate="2014-09-16T16:35:52.827" UserId="471" />
  <row Id="2273" PostId="1126" Score="0" Text="There's huge number of ways to incorporate weights. In simplest case of binary classification you can use equal weights for both answers (from image- and text-based classifiers) and thus simply average probabilities. For multinomial classification you can treat probabilities themselves as weights and compute probability for each class separately. Or you can use adaptive weights like in AdaBoost. Or anything else. Just start reading about weighted ensembles and choose algorithm that in your opinion is the most appropriate for the task." CreationDate="2014-09-16T20:04:51.663" UserId="1279" />
  <row Id="2274" PostId="1128" Score="0" Text="Thanks for those suggestions. I am considering a similar question for the maths site... but hope to discover insights from data-science experts first. The question straddles practical data science and theoretical maths." CreationDate="2014-09-16T22:42:54.807" UserId="3328" />
  <row Id="2275" PostId="1113" Score="0" Text="@ffriend: It's a general methodological question, so I'll pick one at random: splitting the age categories based on the words respondents use." CreationDate="2014-09-17T00:58:38.523" UserId="36" />
  <row Id="2276" PostId="1108" Score="0" Text="http://stats.stackexchange.com/questions/19601/adjusting-kappa-inter-rater-agreement-for-prevalence" CreationDate="2014-09-17T02:16:14.377" UserId="3294" />
  <row Id="2277" PostId="1132" Score="3" Text="A general request for help and advice is not appropriate for StackExchange. You need to ask a more narrow and focused question that can be answered with specific responses." CreationDate="2014-09-17T06:43:45.697" UserId="609" />
  <row Id="2279" PostId="1106" Score="0" Text="@Hendrik: please, use @&lt;username&gt; to address user - SE didn't notify me about your comment. Counting relevancy is the key point in search engines (though normally you compute how relevant is web page to a search query, you need it, right?). Do I understand it right that you just want to know how to compute relevancy of document to a search query when there are similar, but not exact words (e.g. &quot;bike&quot; and, say, &quot;cyclist&quot;)?" CreationDate="2014-09-17T08:12:55.133" UserId="1279" />
  <row Id="2280" PostId="1131" Score="0" Text="Thank you for this interesting answer - it isn't what I anticipated.  Time/frequency domain (Fourier/Wavelet) transforms  operate on individual time-series, a very restrictive algebra.  &quot;Dynamic Systems&quot; - on the other hand - focus on establishing models for time series - as opposed to establishing algebras ranging over time series.  I hoped to discover sets of operators ranging over time-series that are 'somehow adequate'." CreationDate="2014-09-17T09:59:14.570" UserId="3328" />
  <row Id="2281" PostId="1132" Score="2" Text="Some people are rather vocal in their conviction that k-means is not appropriate for categorical data, e.g., [here](http://stats.stackexchange.com/questions/115573/what-are-clustering-techniques-for-this-case/) or [here](http://stats.stackexchange.com/questions/49887/appropriate-cluster-method) or [here](http://stats.stackexchange.com/questions/31308/distance-function-for-categories-in-k-means) or [here](http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust)." CreationDate="2014-09-17T12:27:02.470" UserId="2853" />
  <row Id="2282" PostId="1131" Score="0" Text="Then I'm not sure what you mean by &quot;algebra over time-domain time series&quot;. FWIW, you can operate algebraically in frequency domain to express relations over many time-series equivalents. A simple example: formulas for musical scales, incl. harmonics. Also, I don't know why you'd want to restrict yourself to time domain, given that frequency domain contains all the same information." CreationDate="2014-09-17T12:34:54.860" UserId="609" />
  <row Id="2283" PostId="1087" Score="0" Text="Exploring weighted TD-IDF  to introduce conscious bias of sorts." CreationDate="2014-09-17T13:30:28.420" UserId="3232" />
  <row Id="2284" PostId="1131" Score="0" Text="I didn't intend to 'knock' exploiting the frequency domain... an approach that often yields seemingly 'magical' results. This question is focused exclusively on functions transforming time-domain time-series to time-domain time-series. I've added some details about abstract algebras (and a &quot;Boolean Algebra&quot; analogy) which, I hope, makes my question clearer. I am not excluding the possibility that operations could be defined using the Fourier transform... for example... but I don't want frequency domain representations as &quot;outputs&quot; from the operations which make up the algebra." CreationDate="2014-09-17T13:32:40.867" UserId="3328" />
  <row Id="2285" PostId="1135" Score="1" Text="*Please* don't [cross-post](http://stats.stackexchange.com/questions/115775/statistical-commute-analysis-in-java). I'll flag your post on CrossValidated for closure, even though it already has an answer, since your question is really more appropriate here." CreationDate="2014-09-17T14:19:10.317" UserId="2853" />
  <row Id="2286" PostId="1103" Score="0" Text="I haven't seen any publicly-available models for Stanford NER, other than those distributed by the Stanford NLP Group itself. These include multiple versions for English (including case-insensitive models), as well as models for German, Spanish, and Chinese.&#xA;&#xA;http://nlp.stanford.edu/software/CRF-NER.shtml#Models&#xA;&#xA;However, these models were trained mostly on annotated news articles, which may not work well on other kinds of text data. Here's how you would train your own models for Stanford NER:&#xA;&#xA;http://nlp.stanford.edu/software/crf-faq.shtml#a" CreationDate="2014-09-18T13:31:50.873" UserId="819" />
  <row Id="2287" PostId="1103" Score="0" Text="Also, other NER tools offer languages for other models. For example, Apache OpenNLP offers a model trained on Dutch: http://opennlp.sourceforge.net/models-1.5/ But again, in the long run, the way to get the best performance on your own data is to train custom models on your own data. It takes a TON of work to acquire the training data and painstakingly annotate it by hand, but it *might* be worth it in the end. Once you have the annotated training data, most NER tools have methods for training new custom models on that data." CreationDate="2014-09-18T13:39:42.647" UserId="819" />
  <row Id="2288" PostId="1087" Score="0" Text="If you have a controlled vocabulary of terms you care about, you could boost the score of those specific items." CreationDate="2014-09-18T13:40:31.580" UserId="819" />
  <row Id="2289" PostId="1124" Score="0" Text="Why not test out both approaches on a small sample set and see if there's any difference?" CreationDate="2014-09-18T13:47:50.750" UserId="819" />
  <row Id="2290" PostId="794" Score="0" Text="As madison may, mentioned its all about separating configuration and code. It would be fine if you were running one network and knew all the parameters, but you don't. by splitting config and code, you can run multiple networks - different hidden neurons etc, etc. and source control is straight forward ( how do you keep track of which configuration you have tried if you keep it in the code)." CreationDate="2014-09-18T23:57:27.757" UserId="1256" />
  <row Id="2291" PostId="694" Score="0" Text="If you want to only use Restricted Boltzmann Machine, you can stick with scikit-learn as well." CreationDate="2014-09-17T11:42:42.543" UserId="3342" />
  <row Id="2292" PostId="1101" Score="0" Text="I think we are talking about decision trees. So we are in the field of machine learning! Please read the question more carefully" CreationDate="2014-09-19T12:38:02.420" UserId="979" />
  <row Id="2293" PostId="1144" Score="0" Text="Surely will read it! Really liked the last line...i think the urge and sometimes the pressure to get the results ASAP often leads to such Parodies.&#xA;&#xA;And its equally important to avoid the opposite of this wherein one goes so deep in the learning that it becomes useless for the real world issues.&#xA;&#xA;while growing/learning sometimes its more important to know what NOT to do, thanks a lot for the guidance hope to see more such insights that would enlighten me and others on a similar Journey." CreationDate="2014-09-19T12:15:34.950" UserDisplayName="Vinay Tiwari" />
  <row Id="2294" PostId="1144" Score="0" Text="&quot;what affects the convergence rate of the Fisher scoring algorithm in GLM&quot;- I guess you lost 99% of the Data Scientists here." CreationDate="2014-09-19T12:23:22.917" UserDisplayName="Momo" />
  <row Id="2295" PostId="1145" Score="0" Text="Would appreciate if you could suggest some Algorithms that one should not MISS ON, or better to say are the most useful for solving practical business issues. If possible please mention the best ways to learn them (particular books,self help articles or may be trial and error)" CreationDate="2014-09-19T13:18:33.680" UserDisplayName="Vinay Tiwari" />
  <row Id="2296" PostId="1145" Score="2" Text="I would say pretty much all the algos in ISL: linear regression, logistic regression, tree based methods, SVM; Clustering and dimension reduction eg PCA.  Go through the book and look at the corresponding online course ( http://online.stanford.edu/course/statistical-learning-winter-2014 - maybe on youtube?)." CreationDate="2014-09-19T14:26:34.747" UserId="1256" />
  <row Id="2297" PostId="1135" Score="0" Text="you have no knowledge of machine learning or stats methods, yet any answer to this question is going to involve lots of both. That makes this question off-topic as &quot;too broad&quot;, since its just going to have to tell you all about ML and/or stats. You can't just plug your numbers into a magic box. The one existent &quot;answer&quot; posted already is clearly not an answer, its just chatter." CreationDate="2014-09-19T14:33:47.160" UserId="471" />
  <row Id="2298" PostId="1144" Score="0" Text="@Momo: Well, &quot;data scientist&quot; is one of those ill-starred terms that has barely gained currency before starting to be devalued." CreationDate="2014-09-19T14:40:39.097" UserId="3361" />
  <row Id="2299" PostId="1145" Score="0" Text="Great Resource,good to have a book and videos on the same by the Authors themselves.Thanks a lot for the link,wasn't aware of this." CreationDate="2014-09-19T15:03:13.393" UserId="3360" />
  <row Id="2301" PostId="1135" Score="0" Text="@Spacedman: My response provides the theoretical information you need to solve this problem. To recap: once you have estimated the commute time function, you need but solve a bivariate constrained optimization problem. I made some simplifying assumptions in order to keep it simple. If you see a mistake feel free to leave a comment or edit my answer." CreationDate="2014-09-19T20:30:42.970" UserId="381" />
  <row Id="2302" PostId="1101" Score="0" Text="No need to guess. Did you click the links? Do you see that the gini coefficient in question is not the thing you keep talking about?" CreationDate="2014-09-20T13:21:32.087" UserId="21" />
  <row Id="2303" PostId="1141" Score="0" Text="A major point of clarification is whether you want to be in Academia or Industry. Both have radically different flavors about what is used, how it's used, and end goals." CreationDate="2014-09-22T00:14:16.310" UserId="3378" />
  <row Id="2304" PostId="1150" Score="0" Text="Thank you very much, I'll definitely have a look at it for educational purposes. I fortunately found a ready-made implementation for my system which works for me." CreationDate="2014-09-23T06:37:22.327" UserId="3283" />
  <row Id="2305" PostId="1152" Score="0" Text="Actually I would expect the size of the vocabulary to increase greatly when considering URLs, because the tweets vocabulary is quite limited. Regarding the second misunderstanding, one of the ideas was to simply append the features for URLs to the features for the tweet. But, if a tweet does not have URLs, I will have to put some &quot;impossible&quot; values in the URL features (like -1), because the lack of a certain feature (-1) must be different from the absence of a word (0 if considering term frequencies). This would not happen if considering your initial proposed approach to have one bag of words" CreationDate="2014-09-23T14:09:41.417" UserId="3054" />
  <row Id="2306" PostId="1152" Score="0" Text="I am not sure about your initial approach to have one big bag of words though, because, apart from the eventual different vocabulary between tweets and webpages, this would put on the same level the words from the tweet and the words from the URL, when actually they could have different importance in the classification" CreationDate="2014-09-23T14:15:03.557" UserId="3054" />
  <row Id="2307" PostId="1138" Score="1" Text="I was just trying to figure out if my reasoning was correct, so I will keep using Random Forests, thanks for your help!" CreationDate="2014-09-23T14:32:41.853" UserId="3054" />
  <row Id="2308" PostId="1103" Score="0" Text="Thanks @CharlieGreenbacker - what if I'm looking for models to use in the corporate environment? Meaning, if people talk about their work, teams, etc. Are there existing work around this domain?" CreationDate="2014-09-23T17:47:20.853" UserId="2785" />
  <row Id="2309" PostId="1151" Score="3" Text="I can suggest to try the subject in a simple way, with a little maths, and a little programming (both of which you will need to pursue the subject deeper), look into this Coursera course: https://www.coursera.org/course/ml - the next session also starts quite soon" CreationDate="2014-09-23T20:45:53.967" UserId="836" />
  <row Id="2310" PostId="1151" Score="0" Text="+1 to Neil Slater for the Coursera course. It was a great intro to Machine Learning for me, and would give anyone a good taste of what to study next." CreationDate="2014-09-24T14:50:10.907" UserId="3409" />
  <row Id="2312" PostId="1159" Score="0" Text="Hi, and welcome to DS! Perhaps you could elaborate a bit on your dataset. How many rows and columns do you have? This could have an impact on possible solutions." CreationDate="2014-09-25T11:30:31.503" UserId="2853" />
  <row Id="2313" PostId="1159" Score="0" Text="23711341 rows, and 8 columns. I could try to remove 1-2 columns. They does not seem to related to my problem." CreationDate="2014-09-25T13:16:53.580" UserId="3167" />
  <row Id="2315" PostId="1165" Score="1" Text="This is a very broad question. Please try to narrow it down. What are you interested in?" CreationDate="2014-09-25T21:08:43.703" UserId="21" />
  <row Id="2318" PostId="1159" Score="0" Text="You should sample rows before columns here. Is there a reason you cant randomly sample rows to reduce data size? I'm assuming rows here are related to users or something" CreationDate="2014-09-26T03:15:54.100" UserId="92" />
  <row Id="2319" PostId="1021" Score="0" Text="Would this come under NER (named entity recognition)?" CreationDate="2014-09-26T03:30:30.803" UserId="2474" />
  <row Id="2320" PostId="1163" Score="1" Text="+1 for *you do not want to apply svd to only 8 columns: you apply it when you have a lot of columns.*" CreationDate="2014-09-26T06:47:34.770" UserId="2853" />
  <row Id="2321" PostId="1166" Score="0" Text="Matlab is not the only good tool for maths, there are also Octave, R, Python/NumPy, SciLab, etc. Same applies to NLP - there are NLTK, Standford NLP, GATE and numerous others. Often it's a question of personal preference, so no single good answer can be given to your question. In general, try to avoid questions starting with &quot;what is the best ...&quot; - they are primary opinion-based and are forbidden on most SE sites." CreationDate="2014-09-26T07:34:34.757" UserId="1279" />
  <row Id="2322" PostId="1166" Score="0" Text="@ffriend Thank you I changed the title of question and detailed my question a bit" CreationDate="2014-09-26T07:49:44.047" UserId="3436" />
  <row Id="2323" PostId="1166" Score="2" Text="Then I would suggest trying out [NLTK](http://www.nltk.org/). It's pretty simple, and Python's REPL allows to do quick experiments and immediately see the result." CreationDate="2014-09-26T08:02:07.830" UserId="1279" />
  <row Id="2324" PostId="1101" Score="0" Text="It was not a guess ;-) I am pretty sure about it." CreationDate="2014-09-26T08:54:21.940" UserId="979" />
  <row Id="2325" PostId="1101" Score="0" Text="this is not constructive at this point. I am happy to let anyone read the question and links, my answer, and your comments, and decide what the word means." CreationDate="2014-09-26T08:57:40.643" UserId="21" />
  <row Id="2326" PostId="1159" Score="0" Text="Sorry if I did not made myself clear. My goal is to do PCA. I think SVD on sample data cannot help me to do PCA, right?" CreationDate="2014-09-26T12:36:08.193" UserId="3167" />
  <row Id="2327" PostId="1169" Score="1" Text="Overall, I like your answer but the opening sentence is not quite right. PCA isn't suited for many dimensions with low variance; rather, it is suited for many dimensions with *correlated* variance. For a given data set, the variance could be high in *all* dimensions but as long as there is high covariance, then PCA can still yield significant dimensionality reduction." CreationDate="2014-09-26T15:14:47.117" UserId="964" />
  <row Id="2328" PostId="1154" Score="0" Text="I can understand what's going on by what you have already. Are the pipes unidirectional or bidirectional?" CreationDate="2014-09-26T15:54:18.047" UserId="1241" />
  <row Id="2329" PostId="1151" Score="1" Text="Questions about how to become a data scientist [are off-topic here](http://meta.datascience.stackexchange.com/q/41/322)." CreationDate="2014-09-26T16:25:47.577" UserId="322" />
  <row Id="2330" PostId="1169" Score="0" Text="@bogatron: good catch, thanks. In fact, I was referring to high/low variance in _some_ dimensions, possibly not original ones. E.g. in [this picture](http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png) these dimensions are defined by 2 arrows, not original x/y axes. PCA seeks to find these new axes and sorts them by the value of variance along each axis. Anyway, as you pointed out, it was a bad wording, so I tried to reformulate my idea. Hopefully, now it's more clear." CreationDate="2014-09-26T18:20:18.717" UserId="1279" />
  <row Id="2331" PostId="1169" Score="0" Text="That makes sense to me. +1." CreationDate="2014-09-26T19:13:37.270" UserId="964" />
  <row Id="2332" PostId="1166" Score="0" Text="@ffriend thank you very much, I am going to try it, just a question are these toolkit general purpose? In fact my target language is Persian" CreationDate="2014-09-26T19:30:03.517" UserId="3436" />
  <row Id="2333" PostId="1168" Score="0" Text="Thank you, I think I will use NLTK. The paper was good but not an excellent comparison" CreationDate="2014-09-26T19:43:51.783" UserId="3436" />
  <row Id="2334" PostId="1166" Score="0" Text="Short answer: no, but it is solvable. Natural languages differ in both - words and rules. You can't expect English lemmatizer to correctly find root of a French word. You also can't expect standard whitespace-based tokenizer to split German adjetcive/noun pairs that are written without whitespaces. But most NLP libraries are flexible enough to allow you extend them for your specific purposes. Sometimes it is done via custom modules, sometimes - via training data, and sometimes there's already ready-to-use library for you (e.g. see [Hazm](https://github.com/sobhe/hazm) for Persian support)." CreationDate="2014-09-26T20:27:59.073" UserId="1279" />
  <row Id="2335" PostId="1165" Score="1" Text="Hello Sean, thanks for your answer. I have just edited my question and narrow it down a bit. I specified that I am interested in the area of Education with the new revoluions like MOOCs as an example..." CreationDate="2014-09-27T16:58:50.050" UserId="3433" />
  <row Id="2336" PostId="1165" Score="0" Text="One of the most promising tools for machine learning with big data is [MLlib](https://spark.apache.org/docs/latest/index.html) from Apache Spark project. There's already a number of methods implemented, but much more is to be written yet. If you are also interested in education, take some algorithmic problem from it (e.g. predicting student's score or course efficiency) and design solution for that problem. Having told that, I should warn you that for using big data solutions your data should really be pretty large, and I'm not sure MOOCs will give that much data." CreationDate="2014-09-27T22:10:12.960" UserId="1279" />
  <row Id="2337" PostId="1106" Score="0" Text="I guess you are asking for two things. One is a rather well understood search problem. You can just use Solr or Elasticsearch to do the heavy lifting for you. They both can find relevant documnents in a collection by weighting hits. However if you want some deeper semantic understanding of the text (i.e. &quot;repair&quot; is not mentioned but a typical activity in a bike shop) then the pure search engine might fall flat." CreationDate="2014-09-28T11:28:48.223" UserId="3445" />
  <row Id="2338" PostId="1165" Score="0" Text="I would like to thank you first for your detailed answer. Yes you are right, the availability of the so much large data is very crucial. I will try to think about that and get back to you :) Thanks for your proposition of the subject :)" CreationDate="2014-09-28T14:59:34.200" UserId="3433" />
  <row Id="2339" PostId="1159" Score="0" Text="PCA is usually implemented by computing SVD on the covariance matrix. Computing the covariance matrix is an embarrassingly parallel task, so it should scale easily with the number of records." CreationDate="2014-09-29T00:18:59.397" UserId="924" />
  <row Id="2340" PostId="1154" Score="0" Text="Pipes are unidirectional. In my case there is a only one big receiver tank and many smaller ones that are pumped (one at a time) to the pipelines, so the liquid flows to the big tank." CreationDate="2014-09-29T03:07:22.030" UserId="3400" />
  <row Id="2341" PostId="413" Score="0" Text="I agree mostly, with a precision: Feature selection needs not be done by hand, it can be automatic. See for instance the Lasso method (http://en.wikipedia.org/wiki/Least_squares#Lasso_method)." CreationDate="2014-09-29T09:00:26.110" UserId="3410" />
  <row Id="2342" PostId="1103" Score="0" Text="@UzumakiNaruto The same goes here for really any domain, regardless of topic, etc. -- collect a LOT of relevant real-world data and annotate it manually... this will be the training data you use to train new models.&#xA;&#xA;Also, you might want to look into the work of Giuseppe Carenini at the Univ of British Columbia -- he's done some research applying NLP, etc. to things like business meeting notes: http://www.cs.ubc.ca/~carenini/" CreationDate="2014-09-29T17:12:25.793" UserId="819" />
  <row Id="2343" PostId="1152" Score="0" Text="@markusian re: &quot;the tweets vocabulary is quite limited&quot; -- that's not what I've seen in the real world. What kind of Twitter corpus are you dealing with? re: &quot;simply append the features for URLs to the features for the tweet&quot; What kind of features are you talking about? Something other than bag-of-words? re: &quot;this would put on the same level the words from the tweet and the words from the URL&quot; So just boost the score of the words in the tweets." CreationDate="2014-09-29T17:19:05.177" UserId="819" />
  <row Id="2344" PostId="876" Score="0" Text="I've realized that my data set is um... interesting. &quot;Interesting&quot; in that out of 10 explanatory variables, only one is actually independent." CreationDate="2014-09-30T00:05:12.950" UserId="1241" />
  <row Id="2345" PostId="1173" Score="1" Text="I'm not sure I'm understanding what you're looking for, because it seems like a simple log of what and when is all you need." CreationDate="2014-09-30T04:13:10.900" UserId="1241" />
  <row Id="2346" PostId="1180" Score="0" Text="Sorry, I'm still learning the correct terminology. I'm trying to determine similarity, and believe the Tanimoto  and Pearson coefficients and Euclidean distance calculation can be used interchangeably to do this (source: Programming Collective Intelligence)." CreationDate="2014-09-30T08:32:20.053" UserId="3459" />
  <row Id="2347" PostId="1180" Score="1" Text="Similarity is large, but distance is small, when things are &quot;alike&quot;. They're opposite in that sense, but, you could create some similarity metric out of distance, yes." CreationDate="2014-09-30T09:26:11.523" UserId="21" />
  <row Id="2348" PostId="1182" Score="0" Text="Thanks was looking for ideas for the data structures. Will check out the lib what they use." CreationDate="2014-09-30T15:21:02.593" UserId="3445" />
  <row Id="2349" PostId="1173" Score="0" Text="@AndyBlankertz in that case I was looking for in-memory, but sure I can push the events to a logfile or redis or similiar. Then I still need datastructure(s) for the efficient calculations. I would like to combine the typical values in a way I dont have to have buckets for all seperate." CreationDate="2014-09-30T15:22:49.710" UserId="3445" />
  <row Id="2350" PostId="1183" Score="0" Text="Hi Catalin, and welcome to DS! As it stands, your question is hard to answer. What does &quot;not similar&quot; mean? Are you looking to detect structural changes, like shifting levels? Outliers? Changes in seasonality? What is the &quot;data&quot; (any number, or counts, lots of zeros or few)? Please consider editing your question." CreationDate="2014-10-01T09:50:04.953" UserId="2853" />
  <row Id="2351" PostId="1186" Score="0" Text="You can classify all keywords beforehand and then just pull category from the index." CreationDate="2014-10-01T13:31:28.367" UserId="1279" />
  <row Id="2352" PostId="1186" Score="0" Text="@ffriend seems like an answer for one word query. But if search query is consist more words .. or combinations of words .. i have to create index for all combinations!!!" CreationDate="2014-10-01T13:35:16.603" UserId="3498" />
  <row Id="2353" PostId="1186" Score="1" Text="SVC is fast, so if you want to use it for query classification in a moderate-load application, it will work. But classification by a single (or even several words) is a bad idea in most cases. Take ambiguous words, for example: what if some word belongs to 2 categories with very little difference in probabilities? Are you going to throw just a little bit less probable category out of search? What you most probably want is an additional term in ranking formula while searching, not rejecting less probable categories at all." CreationDate="2014-10-01T13:56:54.813" UserId="1279" />
  <row Id="2354" PostId="1181" Score="2" Text="re: &quot;Is there a better way of using LDA to detect topics in text, there are so provide better results?&quot; &#xA;&#xA;Could you clarify your question a bit? What do you mean by &quot;better?&quot;" CreationDate="2014-10-01T14:33:18.317" UserId="819" />
  <row Id="2355" PostId="1174" Score="1" Text="Can you repost the important details in this question? e.g. what's the data look like, where'd it come from, is it labeled, etc." CreationDate="2014-10-01T14:43:41.097" UserId="403" />
  <row Id="2356" PostId="1183" Score="0" Text="i've updated the question" CreationDate="2014-10-01T15:30:48.633" UserId="3482" />
  <row Id="2357" PostId="1175" Score="0" Text="Thank you for this response. I am still a tad confused: 1) About consecutive updates - why is this better than seeing the same observation the same number of times, but out of order? and 2) I dont get the phrase used regarding &quot;for all importance weights of h, the update is equivalent to two updates with importance weight h=2&quot;. It seems like what is important is that the process is equivalent to h updates with weight =1." CreationDate="2014-10-01T17:24:15.557" UserId="1138" />
  <row Id="2358" PostId="1175" Score="0" Text="consecutive updates- its not that its better ( it isn't), but that it doesn't fit in the online &quot;memory-less&quot; setting of vowpal-wabbit ( in order to reorder the data you need to store the data). second is a typo? In the abstract it says:  &quot;that updating twice with importance&#xA;weight h is equivalent to updating once with&#xA;importance weight 2h&quot;" CreationDate="2014-10-01T17:34:35.127" UserId="1256" />
  <row Id="2359" PostId="1175" Score="0" Text="Sorry, I read your response: &quot;The idea being that presenting it consecutively reduces the problem because the error gradient... &quot; as saying that if you were relying on adding a record with weight n, n times (with weight 1), ignoring the computational burden, it was better to present the record to the algorithm n-times consecutively. So, this was the &quot;goal&quot; VW is trying to achieve, but with something equivalent that only requires a record to be present 1 times in the data." CreationDate="2014-10-01T18:22:29.540" UserId="1138" />
  <row Id="2360" PostId="1175" Score="0" Text="Regarding the typo, YES, seems I should have typed &quot;...h/2&quot;. I changed the question. This is stated in this fashion the way you added in the comment and as the question now states, later in the paper. Even with this new wording, I fail to see what the point it...likely obvious, I am just not latching onto it yet." CreationDate="2014-10-01T18:26:02.160" UserId="1138" />
  <row Id="2361" PostId="1175" Score="0" Text="So which bit? a) can't present the data out of order in online setting ( and computational load). b) why n consecutive updates is better than 1 single update of weight n: gradient descent - &quot;overshoot&quot;, c) why their proposed single update is better than n consecutive updates - computational load d) why importance weights are useful in a statistical learning system" CreationDate="2014-10-01T18:48:33.877" UserId="1256" />
  <row Id="2362" PostId="1175" Score="0" Text="I think I understand b) in that a huge weight multiplied against the gradient will really cause a large update and it will act as an outlier for tuning a learning rate. I think I understand c) in that having to repeat the update for a single observation with weight n, n-times will of course cause more computations. I know d) well - at least in the case of batch learning, where it is for example useful for unbalanced classes. I don't know of an example in true online learning (my VW experience is with a fixed offline data set that is fed into VW because it doesn't all fit in memory)." CreationDate="2014-10-01T19:05:33.057" UserId="1138" />
  <row Id="2363" PostId="1175" Score="0" Text="So, my confusion is for a) as well as the initial question about the invariance property - what it means and why it matters." CreationDate="2014-10-01T19:06:36.847" UserId="1138" />
  <row Id="2364" PostId="1175" Score="0" Text="Does invariance here simply mean that a single update (as they propose) achieves the same thing as seeing the record multiple times? Is that all it is saying?" CreationDate="2014-10-01T19:08:29.410" UserId="1138" />
  <row Id="2365" PostId="1175" Score="0" Text="a) VW basically streams the data- so how do you present the samples out of order, that requires you to store the data. And yes - thats all they mean by invariance." CreationDate="2014-10-01T20:15:48.957" UserId="1256" />
  <row Id="2366" PostId="1175" Score="0" Text="a) I thought that was what you were saying - that if you were presenting an observation multiple times (ignoring the computational cost) it was better that it be consecutive, as that would result in less instability of a large weight!" CreationDate="2014-10-01T20:21:47.670" UserId="1138" />
  <row Id="2367" PostId="1175" Score="0" Text="Oh geez if that is all it means, then that was pretty straightforward - I guess my typo did not help matters for my confusion :) thanks!" CreationDate="2014-10-01T20:22:23.357" UserId="1138" />
  <row Id="2368" PostId="1191" Score="0" Text="Do you use every userid and every itemid as a separate dimension?" CreationDate="2014-10-02T06:42:22.137" UserId="1279" />
  <row Id="2369" PostId="1191" Score="0" Text="When comparing items the number of dimensions is the number of users. Think of items as vectors of 1's and 0's where each dimension corresponds to some user and the value is 1 if the user accessed the item and 0 otherwise." CreationDate="2014-10-02T13:14:43.533" UserId="3506" />
  <row Id="2370" PostId="1194" Score="0" Text="I have tried varying the rank for the approximation. I tried ranks 40, 100, 300, 1000. I expect that with the rank 40 it would classify items into 40 available dimensions. If that classification into 40 classes is good, I would have no issue with that. However, it seems that some dimensions were simply not being used. A lot of dissimilar items ended up in the same dimension." CreationDate="2014-10-02T13:29:39.620" UserId="3506" />
  <row Id="2371" PostId="1191" Score="0" Text="The real question here is: does standard PCA work well with binary variables? And I'd say that the answer is &quot;no&quot;. Binary variables are not continuous (you can't get .73 in your user-item matrix), but instead categorical (only 0s and 1s are allowed, &quot;yes&quot;-s and &quot;no&quot;-s and nothing in between). IIRC, [MCA](http://en.wikipedia.org/wiki/Multiple_correspondence_analysis) is a standard analogue of PCA for categorical data. Though, my personal approach would be to use [RBMs](http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine), which can also handle non-linearities." CreationDate="2014-10-02T21:14:39.957" UserId="1279" />
  <row Id="2372" PostId="1196" Score="0" Text="Simple way to deal with missing data is to use population's average for this variable. Intuition here is that if we don't know anything about it, then we would want to use something &quot;neutral&quot; - not better and not worse than an average. From statistical point of view, we may say that _if that value existed_, its prior would most probably be distributed normally with mean in population's expectation." CreationDate="2014-10-02T22:40:43.443" UserId="1279" />
  <row Id="2373" PostId="1191" Score="0" Text="It is possible that there are algorithms more suited to categorical data than SVD. However, SVD is often cited as the tool for dimensional reduction in the context of latent semantic analysis. See the response by buruzaemon, for example. In that case the SVD is applied to the term incidence matrix which is also made of 1's and 0's. Also some form of matrix factorization (may be SVD with some regularization) was successfully used in the Netflix competition to predict user ratings for movies (the ratings are integers 1-5). Therefore, I don't think it is easy to write SVD off in discrete case." CreationDate="2014-10-03T00:55:33.500" UserId="3506" />
  <row Id="2374" PostId="1191" Score="0" Text="Re using RBMs: Aren't RBMs for classification (supervised learning)? I am interested in clustering (unsupervised learning). Anyway RBM's are probably off limits for me because RBMs are not implemented in Mahout and I cannot use anything licensed by GPL for legal reasons." CreationDate="2014-10-03T00:59:01.023" UserId="3506" />
  <row Id="2376" PostId="1191" Score="0" Text="1. In LSA, SVD is not used for dimension reduction, but instead for finding semantic concepts. If you want dimension reduction, use PCA (or MCA). 2. For continuous variables it makes sense to talk about real values (e.g. .73) and order of values, for categorical - no. For Netflix it makes sense to say &quot;user likes X as much as 4.5&quot; and &quot;mark 4.5 is higher than 3.88&quot;. For values &quot;man&quot; and &quot;woman&quot; (even represented as 1 and 0) there's nothing between them (e.g. what would mean value .73?) and no specific order (&quot;man&quot; &gt; &quot;woman&quot; or vice versa?). Decide for yourself which case is yours." CreationDate="2014-10-03T15:40:06.363" UserId="1279" />
  <row Id="2377" PostId="1191" Score="0" Text="3. RBM may be used for data compression/dimension reduction, see relevant sections of my [earlier answer](http://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma/117188#117188)" CreationDate="2014-10-03T15:44:04.323" UserId="1279" />
  <row Id="2381" PostId="1197" Score="0" Text="n-gram models are often the way to go when considering word order. see http://en.wikipedia.org/wiki/N-gram" CreationDate="2014-10-03T19:25:00.910" UserId="2969" />
  <row Id="2382" PostId="1199" Score="0" Text="Thanks for the input. This means greatly expanding the number of rhs variables, since you now need two variables for each one that has missing values. Also, I'd think you have to be careful about making 0 the default value for lack of information. But this is a good starting point, and I'll think more about it." CreationDate="2014-10-03T20:11:13.263" UserId="3510" />
  <row Id="2383" PostId="1205" Score="0" Text="This is fairly broad, and generally questions asking for recommended tools are off topic on StackExchange. I'd suggest stating more about the nature of your requirements and asking specific questions about tools if possible, to get more specific and useful responses." CreationDate="2014-10-04T10:15:00.067" UserId="21" />
  <row Id="2384" PostId="1189" Score="0" Text="The question here is quite unclear. What are you trying to do, build a confusion matrix? what have you tried? is there more to your goal?" CreationDate="2014-10-04T10:16:11.317" UserId="21" />
  <row Id="2386" PostId="1151" Score="0" Text="AirThomas, I had a similar question, is there another place where I can ask it where it wouldn't be off topic?" CreationDate="2014-10-05T00:28:53.647" UserId="3535" />
  <row Id="2387" PostId="1208" Score="0" Text="Do the test set predictions for the two classifiers correspond exactly? i.e. is every instance classified in the same way?" CreationDate="2014-10-05T01:30:30.080" UserId="51" />
  <row Id="2388" PostId="1208" Score="0" Text="yes the methods are exactly like each other. the only difference is from the feature type" CreationDate="2014-10-05T05:47:30.430" UserId="3530" />
  <row Id="2389" PostId="1214" Score="1" Text="I wanted to add the tag best-practices but I cannot since I don't have 150 reputation points. To be honest, I don't understand how a new comer can effectively be a contributor to the site with all such rules. I see a lot of questions for which I know the answers, but I can't answer or even up vote the answer if it is already there." CreationDate="2014-10-05T06:28:39.150" UserId="3540" />
  <row Id="2390" PostId="1213" Score="1" Text="Does it have to be a supervised approach? A convolutional denoising autoencoder seems like exactly what you're looking for, but it's unsupervised." CreationDate="2014-10-05T12:18:10.090" UserId="2969" />
  <row Id="2391" PostId="1215" Score="0" Text="The first line of your answer means to me you prefer windows but in the rest you count advantages of Linux, I didn't get Windows or Linux?" CreationDate="2014-10-05T12:40:26.250" UserId="3436" />
  <row Id="2392" PostId="1215" Score="0" Text="Linux. I prefer to use windows, but it's the wrong way to go for this problem. Edited the answer for clarity." CreationDate="2014-10-05T14:52:57.497" UserId="2969" />
  <row Id="2393" PostId="1191" Score="0" Text="If every item is assigned some semantic concepts, then that can be used for clustering. I believe that my case is analogous to the latent semantic analysis and the netflix data set because in all examples in the absence of data the default value in the matrix is zero. For netlix: zero doesn't mean that the user gave that movie a zero rating, it means that he didn't give any rating. Therefore netflix rating is also a categorical value. In fact I would experiment with the netflix dataset the same way by mapping all ratings to 1 and mapping absence of a rating to zero." CreationDate="2014-10-05T15:31:11.517" UserId="3506" />
  <row Id="2394" PostId="1191" Score="0" Text="Re RBMs: This is interesting. Thank you for sharing." CreationDate="2014-10-05T15:32:31.903" UserId="3506" />
  <row Id="2395" PostId="1214" Score="0" Text="I think this is more of a generic programming question, so StackOverflow might be better. You can include a snippet of what you are trying to do, and why it's slow, and ask for suggested optimizations." CreationDate="2014-10-05T16:46:40.337" UserId="21" />
  <row Id="2396" PostId="1210" Score="0" Text="I think this answer has a kernel of value but could probably benefit from a more specific example, and some editing." CreationDate="2014-10-05T16:48:11.980" UserId="21" />
  <row Id="2397" PostId="1215" Score="0" Text="@brentlace Thank you so much, I modified my question and asked a recommendation for a programming language. which you recommend?" CreationDate="2014-10-05T19:11:49.257" UserId="3436" />
  <row Id="2398" PostId="1215" Score="0" Text="I tend to do most if my work in python, but i don't actually do much nlp these days. My suggestion would be to use whatever you are comfortable with. Python would be a good place to start. Python also tends to play well with other languages, so if you find that it missing key capabilities, you should be able to integrate them." CreationDate="2014-10-05T19:14:44.210" UserId="2969" />
  <row Id="2399" PostId="1191" Score="0" Text="In Netflix 0 is just an encoding for NA (not available) values. It's not a continuous or categorical variable, it's just missing datum, and should be treated separately. E.g. you want your model to predict values from 1 to 5, but not zero - it would be nonsense (how even would you interpret it? predicting that user haven't given a rating?). So your case is _somewhat_ similar to LSA, but definitely not Netflix dataset." CreationDate="2014-10-05T22:05:34.757" UserId="1279" />
  <row Id="2400" PostId="1208" Score="0" Text="Not the methods, I meant the classified labels. Do they correspond exactly or is it just the aggregate percentages that match?" CreationDate="2014-10-06T04:06:11.803" UserId="51" />
  <row Id="2401" PostId="1076" Score="0" Text="@mike1886: if you already know how to update your model using new data, what else do you want? What is &quot;more of state of the art&quot; in your understanding? Just more recent and sounding cooler?" CreationDate="2014-10-06T05:56:47.770" UserId="1279" />
  <row Id="2403" PostId="1216" Score="2" Text="I think this might be a bit broad for StackExchange, and perhaps considered off-topic if it concerns career advice, but see what others think." CreationDate="2014-10-06T15:12:23.767" UserId="21" />
  <row Id="2407" PostId="1221" Score="0" Text="I do agree with using Linux through SSH, this way I can download any package and there is no cost for my Internet (In my country internet is not much cheap) but that way could I use something like PyCharm? or do I miss some GUI which is important for NLP tasks?" CreationDate="2014-10-06T20:01:40.433" UserId="3436" />
  <row Id="2408" PostId="1221" Score="0" Text="If you SSH into a Linux machine, you will have no GUI. However, you can use any GUI tools in Windows and simply transfer your files to you Linux machine via SSH. In fact, you can use PyCharm in Windows and configure it to transfer your files and execute your code on your Linux machine." CreationDate="2014-10-06T20:06:36.073" UserId="3466" />
  <row Id="2409" PostId="1221" Score="1" Text="You can dual boot and run both on the same machine; it's easy." CreationDate="2014-10-06T21:16:09.877" UserId="381" />
  <row Id="2410" PostId="1208" Score="0" Text="Aaah. I misunderstood, sorry. but it makes no difference, they are the same" CreationDate="2014-10-06T21:21:56.003" UserId="3530" />
  <row Id="2411" PostId="1191" Score="0" Text="I agree it isn't very appropriate to mix NAs with ratings. However, the data set is sparse--no user rates all movies. If we want to apply SVD to the dataset and exclude NA values, we would be applying it to an empty set or we will be narrowing the dataset down extremely to a subset of users and a subset of movies, so that each movie is rated by each user. So NAs have to be assigned some values to perform SVD at all. In this paper (http://research.cs.queensu.ca/TechReports/Reports/2006-527.pdf) the authors treated NAs as no-opinion with score 3 (see page 11). Sounds like it worked for them." CreationDate="2014-10-07T00:10:46.893" UserId="3506" />
  <row Id="2413" PostId="1208" Score="0" Text="I asked a similar question not too long ago: http://datascience.stackexchange.com/questions/992/why-might-several-types-of-models-give-almost-identical-results" CreationDate="2014-10-07T01:49:29.913" UserId="1241" />
  <row Id="2415" PostId="1191" Score="0" Text="Often people use mean of all ratings for specific film. E.g. if movie got ratings (2, 4, 2) from 3 users, all other user ratings (which are NAs in the dataset) are assigned (2+4+2)/3 = 2.66." CreationDate="2014-10-07T06:21:13.437" UserId="1279" />
  <row Id="2418" PostId="1216" Score="6" Text="Don't forget that the people you are comparing yourself against are those that have the knowledge to have well-read blogs, have high stack exchange reps, etc, ie, not a representative sample. You are comparing yourself with the best, not the average. If you are a smart IT guy and you want it badly enough, it is there for the taking. Data is growing exponentially, our ability to analyse and manage it, possibly more slowly. So, there are plenty of opportunities, just grab the bull by the horns." CreationDate="2014-10-07T19:36:54.663" UserId="3571" />
  <row Id="2419" PostId="1191" Score="0" Text="If a user bothered to rate the item, then that means that the user is somewhat interested in this item. I assume that users don't access items at random, but do so according to their personal tastes. Missing data is interpreted as negative feedback. Why would it be more appropriate to give mean rating for all users? The mean would be an estimator only conditionally that the users provide some rating, because this characterizes the sample space. It seems that the mean is appropriate for different purposes than mine, like modeling the rating function with the assumption that a rating is given." CreationDate="2014-10-07T23:28:25.813" UserId="3506" />
  <row Id="2420" PostId="1208" Score="0" Text="thanks @AndyBlankertz that helped a lot." CreationDate="2014-10-08T08:29:15.980" UserId="3530" />
  <row Id="2421" PostId="1191" Score="0" Text="It's up to your assumptions. There are ways to assess performance of both models, so you can always check what assumption was closer to the truth." CreationDate="2014-10-08T08:37:50.453" UserId="1279" />
  <row Id="2423" PostId="1213" Score="2" Text="Supervised is the wrong term here, I believe. The OP essentialy wants to denoise, which is what you typically do with unsupervised methods." CreationDate="2014-10-08T19:32:07.663" UserId="1193" />
  <row Id="2424" PostId="1228" Score="1" Text="Although this might answer the question, link-only answers are highly discouraged. Links may change, thus invalidating references. Try to make your answer self-contained, by adding further information to it." CreationDate="2014-10-09T02:30:56.957" UserId="84" />
  <row Id="2425" PostId="1214" Score="0" Text="Actually I think http://opendata.stackexchange.com/ would be a better fit." CreationDate="2014-10-09T02:36:56.137" UserId="381" />
  <row Id="2426" PostId="1230" Score="0" Text="Thanks, this was really helpful" CreationDate="2014-10-09T05:25:03.783" UserId="3587" />
  <row Id="2427" PostId="1231" Score="0" Text="Wonderful answer. Thanks." CreationDate="2014-10-09T05:25:36.827" UserId="3587" />
  <row Id="2428" PostId="1231" Score="0" Text="Do you see any problems with using the simple difference averaging approach I wrote above? It seems like an ok stand in for a similarity measure in a K-NN approach and it's just as lightweight as Euclidean Distance." CreationDate="2014-10-09T05:36:42.697" UserId="3587" />
  <row Id="2429" PostId="1231" Score="0" Text="No, it actually has a name: [Manhattan similarity](http://en.wikipedia.org/wiki/Taxicab_geometry)." CreationDate="2014-10-09T05:45:32.473" UserId="381" />
  <row Id="2430" PostId="1220" Score="0" Text="I mentioned about my being weak in mathematics, during school days. I have started liking mathematics ever since I have seen its real use in solving real life problems :).  So, you can suggest me ways to study mathematics. I like your answer." CreationDate="2014-10-09T10:38:08.233" UserId="3550" />
  <row Id="2431" PostId="1220" Score="0" Text="I always like to learn about the software problem I'm trying to solve, then learn the mathematics needed to solve the problem. However, it is possible that you won't be able to just pick up the new math and use it right away, depending on your skill level. Be honest with yourself and choose a software problem that has math you think you could pick up. Work on it daily, as part of your portfolio. Broaden your math knowledge with online courses if you find engaging software problems with math you don't understand. The key thing is habit - make time to study or code every day." CreationDate="2014-10-09T16:59:49.030" UserId="3466" />
  <row Id="2436" PostId="1087" Score="0" Text="A quick update .. Ive implemented this logic and the results  have come out pretty well. Built on Python NLTK  and gensim. im exploring opportunities to use information gain to add in some measure of &quot;usefulness&quot; using information gain or something like that . LSI is also an option here." CreationDate="2014-10-09T21:36:42.363" UserId="3232" />
  <row Id="2438" PostId="1199" Score="0" Text="Well, in my particular case the number of new variables was not a problem, since I did not have so many numerical features. And regarding the value 0 for the missing values, it made sense in the specific context of my problem. But maybe these ideas may be adapted somehow to your particular problem." CreationDate="2014-10-10T08:04:03.297" UserId="2576" />
  <row Id="2441" PostId="1246" Score="1" Text="Split dataset into mini-batches and fit model to each mini-batch sequentially." CreationDate="2014-10-10T13:53:58.887" UserId="1279" />
  <row Id="2442" PostId="1246" Score="0" Text="Thank you @ffriend. However, that wouldn't be a pure on-line implementation." CreationDate="2014-10-10T14:05:36.313" UserId="2576" />
  <row Id="2443" PostId="1096" Score="0" Text="This is as easy as it is going to get, if you want to DIY without spending money." CreationDate="2014-10-10T16:22:59.290" UserId="381" />
  <row Id="2444" PostId="1234" Score="0" Text="I really like your engine, it gave me great recommendations for related topics. Well done." CreationDate="2014-10-10T17:21:18.920" UserId="3526" />
  <row Id="2445" PostId="1234" Score="0" Text="Thanks man!! Keep using." CreationDate="2014-10-10T17:28:17.107" UserId="3596" />
  <row Id="2446" PostId="1246" Score="0" Text="What's the reason to use &quot;pure online&quot; implementation if your dataset is fixed? SGD only says that you don't need to iterate the whole dataset at once, but can split it into an arbitrary number of pieces (mini-batches) and process them one by one. Mini-batch of size 1 only makes sense when you have continuous and possibly endless source of data (like twitter feed, for example) and want to update the model after each new observation. But that's very rare case and definitely not for fixed datasets." CreationDate="2014-10-10T18:35:17.400" UserId="1279" />
  <row Id="2447" PostId="1216" Score="1" Text="Every company is different I guess, but in my company we don't do any insane statistics/mathematics. There are a lot of common sense problem solving though. I personally wish that my computer science background was stronger. I'd rank the skills in order of value like this: 1) Common sense, 2) Computer Science / Programming 3) Mathematics / Statistics." CreationDate="2014-10-11T03:54:46.093" UserId="3070" />
  <row Id="2449" PostId="1236" Score="0" Text="I think this will be a much better question if you define polysemes, and state what you have done so far to answer your question. This doesn't seem to be about data science at the moment." CreationDate="2014-10-11T09:44:12.857" UserId="21" />
  <row Id="2450" PostId="1233" Score="1" Text="Link-only answers are generally discouraged. Please summarize what about the post answers the question." CreationDate="2014-10-11T09:46:04.410" UserId="21" />
  <row Id="2452" PostId="1255" Score="0" Text="So one-hot-encoding is still used, just on hashed values *which as you say saves space and can cause dimensionality reduction (given collisions). Is that correct?" CreationDate="2014-10-12T00:08:00.330" UserId="1138" />
  <row Id="2453" PostId="1255" Score="1" Text="One Host Encoding isn't a required part of hashing features but is often used alongside since it helps a good bit with predictive power.  One way to think of one hot encoding is transforming a feature from a set of N discrete values into a set N binary questions.  Perhaps it's not important for me know if feature J is 2 or 3 only that it's not 4.  One Hot makes that distinction specific.  This helps a lot with linear models whereas ensemble approaches (like RF) will scan break points in the feature to find that distinction." CreationDate="2014-10-12T00:15:58.410" UserId="92" />
  <row Id="2454" PostId="405" Score="0" Text="Nice answer, though it fails to cover the OP's point 2, which is the Python 2 vs 3 issue." CreationDate="2014-10-12T03:24:36.873" UserId="3571" />
  <row Id="2455" PostId="2257" Score="1" Text="I think you should clarify your question. What are you trying to predict exactly, what are your constraints, what have you tried, what are your concerns about the approach you cite?" CreationDate="2014-10-12T10:19:04.723" UserId="21" />
  <row Id="2458" PostId="2257" Score="0" Text="I edited the problem, add some clarification. Thank you!" CreationDate="2014-10-12T21:55:40.043" UserId="4619" />
  <row Id="2459" PostId="1236" Score="0" Text="@SeanOwen its always better if you dont comment, if you have no idea on subject.do you know what is polysemes?" CreationDate="2014-10-13T08:34:26.787" UserId="3598" />
  <row Id="2460" PostId="1076" Score="0" Text="Agree---scikit's online learning modules are pretty 'state-of-the-art', whatever that means" CreationDate="2014-10-13T13:56:02.590" UserId="1399" />
  <row Id="2461" PostId="1236" Score="1" Text="of course I do. You misunderstand my suggestion. It's for the benefit of readers who might help you with a resource on &quot;words with multiple meanings&quot; but don't recognize the word 'polyseme'." CreationDate="2014-10-13T20:48:26.457" UserId="21" />
  <row Id="2462" PostId="1250" Score="0" Text="i would like to upvote but I am not allowed." CreationDate="2014-10-13T22:01:16.370" UserId="3540" />
  <row Id="2463" PostId="2266" Score="0" Text="Thanks, Ben. The quality I mentioned, is the quality, or true worth, of a product in an shopping website. Actually, I'm going to measure/inference on its ranking itself, but try to identity two main factors towards the sales of a product: 1.quality(it that product good enough?) 2.ranking in page(a bias contributes to sales, but doesn't reflect product's quality)" CreationDate="2014-10-14T09:22:31.183" UserId="1048" />
  <row Id="2464" PostId="2266" Score="0" Text="Sure---I think you can include whatever predictors are useful (and indeed doing so may improve your model's fit). The point is you should view it as a logistic regression problem and understand which parameters you're doing inference about" CreationDate="2014-10-14T15:42:12.880" UserId="1399" />
  <row Id="2465" PostId="2274" Score="0" Text="Hi ffriend, thanks for your detailed answer. I did not go into much details into the independent variable on purpose, because the focus of my question is the fact that I am using a *single* variable to predict another and I wanted to know the most suitable data mining techniques for this case. You confirmed my feeling that simple statistics (or not so simple) could be appropriated in this case, but the best is to try out things. Regarding the &quot;weather&quot; variable I was actually planning to use one metric and continuous variable such as visibility or rain, just to keep things simple." CreationDate="2014-10-15T08:42:21.740" UserId="3159" />
  <row Id="2466" PostId="2274" Score="0" Text="In fact, using 2 or more variables for linear regression is almost as simple as using only one, but can lead to a much more accurate predictions. [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) (free PDF is available) is a great introduction to linear regression, its use cases and estimation metrics. If you are using specialized software like R, modelling different dependencies is deadly simple, boiling down to only few lines of code." CreationDate="2014-10-15T09:05:25.753" UserId="1279" />
  <row Id="2467" PostId="2278" Score="0" Text="please reformulate your question title. read here http://stackoverflow.com/help/how-to-ask for how to ask a good question" CreationDate="2014-10-15T09:33:36.683" UserId="906" />
  <row Id="2468" PostId="2277" Score="0" Text="Thanks for your comments regarding the meaning of the weather variables; although that was not my original question, it was really helpfull for my problem." CreationDate="2014-10-15T13:52:19.253" UserId="3159" />
  <row Id="2471" PostId="2286" Score="0" Text="thanks for this. But already have completed anaphora resolution part. I already added it in question description" CreationDate="2014-10-16T09:25:30.337" UserId="4662" />
  <row Id="2472" PostId="2286" Score="0" Text="OvisAmmon: also `,` and `and` will not be sentence boundary in all case" CreationDate="2014-10-16T10:03:14.787" UserId="4662" />
  <row Id="2473" PostId="2288" Score="0" Text="That's what I had in mind . They are indeed dependent on the behavior of each other.The problem is that I read about logistic regression  and saw an example with iris dataset. It is kind of a classificiation and the x_prediction  , y_prediction  , z_prediction are going to be float numbers not a value like (type 1 , type 2 etc.) Is that a problem for my case? Thanks for yoru answer" CreationDate="2014-10-16T15:04:30.480" UserId="4668" />
  <row Id="2474" PostId="2288" Score="0" Text="[Logistic Regression](http://en.wikipedia.org/wiki/Logistic_regression) predicts a binary response from categorical variables (type1, type2, etc). If all your variables are continuous, you may be better off using a [linear regression](http://en.wikipedia.org/wiki/General_linear_model) model" CreationDate="2014-10-17T10:20:38.800" UserId="3159" />
  <row Id="2475" PostId="2293" Score="1" Text="This is too vague to be answered. Time series are too different to just throw k-means on them and get out anything useful. It *heavily* depends on your data." CreationDate="2014-10-17T12:14:04.003" UserId="924" />
  <row Id="2476" PostId="2286" Score="0" Text="I'm not sure, that your last comment makes grammatical sense. In both examples comma and conjunction `and` are exactly sentence boundary markers as they split a complex sentence into two simple ones. More of that, Stanford NLP assigns POS tag `CC` to the conjunction `and` (you can try it online @ their site). So even with anaphora present as a prerequisite, basic thing you need to learn how to do is simple sentence borders. After that your task is simplified to binary classification query/not_query." CreationDate="2014-10-17T13:24:31.757" UserId="2573" />
  <row Id="2477" PostId="2287" Score="0" Text="What is the relationship between the variables? Does the value they take depend on t or not? It sounds like a dynamic Bayes net, or MRF, might be neccesary but you need to provide more info" CreationDate="2014-10-17T13:38:58.833" UserId="1399" />
  <row Id="2478" PostId="2288" Score="0" Text="I see thanks for your help , I cannot upvote your answer cause of no reputation but this is a solid answer ty :)" CreationDate="2014-10-17T14:37:03.243" UserId="4668" />
  <row Id="2479" PostId="1128" Score="0" Text="This is a really interesting question and I agree it has practical applications, but I also really think it belongs on Mathematics. It should get a lot more exposure there." CreationDate="2014-10-17T17:00:43.200" UserId="1156" />
  <row Id="2480" PostId="2296" Score="0" Text="How does t-digest compare to the p-square algorithm?" CreationDate="2014-10-17T17:16:09.790" UserId="4683" />
  <row Id="2481" PostId="2304" Score="0" Text="Has one of &quot;Java&quot; and &quot;Energy consumption forecasting&quot; higher priority than the other? (even if ideally you want both)" CreationDate="2014-10-18T17:36:39.233" UserId="3317" />
  <row Id="2482" PostId="1253" Score="6" Text="Basically because they deliver state-of-the-art results, and eliminate the need for feature engineering." CreationDate="2014-10-20T07:39:09.177" UserId="381" />
  <row Id="2483" PostId="2288" Score="0" Text="I am glad it helps; if I answered to your question, maybe you could mark it as &quot;answered&quot;?" CreationDate="2014-10-20T08:08:22.110" UserId="3159" />
  <row Id="2484" PostId="2296" Score="0" Text="Thanks for the answer: this is a simple model to compute extreme quantiles, and I think it will fit my needs. However for more complex time-series that do not have a nearly stationary distribution this approach may fail, and that's when I think we would need something adaptive such as a Markov chain." CreationDate="2014-10-20T09:32:25.270" UserId="3159" />
  <row Id="2485" PostId="2316" Score="0" Text="It's still here." CreationDate="2014-10-20T07:04:46.800" UserDisplayName="davidhigh" />
  <row Id="2487" PostId="2307" Score="0" Text="How can I generate a confusion matrix for the above mentioned input data to check various Performance measures such as variance, precision, recall, F1 measure ?" CreationDate="2014-10-20T15:31:10.347" UserId="3577" />
  <row Id="2488" PostId="2307" Score="0" Text="I do know that I can use the function confusionMatrix(predicted, actual) of the caret package, the only problem I am facing is with the predicted values. Since the data generated is ambiguous, I am not able to understand what the predicted values should be ?" CreationDate="2014-10-20T15:42:48.740" UserId="3577" />
  <row Id="2489" PostId="2307" Score="0" Text="In the worst case (uniform distribution), there is no &quot;predicted value&quot; because there are no clusters in the data. For data generated by other algorithms (e.g. linearly inseparable n-classes XOR), the algorithm does generate *n* classes, so they would be the basis for comparing predicted (i.e. perfect classification) vs actual (what ever your ML method yields)" CreationDate="2014-10-20T17:38:39.237" UserId="609" />
  <row Id="2490" PostId="2307" Score="0" Text="I did get the clusters using the above dataset through kmeans. Hence I wished to find the performance metrics so that after running kernel tricks, I can compare the performance metrics before and after applying kernel tricks" CreationDate="2014-10-20T17:43:39.267" UserId="3577" />
  <row Id="2491" PostId="2261" Score="0" Text="Tnx for the link :D" CreationDate="2014-10-20T17:46:48.557" UserId="4727" />
  <row Id="2492" PostId="2307" Score="0" Text="One more thing I noticed is that, the SSE generated using kmeans lies between 0.2 and 0.35, which makes me wonder whether the dataset can actually be considered 'bad' for kmeans?" CreationDate="2014-10-20T17:49:01.737" UserId="3577" />
  <row Id="2493" PostId="2307" Score="0" Text="Any given realization of a random data set could yield good clustering. But if you run it many times, as you should for statistical testing, and you'll find that it fails as often as it succeeds." CreationDate="2014-10-20T19:13:36.743" UserId="609" />
  <row Id="2494" PostId="2328" Score="1" Text="Be clearer about the structure of your data. Is it already gridded? Or do you need to interpolate it to a grid first? Because that's a whole other big question. Then (once you have a grid) can we talk about 3d graphics. Show us a chunk of your data." CreationDate="2014-10-21T11:44:40.520" UserId="471" />
  <row Id="2495" PostId="2329" Score="0" Text="That is just.. fantastic!" CreationDate="2014-10-21T13:35:38.493" UserId="3443" />
  <row Id="2496" PostId="2324" Score="0" Text="According to the authors, there is indeed a field aware characteristic to the model, relative to the standard implementation - it is stated in the kaggle forums. I was just not able to follow what it meant and what the difference actually was." CreationDate="2014-10-21T13:51:55.417" UserId="1138" />
  <row Id="2497" PostId="2308" Score="1" Text="This is too broad to be a useful question. Narrow it down by stating what you have studied, your interests, and some specific topics you are considering." CreationDate="2014-10-21T13:54:13.613" UserId="21" />
  <row Id="2499" PostId="2285" Score="0" Text="This doesn't make a good question, as there is no detail from which anyone could better guess than you what this term means. I'd elaborate with much more detail and your specific concern, or close this." CreationDate="2014-10-21T13:56:35.100" UserId="21" />
  <row Id="2501" PostId="2287" Score="0" Text="What are these variables you are predicting? it's not clear how they relate to the time series input." CreationDate="2014-10-21T13:58:24.423" UserId="21" />
  <row Id="2502" PostId="2303" Score="1" Text="This is probably off topic if you're just asking for tutorials and resources. Please elaborate with the specific issues you are facing with these tools." CreationDate="2014-10-21T13:59:41.253" UserId="21" />
  <row Id="2503" PostId="2302" Score="0" Text="Can you clarify what these data sets are that you are trying to generate? what have you tried in R so far?" CreationDate="2014-10-21T14:01:17.660" UserId="21" />
  <row Id="2504" PostId="2258" Score="0" Text="Yes, this is too open-ended if you're just looking for tutorials and resources. Maybe you can make this much more specific by specifying what you are trying to do, what you have tried so far and what concepts you found challenging." CreationDate="2014-10-21T14:01:58.440" UserId="21" />
  <row Id="2505" PostId="2299" Score="0" Text="I'm not sure this is a particular problem with R, or that it answers the question of how Python and R differ." CreationDate="2014-10-21T14:02:29.913" UserId="21" />
  <row Id="2510" PostId="2324" Score="0" Text="http://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution/55932#post55932" CreationDate="2014-10-21T14:12:45.363" UserId="1138" />
  <row Id="2511" PostId="197" Score="0" Text="Wanted to provide an update for this question/answer: we've been using JRip with some success, but our new leading contender is FURIA (https://www.cs.uni-paderborn.de/fileadmin/Informatik/eim-i-is/PDFs/furiadraft.pdf). It's generating the best rules for human review/use because it tries to generate an exhaustive ruleset. JRip makes nice rules, but it has a &quot;default&quot; rule for classification when no other rules apply. Default buckets don't work well in our project's business context, we need exhaustive rules." CreationDate="2014-10-21T17:36:54.457" UserId="275" />
  <row Id="2512" PostId="2308" Score="0" Text="thanks @SeanOwen. good, i will add some other information about my studies on Master and my interests :)" CreationDate="2014-10-21T19:33:49.923" UserId="4705" />
  <row Id="2513" PostId="2308" Score="0" Text="&quot;and beyond&quot; part of [this](http://www.meetup.com/spark-users/events/175940092/) and/or [this](http://www.slideshare.net/Hadoop_Summit/th-210p212meng) is probably a good point to start." CreationDate="2014-10-21T21:12:42.143" UserId="1279" />
  <row Id="2515" PostId="2324" Score="0" Text="Based on slie 14, it appears that they based their solution on [this](https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf) paper (_[Ensemble of Collaborative Filtering and Feature Engineered Models for Click Through Rate Prediction](https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/OperaSlides.pdf)_)." CreationDate="2014-10-22T00:20:57.583" UserId="381" />
  <row Id="2516" PostId="2308" Score="1" Text="You are paying to do a Masters, what do your tutors suggest?" CreationDate="2014-10-22T11:08:03.813" UserId="471" />
  <row Id="2518" PostId="2308" Score="0" Text="You might wanna state that Big Data is a leading trend in the Computer industry." CreationDate="2014-10-21T21:21:57.150" UserDisplayName="user4753" />
  <row Id="2522" PostId="2335" Score="0" Text="ok, it is probably the real solution. Thank you." CreationDate="2014-10-23T11:39:54.950" UserId="3281" />
  <row Id="2523" PostId="2332" Score="0" Text="thanks @ssantic , I will consider your proposal for to know the dimensions of this subject according to my ability" CreationDate="2014-10-23T14:55:27.333" UserId="4705" />
  <row Id="2525" PostId="2337" Score="0" Text="How do you decide that &quot;string&quot;, &quot;string2&quot; and &quot;string3&quot; are similar, but &quot;string&quot;, &quot;xstring&quot; and &quot;stringg&quot;, for example, are not?" CreationDate="2014-10-23T19:12:57.943" UserId="1279" />
  <row Id="2526" PostId="2337" Score="0" Text="As I understood it, string, string2 and string3 are placeholders; they are not necessarily similar." CreationDate="2014-10-23T20:05:41.947" UserId="381" />
  <row Id="2527" PostId="2337" Score="0" Text="By string similar I mean they are exactly the same, char by char. Additionally, there might be multiple pairs of exact substrings between the two strings." CreationDate="2014-10-24T05:58:02.363" UserId="4774" />
  <row Id="2528" PostId="2325" Score="0" Text="The implementation that you mentioned follows a tree-growing strategy, like Mondrian forests (http://arxiv.org/abs/1406.2673). Hence, the number of trees is constant while the number of splits is increased. My question focuses on increasing the number of trees for new samples while remaining untouched the previously trained trees." CreationDate="2014-10-24T09:20:03.073" UserId="4719" />
  <row Id="2529" PostId="2306" Score="0" Text="Can you make this question more specific? I am not sure it's productive to speculate on why software doesn't exist." CreationDate="2014-10-24T12:33:25.073" UserId="21" />
  <row Id="2530" PostId="1185" Score="2" Text="Questions asking for online resources are generally discouraged. This might be more useful if you state what aspects you want more insight on, and see if people can write up those insights possibly with links to papers." CreationDate="2014-10-24T12:39:43.657" UserId="21" />
  <row Id="2531" PostId="2325" Score="1" Text="Like [this](http://stats.stackexchange.com/questions/87237/trend-analysis-of-feature-importance-over-time-in-r)? Don't you also want to drop trees if appropriate?" CreationDate="2014-10-24T16:31:03.530" UserId="381" />
  <row Id="2532" PostId="2313" Score="0" Text="I'm not clear on something- are the classes predefined, or supposed to be defined by the data? I.e. are you talking about supervised or unsupervised learning?" CreationDate="2014-10-25T23:16:24.790" UserId="1241" />
  <row Id="2533" PostId="2287" Score="0" Text="@SeanOwen The variables are statistics taken from cores of a computer . So through time I take lets say 'snapshots' of the state of a core and it's variables" CreationDate="2014-10-26T15:42:06.923" UserId="4668" />
  <row Id="2534" PostId="2287" Score="0" Text="@BenAllison The variables are dependent on each other since they are statistics of a core in a computer multicore system." CreationDate="2014-10-26T15:42:42.360" UserId="4668" />
  <row Id="2535" PostId="2287" Score="0" Text="Yeah but what are you predicting? You say that given the values you want to predict the values but you have them." CreationDate="2014-10-26T17:16:18.677" UserId="21" />
  <row Id="2536" PostId="2343" Score="1" Text="I think this is well too vague to make a good question. You haven't given any parameters other than be implementable in 3-4 weeks. Add much more detail at least, but I think this is off topic." CreationDate="2014-10-26T20:12:29.093" UserId="21" />
  <row Id="2537" PostId="2346" Score="0" Text="I think this is just about programming or architecture? I don't see a requirement for engineering+statistics or things people would call ML. I would have said StackOverflow is the best place, but if you can narrow this down to more than just asking for product recommendations." CreationDate="2014-10-26T20:14:00.637" UserId="21" />
  <row Id="2538" PostId="2348" Score="0" Text="How to calculate classification accuracy with confusion matrix ? Thanks" CreationDate="2014-10-27T11:14:43.897" UserId="3503" />
  <row Id="2539" PostId="1189" Score="0" Text="Thanks, I want build confusion matrix to calculate accuracy." CreationDate="2014-10-27T11:17:27.580" UserId="3503" />
  <row Id="2540" PostId="2287" Score="0" Text="@SeanOwen I am using all these values to train a system , so that in the case of 3 new values of x , y , z it will predict what are the new values coming. &#xA;&#xA;The data set I have collected comes from many different benchmarks . So when the program is live I will need based on the current values of the system to predict what the values of the next interval will be." CreationDate="2014-10-27T14:06:23.097" UserId="4668" />
  <row Id="2541" PostId="2337" Score="0" Text="No, the question is if there is another string that has &quot;stringg&quot; then if and how is it different from string2 or string3? The question is about defining EXACTLY what you want to do. Like is there a minimum length of strings you're looking to cluster? Is there a maximum length?" CreationDate="2014-10-27T15:23:23.317" UserId="587" />
  <row Id="2542" PostId="2356" Score="0" Text="I added some more details so as to be more helpful" CreationDate="2014-10-27T16:04:43.740" UserId="4668" />
  <row Id="2543" PostId="2352" Score="0" Text="You really can't draw inferences from *simulated* data, and, in most cases, not even from real data. Since you have so many parameters to play with, you will always (through sheer chance) find some &quot;pattern&quot; that doesn't really exist." CreationDate="2014-10-27T21:10:31.910" UserId="4710" />
  <row Id="2544" PostId="2297" Score="0" Text="NASA approximates the position of planets and satellites and publishes these approximations online with a known margin of error. Using this margin of error, calculate the chances that some catastrophic event will occur in n years (eg, an asteroid hitting the Earth)" CreationDate="2014-10-27T21:14:07.560" UserId="4710" />
  <row Id="2546" PostId="2337" Score="0" Text="I've implemented longest common substring algorithm. It solved my problem but it's expensive computationally." CreationDate="2014-10-28T08:28:14.060" UserId="4774" />
  <row Id="2547" PostId="2362" Score="0" Text="It's a common problem, many techniques invented to try and address it - momentum, adaptive learning rates, batch solvers using second-order approximations etc. Fundamentally, gradient descent is a ropey choice for optimising a complex function, but the best option with neural networks is usually some form of it with some fixes to make it more bearable." CreationDate="2014-10-28T16:57:08.800" UserId="836" />
  <row Id="2548" PostId="2355" Score="0" Text="It is a known issue in GraphLab Create 1.0.1 and earlier that the visualization output will not show in an IPython Notebook running over HTTPS. If you are using HTTPS, try over HTTP instead. This will be fixed in a future release of GraphLab Create." CreationDate="2014-10-28T17:56:44.580" UserId="4834" />
  <row Id="2549" PostId="2297" Score="0" Text="That's very interesting, thanks for your input!" CreationDate="2014-10-29T06:19:44.117" UserId="2475" />
  <row Id="2550" PostId="264" Score="0" Text="@bayer, i think the clustering mentioned here is gaussian mixture model. GMM usually uses EM." CreationDate="2014-10-29T07:17:00.650" UserId="4841" />
  <row Id="2553" PostId="2353" Score="0" Text="I think you should explain the essence of what you are trying to do, and what you have investigated already. &quot;What's wrong with this code&quot; questions aren't that suitable for StackExchange." CreationDate="2014-10-29T08:10:52.140" UserId="21" />
  <row Id="2554" PostId="2359" Score="0" Text="What is the sort criteria? it's not clear. You say &quot;most relevant&quot; but to what?" CreationDate="2014-10-29T08:11:55.803" UserId="21" />
  <row Id="2555" PostId="264" Score="0" Text="I don't think that's what he means, cause GMM does not assume categorical variables." CreationDate="2014-10-29T09:06:51.230" UserId="1193" />
  <row Id="2556" PostId="2355" Score="0" Text="Thank you for your answer" CreationDate="2014-10-29T11:46:59.053" UserId="3281" />
  <row Id="2558" PostId="2346" Score="0" Text="I just made some edits that added questions that are more specific. I think that this question is on topic because once you've done your analyses, you're going to have to communicate the results to others, and that is the gist of Brandon's question." CreationDate="2014-10-29T16:02:34.727" UserId="1241" />
  <row Id="2559" PostId="2372" Score="0" Text="Eh, but the model is fixed here and serialized as PMML. The question is about scoring from a given fixed model." CreationDate="2014-10-29T17:37:22.810" UserId="21" />
  <row Id="2560" PostId="2313" Score="0" Text="the classes are already defined. I'm a little bit confused about the right way of classification. If I have 10 different categories (politics, biology, sports etc.) and I would like to predict new unlabeled data. How do I organize my training-data? I mean, do I build a training-set for predicting politics by creating a set of good samples (politic text) and bad samples (not politic text), the same for all other classes. That would be binary-classification. Or do I create single-class training-sets containing only good samples and use one-class algorithms ..." CreationDate="2014-10-29T21:46:16.737" UserId="4717" />
  <row Id="2561" PostId="2325" Score="0" Text="Thank you. This is more similar to what I am looking for. In this case, the use RF for feature selection of time-variant signals. However, the specific implementation and validity of the method is quite unclear, do you know if they published anything (Google didn't help)?" CreationDate="2014-10-30T13:59:27.840" UserId="4719" />
  <row Id="2562" PostId="2325" Score="1" Text="[Calculating Feature Importance in Data Streams With Concept Drift Using Online Random Forest](https://docs.google.com/presentation/d/19ccixj5ey5ldwpp63x9VGqpTR2Hr-D4nM3l1NRPSPFo/)" CreationDate="2014-10-30T18:01:29.780" UserId="381" />
  <row Id="2563" PostId="2378" Score="0" Text="What you need is a good book on linear algebra, and maybe convex optimization. I can't recommend the one I learned from but [this](http://www.crcpress.com/product/isbn/9781420095388) is relevant." CreationDate="2014-10-30T22:30:17.377" UserId="381" />
  <row Id="2564" PostId="2378" Score="0" Text="This is very broad for StackExchange. Maybe you can start with a specific statement about your understanding and a specific question from there." CreationDate="2014-10-31T08:29:30.863" UserId="21" />
  <row Id="2565" PostId="2325" Score="0" Text="Thanks for the link! I can see that they actually update all the previous trees using a tree-growing strategy, and I am interested in creating new DT's with the new data while keeping untouched the old trees." CreationDate="2014-10-31T10:15:07.017" UserId="4719" />
  <row Id="2566" PostId="2382" Score="0" Text="Poke around things like http://mesowest.utah.edu/cgi-bin/droman/meso_base.cgi?stn=WMGI2" CreationDate="2014-10-31T15:53:38.433" UserId="4710" />
  <row Id="2567" PostId="2381" Score="0" Text="Having trouble understanding your data set.  What do the 1's and 0's correspond to.  For example what does 1011 mean in plain english?" CreationDate="2014-10-31T16:52:03.127" UserId="4808" />
  <row Id="2568" PostId="2381" Score="0" Text="1011 means: Customer booked 2010, 2011 and 2013, but not 2012. It's basically a short time-series and each digit indicates if the customer booked that year (2010,2011,2012,2013)." CreationDate="2014-10-31T17:51:43.593" UserId="723" />
  <row Id="2569" PostId="2376" Score="0" Text="hello sean, thank u for ur reply. there is actually no problem with a binary classifier. I'm just wondering what kind of classification is right for the mentioned problem." CreationDate="2014-10-31T17:58:36.533" UserId="4717" />
  <row Id="2570" PostId="2381" Score="0" Text="Oh, of course I meant 1011 = booked 2010, 2012, 2013 and not 2011." CreationDate="2014-10-31T19:25:41.157" UserId="723" />
  <row Id="2571" PostId="2362" Score="0" Text="Hi Neil, thanks for the comment. Can you suggest other areas I could explore other than gradient descent algorithms?" CreationDate="2014-11-01T07:36:40.033" UserId="4824" />
  <row Id="2572" PostId="2362" Score="0" Text="I don't really think there are any serious contenders for nn optimisation on large complex networks. For small networks, a genetic algorithm search can work (see http://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies or examples)" CreationDate="2014-11-01T08:41:06.957" UserId="836" />
  <row Id="2573" PostId="2362" Score="0" Text="Thanks will take a look" CreationDate="2014-11-01T09:30:30.647" UserId="4824" />
  <row Id="2574" PostId="2382" Score="1" Text="This isn't a good place to ask for data sets." CreationDate="2014-11-01T16:08:44.920" UserId="21" />
  <row Id="2575" PostId="2382" Score="1" Text=". . . but you could try http://opendata.stackexchange.com/ - please check first whether the question is already answered, and please give more details in the question, there are probably lots of variations in energy production data sets, and it is not 100% clear what you are looking for. E.g. why is http://www.nrel.gov/gis/solar.html not suitable from the same site you get the wind data from?" CreationDate="2014-11-01T19:32:25.710" UserId="836" />
  <row Id="2577" PostId="2392" Score="0" Text="There's not much information here. You should provide more detailed diagnostic information, including how you are invoking OMP and with what settings." CreationDate="2014-11-03T08:27:15.523" UserId="21" />
  <row Id="2578" PostId="2391" Score="0" Text="I think you should clarify your problem a bit. So you think metrics1 and metrics2 predict metrics3, and want to know when metrics3 doesn't match the prediction well? that's just a regression problem." CreationDate="2014-11-03T08:28:38.440" UserId="21" />
  <row Id="2579" PostId="2391" Score="0" Text="@SeanOwen : Yes absolutely. metrics1 and metrics2 predict metric3. for example, temperature and pressure metrics predict a metric called constant for a fixed mass of gas. A change in the metric constant means there an anomaly detected in the mass of gas and an action has to be taken. Similarly, at times, we want to predict temperature or pressure metrics from metric constant as well. In the end, all these 3 metric values are streamed to the algorithm." CreationDate="2014-11-03T13:23:01.303" UserId="4887" />
  <row Id="2580" PostId="2394" Score="0" Text="&quot;Best&quot; is massively dependent on your application and probably subjective too." CreationDate="2014-11-03T15:58:56.823" UserId="471" />
  <row Id="2581" PostId="2394" Score="0" Text="Question was edited to be more specific." CreationDate="2014-11-03T16:56:03.183" UserId="3106" />
  <row Id="2582" PostId="2381" Score="0" Text="Haven't you answered your own question? What you provide in the question **is** a probability distribution. Another approach: model the number of hits by averaging/weighting the individual values (eg, chance of 2 hits is approximately .015 or so)" CreationDate="2014-11-03T17:59:57.047" UserId="4710" />
  <row Id="2583" PostId="2393" Score="0" Text="thank you for your reply! =)" CreationDate="2014-11-03T18:37:37.220" UserId="4717" />
  <row Id="2584" PostId="2393" Score="0" Text="You're more than welcome! Good luck!" CreationDate="2014-11-03T19:06:04.653" UserId="4897" />
  <row Id="2585" PostId="2381" Score="0" Text="I want to find a mechanism for this distribution. Therefore reduce all these parameters to only very few. For example just saying $P(1)=0.25$ wouldn't work since a binomial distribution doesnt match. However it almost does if you somehow include another parameter for consecutive hits." CreationDate="2014-11-04T05:54:35.810" UserId="723" />
  <row Id="2586" PostId="904" Score="0" Text="Did you created this dashboard as described above? I would love to have a look at the code!" CreationDate="2014-11-04T07:43:19.750" UserId="497" />
  <row Id="2587" PostId="2392" Score="0" Text="Edited with relevant piece of code and the error as well" CreationDate="2014-11-04T09:52:06.560" UserId="4889" />
  <row Id="2589" PostId="2397" Score="0" Text="If using decimal degrees make sure your metadata includes the Coordinate Reference System (most likely WGS84 lat-long, aka EPSG code 4326). Time stamps need to consider time zones, DST and leap seconds when applicable. Caveat emptor." CreationDate="2014-11-04T17:46:54.797" UserId="471" />
  <row Id="2590" PostId="2398" Score="1" Text="If the question is about finding collaborators... I think this is a bit off topic for this SE site." CreationDate="2014-11-04T19:14:01.473" UserId="21" />
  <row Id="2591" PostId="2398" Score="1" Text="This question appears to be off-topic because it is a request for finding people and not a data science question" CreationDate="2014-11-05T13:29:25.080" UserId="471" />
  <row Id="2592" PostId="2398" Score="0" Text="I would like to learn about data science.  Instead of asking 100 questions on here, why not find someone that would like to work as a team to share his/her insights." CreationDate="2014-11-05T17:40:43.333" UserId="4910" />
  <row Id="2596" PostId="2422" Score="0" Text="How many columns did you get after including the quadratic features? Was it 180? Also, what was the maximum memory usage without including the quadratic term?" CreationDate="2014-11-07T04:52:37.700" UserId="847" />
  <row Id="2597" PostId="2422" Score="0" Text="I'm trying to add all possible quadratic features. That should give about 4275 features I think. &#xA;Adding just the squared features works fine. That would be 180.&#xA;btw how do we check the max memory usage ?" CreationDate="2014-11-07T05:08:12.373" UserId="4947" />
  <row Id="2598" PostId="2422" Score="0" Text="see http://ubuntuforums.org/showthread.php?t=1161120" CreationDate="2014-11-07T05:55:48.770" UserId="847" />
  <row Id="2599" PostId="2318" Score="0" Text="Look pretty cool, but also a bit expensive. Anyway I like the idea." CreationDate="2014-11-07T07:41:25.573" UserId="82" />
  <row Id="2600" PostId="2405" Score="1" Text="Link-only answers are discouraged. Please in-line why you recommend this tool." CreationDate="2014-11-07T08:23:39.860" UserId="21" />
  <row Id="2601" PostId="2403" Score="0" Text="It's not clear what you are asking - is it better to learn tools or gather domain knowledge? probably too open-ended and opinion-based for StackExchange." CreationDate="2014-11-07T08:27:38.663" UserId="21" />
  <row Id="2602" PostId="2412" Score="0" Text="Questions just looking for resources are considered off-topic for StackExchange. Maybe you can refine this into specific questions about specific tools." CreationDate="2014-11-07T08:29:28.533" UserId="21" />
  <row Id="2603" PostId="2413" Score="0" Text="Thank you @Nitesh . I will try this technique" CreationDate="2014-11-07T09:20:29.970" UserId="4887" />
  <row Id="2604" PostId="2423" Score="0" Text="Thank you. &#xA;So adding all quadratic features is not possible. Guess I'll have to try some other methods" CreationDate="2014-11-07T09:36:33.640" UserId="4947" />
  <row Id="2606" PostId="2423" Score="0" Text="Yes, that would make more sense. I think 4275 is too many features, given the number of rows you are dealing with." CreationDate="2014-11-08T00:43:08.737" UserId="847" />
  <row Id="2607" PostId="2426" Score="0" Text="Could you provide more information here? One way would be to see if the data forms a bell (normal) curve, and look for extreme outliers?" CreationDate="2014-11-08T16:08:28.010" UserId="4710" />
  <row Id="2608" PostId="2426" Score="0" Text="Could you try to explain your data format again? I don't really understand it. Also, how big is your data set." CreationDate="2014-11-08T20:13:16.053" UserId="1241" />
  <row Id="2610" PostId="2428" Score="0" Text="Thanks Nitesh, The purpose (Cheat, Yes) is retained in the multi-layer classification rules to do?" CreationDate="2014-11-08T23:03:55.437" UserId="3503" />
  <row Id="2611" PostId="2428" Score="0" Text="@XuanDung: For classification where the response is not binary (multi-class), the confidence of other classes cannot be derived from the confidence of the predicted class (unlike in the binary class version)." CreationDate="2014-11-08T23:05:54.377" UserId="847" />
  <row Id="2612" PostId="2403" Score="0" Text="While your question is valid, this is not the right place for it. Career related questions are considered off topic here." CreationDate="2014-11-05T17:15:38.640" UserId="3466" />
  <row Id="2613" PostId="2428" Score="0" Text="I want to know why need to have (Cheat, yes) in this multi-class classification rule, while the predicted classification (Cheat, No) is selected, so (Cheat, Yes) is redundant." CreationDate="2014-11-08T23:12:37.233" UserId="3503" />
  <row Id="2614" PostId="2428" Score="0" Text="Its not just the predicted class, the probability with which its predicted is also important." CreationDate="2014-11-08T23:20:06.190" UserId="847" />
  <row Id="2615" PostId="2428" Score="0" Text="I still do not understand why need to have (Cheat, yes) in this multi-class classification rule, you can more explain clean, thanks." CreationDate="2014-11-08T23:26:49.070" UserId="3503" />
  <row Id="2616" PostId="2439" Score="1" Text="From a supervised learning perspective, is this a standard problem with 3190 instances and 60 columns where each of the columns is a categorical variable? If that's the case, why not feed in the transition probabilities as additional features and throw your favorite classifiers at it. It could be a neural net, but it could might as well be a GBM? Also, how do you know that its a 2nd order Markov chain? What method are you using to estimate the order (eg. BIC, AIC, etc.)?" CreationDate="2014-11-10T18:10:40.660" UserId="847" />
  <row Id="2620" PostId="2439" Score="0" Text="@nitesh the 2nd order Markov chain is based on the observation that DNA sequences are composed of Codons composed of 3 nucleotides." CreationDate="2014-11-10T21:27:57.150" UserId="136" />
  <row Id="2621" PostId="2439" Score="0" Text="@akellyril: Thanks for clarifying!" CreationDate="2014-11-10T21:40:00.920" UserId="847" />
  <row Id="2624" PostId="2432" Score="0" Text="I think this might be a bit broad, and maybe better suited for StackOverflow." CreationDate="2014-11-11T11:29:02.527" UserId="21" />
  <row Id="2625" PostId="2445" Score="0" Text="Do you have data already aggregated by date? Is it the case when your columns are something like: date_1, #calls, #sms, #internetConnections? How many days do you have such data for (in terms of number of rows)?" CreationDate="2014-11-11T17:56:29.393" UserId="847" />
  <row Id="2626" PostId="2445" Score="0" Text="@Nitesh yes exactly. I have 12 months, millions of records." CreationDate="2014-11-11T21:36:37.423" UserId="989" />
  <row Id="2627" PostId="2449" Score="0" Text="first of all thx. In Clustering: do you mean I should cluster all the days (24 features) and see how it works? And what about trends? I mean, during winter there is a different usage than in summer" CreationDate="2014-11-12T09:40:31.317" UserId="989" />
  <row Id="2628" PostId="2436" Score="0" Text="Since you can create categorical variables out of numeric ones, maybe you should drop &quot;categorical&quot; from the title." CreationDate="2014-11-12T11:22:51.003" UserId="1241" />
  <row Id="2629" PostId="2436" Score="0" Text="Creating categorical variables out of numeric ones is always a possibility, but I'd rather find specific categorical datasets." CreationDate="2014-11-12T11:36:25.860" UserId="2576" />
  <row Id="2630" PostId="2436" Score="0" Text="Why do you want that?" CreationDate="2014-11-12T11:39:27.560" UserId="1241" />
  <row Id="2631" PostId="2452" Score="0" Text="Thanks, @user1808924!" CreationDate="2014-11-12T15:05:28.797" UserId="4999" />
  <row Id="2632" PostId="1246" Score="0" Text="Apologies for my very late response. Please, check the text that I added to the original question." CreationDate="2014-11-12T15:31:05.363" UserId="2576" />
  <row Id="2633" PostId="2436" Score="0" Text="I don't want to introduce any assumption about the structure of the data. I have the impression that creating categorical features from numerical ones would have that effect. But I may be wrong." CreationDate="2014-11-12T15:32:54.937" UserId="2576" />
  <row Id="2634" PostId="2449" Score="1" Text="Yes, for clustering, consider clustering all the 12 months of data using all the features and see how it works. Since its unsupervised, it might as well be the case that clusters are created by trends (for example a cluster with all winter data and another with all summer data and so on). Thinking about this a little more carefully and reading your question again, I think multi-class classification might be a better approach." CreationDate="2014-11-12T16:30:20.480" UserId="847" />
  <row Id="2635" PostId="2436" Score="0" Text="I don't see why it would. In a regression, a variable indicating an income of &lt; 50k is no different from one indicating an income of &gt;= 50k." CreationDate="2014-11-12T17:05:41.460" UserId="1241" />
  <row Id="2636" PostId="2436" Score="0" Text="I think you should just pick _some_ dataset with a lot of categorical variables to start gaining experience. You can just ignore numerical variables. You can keep on looking for your ideal data set at the same time." CreationDate="2014-11-12T17:08:25.713" UserId="1241" />
  <row Id="2637" PostId="2451" Score="0" Text="You'll find lots of hits if you search stats.SE for [&quot;imbalanced&quot;](http://stats.stackexchange.com/search?q=imbalanced) data." CreationDate="2014-11-12T18:13:46.290" UserId="381" />
  <row Id="2638" PostId="2451" Score="0" Text="@Emre: This question is focused more on the solution to fraud like problems rather than handling imbalance in the dataset (also the fact that we could do both classification and regression and its unclear to me how to go about obtaining the best solution). I tried searching for &quot;imbalanced&quot; and found nothing that relates to this. Could you provide a link in case I have missed something that answers this precise question?" CreationDate="2014-11-12T18:31:57.743" UserId="847" />
  <row Id="2639" PostId="1079" Score="0" Text="This is off topic in this Stack Exchange. Your post belongs in: http://opendata.stackexchange.com/" CreationDate="2014-11-12T23:32:15.470" UserId="3466" />
  <row Id="2640" PostId="2268" Score="0" Text="Opinion based posts are off topic here. This site is for discussing solutions to specific technical problems." CreationDate="2014-11-13T00:15:36.367" UserId="3466" />
  <row Id="2641" PostId="2303" Score="0" Text="Here are some guidelines about posts here. http://datascience.stackexchange.com/help/dont-ask" CreationDate="2014-11-13T00:18:16.043" UserId="3466" />
  <row Id="2642" PostId="2456" Score="0" Text="Thanks a lot for sharing your ideas. U mentioned 'my NLTK'...did you write that part of library?" CreationDate="2014-11-13T02:28:58.310" UserId="1165" />
  <row Id="2643" PostId="2445" Score="0" Text="This needs a fair bit more information. What do you think drives similarity, and what things are you looking for similarity in. Times?" CreationDate="2014-11-13T04:39:18.413" UserId="21" />
  <row Id="2647" PostId="2448" Score="0" Text="I'm not sure this is directly related to data science, but just a broad design approach question?" CreationDate="2014-11-13T04:46:21.940" UserId="21" />
  <row Id="2648" PostId="2268" Score="0" Text="state of the art is the most advances in the field and are not opinion based, its based on history of that field." CreationDate="2014-11-13T06:21:45.490" UserId="3436" />
  <row Id="2650" PostId="2445" Score="0" Text="@SeanOwen sorry, I think the right term is &quot;patterns&quot;. Similar amount of calls or similar behaviours (Friday night have a peak like in Saturday night)" CreationDate="2014-11-13T14:41:58.083" UserId="989" />
  <row Id="2651" PostId="2452" Score="0" Text="@SeanOwen - if it prints the specified output, that will be valid PMML (it's an example from the spec). That's all I'm worried about." CreationDate="2014-11-13T15:12:20.990" UserId="4999" />
  <row Id="2652" PostId="2452" Score="0" Text="Ah, row can have any content: http://www.dmg.org/v4-2-1/Taxonomy.html#xsdElement_row  Let me delete/fix comments then." CreationDate="2014-11-13T15:43:17.987" UserId="21" />
  <row Id="2653" PostId="2464" Score="1" Text="[As explained in the tutorial](http://radimrehurek.com/gensim/tut2.html), you can express documents as vectors. Cluster those vectors." CreationDate="2014-11-13T18:54:37.147" UserId="381" />
  <row Id="2654" PostId="2448" Score="0" Text="Yes - I agree it's a rather broad design approach question. If you think I should take down and put elsewhere, happy to do so." CreationDate="2014-11-14T06:22:09.493" UserId="5004" />
  <row Id="2655" PostId="1246" Score="1" Text="Can you show your implementation? I see misunderstanding, but without code sample it will be hard to explain it." CreationDate="2014-11-14T08:21:17.407" UserId="1279" />
  <row Id="2656" PostId="2464" Score="0" Text="I know mate but I have to cluster them according to the topics created after I apply LDA on my collection. Each topic should be represented as a vector in order to compare each document with each topic and find the correspondent topic or topics for each doc." CreationDate="2014-11-14T11:03:49.373" UserId="5029" />
  <row Id="2657" PostId="2468" Score="0" Text="Thanks for a good answer. I guess I am definitely in the 100s of millions ( I have updated my question)." CreationDate="2014-11-14T15:59:55.560" UserId="1256" />
  <row Id="2659" PostId="2474" Score="1" Text="This format is called &quot;JSON&quot;. Curly braces form objects, while square ones represent arrays. Most programming languages have libraries for parsing JSON (e.g. for R see [this](http://stackoverflow.com/questions/2061897/parse-json-with-r) question), so parsing it becomes trivial task." CreationDate="2014-11-14T23:46:31.913" UserId="1279" />
  <row Id="2660" PostId="2474" Score="2" Text="I think this would be more appropriate at StackOverflow. It's not related directly to data science, but just parsing a common data format." CreationDate="2014-11-15T14:20:32.547" UserId="21" />
  <row Id="2661" PostId="2470" Score="0" Text="SVMs support binary classification by nature. Your classes can't be vectors. You're converting vectors to a scalar norm of some vector difference. These still aren't binary classes. I don't think SVM is the tool you want; maybe you can clarify the problem?" CreationDate="2014-11-15T14:22:14.240" UserId="21" />
  <row Id="2662" PostId="2463" Score="1" Text="Can you be more specific about your requirements? are you asking for an algorithm? software tool, library? how big is the data?" CreationDate="2014-11-15T14:22:53.063" UserId="21" />
  <row Id="2663" PostId="2473" Score="0" Text="SVMs do not operate on categorical features. Are you sure that's what you mean?" CreationDate="2014-11-15T14:24:33.907" UserId="21" />
  <row Id="2664" PostId="2469" Score="0" Text="#1 is unclear, and #4 is too open ended. Can you clarify what you mean in #2-3, and what you have tried so far?" CreationDate="2014-11-15T14:25:41.183" UserId="21" />
  <row Id="2666" PostId="1079" Score="0" Text="This question appears to be off-topic because it is seeking a data set, and I believe that is more specifically on-topic at opendata.stackexchange.com" CreationDate="2014-11-15T14:28:46.633" UserId="21" />
  <row Id="2667" PostId="2470" Score="0" Text="Each element in classes A and B are vectors of a given dimension d. In other words you have d features to consider to classify a vector as A or B." CreationDate="2014-11-15T21:40:10.550" UserId="5042" />
  <row Id="2668" PostId="2470" Score="0" Text="A class is like a label that can apply to a feature vector. It makes sense to say a vector can have label A but then what does the notation |A-C| mean? the norm of the difference of vectors that are in class A / C, is a scalar, and you say the scalar belongs to class A'?" CreationDate="2014-11-15T22:00:06.307" UserId="21" />
  <row Id="2669" PostId="2470" Score="0" Text="What I meant by |A-C| is actually the Mahalanobis transformation where I calculate the covariance and centroid of C and then I use those two parameters to transform my classes A and B into A' and B' which are the respective multivariate distances. Effectively now every element of A' and B' are scalars." CreationDate="2014-11-16T00:46:11.980" UserId="5042" />
  <row Id="2670" PostId="2470" Score="0" Text="Basically I want to know if this is a valid transformation in the sense that I am not informing the algorithm (double dipping?) because I know that B is closer to C than A." CreationDate="2014-11-16T00:48:16.663" UserId="5042" />
  <row Id="2671" PostId="1128" Score="1" Text="You can't be successful if you don't have a goal. What kind of insights do you want to get? You have to define that first. Otherwise: Just use element-wise addition and multiplication. You're done now! Or aren't you? If you understand why this doesn't do it - maybe you can make progress." CreationDate="2014-11-16T16:06:42.517" UserId="723" />
  <row Id="2673" PostId="1128" Score="0" Text="@Gerenuk : This would be a reasonable response if I wanted to test a specific hypothesis about specific time series.  I'm interested in the meta-problem... i.e. are there *any* systems that go beyond element-wise addition and multiplication?  If so, what are they, how do they go further, and what sort of (interesting) transformations do they facilitate?" CreationDate="2014-11-16T19:38:34.597" UserId="3328" />
  <row Id="2674" PostId="1128" Score="1" Text="@aSteve: I could make up many non-trivial algebras, too. It would be a very tedious process asking you every time why this particular algebra does not suit your needs. It's really much more fruitful if there is an notion of what you want to achieve." CreationDate="2014-11-16T20:55:24.630" UserId="723" />
  <row Id="2675" PostId="1128" Score="0" Text="@Gerenuk:  I'm trying to establish a taxonomy of algebras with existing (practical or academic) applications.  Every significantly distinct algebra is of interest - as is the context in which it was deemed valuable.  Ultimately, I would like to establish if an (eloquent) universal time-series algebra can be defined, and - if not - why not.  What is the minimum adequate sets of operators and constants?" CreationDate="2014-11-16T23:13:09.507" UserId="3328" />
  <row Id="2676" PostId="2463" Score="0" Text="Thanks for the response, i am looking for Algorithm and library but most importantly algorithm, the data is close be hundred of thousands of records." CreationDate="2014-11-17T09:57:28.420" UserId="5027" />
  <row Id="2677" PostId="2462" Score="0" Text="One of the problems with this is that you don't know which nucleotide starts the codon. So you'd need to create 58 categorical features. Worth a try though." CreationDate="2014-11-17T10:47:45.140" UserId="136" />
  <row Id="2679" PostId="2487" Score="0" Text="What structure is your source data? If it is just the string &quot;Camera&quot;, what kind of associations are you hoping to make - properties that a physical camera might have with variation, properties related to selling cameras? The latter is what your list looks like, but if you are starting with just a string word, you might just as well get into more open-ended word association (e.g. &quot;Camera&quot; is a word derived from a Latin root, it has X million Google hits, it is of interest to photographers etc etc)" CreationDate="2014-11-17T14:07:16.237" UserId="836" />
  <row Id="2680" PostId="2487" Score="0" Text="Thanks for your reply Neil. It would be just a string for &quot;camera&quot; and I would be looking for any words that would be associated to it, whether that be physical properties or non physical such as &quot;light&quot;." CreationDate="2014-11-17T15:20:58.313" UserId="5066" />
  <row Id="2681" PostId="2462" Score="0" Text="That is a good point. However, since 30 are in E and 30 are in I (or the reverse), we can assume that the first neucleotide starts the codon? I might be totally wrong here so please correct my hypothesis/ reasoning." CreationDate="2014-11-17T16:19:40.490" UserId="847" />
  <row Id="2682" PostId="2451" Score="1" Text="heckman sample selection correction.  The regression only applies to records where fraud is observed, controlling for the probability of fraud occuring. Same approach is (famously) taken to estimating labor supply equations when wages are only observed for workers. http://en.wikipedia.org/wiki/Heckman_correction" CreationDate="2014-11-17T16:36:26.473" UserId="5067" />
  <row Id="2683" PostId="2477" Score="0" Text="So you are suggesting we should treat it as a regression problem (never consider it as a classification one). My hunch was that there must be some way to combine regression and classification together to solve such problems better." CreationDate="2014-11-17T17:32:10.240" UserId="847" />
  <row Id="2684" PostId="2477" Score="0" Text="What is your class variable? Please answer that question." CreationDate="2014-11-17T19:46:54.460" UserId="3083" />
  <row Id="2685" PostId="2477" Score="0" Text="The response denotes the transaction value. That is converted to represent 0 if there is no fraud, and to the transaction value, if there is fraud found." CreationDate="2014-11-17T19:48:58.573" UserId="847" />
  <row Id="2686" PostId="2477" Score="0" Text="Seems like you'll have many classes then? Try regression, it's a suggestion.If you predict greater than a threshold, fraud, else, not fraud. Very simple and see if that works.you'll need to define the threshold." CreationDate="2014-11-17T19:53:38.543" UserId="3083" />
  <row Id="2687" PostId="2477" Score="0" Text="Or you may try logistic regression, and for those detected as fraud, model prediction using ols." CreationDate="2014-11-17T19:56:06.640" UserId="3083" />
  <row Id="2688" PostId="2469" Score="0" Text="I'm new for statistics and it was interesting for me, what kind of question i can ask, when i have such data." CreationDate="2014-11-17T20:36:04.127" UserId="3377" />
  <row Id="2689" PostId="2451" Score="0" Text="@justincress: Thanks for the comment. If you could go into more detail about how I could pose it as a supervised learning problem, it would be great." CreationDate="2014-11-17T21:02:45.883" UserId="847" />
  <row Id="2690" PostId="2487" Score="0" Text="This is mostly a request to find a database of particular data. I think it's not quite the right type of question for this StackExchange." CreationDate="2014-11-18T00:23:22.677" UserId="21" />
  <row Id="2693" PostId="2489" Score="0" Text="Could you paste the head of the cleaned data in the question itself? Also, what do you mean by reducing this to a single variable? If you can, please provide some more context. Examples might help too." CreationDate="2014-11-18T10:03:11.987" UserId="847" />
  <row Id="2694" PostId="2489" Score="0" Text="Say I have put up a question to get replies for 5 disabilities, a respondent has 3 disabilities and there are multiple cases like this. How do I club all of them in a single categorical variable representing these disabilities?" CreationDate="2014-11-18T10:07:12.170" UserId="5075" />
  <row Id="2695" PostId="2492" Score="1" Text="The issue becomes bigger when I have 11 such variables" CreationDate="2014-11-18T10:21:23.107" UserId="5075" />
  <row Id="2696" PostId="2492" Score="0" Text="Agreed. It becomes more and more complicated as the number of choices/ options increase. But you choose to restrict the number of categories to, for example, 5. This can be done by using the top 4 choices/ options as they are and treating everything else as &quot;other&quot;." CreationDate="2014-11-18T10:23:00.820" UserId="847" />
  <row Id="2697" PostId="2492" Score="0" Text="Any other way out?" CreationDate="2014-11-18T10:23:28.600" UserId="5075" />
  <row Id="2699" PostId="2492" Score="0" Text="I will probably edit my answer with the above comment." CreationDate="2014-11-18T10:25:54.510" UserId="847" />
  <row Id="2700" PostId="2492" Score="0" Text="How do I figure out the top 4 choices?" CreationDate="2014-11-18T10:31:22.530" UserId="5075" />
  <row Id="2701" PostId="2492" Score="0" Text="Collect data first and then use the collected data to figure out the top 4 choices by count." CreationDate="2014-11-18T10:35:53.553" UserId="847" />
  <row Id="2703" PostId="2495" Score="1" Text="This is very short on details, like your data size, how you're running it now, what parameters, etc." CreationDate="2014-11-18T16:47:36.130" UserId="21" />
  <row Id="2704" PostId="2493" Score="0" Text="This is quite open-ended. I would state more about your data, scale, what skills you have, what you have tried and what problems you're facing." CreationDate="2014-11-18T16:48:16.730" UserId="21" />
  <row Id="2706" PostId="2495" Score="0" Text="Did you code the SVM yourself or are you using a function from a package?" CreationDate="2014-11-18T18:20:57.340" UserId="1156" />
  <row Id="2707" PostId="2495" Score="0" Text="There seem to be no parallel implementations of SVM in R. The testing as one can guess, can be parallelized. See http://vikparuchuri.com/blog/parallel-r-model-prediction-building/." CreationDate="2014-11-18T20:13:20.350" UserId="847" />
  <row Id="2708" PostId="2496" Score="0" Text="This was for a homework assignment related to SVM, I didn't need to create a Visualization, I just was curious if it had ever been dome with anything outside 3 dimensions.  I am not entirely sure I know what you are referring to when you say multiple 3-D images, just so I am clear, I take it to mean creating several models with only 3 explanatory variables in them, correct?" CreationDate="2014-11-18T20:17:52.327" UserId="5023" />
  <row Id="2709" PostId="2496" Score="0" Text="Correct. I sometimes also show images with individual variables on two axis, and a weighted average of the remaining variables (in your case, 4 of them) on the third axis. This can show dense clusters of data that were not obvious by examination of separate 3-D pictures." CreationDate="2014-11-19T00:23:25.227" UserId="5083" />
  <row Id="2710" PostId="2499" Score="0" Text="Not looked very closely, but have you looked at Trulia's API?  The limit here seems to be 5000 a day. There is a chance that if you email them, you might be able to increase your limit. http://developer.trulia.com/" CreationDate="2014-11-19T06:37:26.630" UserId="847" />
  <row Id="2712" PostId="1246" Score="0" Text="I'm afraid I can not share my specific implementation (I'm under a non-disclosure agreement). However, I can post a pseudo-code summary of my on-line implementation, if that is ok." CreationDate="2014-11-19T09:25:43.130" UserId="2576" />
  <row Id="2713" PostId="2500" Score="1" Text="Do you have some other information about which user liked,  rated or bought which product?" CreationDate="2014-11-19T10:09:27.473" UserId="847" />
  <row Id="2714" PostId="2500" Score="1" Text="no only user and product data" CreationDate="2014-11-19T10:27:30.230" UserId="5091" />
  <row Id="2715" PostId="1246" Score="0" Text="pseudocode should be enough, thanks." CreationDate="2014-11-19T10:39:27.193" UserId="1279" />
  <row Id="2716" PostId="2493" Score="0" Text="Thanks. I'll edit the question and add more info" CreationDate="2014-11-19T11:31:08.043" UserId="5077" />
  <row Id="2717" PostId="2496" Score="0" Text="thanks for the tip, i will have to try that out." CreationDate="2014-11-19T13:15:11.243" UserId="5023" />
  <row Id="2718" PostId="2495" Score="0" Text="Data size is 10MB and I am running it on Revolution R 7.2.0" CreationDate="2014-11-19T16:11:27.683" UserId="3551" />
  <row Id="2719" PostId="2501" Score="1" Text="Can you tell us where you got the data set from?  I would suspect that the data-prep/normalization methods will depend on the source of the data." CreationDate="2014-11-19T18:57:01.573" UserId="375" />
  <row Id="2721" PostId="2500" Score="2" Text="Clustering the products is easy enough (look up k-means or Gaussian mixture models), but recommending them will be difficult unless you can propose a model which relates them to the users. Since you have no training data for a supervised model you need to get it from somewhere; e.g., market research. Otherwise you can make up some heuristics yourself and use it to build a Bayesian prior, but I suspect this more complexity than you are comfortable with.&#xA;&#xA;Worry about mahout later; you need an algorithm, a model first." CreationDate="2014-11-19T19:32:03.240" UserId="381" />
  <row Id="2722" PostId="2500" Score="1" Text="Without data associating the products and the customers, there is no way to build a list of recommended products on a per-user basis. It could be possible to group the items based on meta-data about the products - for example, each item would have a list of recommended items. This would be generated by products sharing tags, a price range, a brand name, or some other meta-data you have available. Do you have any additional meta-data?" CreationDate="2014-11-19T20:33:00.233" UserId="3466" />
  <row Id="2724" PostId="2479" Score="0" Text="I couldn't understand the output I got after implementing BBN in R. I changed the no.of nodes a lot to get more than 1 arc. Result :- Nodes: 8 arcs: 1 &#xA;    undirected arcs: 1 &#xA;    directed arcs: 0 &#xA;  average markov blanket size: 0.25 &#xA;  average neighbourhood size: 0.25 &#xA;  average branching factor: 0.00 &#xA;  learning algorithm: Grow-Shrink &#xA;  conditional independence test: Mutual Information (disc.) &#xA;  alpha threshold: 0.05 &#xA;  tests used in the learning procedure:65 &#xA;  optimized: TRUE. Moreover, the plot just gave me a circle of all nodes. What does it mean?" CreationDate="2014-11-19T21:28:29.957" UserId="5043" />
  <row Id="2725" PostId="2479" Score="0" Text="I looked for tutorials related to this and all of them uses a simple set of numerical columns to get the two types of BBN. How to find the effect of combination of several categorical variables over one target categorical variable using BBN?" CreationDate="2014-11-19T21:42:34.043" UserId="5043" />
  <row Id="2726" PostId="2499" Score="0" Text="@nfmcclure, read my post again :)" CreationDate="2014-11-19T22:34:36.093" UserId="5086" />
  <row Id="2730" PostId="2479" Score="0" Text="I don't use R for this BBN learning. I use BayesiaLab and Hugin. Maybe this is a separate question on howto use the R BBN package that you're using. Some code and references you're using would be nice on that new thread." CreationDate="2014-11-20T00:19:58.583" UserId="3083" />
  <row Id="2731" PostId="2501" Score="0" Text="Can you also tell us some other basic characteristics of the dataset like the # of users, # of items, total # of ratings in your dataset? Why are you taking the log of the ratings instead of the raw ratings?" CreationDate="2014-11-20T00:32:17.953" UserId="847" />
  <row Id="2732" PostId="2500" Score="0" Text="Thanks for you reply @Emre,@ sheldonkreger. So how similar products can be found out? as product attributes i have name,category,cost and ingredients (some product may have 1 ingredients where as some other have 10..) I am new to recommendation system and I would like to know which are all the best algorithms and tools available to implement this problem.." CreationDate="2014-11-20T04:17:44.077" UserId="5091" />
  <row Id="2733" PostId="2500" Score="0" Text="Since you're new I suggest [reading a book](http://www.springer.com/computer/ai/book/978-0-387-85819-7) and/or watching [these](https://www.youtube.com/watch?v=bLhq63ygoU8) [videos](http://www.youtube.com/watch?v=mRToFXlNBpQ) by the director of Research &amp; Engineering @Netflix." CreationDate="2014-11-20T06:02:13.817" UserId="381" />
  <row Id="2734" PostId="2501" Score="0" Text="Thank you, I add some additional information to the main post" CreationDate="2014-11-20T07:43:41.993" UserId="3281" />
  <row Id="2735" PostId="2504" Score="2" Text="I think unless you have some good intuition about the data properties, you end up exploring both options (perhaps on just 500,000 rows) and cross-validate. But perhaps there are visualisations or other analyses that can help you get that intuition." CreationDate="2014-11-20T08:43:52.910" UserId="836" />
  <row Id="2736" PostId="2490" Score="0" Text="Thanks, this was really helpful" CreationDate="2014-11-20T12:33:13.473" UserId="5027" />
  <row Id="2737" PostId="2504" Score="1" Text="I've actually been planning to do a big model comparison for my own research on real data this week. I'll clean up the results a bit and post them here. Also at least one CS student has studied the question: https://www.academia.edu/3526056/Algorithms_Comparison_Deep_Learning_Neural_Network_AdaBoost_Random_Forest" CreationDate="2014-11-20T14:53:25.473" UserId="1156" />
  <row Id="2738" PostId="2504" Score="1" Text="@NeilSlater Id like to see an answer addressing what that intuition might/could/should be" CreationDate="2014-11-20T14:54:14.623" UserId="1156" />
  <row Id="2739" PostId="2508" Score="0" Text="Several solutions are posted here: http://stackoverflow.com/q/3301694/2954547" CreationDate="2014-11-20T15:21:45.087" UserId="1156" />
  <row Id="2742" PostId="2505" Score="0" Text="I will have to try this out, thanks for the tip! I would vote both you and Oleg up but I don't have enough reputation points." CreationDate="2014-11-20T21:03:52.067" UserId="5023" />
  <row Id="2743" PostId="2495" Score="0" Text="See a similar question on stats.se: http://stats.stackexchange.com/questions/825/any-suggestions-for-making-r-code-use-multiple-processors" CreationDate="2014-11-20T23:32:10.670" UserId="847" />
  <row Id="2744" PostId="2507" Score="0" Text="Can you provide a representative selection of examples that demonstrate the variety of items the algorithm will need to parse?" CreationDate="2014-11-21T04:20:15.347" UserId="381" />
  <row Id="2745" PostId="1133" Score="0" Text="ALGLIB looks really interesting! I'm a Data Scientist working in a .NET shop, and am looking to learn C#. Could you tell me a little more about the library?" CreationDate="2014-11-21T09:38:54.500" UserId="1127" />
  <row Id="2746" PostId="1246" Score="0" Text="I just added the pseudo-code as an edit. Please, do not hesitate to ask for further detail if needed." CreationDate="2014-11-21T10:03:49.387" UserId="2576" />
  <row Id="2747" PostId="2507" Score="1" Text="There are a hundred ways to do this. Give some sense of what tools or language you need to do this in. Is there a data science aspect to this? seems like just log parsing." CreationDate="2014-11-21T10:32:26.920" UserId="21" />
  <row Id="2748" PostId="2510" Score="1" Text="Career questions are generally considered off-topic, but maybe you can modify this to ask about specific problems and areas you are interested in." CreationDate="2014-11-21T10:37:08.433" UserId="21" />
  <row Id="2749" PostId="2506" Score="0" Text="Neo4j itself does not provide visualization. I don't think this addresses the question, but maybe you can edit it to expand on how you would use this to make a visualization." CreationDate="2014-11-21T10:39:03.147" UserId="21" />
  <row Id="2750" PostId="2512" Score="0" Text="This is fairly terse. Maybe you can expand on why you think this is a good choice?" CreationDate="2014-11-21T10:40:17.307" UserId="21" />
  <row Id="2751" PostId="2512" Score="2" Text="ok I have made edits, hope this helps." CreationDate="2014-11-21T10:51:53.487" UserId="5110" />
  <row Id="2752" PostId="1246" Score="0" Text="In your code, what do you mean by &quot;sample&quot;? Single observation?" CreationDate="2014-11-21T10:56:47.013" UserId="1279" />
  <row Id="2753" PostId="1246" Score="0" Text="Yes. The training set consists of a set of training samples." CreationDate="2014-11-21T11:00:30.950" UserId="2576" />
  <row Id="2754" PostId="1133" Score="0" Text="Hi, in couple words... Old, solid, rich enough, made by one author. For many languages. I have used it several times for Dot Net." CreationDate="2014-11-21T12:36:56.283" UserId="97" />
  <row Id="2755" PostId="2516" Score="1" Text="+1 Good question and well explained. I don't think there will be a straightforward answer to this. Here are a few follow up questions. What is the cardinality of each of the columns? Also, what is the target column here? Are you interested in predicting Var1?" CreationDate="2014-11-21T18:38:30.060" UserId="847" />
  <row Id="2756" PostId="2496" Score="0" Text="I'm downvoting this answer because it's far too specific, to the point where I think it's incomplete. Rather than this _particular_ 6D visualization (which, I'm sorry to say, is not new at all), you can just apply the general principle of using color/shape/etc. to represent higher dimensions. It's also important to realize that you can in fact have _too many_ dimensions on one plot, and that mapping features to non-spatial scales needs case-by-case judgement." CreationDate="2014-11-21T18:58:37.920" UserId="1156" />
  <row Id="2757" PostId="2518" Score="0" Text="Mahout does item based similarity. However, item based similarity in Mahout is calculated through users ratings of these items. In the dataset as described in the question, an item based similarity is not possible (at least in the framework used in Mahout's item-based similarity)." CreationDate="2014-11-21T22:51:33.710" UserId="847" />
  <row Id="2758" PostId="2515" Score="1" Text="I think this is the way to go. Mini-batches with a well-chosen size can actually converge faster than either batch or online version (former only updates weights once per whole set, and latter cannot be vectorised, plus has additional weight update steps more often)" CreationDate="2014-11-22T08:39:12.487" UserId="836" />
  <row Id="2759" PostId="2510" Score="1" Text="@Chuck D. When you say _with an eye toward entering the field of data science_, do you mean working in an industry position?" CreationDate="2014-11-22T19:28:34.770" UserId="4621" />
  <row Id="2760" PostId="2516" Score="1" Text="I could see using a well-indexed MySQL database to do this." CreationDate="2014-11-22T19:30:43.100" UserId="4710" />
  <row Id="2761" PostId="2510" Score="0" Text="@RobertSmith: Yes, I'm definitely interested in an industry position." CreationDate="2014-11-22T22:51:09.397" UserDisplayName="user5007" />
  <row Id="2762" PostId="2510" Score="1" Text="Then I have a quick recommendation. If you don't have previous experience in statistics, it would be good to pursue a master's degree (assuming you won't be left with a huge debt.) to get the basics. A PhD simply takes too long and it would be more valuable for you to have those years as work experience. Keep in mind that neither a MSc nor a PhD in a traditional department will teach you most of what you need in practice. That's why the people answering your question are suggesting departments geared toward data science. You need to learn quite a bit on your own." CreationDate="2014-11-23T01:11:44.383" UserId="4621" />
  <row Id="2763" PostId="2521" Score="0" Text="This doesn't answer your question but the first four lines of Athlete.csv list the fields for that CSV file. So it seems they only pull specific fields for athletes, and the extra fields are ignored. This isn't mentioned explicitly on the page you note, but they do say &quot;we provide *some* of the core DBpedia&quot; (emphasis added). I assumed they meant specific records, but apparently it also means only specific fields." CreationDate="2014-11-23T01:23:39.870" UserId="4710" />
  <row Id="2764" PostId="2526" Score="0" Text="Yeah, I had put that question on that thread - But I did not receive this mail??" CreationDate="2014-11-23T06:07:08.073" UserId="5126" />
  <row Id="2765" PostId="2495" Score="0" Text="If you really have to stay in R: http://amplab-extras.github.io/SparkR-pkg/" CreationDate="2014-11-23T17:52:09.700" UserId="381" />
  <row Id="2766" PostId="2503" Score="1" Text="You'll find lots of leads if you look up &quot;encoding categorical variables&quot;; e.g. http://stats.stackexchange.com/questions/21770/" CreationDate="2014-11-23T18:27:27.070" UserId="381" />
  <row Id="2767" PostId="2510" Score="0" Text="@SeanOwen, I've been thinking about how to reword this but I'm at a loss as to how to bring this back on-topic. I'm not to the point where I'm ready to ask about specific schools - I'm just looking for a broader perspective as to whether there is a general consensus concerning education credentials in the field. Do you have any further suggestions that could help me out? Thanks." CreationDate="2014-11-23T22:35:19.777" UserDisplayName="user5007" />
  <row Id="2768" PostId="2515" Score="0" Text="Thank you both. Apologies for stubbornly rejecting mini batches before, but I was unsure of the implications of this method on the convergence rate. Neil, is your affirmation coming from your own experience, or are there any theoretical/empirical published results?" CreationDate="2014-11-24T09:56:03.613" UserId="2576" />
  <row Id="2769" PostId="2518" Score="0" Text="I believe that in a proper case, the data should contain username, product name and rating to generate the recommendation system. Consider an overview of this scenario, you just need a couple of categorical &amp; numerical variables. Can't we use category,product name and cost to replicate this case in here? It may not be a good way to do it but,do you think it is not possible to use item-based similarity here?" CreationDate="2014-11-24T14:44:18.647" UserId="5043" />
  <row Id="2773" PostId="2518" Score="0" Text="Of course its possible to use the item categories/ features to create similarity. However, that representation will be sparse and might not yield meaningful similarities. Notice that when we have ratings, then for each item's vector, a component denotes a particular user's ratings. This inherently ensures that each user's rating is weighted equally when calculating similarity. In any case, one would have to do extensive preprocessing in order to get this dataset to be ready to be fed into Mahout." CreationDate="2014-11-24T17:58:38.407" UserId="847" />
  <row Id="2776" PostId="2530" Score="0" Text="In this case, you are suggesting the data to be fed something along the following lines, then? category_i, item_id_j, value_{i,j}. Here category_i denotes the i-th category and item_id_j denotes the j-th item and value_{i,j} denotes the value for the item category combination. For example, color can be one category and its value can be red for a given item. However, to feed it in, one would have to transform the dataset to read something like: color_red, item_i, 1 as one row and may be color_blue, item_i, 0 as another row. This way, we run into values that are both unary and numeric." CreationDate="2014-11-24T19:41:07.037" UserId="847" />
  <row Id="2777" PostId="2523" Score="1" Text="By KS, do you mean the Kolmogorov-Smirnov statistic? AUROC is probably the area under the ROC curve?" CreationDate="2014-11-24T19:49:59.380" UserId="847" />
  <row Id="2778" PostId="2525" Score="0" Text="Please provide more context here. Consider providing an example or the task that you are interested in. Why is it that you think there is a connection in the first place? May be cite a source?" CreationDate="2014-11-25T03:53:05.710" UserId="847" />
  <row Id="2779" PostId="2525" Score="0" Text="The task is to find clusters (non parametric) for given set of data points. So I am interested to find mean components in the case of GMM. The claim is these mean components lie in the span of eigen vectors of second order moment matrix . Can you give me some intuition behind this claim." CreationDate="2014-11-25T05:11:41.057" UserId="4686" />
  <row Id="2782" PostId="2451" Score="0" Text="I don't really know how to recast econometrics into data science speak. It's just a two stage regression model where the first step is a probit and the second is OLS.  To the extent that the models are specified directly (instead of learned from data) doesn't that make the algorithm &quot;supervised&quot; ? Not my expertise, sorry, but I think the heckman correction is exactly what you're looking for." CreationDate="2014-11-25T14:23:42.120" UserId="5067" />
  <row Id="2783" PostId="851" Score="1" Text="@ffriend: Please post your comment as an answer so it can be accepted." CreationDate="2014-11-25T17:29:25.573" UserId="847" />
  <row Id="2784" PostId="2538" Score="0" Text="More information would be very helpful. What are you forecasting? what is the error distribution?" CreationDate="2014-11-25T23:18:12.757" UserId="21" />
  <row Id="2785" PostId="2518" Score="0" Text="Thanks for your comments Nitesh Srinath. But it seams ( I am new to mahout and still experimenting) mahout is good at collaborative filtering. And what i need is something related to contend based filtering. I do not have any user ratings/preference value available. So is there any way to implement content based filtering in mahout or is there any other tools/libraries available.." CreationDate="2014-11-26T04:21:52.047" UserId="5091" />
  <row Id="2786" PostId="2506" Score="0" Text="https://gephi.github.io/  It support connecting to neo4j" CreationDate="2014-11-26T04:53:29.717" UserId="5091" />
  <row Id="2787" PostId="2538" Score="0" Text="@SeanOwen I am forecasting wind-speed at a particular location, The error series is obeying normal distribution.  please intimate me if you need any other information." CreationDate="2014-11-26T04:53:40.640" UserId="5099" />
  <row Id="2789" PostId="2540" Score="0" Text="Thank you very much for your full response. I check them." CreationDate="2014-11-26T06:49:38.047" UserId="3436" />
  <row Id="2790" PostId="2523" Score="0" Text="Seems like starting from Wikipedia and going through the original references would be a good place to start." CreationDate="2014-11-26T10:08:51.270" UserId="587" />
  <row Id="2792" PostId="2506" Score="0" Text="@SeanOwen is there a Javascript graph lib that works well with Neo4J that you will recommend. by the way, i have started learning Neo4j" CreationDate="2014-11-27T08:33:33.127" UserId="5027" />
  <row Id="2794" PostId="2506" Score="0" Text="check this http://neo4j.com/developer/javascript/" CreationDate="2014-11-27T08:55:50.247" UserId="5091" />
  <row Id="2798" PostId="2543" Score="1" Text="This is too broad as a question here, as you're asking what your question is. Please refine it with detail about what you are trying to accomplish." CreationDate="2014-11-27T20:09:31.973" UserId="21" />
  <row Id="2799" PostId="2553" Score="0" Text="+1. A reference: Good, in his *Resampling Methods* (3rd ed., 2006, p. 19) notes that the bootstrap may be unstable for sample sizes n&lt;100. Unfortunately, I don't have the book at hand, so I can't look up his argumentation or any references." CreationDate="2014-11-27T21:29:16.843" UserId="2853" />
  <row Id="2800" PostId="2553" Score="0" Text="@goangit, thank you for your reply. I don't have the software you used and don't quite follow why 0.05 and 0.95 is used in your first solution. Nonetheless, your answer inspired me to program a bootstrap which I will provide details of as &quot;an answer&quot;. Comments on the validity of what I've done from anyone would be appreciated. Thanks" CreationDate="2014-11-27T21:41:36.307" UserId="5180" />
  <row Id="2802" PostId="2553" Score="0" Text="@Steve, the software used here is R, it's freely [available](http://www.r-project.org/), in both senses of the word. The cutoffs used in the two tail case provide the central 90% confidence interval." CreationDate="2014-11-27T23:46:10.697" UserId="5153" />
  <row Id="2804" PostId="2362" Score="0" Text="NN initialization is also an open issue." CreationDate="2014-11-28T06:07:40.173" UserId="5198" />
  <row Id="2805" PostId="806" Score="0" Text="&quot;The implicit goal of AUC is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.&quot; I thought that these situations were where AUC performed poorly and precision-recall graphs/area under them were used." CreationDate="2014-11-26T20:11:42.207" UserId="1241" />
  <row Id="2806" PostId="2553" Score="0" Text="@goangit, thanks. I've heard of R but can't justify the time to learn a new sw package, Mathematica is what I use, but this is not a Mathematica question. Could you explain what the (5%,-0.29) and (95%, 2.95) actually mean ? Is this saying that we are 90% confident that the ratio of the means will fall in the interval -0.29 to 2.95 ?" CreationDate="2014-11-28T13:06:23.920" UserId="5180" />
  <row Id="2807" PostId="2566" Score="0" Text="Thank You very much for valuable suggestion" CreationDate="2014-11-28T16:19:00.843" UserId="5099" />
  <row Id="2808" PostId="2567" Score="0" Text="If you are allowed to change your output to binary (depending on which party won), [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logistic_regression) is a good fit. It still considers independent output which may not be what you want." CreationDate="2014-11-29T17:20:27.880" UserId="4621" />
  <row Id="2810" PostId="2578" Score="0" Text="This question is potentially opinion-based, but I think it's specific enough to be valid. I'd like to see if a moderator has any thoughts on this. This exchange is still young, so we are still setting the standards." CreationDate="2014-12-02T19:48:17.897" UserId="3466" />
  <row Id="2811" PostId="2576" Score="0" Text="For Web entities there's WHOIS." CreationDate="2014-12-03T05:17:35.610" UserId="381" />
  <row Id="2812" PostId="712" Score="1" Text="@indico for most practical problems kernel SVM training complexity is closer to quadratic. Platt's cubic SMO has been out of use for quite some time. That's still too high for truly large data sets, but it's not as bad as you portray. Linear SVM is highly efficient, with sublinear complexity." CreationDate="2014-12-03T10:12:08.473" UserId="119" />
  <row Id="2813" PostId="2574" Score="1" Text="These don't relate to R or MapReduce, which seem to be requirements here." CreationDate="2014-12-03T13:51:27.717" UserId="21" />
  <row Id="2814" PostId="2584" Score="0" Text="You might instead reproduce your answer here, although ideally it wouldn't have been cross-posted as a question initially." CreationDate="2014-12-03T13:52:22.533" UserId="21" />
  <row Id="2815" PostId="2568" Score="1" Text="What debugging have you done? This sounds like a pure code review question, and there are other SE sites for that." CreationDate="2014-12-03T14:31:55.107" UserId="21" />
  <row Id="2816" PostId="2574" Score="0" Text="On spark You can write in python. Use RPy library for integrating R an Python." CreationDate="2014-12-03T14:40:02.177" UserId="5224" />
  <row Id="2817" PostId="2585" Score="0" Text="Hi Robert, thanks for the reply! Good point about the representation of the training/test set but this is time series data so as new data comes in I cannot guarantee that values will be similar in range to what they were before. I have the same exact feeling as you that I have no good reason to think it won't affect a SVM in any circumstance." CreationDate="2014-12-03T14:59:36.297" UserId="802" />
  <row Id="2818" PostId="2593" Score="0" Text="One solution which goes in the direction I am looking for is [fluentd](http://fluentd.org)." CreationDate="2014-12-03T16:36:50.713" UserId="5266" />
  <row Id="2819" PostId="2585" Score="0" Text="Have you compared predictions using MinMaxScaler and standardization?" CreationDate="2014-12-03T17:40:51.910" UserId="4621" />
  <row Id="2820" PostId="2585" Score="0" Text="I have and the results are similar, but that doesn't really tell me if things are getting screwy with the MinMaxScalar." CreationDate="2014-12-03T18:12:11.077" UserId="802" />
  <row Id="2821" PostId="2585" Score="0" Text="Sure. In any case, it would be better to use standardization if you're not getting something valuable from `MinMaxScaler`." CreationDate="2014-12-03T19:47:44.527" UserId="4621" />
  <row Id="2822" PostId="2595" Score="0" Text="great insight, thanks!" CreationDate="2014-12-03T20:28:58.367" UserId="802" />
  <row Id="2823" PostId="2584" Score="0" Text="Thank you. I guess I don't have much of a choice here. It's not just about cameras, basically parent category is, lets say, electronics with all sub-cats: cameras, smartphones, laptops etc. But so far there is no better solution than to build manually. Online catalogs would help me, but like I said, most of them contain more irrelevant info than relevant-at least in my case." CreationDate="2014-12-04T08:33:09.717" UserId="5241" />
  <row Id="2824" PostId="2584" Score="0" Text="Electronics in general is a huge category with long tail distribution. A lot would depend on your specific needs. For example, taking the &quot;assorted&quot; category in: if you do that, you will be swamped with tons of Chinese small name brands, and your best bet would be reliable feature recognition (memory size, color, domain keywords, etc.). If your target metric is purely numerical, &quot;Panasonics&quot; would suddenly constitute a shrinking minority. But if it's &quot;major electronics&quot;, try compiling a brand dictionary from web sites first." CreationDate="2014-12-04T10:02:06.847" UserId="5249" />
  <row Id="2825" PostId="2584" Score="0" Text="Btw, I would be interested to see the results of your experiments - if you intend making them public." CreationDate="2014-12-04T10:08:15.097" UserId="5249" />
  <row Id="2826" PostId="2597" Score="0" Text="This presupposes you know the total number of contestants and that number isn't infinite? And so when a new guy shows up to the race track you have to re-train the entire model?" CreationDate="2014-12-04T14:53:32.377" UserId="5247" />
  <row Id="2827" PostId="2600" Score="0" Text="That's fairly open ended. Can you say more about your requirements and what you have tried?" CreationDate="2014-12-04T15:21:23.807" UserId="21" />
  <row Id="2828" PostId="2600" Score="0" Text="With many LP (or MILP) solvers, you can easily add the convex and the constraints. For example, in scipy.optimize.anneal I could try f(x) = sum(x) as simply summing the input, e.g. in the case of a Graph's nodes. But two things are not clear: How do we pass in graphs to begin with and how do we impose further constraints, i.e. the requirement that every connected edge must contain different node colors?" CreationDate="2014-12-04T15:45:15.177" UserId="5273" />
  <row Id="2829" PostId="2602" Score="0" Text="Fair warning: Links aren't accepted as answers on this site. I recommend editing or deleting before you get any downvotes!" CreationDate="2014-12-04T18:56:36.163" UserId="3466" />
  <row Id="2830" PostId="2597" Score="0" Text="I updated my answer." CreationDate="2014-12-04T19:53:53.070" UserId="381" />
  <row Id="2831" PostId="2606" Score="0" Text="I'd also take relevant classes from the [statistics](http://handbook.uts.edu.au/directory/maj01111.html) or [maths](http://handbook.uts.edu.au/directory/maj01110.html) [departments](http://handbook.uts.edu.au/directory/stm90681.html). And do buy that book Charlie recommended!" CreationDate="2014-12-04T22:45:47.173" UserId="381" />
  <row Id="2832" PostId="2607" Score="0" Text="I was thinking more along the lines of [Bayesian nonparametric ranking](http://papers.nips.cc/paper/4624-bayesian-nonparametric-models-for-ranked-data.pdf)." CreationDate="2014-12-04T23:59:52.197" UserId="381" />
  <row Id="2833" PostId="2607" Score="0" Text="Sure. That is definitely a good fit. It could be a bit difficult to implement, though." CreationDate="2014-12-05T01:51:55.813" UserId="4621" />
  <row Id="2834" PostId="2604" Score="0" Text="What are the parameters for the custom search link you provided? Does it search in a list of websites, keywords, etc.?" CreationDate="2014-12-05T07:42:26.397" UserId="227" />
  <row Id="2835" PostId="2604" Score="0" Text="@AmirAliAkbari It searches through sources like Data.gov, Quandl, and other major data warehouses." CreationDate="2014-12-05T12:39:34.210" UserId="5279" />
  <row Id="2836" PostId="2605" Score="2" Text="It is discouraged to post link-only answers. The link may eventually break, and the answer will be thus harmed. It will be better to post it as a comment, or even add further useful information to your post." CreationDate="2014-12-05T15:14:16.597" UserId="84" />
  <row Id="2837" PostId="2609" Score="1" Text="This type of question is considered off topic for StackExchange." CreationDate="2014-12-05T15:48:27.397" UserId="21" />
  <row Id="2838" PostId="2493" Score="0" Text="Even after the editing of the question, I'm still not sure what you're trying to do. Do you want to cluster the paragraphs?" CreationDate="2014-12-05T15:48:53.937" UserId="819" />
  <row Id="2840" PostId="334" Score="0" Text="This discussion was useful to me. I have worked on the &quot;edges&quot; of data science with BI and analytics and have taken data mining and statistics courses for a Master's Degree several years ago. Right now I primarily work on the enterprise information management aspect of corporate data but would like to switch to a data science job. Therefore I am looking at best way to &quot;present myself&quot; on a resume and with training/experiences to potential future employer." CreationDate="2014-12-04T18:01:58.977" UserDisplayName="user5278" />
  <row Id="2841" PostId="2601" Score="1" Text="If you have a specific, reproducible example then that would probably add value. Also, are you sure that you're talking about a *package* in R? I see no package called `sann`. I do see a `sann` function in the `ConsPlan` package...  I'm pretty sure that's what you meant, so I'm going to edit the question. Please let me know if I'm off base." CreationDate="2014-12-05T15:53:58.770" UserId="2723" />
  <row Id="2842" PostId="2601" Score="1" Text="Upon further investigation, you actually could've gotten this `sann` function from a few different packages. The definition of `sann` varies between them, based on the developers comments in various R mailing lists. Were you perhaps using the `optim` package?" CreationDate="2014-12-05T16:13:40.923" UserId="2723" />
  <row Id="2843" PostId="2593" Score="0" Text="Are you interested only in libraries available within Google Go?" CreationDate="2014-12-05T16:38:27.373" UserId="2723" />
  <row Id="2844" PostId="2590" Score="0" Text="Can you provide some reproducible code/data to work with? I've completed a cross-sell model recently and I'd like to give it a shot." CreationDate="2014-12-05T16:46:19.343" UserId="2723" />
  <row Id="2845" PostId="2624" Score="0" Text="thank you, that's very useful. +1." CreationDate="2014-12-05T17:35:15.847" UserId="2723" />
  <row Id="2846" PostId="2612" Score="2" Text="Which tools have you already looked at, and why don't they solve your problem?" CreationDate="2014-12-05T18:24:02.693" UserId="3466" />
  <row Id="2847" PostId="2626" Score="0" Text="If price is no object you could check out Netezza..." CreationDate="2014-12-05T23:27:40.047" UserId="5247" />
  <row Id="2848" PostId="2615" Score="0" Text="I've looked for spaces after each of the state names. There aren't any." CreationDate="2014-12-06T00:12:42.247" UserId="2647" />
  <row Id="2849" PostId="2629" Score="1" Text="It means different things to different people. Give it time and people may come to an agreement. Until then: [Six categories of Data Scientists](http://www.datasciencecentral.com/profiles/blogs/six-categories-of-data-scientists), [16 analytic disciplines compared to data science](http://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared)." CreationDate="2014-12-06T08:12:06.730" UserId="381" />
  <row Id="2850" PostId="2629" Score="1" Text="This is quite open-ended and opinion based, which is viewed as off topic for StackExchange." CreationDate="2014-12-06T13:29:27.300" UserId="21" />
  <row Id="2851" PostId="2354" Score="0" Text="I'd recommend getting rosetta by going directly go GitHub.  That ensures you get the latest version.     https://github.com/columbia-applied-data-science/rosetta" CreationDate="2014-12-06T23:42:17.337" UserId="5313" />
  <row Id="2852" PostId="2544" Score="0" Text="Thanks a lot, IharS" CreationDate="2014-12-07T04:42:14.747" UserId="5172" />
  <row Id="2853" PostId="2543" Score="0" Text="Hi Sean, I'm just trying to explore the possible options with the data available" CreationDate="2014-12-07T04:45:02.923" UserId="5172" />
  <row Id="2854" PostId="2622" Score="0" Text="The `Data Science Toolkit` is very interesting but not what I am looking for. I am looking for some high performant stream based protocol which allows me to stream (and buffer) data from n data-miners to m data-processors." CreationDate="2014-12-07T11:21:16.863" UserId="5266" />
  <row Id="2855" PostId="2593" Score="0" Text="@Hack-R if it is a more complex protocol which requires some heavy logic I would prefer that a library would be available in Go but I would even more prefer if the library would be a available for other languages too. What do you think of a message queue like [nsq](http://nsq.io)." CreationDate="2014-12-07T11:22:41.060" UserId="5266" />
  <row Id="2856" PostId="2625" Score="0" Text="This is a *pure* python solution?" CreationDate="2014-12-07T11:23:28.320" UserId="5266" />
  <row Id="2857" PostId="2633" Score="0" Text="Install it on your own computer. It has more than a gig right?" CreationDate="2014-12-07T12:00:56.413" UserId="381" />
  <row Id="2858" PostId="2633" Score="0" Text="yep, but my system has only 4GB of RAM. I tried using ubuntu in VM. It's very low." CreationDate="2014-12-07T12:59:35.613" UserId="5172" />
  <row Id="2859" PostId="2637" Score="0" Text="yes in fact i have the same questions in my mind. No other choice left except setting up a physical machine with good amount of memory or creating a large instance in AWS." CreationDate="2014-12-07T13:49:30.050" UserId="5172" />
  <row Id="2860" PostId="2642" Score="2" Text="Could you clarify, in what way the equations should be part of the ML problem or solver? Error gradient backpropagation in neural networks is pretty much entirely the chain rule applied repeatedly to a set of partial differential equations. Would that count?" CreationDate="2014-12-07T22:41:31.310" UserId="836" />
  <row Id="2861" PostId="2643" Score="0" Text="[Spark Cluster on Google Compute Engine](https://greenido.wordpress.com/2014/05/13/spark-cluster-on-google-compute-engine/)" CreationDate="2014-12-07T23:04:13.033" UserId="381" />
  <row Id="2862" PostId="2637" Score="0" Text="Or you could upgrade your computer; memory is cheap." CreationDate="2014-12-07T23:05:40.490" UserId="381" />
  <row Id="2863" PostId="2639" Score="0" Text="thanks. I am familiar with adaboost, and boosting in general. The problem I'm describing has 2 differing qualities though - 1. It is an estimator not classifier we want to get (seems simple to get around), and 2. looking for an online learning method, as all the estimators R_i are online. Would you suggest online boosting? I have no relevant exp, would appreciate some pointers." CreationDate="2014-12-08T07:09:13.367" UserId="5314" />
  <row Id="2864" PostId="2644" Score="0" Text="atmosphere, it is possible that radioactivity is&#xA;responsible for the fogging. At the same time, it is&#xA;certainly possible that proximity to x-ray machines in&#xA;use, or to other sources of radiation, is responsible for&#xA;the fogging.&#xA;We propose an emergency budget of $45,000 be set&#xA;aside for an immediate study to:&#xA;1. Locate sources of radioactivity in the Denver&#xA;Plant, in our field warehouses and throughout&#xA;the distribution pipeline;" CreationDate="2014-12-08T08:16:39.743" UserId="5328" />
  <row Id="2865" PostId="2644" Score="0" Text="2. Run a correlation analysis on the incidence of&#xA;radioactivity in a geographical area&#xA;corresponding with the complaining dealers and&#xA;their customers.&#xA;A problem of this magnitude and it appears to be&#xA;growing rapidly, requires and warrants thorough&#xA;analysis. How should they tackle this problem and what&#xA;is the action plan?" CreationDate="2014-12-08T08:17:46.177" UserId="5328" />
  <row Id="2866" PostId="2575" Score="0" Text="I have never seen such a thing in Hive." CreationDate="2014-12-08T14:23:45.440" UserId="21" />
  <row Id="2867" PostId="660" Score="0" Text="Is this a single machine cluster? Can you provide the stderr log?" CreationDate="2014-12-08T14:34:52.917" UserId="2723" />
  <row Id="2868" PostId="2644" Score="1" Text="Can you please clean up the formatting in your question? It is very hard to read." CreationDate="2014-12-08T22:36:41.757" UserId="3466" />
  <row Id="2869" PostId="2654" Score="0" Text="http://prediction.io/" CreationDate="2014-12-09T02:46:24.813" UserId="381" />
  <row Id="2870" PostId="2642" Score="1" Text="By PDE, I meant some set of [equations](http://en.wikipedia.org/wiki/Convection%E2%80%93diffusion_equation) which are generally used to describe physical phenomena and using that as a learning/modeling process. The solution to such equations gives us some sort of error estimation to gauge the strength of these models and thereby, reduce the unpredictability in certain situations." CreationDate="2014-12-09T04:20:40.940" UserId="1131" />
  <row Id="2871" PostId="2643" Score="0" Text="@Emre, I've also looked into that blog, But there are no steps provided for cluster formation. It is single node." CreationDate="2014-12-09T06:20:56.250" UserId="5172" />
  <row Id="2872" PostId="2643" Score="0" Text="You indicated that you wanted 1 slave, so [setting the second parameter to 1](https://github.com/sigmoidanalytics/spark_gce#usage) should do the trick." CreationDate="2014-12-09T06:31:02.437" UserId="381" />
  <row Id="2873" PostId="2648" Score="1" Text="Yes, that would probably be it. I could also add additional weights to some words, that I already know that are informative. Thanks for your help and useful links." CreationDate="2014-12-09T09:33:49.713" UserId="2750" />
  <row Id="2874" PostId="2642" Score="0" Text="@Sidha: That doesn't clarify it for me. Gradient descent, (and the variants of it), is an attempt to find a solution to the minimum of an error function, and *uses* partial differential equations in its formulation. Please clarify, does that count (because if it does, a large percentage of ML could be said to use PDEs)? If you are looking for a machine learning process that uses PDEs in some other way, it needs to be clearer, because simply &quot;uses some PDEs&quot; applies to anything that can be numerically optimised with gradient descent." CreationDate="2014-12-09T10:23:06.047" UserId="836" />
  <row Id="2875" PostId="2651" Score="4" Text="-1: Where have you looked already? Found anything?" CreationDate="2014-12-09T13:59:00.947" UserId="471" />
  <row Id="2876" PostId="2642" Score="1" Text="Gradient descent uses partial derivatives, agreed. What I intended to say was a system of PDE's and not in an optimization problem. Please check the link in the previous comment. This [quora discussion](http://www.quora.com/Are-Differential-Equations-relevant-to-Machine-Learning) mentions some usage in optical flow applications in computer vision. But I am wondering if there are more applications out there using this approach." CreationDate="2014-12-09T15:18:47.340" UserId="1131" />
  <row Id="2877" PostId="2658" Score="0" Text="Do you have any programming skills? There are many ways to do this, I can suggest something hopefully based on something you have used in the past." CreationDate="2014-12-09T20:51:47.973" UserId="3466" />
  <row Id="2878" PostId="2658" Score="0" Text="I have a good knowledge in Java. I have used R for a couple of data mining tasks. Currently I am studying Python for using NLP." CreationDate="2014-12-09T20:53:30.183" UserId="5043" />
  <row Id="2879" PostId="2660" Score="0" Text="This basic strategy would also work if you found an NLP toolkit in another language you know, like Java. I'm just not familiar with those." CreationDate="2014-12-09T22:00:30.657" UserId="3466" />
  <row Id="2880" PostId="2293" Score="0" Text="For outlier detection, have a look at the algorithms in ELKI. That seems to be the most complete collection of outlier detection." CreationDate="2014-12-09T22:37:32.467" UserId="924" />
  <row Id="2881" PostId="2633" Score="1" Text="Talk to your professors and tell them you need a bigger machine to use. Most CS departments will help you out." CreationDate="2014-12-10T01:39:13.597" UserId="3466" />
  <row Id="2886" PostId="2575" Score="0" Text="That answer does not solve the problem :)" CreationDate="2014-12-10T09:57:34.930" UserId="5224" />
  <row Id="2887" PostId="2575" Score="0" Text="Is there a way of implementing a function on my own on Hive or maybe should I use different tool - maybe should I check Apache Spark?" CreationDate="2014-12-10T10:13:32.133" UserId="5224" />
  <row Id="2888" PostId="2668" Score="1" Text="Do you also want to know about Machine Learning toolkits such as e.g. FACTORIE?" CreationDate="2014-12-10T11:40:40.087" UserId="5370" />
  <row Id="2889" PostId="2575" Score="0" Text="Heh, yeah that's why I only commented. What I mean is that, having sniffed around Hive a long time, I've never heard of this, so it's unlikely to exist, but I don't know for sure that this is the answer. Spark does not have it either." CreationDate="2014-12-10T12:11:32.657" UserId="21" />
  <row Id="2890" PostId="2662" Score="0" Text="*facepalm* I meant kNN. XD I will change the title. Sorry for that. Started learning about these things not long ago and I'm still confusing the terms. +1 from me for the &quot;You are mixing up kNN classification and k-means&quot;, which pointed out the mistake I've made in my question!" CreationDate="2014-12-10T12:24:45.520" UserId="5356" />
  <row Id="2891" PostId="2661" Score="0" Text="Sorry, I wrote k-means but meant kNN (see question again but view it as kNN not k-means). Thanks for the fast reply though!" CreationDate="2014-12-10T12:29:17.380" UserId="5356" />
  <row Id="2892" PostId="2546" Score="0" Text="Thanks Laurik. Unfortunately not only numbers, and I don't know also what future messages will be. So, I really need AI." CreationDate="2014-12-10T13:32:29.570" UserId="3024" />
  <row Id="2893" PostId="2668" Score="0" Text="That sounds relevant to my question, feel free to share more." CreationDate="2014-12-10T15:40:50.120" UserId="3466" />
  <row Id="2894" PostId="2675" Score="0" Text="Software/Hardware classification is like the sample task that I tried to work with the categorization. There are several other categories I have on my mind which would take a deep understanding of what is wrong with the products, by reading the customer's case, and tag the appropriate category. I started reading NLPTK using python but I would like to know the kind of functions that I should be looking for to address this case" CreationDate="2014-12-10T15:44:21.057" UserId="5043" />
  <row Id="2895" PostId="2671" Score="0" Text="Yeah, I actually updated only &quot;k-means-&gt;kNN&quot;. Forgot to change the rest but you managed to see through. I am actually interested exactly in the last paragraph of your answer - EXACT same distance (no matter how probable that is in real-life data evaluation). I don't get &quot;But you would still simply pick the class that is most prevalent among that group of observations.&quot; What criterion/criteria do I have to observe is such a situation? PS: I'll update my question again so that it fits &quot;kNN&quot; exactly." CreationDate="2014-12-10T16:16:23.637" UserId="5356" />
  <row Id="2896" PostId="2662" Score="0" Text="I've updated the answer to also talk about kNN classification." CreationDate="2014-12-10T16:21:38.553" UserId="924" />
  <row Id="2897" PostId="2662" Score="0" Text="Oh, really cool. Thanks. Exactly what I wanted to know! Sorry again for asking about the wrong thing. :D" CreationDate="2014-12-10T16:24:43.320" UserId="5356" />
  <row Id="2898" PostId="2676" Score="0" Text="Ok thanks, that's somehow more clear, especially for the XOR. For the gravity thing, I don't know if it's a good example, I guess machine learning doesn't really apply as we know how gravity works we don't really need machine learning, right? The question was more about teaching something with a small array of inputs, and apply it to a larger array." CreationDate="2014-12-10T16:26:27.383" UserId="5369" />
  <row Id="2899" PostId="2675" Score="0" Text="This isn't a simple matter of looking for magic functions. What you want to do is build a classifier using supervised machine learning. These are the steps... 1. manually annotate a sample of your data as a training set, 2. extract features from your data to train on (for text, this might be something like ngrams), 3. build the classifier model using a machine learning library, 4. apply the classifier model to new data. Some libraries like Stanford Classifier will help you with steps 2 &amp; 3." CreationDate="2014-12-10T17:27:06.407" UserId="819" />
  <row Id="2900" PostId="2679" Score="0" Text="Thanks! Interestingly enough they chose to use multiple Hinton Diagrams to plot their weights. I still think it's hard to interpret as soon as you have too many layers/connections but it's good to see it in action at least." CreationDate="2014-12-10T20:52:55.593" UserId="5316" />
  <row Id="2903" PostId="2678" Score="0" Text="Prior to training the classifier, I do format the data in tuples of `({tokenized content}, category)`. As I have the training set websites in a database and already categorized, it is not a problem. The labels that the classifier will be able to apply will only be those it has seen from the annotated training set, correct?" CreationDate="2014-12-11T00:18:36.850" UserId="5199" />
  <row Id="3904" PostId="2682" Score="1" Text="Hi, I do have the same query regarding scaling issue of Neo4j. And now I decided to go for Titan DB. Titan is an open source distributed graph database build on top of Cassandra." CreationDate="2014-12-11T07:18:18.570" UserId="5091" />
  <row Id="3905" PostId="2678" Score="0" Text="Re: &quot;The labels that the classifier will be able to apply will only be those it has seen from the annotated training set, correct?&quot; Correct. In supervised learning, the classifier won't be able to create new/unseen categories. If you want to do that, you should look into something like clustering or topic modeling." CreationDate="2014-12-11T11:36:17.293" UserId="819" />
  <row Id="3906" PostId="2678" Score="0" Text="Thank you very much for the information! As you answered my question as well, I'll be accepting this as the answer." CreationDate="2014-12-11T12:10:29.753" UserId="5199" />
  <row Id="3907" PostId="3685" Score="1" Text="Very interesting idea! Really, visualizing a deep net like a social network is something I've not thought about. The main difference between the models is that these graphs code information in their nodes while neural networks do it within their connections. But it could be modified, e.g. by setting the social network node values to the outgoing connections weights of the neural network." CreationDate="2014-12-11T12:49:38.227" UserId="5316" />
  <row Id="3908" PostId="3685" Score="0" Text="I'm glad that you like the idea. Feel free to upvote/accept. And don't forget to review SoNIA software, with link to which I recently updated my answer. Finally, if you use (or plan to use) R, here's another relevant interesting info for you: http://sna.stanford.edu/rlabs.php." CreationDate="2014-12-11T12:57:07.617" UserId="2452" />
  <row Id="3909" PostId="2678" Score="0" Text="My apologies for bringing this back after accepting the above answer, but I reckoned that I would have better odds of getting an answer to my update if I asked you directly. So as to avoid long comments, I would greatly appreciate it if you could take a look at my edit in the original post." CreationDate="2014-12-11T16:28:45.077" UserId="5199" />
  <row Id="3912" PostId="3689" Score="1" Text="This is great info about Scala in general, but I'm looking for information about specific data science tools or frameworks which have Scala integration, and why Scala is a good language for those tools to use." CreationDate="2014-12-11T20:31:54.600" UserId="3466" />
  <row Id="3914" PostId="3691" Score="0" Text="thank you for your input, if you have an idea what would be better then NB, please let me know." CreationDate="2014-12-12T00:17:13.170" UserId="6387" />
  <row Id="3916" PostId="3688" Score="0" Text="@fordprefect Multinomial Naive Bayes uses a multinomial distribution for the probabilities of some feature given a class: $p(f_i|c)$. The OP wants a classifier to manage multiple outputs as TheGrimmScientist described." CreationDate="2014-12-12T01:03:52.467" UserId="4621" />
  <row Id="3917" PostId="3691" Score="1" Text="the &quot;existing solutions&quot; section of [this slide deck](http://www.slideshare.net/sympapadopoulos/a-graphbased-clustering-scheme-for-identifying-related-tags-in-folksonomies) holds everything I would know to reply with, plus more (assuming you're not needing specifically a classifier and just want a way to use tags).  I hope it's useful to you." CreationDate="2014-12-12T01:48:22.737" UserId="6391" />
  <row Id="3918" PostId="3691" Score="0" Text="@TheGrimmScientist Wouldn't it be reasonable to use the first approach? If you have a vector of features f1, f2, f3 and let's say, 3 labels for this vector, we can partition that into 3 vectors (all containing the same features f1, f2, f3) with different labels as outputs. Then it is possible to use Naive Bayes as usual. I'm not sure if that is what you had in mind." CreationDate="2014-12-12T02:57:27.040" UserId="4621" />
  <row Id="3919" PostId="3693" Score="1" Text="Why collect the names at all?" CreationDate="2014-12-12T08:33:39.923" UserId="381" />
  <row Id="3920" PostId="3690" Score="0" Text="Writing a conversion script seems the most straightforward way to do this." CreationDate="2014-12-12T16:18:43.763" UserId="227" />
  <row Id="3921" PostId="1243" Score="0" Text="Is it possible to run A/B tests in your situation? That could give you some training data, or even just a general idea." CreationDate="2014-12-12T21:54:47.650" UserId="3466" />
  <row Id="3923" PostId="3690" Score="0" Text="@AmirAliAkbari - Have you done something like that with R and OpenCV? If so, can you provide an example in the form of an answer? If not, can you point out some promising approaches for writing such a conversion script?" CreationDate="2014-12-12T22:52:44.337" UserId="6390" />
  <row Id="3924" PostId="37" Score="1" Text="You're talking about unstructured data, not big data.  Unstructured data usually leads to NoSQL solutions and big data in application, but they are still different." CreationDate="2014-12-12T22:55:16.847" UserId="6391" />
  <row Id="3925" PostId="2650" Score="0" Text="It would be nice if you could provide us with an example of gradient application in data mining/machine learning. This would make your answer self contained and more useful to others." CreationDate="2014-12-12T23:41:48.410" UserId="84" />
  <row Id="3926" PostId="2631" Score="2" Text="If you have such a question, you probably have an intuition on why using clustering for outlier detection would be a nice strategy. If you add such information to your post, it shall definitely increase the visibility and the interest from others in answering your question." CreationDate="2014-12-12T23:43:45.277" UserId="84" />
  <row Id="3928" PostId="2586" Score="0" Text="Please, provide us with the main (and just the main!) topics discussed in each subject. This would make your post self contained, instead of forcing others to flick through your course webpages." CreationDate="2014-12-12T23:49:42.287" UserId="84" />
  <row Id="3929" PostId="2579" Score="0" Text="I Found this blog post very useful : http://shanon-shanghai.blogcn.com/articles/mahout-based-recommender-system-introduction.html#5-3" CreationDate="2014-12-11T06:16:59.327" UserId="5091" />
  <row Id="3930" PostId="2667" Score="0" Text="[Bulbflow- A python interface to graph databases](http://bulbflow.com/overview/); [Bulb Docs](http://bulbflow.com/docs/); [A snarky answer](http://bit.ly/1zvehXv). Note this question has been solved already on [stackoverflow.com](http://stackoverflow.com/questions/24990607/bulbs-python-connection-to-a-remote-titandb-rexster), which is where it belongs because it specifically relates to programming." CreationDate="2014-12-10T14:13:56.900" UserId="5247" />
  <row Id="3931" PostId="2676" Score="0" Text="There are different strategies for neural networks to work with unknown data, and there are also different strategies to let a neural network grow. But all of them wouldn't really fit to the question." CreationDate="2014-12-13T09:00:51.660" UserId="3132" />
  <row Id="3932" PostId="3693" Score="0" Text="Records need to be unique, and everybody knows their own name (that sounds sarcastic, but it's just simplicity)." CreationDate="2014-12-13T13:43:37.490" UserId="2742" />
  <row Id="3933" PostId="3694" Score="0" Text="I wasn't aware of those alternative naming schemes. Thank you. On the conflict issue, a high vote answer on [SO suggests](http://stackoverflow.com/questions/297960/hash-collision-what-are-the-chances) this about SHA-1, &quot;To address the birthday paradox, a database with 10^18 (a million million million) entries has a chance of about 1 in 0.0000000000003 of a collision.&quot;" CreationDate="2014-12-13T13:49:43.550" UserId="2742" />
  <row Id="3934" PostId="2625" Score="0" Text="No, using mongodb for caching.  I think Mongodb is written in Java if that's what you mean?" CreationDate="2014-12-13T16:40:30.897" UserId="5247" />
  <row Id="3935" PostId="3699" Score="0" Text="What algorithm, meaning, what loss function are you regularizing?" CreationDate="2014-12-13T20:32:15.557" UserId="21" />
  <row Id="3936" PostId="2659" Score="0" Text="It might have been better to make a new question since now the answers don't match your new, reasonably different question." CreationDate="2014-12-13T20:33:44.490" UserId="21" />
  <row Id="3937" PostId="3701" Score="0" Text="I get the point, but just to be clear, it sounds like you're mixing two options. The first is the option to take student ID together with the name, which would be likely unique and somewhat obscure to anyone who would obtain the data from the internet. Plus a second option to take this encoded ID+Lname and has that as well. Plus a further measure to use a salt of their last name. In other words, pre-computer I imagine the ID+Lname could be an acceptable obfuscation?" CreationDate="2014-12-14T01:17:24.630" UserId="2742" />
  <row Id="3938" PostId="3701" Score="0" Text="No, I did not say to take the PIN and the name; just the PIN, as it is already unique. Using the last name as the salt was just a suggestion; you can use any user-specific information." CreationDate="2014-12-14T01:36:44.303" UserId="381" />
  <row Id="3939" PostId="3702" Score="0" Text="Before I suggest the answer directly -- what did you find that makes you think it's discriminative? are you sure k-NN always involves Bayes's theorem or priors?" CreationDate="2014-12-14T11:26:50.307" UserId="21" />
  <row Id="3940" PostId="3702" Score="0" Text="The problem is that I'm not sure about the definition of these two types classifiers, my book isn't clear. That being said, I think that the generative classifier must be able to generate data poins as well, and to do that it learns the joint probability $p(x,C_{k})$. The discriminative classifier does not learn the joint probability, and so does KNN. While I see that it does use Baye's theorem, it only computes the posterior. Tell me if this is correct or not, but also please tell me the definitive difference between the two classifiers because I'm confused." CreationDate="2014-12-14T11:32:40.510" UserId="6419" />
  <row Id="3941" PostId="3702" Score="0" Text="Also from what I understand in the book, it always uses Baye's theorem to compute the posteriors. The class priors $p(C_{k})$ are given as just $\frac{N_{k}}{N}$ where the numerator is the number of points in class k, and N is the total number of points in our set." CreationDate="2014-12-14T11:51:57.360" UserId="6419" />
  <row Id="3942" PostId="2678" Score="1" Text="RE: &quot;would I have to feed the entirety of my data into TF-IDF at once?&quot; Yes, that's how it works. RE: &quot;I have been looking at the scikit-learn TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.&quot; Here's an example I wrote: https://github.com/charlieg/A-Smattering-of-NLP-in-Python#building-a-term-document-matrix -- it's probably best if you use a corpus of documents as input, rather than some dict+string tuple you created." CreationDate="2014-12-14T14:51:04.237" UserId="819" />
  <row Id="3943" PostId="3699" Score="0" Text="I'm using the logistic loss function." CreationDate="2014-12-14T15:11:50.157" UserId="6418" />
  <row Id="3944" PostId="3699" Score="1" Text="Take a look at [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV) which is generally used for parameter estimation to improve score. Hope it helps." CreationDate="2014-12-15T02:51:13.053" UserId="1131" />
  <row Id="3945" PostId="3703" Score="1" Text="Unfortunately, it looks as if `r-opencv` is relatively unmaintained (last updates 9 months ago, followed by 5 years ago). The PMML option and associated link appear to be great resources. Thanks!" CreationDate="2014-12-15T02:53:26.640" UserId="6390" />
  <row Id="3946" PostId="3703" Score="0" Text="I see. Well, depending on how you look at it, this might be an opportunity to revive the project (of course, if you're interested and can devote some time to it). I'm glad the other information is helpful to you. Thanks for accepting the answer, which you can also upvote, if you feel generous today :-). Good luck with your project! And keep an eye on [Azure Machine Learning](http://azure.microsoft.com/en-us/services/machine-learning), which I haven't included in my answer, as I consider this offering currently 1) still too raw; 2) not cost-effective; 3) not flexible enough (vs. pure R-based)." CreationDate="2014-12-15T03:53:06.580" UserId="2452" />
  <row Id="3948" PostId="2509" Score="0" Text="Thanks a lot. I didn't knew alpha-algorithms. I'll check in that direction." CreationDate="2014-12-15T12:17:05.737" UserId="3024" />
  <row Id="3951" PostId="2609" Score="0" Text="Is it? That's quite interesting question." CreationDate="2014-12-15T22:12:04.203" UserId="5224" />
  <row Id="3953" PostId="3711" Score="1" Text="Maybe Laplace Smoothing is what you are looking for? http://en.wikipedia.org/wiki/Additive_smoothing" CreationDate="2014-12-16T18:59:17.717" UserId="3132" />
  <row Id="3955" PostId="1190" Score="1" Text="This question appears to be off-topic because it is about statistics and should be on crossvalidated, not stack overflow!" CreationDate="2014-12-17T10:23:51.570" UserId="471" />
  <row Id="3958" PostId="3707" Score="1" Text="Link-only answers are discouraged. Can you expand on the content in this link that you think is relevant?" CreationDate="2014-12-17T13:49:50.817" UserId="21" />
  <row Id="3959" PostId="156" Score="0" Text="Freebase is [closing down](https://groups.google.com/forum/#!msg/freebase-discuss/s_BPoL92edc/Y585r7_2E1YJ) and its database will move to [Wikidata](https://www.wikidata.org) soon." CreationDate="2014-12-17T14:39:02.783" UserId="43" />
  <row Id="3962" PostId="3718" Score="0" Text="You have mentioned neither *types* of data you collect, nor its volume's *order of magnitude*. This information would be helpful in providing a more targeted advice." CreationDate="2014-12-18T02:35:11.440" UserId="2452" />
  <row Id="3963" PostId="3719" Score="1" Text="Also consider Graphlab (python based): http://graphlab.com/products/create/overview.html&#xA;Here's a good blog post about it as well:&#xA;http://bugra.github.io/work/notes/2014-04-06/graphs-databases-and-graphlab/&#xA;I can't help you with the Titan vs oriebtDB discussion though.  Hopefully someone will chime in with that." CreationDate="2014-12-18T18:03:46.183" UserId="375" />
  <row Id="3966" PostId="3730" Score="2" Text="This one should go to DataScience as it is about managing large scale projects." CreationDate="2014-12-19T16:06:58.140" UserId="1237" />
  <row Id="3967" PostId="3730" Score="0" Text="I agree with @StasK: this question should be migrated to either Data Science or, alternatively, to StackOverflow (I just flagged it)." CreationDate="2014-12-19T16:37:38.783" UserId="2452" />
  <row Id="3968" PostId="3734" Score="0" Text="I flagged this question for migration to Data Science SE site, as I think it's not a statistics question, but a data mining (data science) one." CreationDate="2014-12-20T08:10:50.377" UserId="2452" />
  <row Id="3969" PostId="3733" Score="1" Text="So you want to estimate the function and its extrema too? Or you have a black box operating as a function which you need to strategically query to determine the extremum?" CreationDate="2014-12-20T20:42:59.690" UserId="381" />
  <row Id="3970" PostId="3719" Score="0" Text="Also possible to use Spark and GraphX" CreationDate="2014-12-20T21:00:53.057" UserId="3466" />
  <row Id="3971" PostId="3733" Score="0" Text="I have a black box operating as a function and I need to strategically query it in order to determine the optimum. Of course, I am not looking for the global maximum but anything that is smarter than a grid search over a space of parameters." CreationDate="2014-12-20T22:16:39.243" UserId="847" />
  <row Id="3972" PostId="3735" Score="1" Text="Thank you Aleksandr. I didn't consider it, but there are factors instead of characters in the data set. I will coerce as character and run again!" CreationDate="2014-12-21T06:04:58.283" UserId="6506" />
  <row Id="3973" PostId="3735" Score="0" Text="@Chris: My pleasure, Chris! Would love to hear about the results. Don't forget to accept/upvote, if my answer is helpful :-)." CreationDate="2014-12-21T06:57:55.040" UserId="2452" />
  <row Id="3974" PostId="1253" Score="0" Text="I would suggest not to use NLP and ML together in such context, as if they were comparable entities. They are IMHO two distinctly separate fields. Despite the fact that NLP uses a lot of ML approaches, methods, algorithms and tools, NLP is an applied field, whereas ML is a (more) generic one." CreationDate="2014-12-21T10:34:29.063" UserId="2452" />
  <row Id="3975" PostId="3739" Score="0" Text="The work is (for now) academic, rather than purely practical. I may do some forecasting very late on or in future, but I'm more interested in exploring the past data for now. The clustering is a goal in and of itself, as well as some ideas I want to explore past that point." CreationDate="2014-12-22T05:24:51.753" UserId="5246" />
  <row Id="3976" PostId="3739" Score="0" Text="Sorry, hit enter prematurely. I've looked into autocorrelation to some extent and ran it on a subset of my data a whole ago but it wasn't really clear to me what I could get out of it. The data is pretty noisy. The seasonality patterns are sometimes pretty obvious on visualisation, but inexact in their timings - so I may be looking for similar patterns but not on a nice, even schedule. I was told that autocorrelation was likely to be problematic on such data, but happy to have another look if there's value in it. I don't want to *just* find seasonality, but understanding it is a goal." CreationDate="2014-12-22T05:47:47.340" UserId="5246" />
  <row Id="3977" PostId="3739" Score="0" Text="Work through that tutorial at least up to and including 2.5.  It uses R which is especially good for your academic environment.  It will teach you autocorrelation which sounds like exactly what you're looking for (can't tell if it wasn't a fit because you didn't know what you were looking at, or the data is actually too noisy).  If noise is the issue, exponential smoothing is one way to help with that, which will be taught as a part of the holt-winters model.  Even if all of that doesn't give you the answer, it will certainly make your next step way clearer." CreationDate="2014-12-22T06:45:44.040" UserId="6391" />
  <row Id="3979" PostId="3738" Score="0" Text="+1 Very nice question, and it is great to see so much enthusiasm! I think you could nail down your question a little bit, so it's more inviting for others to read, and then give you an answer." CreationDate="2014-12-22T09:56:32.873" UserId="84" />
  <row Id="3981" PostId="3738" Score="0" Text="@Rubens Thanks! I'll re-work it when I'm home this evening, I can see where it'd be useful to include some more information about how I've gotten to this point and why. I was worried about it getting too long, but I'll separate out the background and question a bit more to avoid it getting unreadable." CreationDate="2014-12-22T10:04:44.587" UserId="5246" />
  <row Id="3982" PostId="3742" Score="0" Text="R is a good way to go! Can you please add a data sample and desired output you want to get?" CreationDate="2014-12-22T14:52:17.677" UserId="97" />
  <row Id="3983" PostId="3742" Score="0" Text="Columns are separated by commas and lines by semicolon:&#xA;ID, timeframe, fruit_amt, veg_amt;&#xA;4352, before, 0.25, 0.75;" CreationDate="2014-12-22T15:17:29.737" UserId="6491" />
  <row Id="3984" PostId="3742" Score="0" Text="Trying again.  Columns are separated by commas and lines by semicolon:&#xA;ID, date, timeframe, fruit_amt, veg_amt;&#xA;4352, 05/23/2013, before, 0.25, 0.75;&#xA;5002, 05/24/2014, after, 0.06, 0.25;&#xA;4352, 04/16/2014, after, 0, 0;&#xA;4352, 05/23//2013, after, 0.06, 0.25;&#xA;5002, 05/24/2014, before, 0.75, 0.25;&#xA;I want to have all the observations for a single ID (ex, 4352 or 5002) on one row (making the data set much wider).  My data set is much wider than this to begin with (I've got about 50 different columns per current observation) but this is the basic idea." CreationDate="2014-12-22T15:23:09.600" UserId="6491" />
  <row Id="3985" PostId="3742" Score="0" Text="You could better include it into your post and format in proper way..." CreationDate="2014-12-22T15:40:50.030" UserId="97" />
  <row Id="3988" PostId="3746" Score="0" Text="thank you very much!" CreationDate="2014-12-22T19:35:16.133" UserId="3132" />
  <row Id="3989" PostId="3746" Score="0" Text="You are welcome." CreationDate="2014-12-22T19:39:25.617" UserId="3466" />
  <row Id="3990" PostId="3723" Score="1" Text="Thanks so much Aleksandr Blekh &amp; Nitesh for your insight. I'll definitely look into these options. One thing i should have been more clear about is that i'm capturing live data from a laser-micrometer, sampling at about 60 samples per second.  Right after i asked this question, I found a software called  KST (https://kst-plot.kde.org/). It works nicely except that it isn't stable and isn't particularly easy to pull older data, zoom in/out, etc." CreationDate="2014-12-22T23:13:29.220" UserId="6469" />
  <row Id="3991" PostId="1081" Score="0" Text="@mateuz I think you're saying the same thing that I am saying, just in a different way. Data is the approximation of some element which we have decided to measure in some way." CreationDate="2014-12-23T00:28:37.500" UserId="3152" />
  <row Id="3992" PostId="3723" Score="0" Text="@ClaytonPipkin: You're very welcome. I'm glad to help. Don't forget to upvote both answers and accept the best, if satisfied. KST looks interesting, but keep in mind that it's an application, hence, it doesn't have a native ability to visualize data for the Web." CreationDate="2014-12-23T00:38:24.840" UserId="2452" />
  <row Id="3995" PostId="3751" Score="2" Text="Deep Learning is highly related to the theory of neural networks, so it would be good to start learning the basics of artificial neural networks first. See also http://datascience.stackexchange.com/questions/2651/deep-learning-basics" CreationDate="2014-12-23T09:43:34.677" UserId="3132" />
  <row Id="3996" PostId="3745" Score="0" Text="This is too open-ended to be a good question. Instead of asking for recommendations, can you specify more about what you want to know, what you know so far, and what your question is?" CreationDate="2014-12-23T15:41:27.437" UserId="21" />
  <row Id="3997" PostId="3751" Score="2" Text="This is probably too vague to make a good question. Add some detail about what your background is, what specifically about AI you're wondering if you need to know, etc." CreationDate="2014-12-23T15:42:10.027" UserId="21" />
  <row Id="3999" PostId="3751" Score="0" Text="I am a fresh graduate from a computer science program and have taking classes in algorithm and AI. To my understanding deep learning is a branch under AI, we were taught the basic concept of AI but I was wondering which are should I work on mastering and where should I move on to ." CreationDate="2014-12-24T02:28:02.930" UserId="6536" />
  <row Id="4002" PostId="3751" Score="2" Text="https://www.coursera.org/course/neuralnets" CreationDate="2014-12-25T10:27:24.407" UserId="2752" />
  <row Id="4003" PostId="3761" Score="0" Text="thank you @Javierfdr , I would have a question:&#xA;Do you suggest the use of genetic algorithm after selecting the number of hidden neurons (and so for start the training with the selected architecture) or for each hidden neuron number trial, in order to have a better starting point before select the hidden neuron number?" CreationDate="2014-12-25T20:37:09.897" UserId="6559" />
  <row Id="4004" PostId="3739" Score="0" Text="I had a read through the tutorial, but it mostly goes over things I already know. I'm actually working in Python and I'm a bit too far in to things to switch to R, although I've intended to grab rpy at some point in case there were some things I couldn't find in any Python libraries. I've re-written my question in case it helps any - like I say, the clustering is a goal in and of itself, I'm not looking for an entirely different direction to go in. I'm afraid the tutorial doesn't really answer my question." CreationDate="2014-12-25T23:37:03.877" UserId="5246" />
  <row Id="4005" PostId="3764" Score="0" Text="It seems that `scikit-learn` currently does not offer a DTW implementation, hence my reference to the older `mlpy`." CreationDate="2014-12-26T10:11:18.890" UserId="2452" />
  <row Id="4006" PostId="3738" Score="0" Text="It may not be a &quot;pure statistics&quot; question but it needs a pure statistics answer. You will struggle until you can think about it in pure statistics terms." CreationDate="2014-12-26T11:02:20.280" UserId="471" />
  <row Id="4007" PostId="3761" Score="0" Text="You can do both @Daniel, it depends on what can you actually compute. The input optimization with GA will tend to make a specific architecture of NN to work better. So if you can do it for every architecture you are trying it you will have more specific results to analyze. So if your final methodology will be: Optimize initial weights with a GA and use them to a given NN architecture, then yes you will need to do it for each NN to test which works better with this hybrid approach. Please notice that the objective function of the GA will be the calculation of the whole NN for a given input." CreationDate="2014-12-26T15:42:28.977" UserId="5143" />
  <row Id="4008" PostId="3761" Score="0" Text="Also notice @Daniel that if you are using binary input data you must look for Genetic Algorithms, and if using not binary input data then Evolutionary Algorithms will work better (based in the same principle but better architected for non input data)" CreationDate="2014-12-26T15:45:22.660" UserId="5143" />
  <row Id="4009" PostId="3761" Score="0" Text="Thank you @Javierfdr ! I have performed my test, in the question now there is the result. Can you confirm/comment my deduction? Thanks" CreationDate="2014-12-26T20:43:46.460" UserId="6559" />
  <row Id="4010" PostId="3761" Score="0" Text="@Daniel in general it looks like a good solution. Both 12 neurons and 18 neurons give you a low difference error between the sets. I would chose 12 neurons because it will tend to overfit less than a higher number of neurons. Also, are you training the network long enough? You might use a sort of early stopping to stop learning when a certain threshold of the difference of learnings in two consecutive steps is not surpassed. Look at these nice rule of thumbs for picking number of hidden neurons http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html" CreationDate="2014-12-26T23:30:06.450" UserId="5143" />
  <row Id="4011" PostId="3767" Score="0" Text="Thank you Max, it works fine now" CreationDate="2014-12-27T12:46:10.343" UserId="6572" />
  <row Id="4012" PostId="3764" Score="1" Text="A number of these are resources I've been looking at - I've implemented a modified version of the work in points 2 and 4, for instance - so we're probably on the same-ish page now. And the vast majority of what I know is based on Eamonn Keogh's papers or articles based on them. But there are some here I hadn't read, and the one about bike share time series clustering is interesting - thanks! I'm not seeing anything that specifically answers my question, but do point it out if I've missed something while reading." CreationDate="2014-12-27T13:43:28.367" UserId="5246" />
  <row Id="4013" PostId="3764" Score="1" Text="Also, if you're still finding this interesting, Keogh's papers are really worth a read. They're surprisingly easy to read and quite practical given the focus on using many data sets, and providing enough information that someone could re-create all experiments. The most recent one is interesting,and is what I was working my way through when I got sidelined by my question. http://www.cs.ucr.edu/~eamonn/selected_publications.htm" CreationDate="2014-12-27T13:46:44.573" UserId="5246" />
  <row Id="4014" PostId="3738" Score="0" Text="@Spacedman - I welcome answers in whatever manner people feel is the best way to answer it, with the caveat that I may have further questions if the answer is heavy on formulas or references to statistical concepts that I don't understand yet." CreationDate="2014-12-27T13:55:40.887" UserId="5246" />
  <row Id="4015" PostId="3764" Score="1" Text="@JoDouglass: You're welcome! I didn't intend to answer your question directly (due to my limited knowledge of the topic), but hoped that it would be helpful, which appears to be the case. Thank you for nice comments and the reference - I will browse the papers and try to get a better idea. There is so much to learn, it's overwhelming a bit." CreationDate="2014-12-27T16:05:26.760" UserId="2452" />
  <row Id="4016" PostId="3764" Score="1" Text="Overwhelming is right, I was kicking myself for choosing this topic for a while! I feel like I'm getting there, though - and it's been really interesting to learn about. I have a number of things up and running as sort of rough versions of what I need to do, and I think it's more about figuring out how to process my data before running it through my models, now. That bike share link is interesting to me as it's the first I've seen discussing averaging of time series since reading the recent Keogh paper I mentioned." CreationDate="2014-12-27T16:30:37.450" UserId="5246" />
  <row Id="4017" PostId="3764" Score="1" Text="@JoDouglass: When I said &quot;overwhelming&quot;, I meant the whole data science domain (including AI/ML and statistics, specifically). I'm yet to find a **resource**, which presents a _high-level_ discussion of various _approaches_ and/or _methods_ as **themes**, integrated into a _comprehensive_, yet _parsimonious_, **framework**." CreationDate="2014-12-27T17:09:15.957" UserId="2452" />
  <row Id="4018" PostId="3764" Score="1" Text="That's fair! I have no background in it myself, and feel like I jumped into the deep end by starting with time series. I would have dearly loved a resource like that when I started out a year ago - and I still would now." CreationDate="2014-12-27T17:43:33.170" UserId="5246" />
  <row Id="4019" PostId="3764" Score="0" Text="@JoDouglass: I just posted a corresponding question [here](http://datascience.stackexchange.com/q/3769/2452) and [on Quora](http://qr.ae/zb6WL)." CreationDate="2014-12-27T17:55:03.700" UserId="2452" />
  <row Id="4026" PostId="2629" Score="0" Text="Here's my opinion:  http://thegrimmscientist.com/2014/05/05/what-is-data-science/" CreationDate="2014-12-28T05:30:05.677" UserId="6391" />
  <row Id="4028" PostId="3772" Score="0" Text="When you say you're using a moving window, do you mean you are performing some manipulation of the data within that window or just using the previous 6 values?" CreationDate="2014-12-28T20:15:35.377" UserId="4621" />
  <row Id="4029" PostId="3772" Score="0" Text="Yes exactly, and the previous value of the serie I am trying to predict." CreationDate="2014-12-28T22:54:46.593" UserId="303" />
  <row Id="4030" PostId="3773" Score="0" Text="The question about previous value was overall usefullness. If the previous value is a function of past rates, how is this different from just taking a bigger window." CreationDate="2014-12-28T22:57:06.720" UserId="303" />
  <row Id="4031" PostId="3773" Score="0" Text="For the algorithm: regularized polynomial regression (Followed Andrew Ng course on Coursera)" CreationDate="2014-12-28T23:03:44.080" UserId="303" />
  <row Id="4032" PostId="3773" Score="0" Text="For the time-serie parts: I (more or less) know how to pre-process data. I was wondering if there is algorithm for time series and not a specific number of features." CreationDate="2014-12-28T23:06:45.067" UserId="303" />
  <row Id="4033" PostId="3773" Score="0" Text="Great, I can comment here! Apologies for not being able to ask you questions in the comments before. The time series that you're predicting - does it have any historic data, or is it made up purely of your predictions?" CreationDate="2014-12-28T23:23:49.420" UserId="5246" />
  <row Id="4034" PostId="3773" Score="0" Text="@Imorin - I would agree that if you're only looking at the 6 previous values in a time series, you're possibly not using all available information. Have you had a look at Rob Hyndman's blog on forecasting? There's a lot about forecasting and time series, and there's a post from Oot 4 2010 where he discusses cross validation as a method to check on the usefulness of variables in a forecasting model. Time series are mentioned at the end. There are also often helpful discussions in the comments.  http://robjhyndman.com/hyndsight/forecasting/" CreationDate="2014-12-29T00:30:54.210" UserId="5246" />
  <row Id="4038" PostId="3774" Score="0" Text="I strongly suspect non-linear dependencies. That is why I wanted to try Machine Learning on multiple features, their respectives powers and cross products. Your answer is still very helpfull as I just understood that the machine learning process I wanted to do here is strictly the same as fitting a model to my time-serie. (wich I am very familiar with to say the least)" CreationDate="2014-12-29T14:25:10.170" UserId="303" />
  <row Id="4039" PostId="449" Score="1" Text="Keep in mind that Sean Owen is a co-author of the new O'Reilly book on Spark. :-)" CreationDate="2014-12-29T17:05:50.890" UserId="3466" />
  <row Id="4041" PostId="3774" Score="0" Text="Sure. If there is a non-linear dependency between series, then a feature based approach is a good approach." CreationDate="2014-12-29T20:12:19.303" UserId="4621" />
  <row Id="4048" PostId="3783" Score="0" Text="Before anything, to model this as a &quot;data science&quot; problem, you need to define a metric to determine how &quot;close&quot; the encryption is to the original text. I am not sure this is feasible, since the computing time required to decrypt/encrypt isn't really a good metric for how good/bad an encryption algorithm is." CreationDate="2014-12-30T14:14:02.160" UserId="6529" />
  <row Id="4051" PostId="3783" Score="0" Text="You need a model for the encryption algorithm, then you set about estimating the model parameters." CreationDate="2014-12-30T19:10:48.970" UserId="381" />
  <row Id="4053" PostId="3783" Score="0" Text="Unless you are working with weak or broken encryption, then the amount of processing and data you would need to differentiate between two modern schemes is likely to be prohibitive. Most of the pad generating algorithms are statistically random, and very hard to differentiate from truly random sources without knowing the decryption key." CreationDate="2014-12-30T21:25:39.883" UserId="836" />
  <row Id="4054" PostId="3740" Score="0" Text="It's hard to say about Gaussian NB and tf*idf, but I have successfully used multinomial (and sometimes binomial one). Multinomial distribution is much easier to reason about, and it clearly maps to text classification task (value of a feature = number of occurrences of some word in text, probability to get specific text ~ multinomial distribution over all words in this text). Imbalance in dataset is one of the most important components in NB classifier (recall `P(C)`). You _can_ omit it, if it improves results, but it won't be NB anymore, but more like MLE." CreationDate="2014-12-30T22:58:11.760" UserId="1279" />
  <row Id="4055" PostId="2382" Score="0" Text="This question is more appropriate for opendata.stackexchange.com" CreationDate="2014-12-31T12:39:30.103" UserId="21" />
  <row Id="4056" PostId="3788" Score="0" Text="Well even if you do it in Python or R you will have to change the data into coordinates. What are you trying to do? Try gis.stackexchange.com for more specialised advice in this." CreationDate="2014-12-31T17:00:27.617" UserId="471" />
  <row Id="4057" PostId="3792" Score="0" Text="Know it is at least being researched (in case of Amazon) don't know state of implementation." CreationDate="2015-01-01T14:35:24.813" UserId="1256" />
  <row Id="4061" PostId="3797" Score="0" Text="You are likely querying the last known output of batch, not running a batch process." CreationDate="2015-01-03T18:52:57.727" UserId="21" />
  <row Id="4062" PostId="3797" Score="0" Text="OK. so how do I merge the lats known output of batch with the streaming data stored inside spark discrete RDD?" CreationDate="2015-01-03T19:02:11.967" UserId="6644" />
  <row Id="4063" PostId="3804" Score="2" Text="I think that the optimal solution depends on various factors, such as the data volume, project's performance requirements and intended types of analysis." CreationDate="2015-01-04T05:03:15.673" UserId="2452" />
  <row Id="4064" PostId="3804" Score="0" Text="I'm crawling around 5,000 posts or so, and intend to perform some unsupervised topic modelling on them." CreationDate="2015-01-04T13:50:06.517" UserId="6660" />
  <row Id="4065" PostId="3808" Score="1" Text="Why don't you have a validation set? Is it just because the implementation doesn't create one for you? Not knowing a lot about bayesian regularisation, my naive answer would be &quot;so set aside a validation or test set manually&quot; . . . in fact, as long as you are not *tuning* your params using the set, but performing a single pre-decided comparison of generalisation success, then I would call that a &quot;test set&quot;" CreationDate="2015-01-04T14:36:15.570" UserId="836" />
  <row Id="4066" PostId="3808" Score="0" Text="maybe I can compare the validation mse(with the name 'validation' because it is used for parameters tuning) of the trainlm with the test mse (called test because the paramerets are tuned in another way)?" CreationDate="2015-01-04T16:34:44.153" UserId="6559" />
  <row Id="4067" PostId="3808" Score="0" Text="@NeilSlater I forgot to tag" CreationDate="2015-01-04T17:33:19.287" UserId="6559" />
  <row Id="4068" PostId="3808" Score="0" Text="I would not know how to assess whether it was safe to compare those two metrics across the types of NN training you are using, maybe you should wait for a full answer. However, I'd just hold out a test set, not used to train or optimise params for either network, and compare results from both networks on that. If you can afford to keep enough labeled data aside, this is the simplest and least susceptible to errors in assessment." CreationDate="2015-01-04T18:41:24.753" UserId="836" />
  <row Id="4069" PostId="3770" Score="0" Text="Care to expand a bit on what you mean by &quot;unknown property&quot;?" CreationDate="2015-01-04T21:20:51.923" UserId="6391" />
  <row Id="4070" PostId="2580" Score="1" Text="Mahout is slow, and didn't make a lot of progress recently. It's dead, IMHO." CreationDate="2015-01-04T22:24:01.307" UserId="924" />
  <row Id="4071" PostId="3804" Score="2" Text="Based on your comment, I think that the storage method is not very important and may be based on your convenience and/or tools availability. Let's see what other people think." CreationDate="2015-01-04T23:41:09.030" UserId="2452" />
  <row Id="4073" PostId="3804" Score="1" Text="This needs more clarification, and does not seem data science specific -- not the scraping and storage part." CreationDate="2015-01-05T11:28:10.083" UserId="21" />
  <row Id="4074" PostId="3807" Score="2" Text="This is probably not suitable as a question as-is. You should indicate what you have tried and what your specific question is about the input format and package you are using." CreationDate="2015-01-05T11:29:56.683" UserId="21" />
  <row Id="4075" PostId="2580" Score="0" Text="AFAIK it is not dead, they are creating bindings to Spark (cf MLLib) https://mahout.apache.org/users/recommender/intro-cooccurrence-spark.html" CreationDate="2015-01-05T18:50:21.613" UserId="1256" />
  <row Id="4076" PostId="3814" Score="1" Text="I think that adding clarifying words to terms, _ambiguous_ in a particular **context** (such as _sample_), is inevitable, if you want to communicate with the maximum _clarity_ and prevent any _miscommunication_." CreationDate="2015-01-05T19:49:28.460" UserId="2452" />
  <row Id="4077" PostId="3760" Score="0" Text="From the shape of the curve, I want to say to run with 6 or 7 since you 'should' be looking for the 'shoulder'.  However, your validation and testing scores look to be doing surprisingly well even at the highest number of hidden nodes.  Any chance you could try the test for even more nodes (upwards of 35 or 40) to see if you can start breaking your validation and testing sets?" CreationDate="2015-01-05T22:27:50.000" UserId="6391" />
  <row Id="4078" PostId="3821" Score="0" Text="If you are trying to forecast the time series based on its history, use the [entropy rate](http://en.wikipedia.org/wiki/Entropy_rate)." CreationDate="2015-01-06T05:10:56.850" UserId="381" />
  <row Id="4080" PostId="3814" Score="0" Text="@AleksandrBlekh you mean writing something like &quot;I analyzed a statistical sample of rock samples&quot;? Honestly I'm not sure there is a distinction; just hierarchical data." CreationDate="2015-01-06T16:47:53.833" UserId="1156" />
  <row Id="4081" PostId="3825" Score="0" Text="Is it possible to use a programming language or any other software to build such system &lt;- sure. but why are you not using mahout? maybe you could rephrase the question a little, but overall what you are about to do with symptoms is not that different from what recommenders do with products." CreationDate="2015-01-06T21:34:46.957" UserId="3132" />
  <row Id="4082" PostId="3825" Score="0" Text="I guess so.Technically, I am not recommending symptoms here, I am just pointing the possible symptoms a user might report on. That is why I termed it as a deterministic system as opposed to recommendation system." CreationDate="2015-01-06T21:51:38.253" UserId="5043" />
  <row Id="4083" PostId="3825" Score="0" Text="The format of the data needed for mahout is bit different from the one that I have now. If I arrange the data set in the format **Account Name**, **Symptom**,**Count** ;would it be enough to work with Mahout?" CreationDate="2015-01-06T21:53:21.897" UserId="5043" />
  <row Id="4084" PostId="3825" Score="0" Text="Yes, the input format is UserId, ItemId, &quot;Preference&quot;. Maybe just give it a try: https://mahout.apache.org/users/recommender/userbased-5-minutes.html" CreationDate="2015-01-06T22:19:48.633" UserId="3132" />
  <row Id="4086" PostId="3814" Score="0" Text="@ssdecontrol: Obviously, I didn't mean that. No need to combine together the same terms in the same sentence. What I meant is what you expressed by saying &quot;rock samples&quot;. Using &quot;rock&quot; here makes it clear that geological, not statistical, meaning is implied. So, you can rewrite your phrase as follows: &quot;I collected rock samples from &lt;...&gt; site (N=100). I used stratified random sampling for collection&quot;. No ambiguity, everybody's happy." CreationDate="2015-01-06T23:29:04.670" UserId="2452" />
  <row Id="4087" PostId="3814" Score="0" Text="@AleksandrBlekh that's definitely more elegant. But I'm legitimately wondering if &quot;statistical sample&quot; is too cumbersome. The qualification should be bilateral IMO. Maybe &quot;random sample&quot; would be unambiguous enough" CreationDate="2015-01-07T00:51:23.233" UserId="1156" />
  <row Id="4088" PostId="3814" Score="0" Text="@ssdecontrol: I see your point and agree that &quot;statistical sample&quot; is an overkill. However, other than that particular word combination, I still stand by my opinion. I don't see how &quot;sample&quot; can qualify some other term, while the reverse is IMHO true in most cases." CreationDate="2015-01-07T01:00:22.587" UserId="2452" />
  <row Id="4089" PostId="3818" Score="0" Text="Spreading the day column into 5 independent binary output columns is a good idea, thank you. I'm not sure I understand what you mean by `plot the distribution of spend as a function of day`." CreationDate="2015-01-07T02:08:50.653" UserId="6669" />
  <row Id="5081" PostId="3787" Score="0" Text="I think if you model temperature, than a numeric variable should be considered. In you specific case, I would think that building an empirical distribution from conditioned data would work much better than considering temperature as a nominal variable." CreationDate="2015-01-07T07:56:26.207" UserId="108" />
  <row Id="5082" PostId="3818" Score="0" Text="typo: binary input column." CreationDate="2015-01-07T17:22:57.637" UserId="6669" />
  <row Id="5083" PostId="3825" Score="0" Text="Is there a way to work with Mahout for the data that has userName, SymptomName as opposed to Ids[numerical val]? Because, when I tried to run the program for such a data, it threw **Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;** *Number Format Exception error*" CreationDate="2015-01-07T17:31:06.840" UserId="5043" />
  <row Id="5084" PostId="3787" Score="0" Text="@rapaio can you expand a bit?  The only definitions of 'empirical distribution' I know are either doing the same thing I did in the example or are not applicable to the current problem." CreationDate="2015-01-07T19:40:05.437" UserId="6391" />
  <row Id="5085" PostId="3816" Score="1" Text="ideally also ensure the test set is properly separate from the training set. Eg if doing handwriting recognition have the test set have writing done by people who's writing (on other letters) is not in the training set." CreationDate="2015-01-08T01:09:26.593" UserId="2680" />
  <row Id="5086" PostId="3787" Score="0" Text="You built an empirical distribution for a nominal variable. My proposal was to build an E.D. for a continuous variable, for example a KDE ( see http://en.wikipedia.org/wiki/Kernel_density_estimation ). Using a KDE the probability from each sample value 'concentrated' in each point is 'spreaded' in its neighburhood. The result is that for each possible temperature value you will get a probability from the sample points in its neghbourhood and you don't need to build intervals, which are arbitrary and thus have good chances to loose a lot of valuable information." CreationDate="2015-01-08T06:53:31.653" UserId="108" />
  <row Id="5087" PostId="4830" Score="0" Text="How does time series come into the question?" CreationDate="2015-01-08T14:18:59.350" UserId="21" />
  <row Id="5090" PostId="4834" Score="1" Text="There's a good rundown of missing data methods that might help get you started: http://www.stat.columbia.edu/~gelman/arm/missing.pdf" CreationDate="2015-01-08T15:17:46.693" UserId="1156" />
  <row Id="5091" PostId="4834" Score="0" Text="And another approach that I haven't tried myself but comes from a very respected author and sounds good in the abstract: http://gking.harvard.edu/files/gking/files/measure.pdf" CreationDate="2015-01-08T15:21:03.827" UserId="1156" />
  <row Id="5092" PostId="4836" Score="0" Text="Not a direct answer and not tutorials either, but if you're interested in hierarchical models, you might find [my recent answer](http://datascience.stackexchange.com/a/4833/2452) on the topic helpful (a collection of research papers)." CreationDate="2015-01-08T18:12:36.247" UserId="2452" />
  <row Id="5093" PostId="4834" Score="0" Text="Sorry, I wanted just to add paragraphs for clarity in the question -- but Stack Overflow only accepts edits if they are longer than 6 characters.  So I also rephrased the problem description to 'A/B measurements'.  Why did you call them 'B/A measurements', by the way?" CreationDate="2015-01-08T19:51:25.907" UserId="1367" />
  <row Id="5094" PostId="3804" Score="0" Text="I agree with @SeanOwen that this question needs clarification: at least, the OP should include his size, performance and expected use (topic modeling) requirements in the question (taking it from his own comment); and he should make it clear why is this a Data Science question (maybe rephrase the question to: does storage format of textual data matter for later modeling?)" CreationDate="2015-01-08T20:14:22.880" UserId="1367" />
  <row Id="5095" PostId="2346" Score="0" Text="I think the question is clearly too wide: consider how many answers there is to &quot;What are the practical problems I might run into?&quot;.  In the end, the collection of all answers to that question *is* the system you want to create. :)  IMHO, you should narrow this down to &quot;How should I define an exception? How can I know if my definition is a good one?&quot;; it is the most related to Data Science, the others are for Programmers SE." CreationDate="2015-01-08T21:20:54.687" UserId="1367" />
  <row Id="5096" PostId="4830" Score="0" Text="Well, it depends what one is trying to do with the textual data right. For e.g. I have a use-case where i am tracking how elaborate explanation does a user give when asked the same question by different ppl but over a period of time." CreationDate="2015-01-08T23:06:21.697" UserId="5179" />
  <row Id="5097" PostId="4837" Score="0" Text="I think this is hard to answer if you have the formula in front of you but are saying you just don't understand it. Can you break down specifically what you are having trouble with?" CreationDate="2015-01-08T23:42:28.413" UserId="21" />
  <row Id="5098" PostId="4836" Score="1" Text="Generally questions that just ask for off-site resources are considered off topic. Maybe you can narrow down what you are looking for in a tutorial. What do you know, what do you want to know, do you want code in a particular language, etc." CreationDate="2015-01-08T23:43:36.520" UserId="21" />
  <row Id="5099" PostId="4837" Score="0" Text="just added a sample dataset. Does this help? I thought cost function for linear regression is same." CreationDate="2015-01-09T01:01:55.267" UserId="7712" />
  <row Id="5100" PostId="3767" Score="0" Text="@RobertoDotti, then mark the question as answered (and post your answer if topepo's wasn't what you did)?" CreationDate="2015-01-09T01:22:23.070" UserId="6391" />
  <row Id="5101" PostId="4846" Score="0" Text="I played a bit on the Dell's website and was able to configure two systems, which approach your main desired parameters (6-core, 64G of RAM, Ubuntu 12.04LTS) - albeit disks size is smaller (it's tricky) - and the total came to just a little above 3K USD. Models that allow you to do that are: Dell Precision Tower 5810 Workstation and Dell Precision Tower 7810 Workstation. Note that 7810 has additional benefit of optional second processor with a relatively cheap upgrade." CreationDate="2015-01-09T09:35:21.827" UserId="2452" />
  <row Id="5102" PostId="4833" Score="1" Text="Thanks for those links, turns out the 2nd one down was almost exactly what I was after." CreationDate="2015-01-09T10:23:40.893" UserId="474" />
  <row Id="5104" PostId="4833" Score="1" Text="@DaveChallis: My pleasure! Glad to be able to help." CreationDate="2015-01-09T11:24:46.347" UserId="2452" />
  <row Id="5105" PostId="4848" Score="0" Text="this means Lambda architecture a little bit of airy fairy thing. easy to talk on slides and looks  pretty but then in reality it not so easy to implement." CreationDate="2015-01-09T15:11:47.707" UserId="6644" />
  <row Id="5106" PostId="4848" Score="0" Text="or a better analogy is the mice deciding to &quot;bell the cat&quot;. great architecture... but who is going to do it?" CreationDate="2015-01-09T15:13:18.613" UserId="6644" />
  <row Id="5107" PostId="4834" Score="0" Text="Thanks for the comments I will take a look at these links.  B/A measurements was a typo...thanks for the fix." CreationDate="2015-01-09T15:14:56.857" UserId="7713" />
  <row Id="5108" PostId="4835" Score="0" Text="Sean-Thank you for the response, much appreciated!  I thought it seemed like an OK path forward but I did not have anything to base that on.  Will read up on Laplace Smoothing." CreationDate="2015-01-09T15:16:07.760" UserId="7713" />
  <row Id="5110" PostId="3802" Score="0" Text="Interesting links. Thanks" CreationDate="2015-01-09T15:34:39.513" UserId="303" />
  <row Id="5112" PostId="3805" Score="0" Text="usefull library but not really an answer to my question" CreationDate="2015-01-09T15:37:03.023" UserId="303" />
  <row Id="5113" PostId="3801" Score="0" Text="Seems to be a practical answer. Not sure if this is applicable to financial time series because of arbitrage." CreationDate="2015-01-09T15:38:33.107" UserId="303" />
  <row Id="5114" PostId="3802" Score="0" Text="@lmorin: You're welcome." CreationDate="2015-01-09T15:38:33.467" UserId="2452" />
  <row Id="5115" PostId="3771" Score="0" Text="Isn't that a [differenced first-order autoregressive model](http://people.duke.edu/~rnau/411arim.htm) or ARIMA(1,1,0)?" CreationDate="2015-01-09T16:45:29.570" UserId="2452" />
  <row Id="5117" PostId="3771" Score="0" Text="One more note: [this discussion](http://stats.stackexchange.com/q/50807/31372) and, especially, Matt Krause's answer might be helpful." CreationDate="2015-01-09T17:42:19.213" UserId="2452" />
  <row Id="5118" PostId="1216" Score="0" Text="You might want to read [my related answer](http://datascience.stackexchange.com/a/742/2452)." CreationDate="2015-01-10T09:28:17.187" UserId="2452" />
  <row Id="5119" PostId="4844" Score="0" Text="I think this is a bit off-topic, as you're asking about hardware requirements for software. But it's also quite open ended since you're covering OS, to stats environments, to distributed cluster computing frameworks and databases. It's not clear what your use case requires from the sentence of description." CreationDate="2015-01-10T11:13:41.343" UserId="21" />
  <row Id="5120" PostId="1216" Score="0" Text="If you're a good programmer then you probably already use quite a bit of math.  I can't imagine a programmer that is good and doesn't use math on a daily basis.  What is the highest level of math you've used? What programming language do you use and what do you use it for?  You certainly don't need a PhD to do data science, but the math is essential." CreationDate="2015-01-10T19:51:37.717" UserId="4697" />
  <row Id="5121" PostId="4827" Score="1" Text="Questions about installing R packages aren't about Data Science and so aren't on-topic here." CreationDate="2015-01-10T22:40:02.057" UserId="471" />
  <row Id="5122" PostId="3781" Score="0" Text="Do you consider your data set time series?" CreationDate="2015-01-11T00:08:06.653" UserId="2452" />
  <row Id="5123" PostId="4844" Score="1" Text="@SeanOwen, you will see that I already accepted Aleksandr Blek's answer before you put the question on hold. It was very well thought out and supplied me with heaps of material to work from - exactly what I was after. Please take the hold off unless you are on a power trip of course." CreationDate="2015-01-11T01:36:30.707" UserId="7722" />
  <row Id="5124" PostId="4846" Score="1" Text="Thanks @Aleksandr Blekh, this provided me with so much more to take into consideration. I realized that I have to up my budget to ~$6K and I am going to go for a Lenovo P700 Thinkstation. Most of the Lenovo hardware is certified for Ubuntu. I am going for a single core that can support 64GB RAM (8x8), with ample cache 30MB and fast speed 3.4GHz. I also opted for an SSD drive for the OS and APPS, but mechanical HDD's for the data side." CreationDate="2015-01-11T01:41:04.203" UserId="7722" />
  <row Id="5125" PostId="4846" Score="0" Text="@RUser: I am very glad that my answer was helpful. As for your choice, I can't say much, as I haven't had direct experience with the brand, just read some mixed reviews (in terms of quality control), but not too many to generalize. I hope that this option will work for you well. Would be interested in and appreciate your feedback after you set the system up and perform some R analyses." CreationDate="2015-01-11T01:50:31.493" UserId="2452" />
  <row Id="5129" PostId="4844" Score="0" Text="Just ran across [this comparative analysis article](http://www.tomsitpro.com/articles/thinkstation-p300-vs-hp-dell-workstation,1-1955.html) and thought that might want to read it, despite your focus/decision on Lenovo system." CreationDate="2015-01-11T05:10:51.427" UserId="2452" />
  <row Id="5130" PostId="4844" Score="0" Text="@RUser I stand by my comment but happy for other mods to weigh in. Easy on the accusations, it's inappropriate. It is good you got a usable answer but it doesn't necessarily mean the question is on topic here." CreationDate="2015-01-11T09:42:52.970" UserId="21" />
  <row Id="5131" PostId="4855" Score="0" Text="What classifier are you using?" CreationDate="2015-01-11T15:43:03.903" UserId="1156" />
  <row Id="5132" PostId="4855" Score="0" Text="logistic regression in apache mahout" CreationDate="2015-01-11T17:28:09.863" UserId="6636" />
  <row Id="5133" PostId="4855" Score="0" Text="The problem is not with zeroes per se but with too many of one class and not enough of the other; logistic regression is usually fitted with maximum likelihood (although I can't speak specifically for the Mahout implementation) and that causes problems for the algorithm. A search for &quot;rare events logistic regression&quot; on the Stats.StackExchange might help you. I'd write a bigger answer but right now I'm posting from my phone and only have a few minutes" CreationDate="2015-01-11T18:31:33.873" UserId="1156" />
  <row Id="5135" PostId="4856" Score="0" Text="how would you add a &quot;derivative&quot; to a model ?" CreationDate="2015-01-11T23:46:54.457" UserId="303" />
  <row Id="5136" PostId="4835" Score="0" Text="This is also a well known technique in compositional data analysis (data in the interior of a simplex), for handling zero-valued components. The idea is that, in many physical applications such as geology (where the methods were first developed), zeroes were more likely due to detector limitations than being true &quot;structural&quot; zeroes. So adding some small value to the zeroes allows analysts to apply log transforms to the composition without having missing values or infinities" CreationDate="2015-01-12T04:23:49.017" UserId="1156" />
  <row Id="5137" PostId="3825" Score="0" Text="It sounds like you just need to write some wrapper functions, or a wrapper program." CreationDate="2015-01-12T04:31:34.333" UserId="1156" />
  <row Id="5138" PostId="4865" Score="2" Text="Link-only answers are generally discouraged on StackExchange. It would be better to rewrite this to summarize the key points in the resource that answer the question." CreationDate="2015-01-12T10:58:18.760" UserId="21" />
  <row Id="5139" PostId="4867" Score="0" Text="The apache mahout does indeed support three class classifiers. And the positive negative values were features, but I will indeed give multiple classifiers a shot. Thanks for the input" CreationDate="2015-01-12T11:45:16.833" UserId="6636" />
  <row Id="5140" PostId="4866" Score="0" Text="That is a really good answer.  Just a formatting issue: could you please add newlines before points 3 and 4?  Currently they are lost in a single paragraph ..." CreationDate="2015-01-12T14:14:17.683" UserId="1367" />
  <row Id="5141" PostId="4853" Score="0" Text="Hi user2313838, thanks for your reply! I am not familiar with spatial indices. From what I read it is a way to store spatial information regarding shapes/lines/etc..Could you expand on how the usage of spatial indices can extend into the question?" CreationDate="2015-01-12T23:17:26.133" UserId="7739" />
  <row Id="5142" PostId="1216" Score="0" Text="@Amstell: I studied mathematics till high school only, but I have used maths in  programming (Visual basic and C#), however, not at an advance level. I understand that math is essential. I am currently working on R Language, due to my interest in learning statistics." CreationDate="2015-01-13T04:04:09.450" UserId="3550" />
  <row Id="5143" PostId="4853" Score="0" Text="An example would be a [Ball tree](http://en.wikipedia.org/wiki/Ball_tree) structure." CreationDate="2015-01-13T04:04:24.223" UserId="7740" />
  <row Id="5144" PostId="4853" Score="0" Text="I revised that first part. The spatial index would be used to efficiently query for nearby items provided your metric was based on some kind of vector representation. It may or may not be relevant to your application." CreationDate="2015-01-13T04:27:40.623" UserId="7740" />
  <row Id="5145" PostId="1216" Score="0" Text="@Kurio27 I would say you've got quite a bit of learning ahead of you then, but using R to learn statistics will be a great resource. In my experience, hard coding statistical models builds a lot of understanding and intuition and I couldn't recommend it enough.  Stick with it and don't be discouraged by those saying you don't have a shot.  I was 30 when I started learned advanced mathematics (i.e. dyn. programming) having barely passed high school and not touching basic math until I was 28. I'm 32 now and feel very comfortable with a lot of techniques I had no idea about 2 years ago.Good luck!" CreationDate="2015-01-13T04:55:07.690" UserId="4697" />
  <row Id="5146" PostId="3781" Score="2" Text="If you want to classify data, you should not use a clustering method." CreationDate="2015-01-13T10:30:57.763" UserId="1193" />
  <row Id="5147" PostId="327" Score="0" Text="Yes, a good programmer can do the same in C. BUT a bad programmer can do it in Python as fast as an experienced programmer can do it in C." CreationDate="2015-01-13T13:38:41.387" UserId="7784" />
  <row Id="5148" PostId="327" Score="0" Text="That's true @Pithikos" CreationDate="2015-01-13T14:38:23.670" UserId="115" />
  <row Id="5149" PostId="3801" Score="0" Text="I think the answers to your questions are still valid. For the timeseries model you may want to look at ARCH (AutoRegressive Conditional Heteroskedasticity) models." CreationDate="2015-01-13T17:27:15.343" UserId="6648" />
  <row Id="5150" PostId="2514" Score="0" Text="Just to clarify, I meant unseen item types in the population being sampled; obviously there are none in the sample itself." CreationDate="2015-01-13T18:58:15.040" UserId="5095" />
  <row Id="5151" PostId="4856" Score="0" Text="Hi @Imorin, I've updated the answer with a bit more information. Hope that helps." CreationDate="2015-01-13T22:56:34.027" UserId="7738" />
  <row Id="5152" PostId="1154" Score="0" Text="If you are interested in visualizing not only tanks' positions, but also substance flows between the tanks, I would use **Sankey diagrams** (_network flow diagrams_). For more details and `R` examples, check [this blog post](http://blog.ouseful.info/2013/07/23/generating-sankey-diagrams-from-rcharts). If you don't use `R`, I'm sure that other programming languages, frameworks and ecosystems have similar libraries (for example, `d3.js`, if you want your diagrams to be Web-enabled)." CreationDate="2015-01-14T00:15:55.493" UserId="2452" />
  <row Id="5153" PostId="4864" Score="1" Text="[If you *do* want to post related questions at multiple SE sites](http://stats.stackexchange.com/questions/133340/giving-100-to-whoever-can-identify-the-algorithms-behind-these-440-instances), please link them together." CreationDate="2015-01-14T07:35:10.523" UserId="2853" />
  <row Id="5154" PostId="4873" Score="2" Text="Asking for all the plugins/API is waaay too broad for this. StackExchange is not a replacement search engine for the lazy. I could show you one way to do it, but you are asking for every possible way. Refine your question or it'll probably get closed." CreationDate="2015-01-14T11:50:54.030" UserId="471" />
  <row Id="5155" PostId="4873" Score="0" Text="Hi Spacedman, Which is the best api and bet way to get the data from DBpedia" CreationDate="2015-01-14T12:33:05.950" UserId="5091" />
  <row Id="5156" PostId="4874" Score="0" Text="Thank you logc. Yes here you are limiting attribute as 'EuropeanCountries' from all countries but what I would like to have is given a country name and then list (in json or just print) all information about that country from dbpedia." CreationDate="2015-01-14T13:33:53.857" UserId="5091" />
  <row Id="5157" PostId="4874" Score="0" Text="I am aware of that, that is why I said 'a bit of work is needed'. If I have time, I will edit the answer; otherwise, consider it a starting point :)" CreationDate="2015-01-14T14:01:37.580" UserId="1367" />
  <row Id="5158" PostId="4874" Score="0" Text="Edited to fit the question more precisely" CreationDate="2015-01-14T16:39:28.750" UserId="1367" />
  <row Id="5160" PostId="3781" Score="0" Text="Please include a printout of `process_set` and `process_set.hpc` (the first few lines as printed by `pandas` are enough).  Also, please clarify if you want to **cluster** or **classify** this dataset.  You cannot classify if you do not have pre-existing labels." CreationDate="2015-01-14T16:55:48.497" UserId="1367" />
  <row Id="5161" PostId="4877" Score="0" Text="I have done point-biserial correlations before, but I don't think this works for what I want to achieve. As you wrote - it calculates correlation between variables - in their entirety. I am looking for correlation for parts/segments of a variable, not all values, e.g. when continuous variable X &gt; 3000, not for all values of variable X. Basically something that automatically, or via some machine learning algorithm, &quot;discovers&quot;/data-mines different combinations of variable segments that highly correlate to the DV. Maybe I can try to make my question clearer?" CreationDate="2015-01-14T20:30:21.177" UserId="102" />
  <row Id="5162" PostId="4879" Score="0" Text="How is the output related to the input, other than being of the same dimension? What are you trying to do?" CreationDate="2015-01-15T04:18:36.803" UserId="381" />
  <row Id="5163" PostId="4866" Score="0" Text="@logc: Thank you. You can correct the formatting issues and such yourself using the edit button under the post." CreationDate="2015-01-15T07:53:14.430" UserId="6496" />
  <row Id="5164" PostId="4873" Score="0" Text="The best API for DBpedia is the RDF download, so you can process it locally without hitting on an API at all." CreationDate="2015-01-15T08:01:18.573" UserId="924" />
  <row Id="5165" PostId="4870" Score="0" Text="Aleksandr Blekh and @Max Gibiansky, thanks for the starting guidelines." CreationDate="2015-01-15T08:40:45.303" UserId="7750" />
  <row Id="5167" PostId="4881" Score="0" Text="Can you clarify what you mean? not all models assume hidden/latent variables. There is always a distribution of output given input, just because the data exists." CreationDate="2015-01-15T13:42:42.337" UserId="21" />
  <row Id="5168" PostId="4866" Score="0" Text="No, I couldn't because edits have to be at least 6 characters long.  :)" CreationDate="2015-01-15T15:09:38.520" UserId="1367" />
  <row Id="5169" PostId="4879" Score="0" Text="Thanks, I've edited the question." CreationDate="2015-01-15T17:08:07.347" UserId="7806" />
  <row Id="5170" PostId="4884" Score="0" Text="Hi, this topic is very broad. Please revise your question to be specific to machine learning methods in one field. A list for every field is too broad for this site." CreationDate="2015-01-15T20:26:54.640" UserId="3466" />
  <row Id="5171" PostId="2311" Score="1" Text="Deep learning cannot figure out the _labels_ (how could it be?), but only learn features. You still need some amount of labeled data and supervised learning to solve tasks like classification or regression." CreationDate="2015-01-16T08:28:22.303" UserId="1279" />
  <row Id="5173" PostId="4870" Score="0" Text="You are welcome, @hossain. Thank you for kind words, Max." CreationDate="2015-01-16T10:11:23.243" UserId="2452" />
  <row Id="5174" PostId="4891" Score="1" Text="I don't think this is true, but I think I know what you're getting at: in-memory works a lot faster for iterative computation and a lot of ML is iterative." CreationDate="2015-01-16T17:18:15.257" UserId="21" />
  <row Id="5175" PostId="4882" Score="0" Text="I think you'd have to narrow this down by talking about your data, scale and purpose." CreationDate="2015-01-16T17:19:06.810" UserId="21" />
  <row Id="5176" PostId="4887" Score="0" Text="Thanks @Javierfdr ! I tought that would not have such method. I've already searched at acm, ieee, elsevier, data direct, and so., but even so, I decided to ask here to confirm such expectation. If we narrow to the deep package inspection field, would you recommend any particular method?" CreationDate="2015-01-17T12:56:06.803" UserId="6560" />
  <row Id="5177" PostId="4884" Score="0" Text="I apologize for asking such obvious question, but the answer from @javierfdr was exacly the confirmation which I wanted. Anyway, please tell me what might I do. Is a case to delete the my question? Thanks in advance!" CreationDate="2015-01-17T15:37:34.240" UserId="6560" />
  <row Id="5178" PostId="4887" Score="0" Text="Could you please reply with the papers you've mentioned?" CreationDate="2015-01-17T15:39:11.350" UserId="6560" />
  <row Id="5179" PostId="3788" Score="0" Text="Address data doesn't necessarily have to be a graph. If you want to store relationships between the nodes (not just distances) then a graph makes sense." CreationDate="2015-01-17T17:02:06.187" UserId="7842" />
  <row Id="5180" PostId="4881" Score="0" Text="@SeanOwen: I have edited the question. Please have a look." CreationDate="2015-01-18T10:15:24.887" UserId="7811" />
  <row Id="5182" PostId="4882" Score="0" Text="This is like asking for &quot;machine learning tools.&quot; A generative model is any model that posits a probabilistic data-generating process" CreationDate="2015-01-18T22:21:46.273" UserId="1156" />
  <row Id="5183" PostId="1137" Score="2" Text="You might want to check my answers on the _Cross Validated_ site on _using DTW for time series analysis_: [DTW for clustering and/or classification](http://stats.stackexchange.com/a/131284/31372) and, if applicable, [DTW for irregular time series](http://stats.stackexchange.com/a/133826/31372)." CreationDate="2015-01-19T08:51:37.100" UserId="2452" />
  <row Id="5184" PostId="4904" Score="0" Text="Nice answer! (+1)" CreationDate="2015-01-19T15:31:39.983" UserId="2452" />
  <row Id="5185" PostId="4904" Score="0" Text="Thanks, Aleksandr!" CreationDate="2015-01-19T15:52:57.700" UserId="2723" />
  <row Id="5186" PostId="4904" Score="0" Text="It is my pleasure!" CreationDate="2015-01-19T16:34:14.103" UserId="2452" />
  <row Id="5188" PostId="4904" Score="0" Text="Thank you.. Is there any methods to perform feature space enrichment?" CreationDate="2015-01-19T17:39:32.613" UserId="7873" />
  <row Id="5189" PostId="4879" Score="0" Text="neural networks have been trained for filtering." CreationDate="2015-01-19T20:34:23.407" UserId="3132" />
  <row Id="5190" PostId="4904" Score="0" Text="Sure. There are many such methods. For instance the Gabor filter is a bandpass filter edge detection algorithm commonly used for feature generation in facial recognition and texture classification. This can be used in combination with classification algorithms such as support vector machines." CreationDate="2015-01-20T02:48:02.920" UserId="2723" />
  <row Id="5192" PostId="4904" Score="0" Text="Can I use that for feature enrichment in image classification?" CreationDate="2015-01-20T07:45:58.367" UserId="7873" />
  <row Id="5193" PostId="4904" Score="0" Text="@SarathaPriya Sure. Facial recognition and texture classification are actually specific types of image classification." CreationDate="2015-01-20T15:08:28.887" UserId="2723" />
  <row Id="5194" PostId="4887" Score="0" Text="@fabraz This post I published could be helpful for your question http://datascience.stackexchange.com/questions/4914/when-to-use-what-machine-learning" CreationDate="2015-01-20T15:28:54.607" UserId="5143" />
  <row Id="5195" PostId="4882" Score="0" Text="It sounds like you're just asking how to combine probabilities from several generative models. You'd multiply and normalize them. Is there more to it?" CreationDate="2015-01-21T06:47:54.730" UserId="21" />
  <row Id="5196" PostId="711" Score="1" Text="I don't get it - isn't this a question that should be posted on CrossValidated? I continue to be confused about what goes where between DataScience and CrossValidated." CreationDate="2015-01-21T13:31:03.130" UserId="6672" />
  <row Id="5197" PostId="711" Score="0" Text="@fnl: svms have some competition as classifiers from less mathematically &quot;pure&quot; engineered solutions, so I think DataScience is in a better position to make the comparison here. Although I share your confusion!" CreationDate="2015-01-21T13:59:40.567" UserId="836" />
  <row Id="5199" PostId="4920" Score="0" Text="I understand what you say @ssdecontrol, actually having a comprehensive list of solution to typical problems as you mention could also be very useful. Now, the main difference between the two approaches is that what I'm proposing is directly linked to the technical questions you might ask yourself when you are already trying alternatives,and in that point you already made some assumptions.So, if you have assumed that your features are not-gaussian, should I use PCA for dimensionality reduction? No. Your approach is wider: What to use for dim. reduction -&gt; PCA, but assume gaussian features. Thx" CreationDate="2015-01-21T15:46:48.233" UserId="5143" />
  <row Id="5200" PostId="4916" Score="0" Text="That framework you mentioned would be a great thing to have! Is there anything similar being written?" CreationDate="2015-01-21T15:49:49.083" UserId="5143" />
  <row Id="5201" PostId="4916" Score="0" Text="@Javierfdr: Nothing that I'm aware of. However, I keep looking." CreationDate="2015-01-21T16:05:02.417" UserId="2452" />
  <row Id="5202" PostId="4920" Score="0" Text="@Javierfdr my point is that the technical questions are a distraction if you don't have a substantive question in mind." CreationDate="2015-01-21T16:08:09.970" UserId="1156" />
  <row Id="5203" PostId="4916" Score="0" Text="@AleksandrBlekh the more I think about it the more I think the search for a _statistical_ framework is misguided. See Frank Harrell's answer on your question, and my answer to this one. But Harlow's book  sounds really interesting and I'm gonna pick it up from the library this week." CreationDate="2015-01-21T16:12:28.297" UserId="1156" />
  <row Id="5204" PostId="4916" Score="0" Text="@ssdecontrol: I respectfully disagree. Assuming that such framework doesn't exist (which is most likely the case at the present time) and realizing that it's not an easy task to create one, I strongly believe that it's very much possible, nevertheless. As for the answers you've mentioned (I always read all of them), I read both, but they don't prove that creating such framework is impossible - just difficult, as I've mentioned. That's not something that should stop people from thinking about it and even working toward that. Enjoy Harlow's book." CreationDate="2015-01-21T16:29:22.677" UserId="2452" />
  <row Id="5205" PostId="4921" Score="0" Text="I've answered a similar question a while ago. You can check my answer here: http://datascience.stackexchange.com/a/843/2452." CreationDate="2015-01-22T11:42:20.250" UserId="2452" />
  <row Id="5206" PostId="4927" Score="0" Text="Thank you so much for mentioning this option. I will definitely give it a try. However, I want an image that has exactly like this AMI, but can be run with VirtualBox on my laptop." CreationDate="2015-01-23T03:08:12.367" UserId="7923" />
  <row Id="5207" PostId="4927" Score="0" Text="I watched a tutorial recently about Docker, tested it  and found it easy to understand. What part did you find not user-friendly?" CreationDate="2015-01-23T03:24:57.253" UserId="4621" />
  <row Id="5208" PostId="1227" Score="0" Text="Are you sure you can't code that logic using a many-to-many relationship between your models. Hopefully, when you start writing your server-side code, your favorite framework will allow you to declare models with some fields. Therefore, you will be able to instruct that the model `Team` should a have a field called `players` that requires a many-to-many relationship to your other model `Player`. This allows you to retrieve which players are in each team and which teams are associated to each player." CreationDate="2015-01-23T03:47:47.867" UserId="4621" />
  <row Id="5209" PostId="4927" Score="0" Text="@JeanVids: You're very welcome. I understand your desire to have a local VM - that was the reason I've tried Docker on my computer. I will let you know, if I find a VirtualBox VM image focused on data science (hopefully, R-based)." CreationDate="2015-01-23T03:49:21.143" UserId="2452" />
  <row Id="5210" PostId="4927" Score="0" Text="@RobertSmith: It is easy to understand until things don't work. I've tried to install and use Docker toolset some time ago, but I was unable to get it installed completely (used several tutorials, including the official one, plus lots of Internet searching, but to no avail). I don't remember exactly, but the issue was pretty technical, which resulted in inability to even run Docker virtualization software on my computer." CreationDate="2015-01-23T03:54:19.150" UserId="2452" />
  <row Id="5211" PostId="4927" Score="0" Text="@AleksandrBlekh Maybe Docker has been improving lately because I had very good experience. It was very easy to install (at least, on Linux) and I was able to test the main features without any issues." CreationDate="2015-01-23T04:13:50.663" UserId="4621" />
  <row Id="5212" PostId="4927" Score="1" Text="@RobertSmith: I understand. Perhaps, the problem was that I was trying to set it up on my Windows machine. Anyway, I will give it a try some time later. Thanks for your comments." CreationDate="2015-01-23T04:48:43.470" UserId="2452" />
  <row Id="5213" PostId="4927" Score="1" Text="@AleksandrBlekh Yes, that might be the main problem. Unfortunately there are many issues when installing this sort of thing on Windows." CreationDate="2015-01-23T05:08:41.310" UserId="4621" />
  <row Id="5214" PostId="4925" Score="0" Text="Although this question could be viewed as borderline offtopic I somehow find it a good one for the site IMHO." CreationDate="2015-01-23T10:21:51.133" UserId="21" />
  <row Id="5215" PostId="4921" Score="0" Text="This is too broad to be an effective question here. Maybe you can list a couple projects you are interested in, what you've tried, and name the obstacle you face in pursuing each." CreationDate="2015-01-23T10:23:36.417" UserId="21" />
  <row Id="5216" PostId="4929" Score="0" Text="Interesting project (+1). Thank you for sharing! It might be easier to use it than to figure out why Docker didn't want to work on my Win 7 laptop (see above). However, it still might be a good idea to learn Docker, considering recent trends." CreationDate="2015-01-23T10:45:15.893" UserId="2452" />
  <row Id="5217" PostId="4925" Score="2" Text="In addition to the awesome comments, there's a (somewhat older) blog post comparing several different solutions: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html" CreationDate="2015-01-23T13:29:46.980" UserId="587" />
  <row Id="5218" PostId="4922" Score="0" Text="Thank you for your specific response to a rather vague question. &#xA;This was EXACTLY what I was looking for! (I don't have enough Rep to &quot;Vote Up&quot;" CreationDate="2015-01-23T15:13:54.130" UserId="7909" />
  <row Id="5219" PostId="4924" Score="0" Text="Thank you very much.&#xA;I would &quot;Vote Up&quot; this response, but I do not have enough Rep." CreationDate="2015-01-23T15:14:33.967" UserId="7909" />
  <row Id="5220" PostId="4929" Score="0" Text="Nice information. Comparing to vm tools, it needs some time to understand how docker operates. If you are already familiar with vm, it's a good idea to use this toolbox. Thank you for sharing." CreationDate="2015-01-23T15:55:01.110" UserId="1003" />
  <row Id="5222" PostId="4901" Score="0" Text="By the way, what is the &quot;download ratio&quot; of a video?" CreationDate="2015-01-23T20:27:10.383" UserId="4621" />
  <row Id="5223" PostId="4929" Score="0" Text="Thank you for sharing. It is definitely interesting. But I don't see how someone can use it without a graphical interface. I would need R-studio, and PyCharm for Python.( iPython notebook is there). I will need to play with a bit to understand it completely." CreationDate="2015-01-24T00:03:08.927" UserId="7923" />
  <row Id="5224" PostId="4901" Score="0" Text="download ratio = downloaded bytes / file size in byte" CreationDate="2015-01-24T05:45:16.223" UserId="7867" />
  <row Id="5225" PostId="4901" Score="0" Text="Okay, but then most videos have a download ratio of 1? In which situations you have a different ratio? Maybe if the user can't complete the download?" CreationDate="2015-01-24T20:49:30.420" UserId="4621" />
  <row Id="5226" PostId="4937" Score="0" Text="so how do apps know where to query. should they choose impala with 10 years of data? or choose oracle with 2 years of data?" CreationDate="2015-01-24T23:11:15.330" UserId="6644" />
  <row Id="5227" PostId="4937" Score="0" Text="also, both systems would need business friendly data views. so won't this approach lead to duplicate development?" CreationDate="2015-01-24T23:11:57.050" UserId="6644" />
  <row Id="5228" PostId="4937" Score="1" Text="I presume that there are different use cases for the data warehouse versus active archive, so no I don't see that there is necessarily any duplication. You query the data source that fits your use case." CreationDate="2015-01-24T23:31:20.353" UserId="21" />
  <row Id="5229" PostId="4887" Score="0" Text="It's really helpful! Thank you!" CreationDate="2015-01-25T00:18:06.840" UserId="6560" />
  <row Id="5230" PostId="4917" Score="0" Text="I find a &quot;Goal&quot; horizontal line at 74% confusing. If the goal is 100%, then what is the meaning of a &quot;goal at 74%&quot; line?" CreationDate="2015-01-25T09:13:05.293" UserId="2452" />
  <row Id="5231" PostId="4938" Score="0" Text="thanks for your answer. So the SVM packages in eg R or Python do not use quadratic programming methods when the data is non-linearly separable?" CreationDate="2015-01-25T10:07:58.323" UserId="7944" />
  <row Id="5232" PostId="4938" Score="0" Text="Not sure about what svm libraries you use. I use libsvm and different svm tools may use different svm solvers. To find better svm solvers is another research topic. QP is the basic way to solve svm." CreationDate="2015-01-25T10:37:58.553" UserId="1003" />
  <row Id="5233" PostId="4941" Score="2" Text="Providing a summary of external links' content would be useful." CreationDate="2015-01-25T16:40:07.080" UserId="227" />
  <row Id="5234" PostId="4914" Score="0" Text="Really broad. I think each sub-question must be a separate question in order to have meaningful answer." CreationDate="2015-01-25T16:50:46.647" UserId="227" />
  <row Id="5235" PostId="4842" Score="0" Text="The question seems to be more about the cleaning part than the anonymization part. Any ideas how to do the cleaning?" CreationDate="2015-01-25T16:56:47.333" UserId="227" />
  <row Id="5236" PostId="4928" Score="0" Text="Thank you. how can I implement the SIFT features in matlab? and May I know how to implement Bag of Phrases?" CreationDate="2015-01-25T17:54:55.853" UserId="7873" />
  <row Id="5237" PostId="4904" Score="0" Text="Thank you. Can you tell me how Bag of phrases is generated in image?" CreationDate="2015-01-25T17:56:12.247" UserId="7873" />
  <row Id="5238" PostId="4928" Score="0" Text="I don't recommend you to implement your own SIFT algorithm for purposes other than educational. I'm not a MATLAB user but I know VLFeat (http://www.vlfeat.org/matlab/vl_sift.html) provides a MATLAB API, which is what other libraries tend to use. You might want to create another question regarding &quot;bag of phrases&quot;." CreationDate="2015-01-25T19:46:52.310" UserId="4621" />
  <row Id="5239" PostId="4945" Score="0" Text="Nice pictures! Did you create them yourself? If yes, maybe you can share the code for drawing them?" CreationDate="2015-01-27T08:34:15.850" UserId="816" />
  <row Id="5240" PostId="4901" Score="0" Text="Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytes" CreationDate="2015-01-27T11:59:01.587" UserId="7867" />
  <row Id="5241" PostId="4919" Score="0" Text="your question is too vague for us to answer. Please give more details or an example. Are you talking about spatial autocorrelation or about objects that are co-located at a coordinate? Perhaps this is about coordinate quantization?" CreationDate="2015-01-27T16:04:38.770" UserId="7964" />
  <row Id="5242" PostId="4957" Score="2" Text="Are there any specific reasons to do the analysis inside Excel? Maybe exporting the data to a data science tool is more appropriate." CreationDate="2015-01-27T16:04:52.920" UserId="227" />
  <row Id="5244" PostId="4955" Score="0" Text="You can start with converting each email into tf-idf vector http://scikit-learn.org/stable/modules/feature_extraction.html. Finding the right number of K is a tricky problem, this might help http://stackoverflow.com/questions/1793532/how-do-i-determine-k-when-using-k-means-clustering" CreationDate="2015-01-27T16:14:45.930" UserId="7983" />
  <row Id="5245" PostId="4955" Score="0" Text="Very nice use case. But I think you are mixing &quot;feature extraction&quot;, &quot;classification accuracy&quot; and &quot;supervised learning&quot;. I mean not being able to parse an email easily does not imply you can not do supervised learning. It's all about knowing the classes in advance and having classified train data, and both can be gathered in your case." CreationDate="2015-01-27T16:20:42.827" UserId="227" />
  <row Id="5246" PostId="4957" Score="0" Text="I basically have Excel spreadsheets where we do a lot of our analysis and fed up of exporting the data to other tools. I just want to do the analysis in Excel." CreationDate="2015-01-27T16:21:27.297" UserId="7982" />
  <row Id="5247" PostId="4954" Score="0" Text="Can you explain more? Showing the way to find &quot;average monthly search&quot; for a given term is also appreciated." CreationDate="2015-01-27T16:25:52.127" UserId="227" />
  <row Id="5248" PostId="4957" Score="1" Text="Mathematica has some machine learning algorithms and can read Excel spreadsheets directly." CreationDate="2015-01-27T17:38:34.210" UserId="7980" />
  <row Id="5249" PostId="4954" Score="0" Text="Amir, see my update about Google Keyword Planner." CreationDate="2015-01-27T18:24:11.343" UserId="7961" />
  <row Id="5250" PostId="4901" Score="0" Text="Great. What correlations do you expect? Are you sure file size is helping you to find a correlation?" CreationDate="2015-01-28T01:35:26.287" UserId="4621" />
  <row Id="5251" PostId="4961" Score="0" Text="I work with many clients who do a lot of their work in Excel. You can mess up your R scripts like you can mess up in Matlab or anything else. I totally disagree." CreationDate="2015-01-28T09:55:23.497" UserId="7982" />
  <row Id="5252" PostId="4962" Score="1" Text="I guess you haven't had the experience I had with a lot of my clients working in Excel. &quot;Nobody does serious ML in Excel&quot;, really??? That's the best you can do?" CreationDate="2015-01-28T09:56:31.473" UserId="7982" />
  <row Id="5253" PostId="4966" Score="0" Text="That's what I was asking for." CreationDate="2015-01-28T10:39:38.717" UserId="7982" />
  <row Id="5254" PostId="4966" Score="0" Text="@Dimitris: You're welcome. Glad that you like my answer." CreationDate="2015-01-28T11:03:23.553" UserId="2452" />
  <row Id="5256" PostId="4914" Score="0" Text="This question could be qualified as too broad or not too broad, depending on how you look at it. If the question would imply a _detailed_ description of tasks and methods, that would be surely broad not only for a question, but even for a single book. However, I don't think that this question implies that _interpretation_. I believe that this question seeks a **framework** or a **taxonomy**, matching tasks with _approaches_ or _methods_ (_algorithms_ and _concepts_ should be ignored due to granularity issues). From that perspective, this answer is not too broad and, thus, is IMHO valid." CreationDate="2015-01-28T13:16:53.377" UserId="2452" />
  <row Id="5257" PostId="4914" Score="0" Text="@AleksandrBlekh Exactly a framework of the kind you mention is the intention of the question. I'm editing it to clarify. Thank you" CreationDate="2015-01-28T14:22:17.073" UserId="5143" />
  <row Id="5258" PostId="4962" Score="0" Text="If you want to make your work difficult, be my guest :) Once I was asked to deliver an ML presentation at a company that used Excel. They liked the content but were initially taken aback by the software. Well, I had better things to do than try to craft a silk purse out of a sow's ear. Give pandas a try; you'll quickly come to like it." CreationDate="2015-01-28T15:12:16.767" UserId="381" />
  <row Id="5259" PostId="4962" Score="1" Text="I have worked with Pandas and I regularly use R however I cannot dismiss Excel simply because there is a perception out there that its old, cumbersome and must go. I think there is a lot you can do with it and our clients rely on it." CreationDate="2015-01-28T16:52:08.723" UserId="7982" />
  <row Id="5260" PostId="4961" Score="0" Text="The fact that other people do it doesn't mean it is the right to thing to do. It is not about messing up because as you said, it is quite possible to make mistakes in R or Python. However, the difference is that you are more likely to  mess up in Excel than in other tools more suitable to analytics, statistics or machine learning. I added some more links in case you need convincing." CreationDate="2015-01-28T16:57:48.333" UserId="4621" />
  <row Id="5261" PostId="4961" Score="0" Text="I have been using and customising Excel for all sorts of things for the last 15 years. I am sure you can produce 100 links discussing how bad it can be but from personal experience I know you can control it and use all its capabilities without messing up anything. Visual programming which is what Excel offers is a brilliant concept. Just because something is trendy like Python at the moment doesn't necessarily mean is the best option." CreationDate="2015-01-28T17:08:28.527" UserId="7982" />
  <row Id="5262" PostId="4919" Score="0" Text="Here is a document: http://users.cis.fiu.edu/~taoli/pub/Lu-06589200-TMM.pdf" CreationDate="2015-01-28T17:10:16.407" UserId="7873" />
  <row Id="5263" PostId="4961" Score="0" Text="And how do you know you're not messing up anything? People make mistakes precisely because they are not easy to spot. Again, this happens in all sorts of environments, but it is more likely in Excel. However, if you want to keep using Excel for heavy calculations, go ahead." CreationDate="2015-01-28T18:18:18.003" UserId="4621" />
  <row Id="5264" PostId="4961" Score="0" Text="I know I am not messing up anything because I design the entire thing and have control. So far I haven't had any difficult situation in Excel. I guess Microsoft Research looking into using Excel Add-ins says something to you." CreationDate="2015-01-28T19:20:41.647" UserId="7982" />
  <row Id="5265" PostId="4941" Score="0" Text="I will accept the answer after you provide a summary from the external link. Thanks." CreationDate="2015-01-28T19:49:34.730" UserId="3466" />
  <row Id="5266" PostId="4914" Score="0" Text="@Javierfdr: You're welcome." CreationDate="2015-01-28T20:43:01.783" UserId="2452" />
  <row Id="5267" PostId="4952" Score="0" Text="That depends on how you receive/process the training data. If you receive the training examples in a batch (without a time associated with each example) and you want to build a classifier that you will apply to all future observations, then no, you do not need to update the distribution parameters. But if you are attempting to do [online learning](http://en.wikipedia.org/wiki/Online_machine_learning) where the classifier is updated after each example, then you may want to update the parameters (e.g., using the `N` previous observations)." CreationDate="2015-01-29T00:52:13.443" UserId="964" />
  <row Id="5268" PostId="4967" Score="0" Text="Use both variables and let the model figure out the coefficients?" CreationDate="2015-01-29T03:57:08.760" UserId="381" />
  <row Id="5269" PostId="4955" Score="0" Text="@SaurabhSaxena Running TF-IDF in real time as and when we receive a new alert email is impossible, as TF-IDF needs to run on the whole document corpus and not just the single new email. Thanks for the links and info." CreationDate="2015-01-29T06:09:22.493" UserId="7979" />
  <row Id="5270" PostId="4955" Score="0" Text="@AmirAliAkbari I do not understand. The use case I have, has to have the mix of &quot;feature extraction&quot; of important info from the mail, &quot;supervised clustering&quot; of emails into groups for email routing and &quot;classification&quot; of new emails into proper cluster with accuracy. Accuracy is important here because a CPU Load on app server alert email should be classified as MySQL DB server disk space alert mail. :)" CreationDate="2015-01-29T06:13:31.570" UserId="7979" />
  <row Id="5271" PostId="4972" Score="0" Text="Interesting. On a somewhat different problem I ended up using an expectation maximization approach, where where you  ascribe a &quot;probability of belonging to a cluster&quot; rather than strict for every sample, and a &quot;probability of electrical output given that you belong to said cluster&quot;, and maximize over probabilities.&#xA;&#xA;Thanks for the link (I'd upvote if I had the reputation for it), I'll look into it..." CreationDate="2015-01-29T09:41:02.003" UserId="7999" />
  <row Id="5272" PostId="4963" Score="0" Text="Just a comment - there's another good tool comparable with Spark which is called &quot;Apache Flink&quot;. In some cases it's better, so you might want to check it out as well." CreationDate="2015-01-29T11:19:36.933" UserId="816" />
  <row Id="5274" PostId="4972" Score="0" Text="Probability also makes sense, though I find it an overkill for simple tasks. When I face same tasks which involve regression and hour of day do matter I usually rely on simple 'hours to midnight' transformation - in majority of problems out there it works best of all. I just calculate the amount of hours between current day hour and the midnight (if hour&gt;12 return 24 - hour, else return hour). More complicated transformations usually cause my models to overfit." CreationDate="2015-01-29T16:09:23.243" UserId="7969" />
  <row Id="5275" PostId="4914" Score="0" Text="@SeanOwen I modified the main question. Please tell me if is still broad and I would need to make it sharper. Thx!" CreationDate="2015-01-29T16:41:34.967" UserId="5143" />
  <row Id="5277" PostId="4987" Score="0" Text="Could you please explain more detaily. I will give the list of my data(an array) to scale() function and it will return an array. Then should I divide my real list of data to the other array(which is the result of scale) ?" CreationDate="2015-01-29T20:52:30.333" UserId="8018" />
  <row Id="5278" PostId="974" Score="1" Text="I've just been investigating this area and wrote a blog post about what I found.  I used an LSH, but I think my sparsity level was higher than you are looking for.  http://tttv-engineering.tumblr.com/post/109569205836/scaling-similarity" CreationDate="2015-01-30T11:04:54.227" UserId="8030" />
  <row Id="5280" PostId="4990" Score="0" Text="This is not related to my question." CreationDate="2015-01-30T12:43:44.360" UserId="3430" />
  <row Id="5281" PostId="4952" Score="0" Text="I believe my question was not clearly stated initially." CreationDate="2015-01-30T12:45:16.903" UserId="3430" />
  <row Id="5282" PostId="4990" Score="0" Text="The model you're describing is an instance of a cascading classifier. Therefore, this is a legitimate practice used fairly frequently, so this information is very related to your question." CreationDate="2015-01-30T21:11:36.943" UserId="4621" />
  <row Id="5283" PostId="4990" Score="0" Text="I noticed you changed your question. If the first model was computed without looking at the labels and then its output was taken as features for the second classifier, that's not cheating. You don't say if the first model was built from cross validation, though." CreationDate="2015-01-30T21:17:57.050" UserId="4621" />
  <row Id="5284" PostId="4997" Score="1" Text="Can you please provide us with some information on both datasets/links? This will indeed ease the burden of those looking for specific types of data set. Take a look at other posts to see what kind of information your references are missing." CreationDate="2015-01-30T21:31:13.203" UserId="84" />
  <row Id="5286" PostId="4981" Score="0" Text="ok, so if one of the input is 2d then it is expanded, yes, this is exactly what i wanted to know...  thanks a lot" CreationDate="2015-01-31T09:56:02.603" UserId="8013" />
  <row Id="5287" PostId="4978" Score="0" Text="It's not clear what you mean. Is the input to SVM a vector? yes, that's true for most ML algorithms. Multiple vectors? yes, training happens on &gt; 1 data point. 2D? yes. Somehow, several 2D vectors at once? not clear what that means. Please clarify." CreationDate="2015-01-31T11:32:41.617" UserId="21" />
  <row Id="5288" PostId="5001" Score="2" Text="Have you searched at the Open Data Stack Exchange? http://opendata.stackexchange.com/" CreationDate="2015-01-31T00:07:33.683" UserId="8005" />
  <row Id="5291" PostId="5001" Score="0" Text="I very much doubt that you'll find that kind of data as **general** open data sets. You will have much more chances to find what you're looking for in **industry-focused** databases or **market research** data sets." CreationDate="2015-01-31T20:19:43.320" UserId="2452" />
  <row Id="5292" PostId="4978" Score="1" Text="I have got my answer from climbs. Thank you!" CreationDate="2015-02-01T05:05:48.883" UserId="8013" />
  <row Id="5293" PostId="5002" Score="2" Text="Thanks Aleksandr. I agree with your answer. But I just want to hear opinion of other experts in base of their experiences and their sense with various interfaces. For example you maybe work with various frameworks. I want to know which of them has better interface in your opinion. Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. As your answer detailed comparison is so lengthy and this is not my purpose." CreationDate="2015-02-01T07:09:09.393" UserId="7977" />
  <row Id="5294" PostId="5002" Score="2" Text="I edited my question based on your answer. Thanks:)" CreationDate="2015-02-01T07:16:00.560" UserId="7977" />
  <row Id="5295" PostId="5002" Score="2" Text="@OmidEbrahimi: You're very welcome :-). I very much understand the reasoning behind your question. It's just that this type of questions (opinion-based) are not in favor on StackExchange (and I actually disagree with that, but most of the time try to follow the guidelines). Despite that and surprisingly, opinion-based questions and answers are no such rarity on StackExchange. I guess, it's more of issue of moderators' discretion and personal (subjective) tolerance for a particular question/topic. My main point is that SE is unlikely the best outlet for a _comprehensive_ treatment of the topic." CreationDate="2015-02-01T07:40:34.867" UserId="2452" />
  <row Id="5296" PostId="4990" Score="0" Text="There is only one model. I did reworded my question since it seems people are confused." CreationDate="2015-02-01T12:40:21.317" UserId="3430" />
  <row Id="5297" PostId="5000" Score="0" Text="would you consider a function which more accurately reflects the data you have rather than a linear regression model ?" CreationDate="2015-02-01T13:26:46.637" UserId="7980" />
  <row Id="5298" PostId="5000" Score="0" Text="I tried different models, but so far all of them sometimes output negative values. This is not really wrong - if you think of linear regression as a line it is possible that it is not able to build a function which will be ALWAYS positive, or even carefully 'touch' the axis (to output 0s). But once again, whatever I tried, for new data I sometimes keep getting negatives." CreationDate="2015-02-01T16:39:48.950" UserId="7969" />
  <row Id="5299" PostId="5010" Score="0" Text="Is it possible to do this using HUE?" CreationDate="2015-02-01T18:35:26.717" UserId="5224" />
  <row Id="5300" PostId="5009" Score="0" Text="Is it possible to do this using HUE?" CreationDate="2015-02-01T18:36:06.860" UserId="5224" />
  <row Id="5301" PostId="5000" Score="0" Text="Hard to know for definite without sight of the data, but one could imagine a piecewise linear approximation that could be shaped not to go negative, or a high order polynomial perhaps. :)" CreationDate="2015-02-01T18:58:39.890" UserId="7980" />
  <row Id="5302" PostId="5011" Score="0" Text="Thank you Jake for the second point, as it is really accurate - I've somehow forgotten that you should not touch the test set in model building phase :)." CreationDate="2015-02-01T22:31:12.023" UserId="8017" />
  <row Id="5303" PostId="5011" Score="0" Text="Regarding the first one - the thing is I cannot assume that every &quot;sub-training&quot; set is statistically representative. For many classification problems with highly imbalanced class labels, it is recommended NOT to use the real fraction of class labels expected in future, but rather to make them equally big (by under/over sampling for example). Thus, I cannot assume that resampled training set characteristics are representative over the real population." CreationDate="2015-02-01T22:37:40.467" UserId="8017" />
  <row Id="5304" PostId="4941" Score="1" Text="I just updated the answer to add a summary" CreationDate="2015-02-02T07:32:25.093" UserId="7950" />
  <row Id="5305" PostId="4986" Score="1" Text="What range you normalise to would depend on what you wish to do with the transformed data." CreationDate="2015-02-02T11:50:30.607" UserId="7980" />
  <row Id="5306" PostId="5011" Score="0" Text="@Matek It is quite common to resample the training set and include the resampled sets in an aggregated predictor, in Bagging for instance. So the principle that resampling can still be considered representative is established in some contexts. Whilst for your final classifier you might want to use all available data to build your mode, train and test sets, during model exploration any input from the test set during training will pollute your model and skew your results. Class balancing is often an attempt to minimise a loss function on predictions rather than maximise prediction accuracy." CreationDate="2015-02-02T12:07:55.633" UserId="7980" />
  <row Id="5307" PostId="5015" Score="0" Text="i will try to take a further look at logistic regression. Do you know any good source/webpage which explains logistic regression as easy as possible?" CreationDate="2015-02-02T18:13:46.427" UserId="8063" />
  <row Id="5308" PostId="5015" Score="0" Text="Is pretty well explained in this set of notes http://cs229.stanford.edu/notes/cs229-notes1.pdf" CreationDate="2015-02-02T19:13:16.487" UserId="8065" />
  <row Id="5309" PostId="2360" Score="0" Text="Will you clarify one point, the value transform from one scale to another in relation to the parentheses are:`target scale`(`current scale`)" CreationDate="2015-02-02T23:36:31.750" UserId="2742" />
  <row Id="5310" PostId="5024" Score="0" Text="This is too broad. What specifically are you hoping to learn? what problems are you trying to solve?" CreationDate="2015-02-03T11:49:19.860" UserId="21" />
  <row Id="5311" PostId="5014" Score="0" Text="Their calculation looks simply wrong. The F1 column is not the harmonic mean, not even to the number of decimal places shown." CreationDate="2015-02-03T11:55:51.240" UserId="21" />
  <row Id="5312" PostId="5024" Score="0" Text="I'm trying to learn how to use Genetic Algorithms and try to learn when to apply, for example answers like Stephan are welcome to improve the investigation with Genetics and R." CreationDate="2015-02-03T11:58:43.380" UserId="8076" />
  <row Id="5313" PostId="5018" Score="0" Text="Looks like it does not work somehow on mine Hue.. Probably I have mb older version or something" CreationDate="2015-02-03T17:39:12.493" UserId="5224" />
  <row Id="5314" PostId="5025" Score="0" Text="Thank you so much Stephan! I will do a deep sight in this packages." CreationDate="2015-02-03T18:57:22.063" UserId="8076" />
  <row Id="5315" PostId="5019" Score="0" Text="Hmmm, interesting, thank you! I will try it, are there any good links on GLM\Poisson regression articles which you could suggest?" CreationDate="2015-02-03T21:13:15.067" UserId="7969" />
  <row Id="5316" PostId="5022" Score="0" Text="Thanks, your answer and some extra research seems to indicate to me that pure statistical data would be better. I'm not worried much about ranking...this is more along the lines of analyzing a trade for players in a fantasy sports league to determine if trading player A for player B would be recommended or not. Hope that helps you better understand my intention :)" CreationDate="2015-02-04T00:02:09.220" UserId="8074" />
  <row Id="5317" PostId="5019" Score="0" Text="I'm not aware of any particularly outstanding general guides on Poisson regression. As a first step, I would recommend looking up GLMs in whatever programming language you are comfortable with.  Most implementations provide support for Poisson regression and getting started is often as easy as plugging in your training data and specifying a &quot;Poisson&quot; error distribution or a &quot;log&quot; link function." CreationDate="2015-02-04T02:40:43.307" UserId="182" />
  <row Id="5318" PostId="5016" Score="0" Text="SVMs are more suitable for our use case but my team wanted to explore more into deep networks and how will these help us. DBNs solved our water scheduling problem with .1% more accuracy and lesser time as compared to shallow networks so we are wondering if such an approach might help in this case as well. How would you suggest we deal with point clouds data for input to network where it has (x,y,z,r,g,b) values for each point ?" CreationDate="2015-02-04T06:30:18.020" UserId="8051" />
  <row Id="5319" PostId="5039" Score="0" Text="No luck with this solution.  I'm also only using 100 examples/cases." CreationDate="2015-02-04T06:51:14.040" UserId="985" />
  <row Id="5320" PostId="5039" Score="0" Text="Can you expect the output values from each iteration of the optimization? Is it ever non-zero?" CreationDate="2015-02-04T07:02:22.573" UserId="182" />
  <row Id="5321" PostId="5042" Score="0" Text="yes I have other features like num of room, num of stars.. but my question ih how the model predict the number oclicks for each num if hotel and it dont know num of hotel if i dont use these feature in training model?" CreationDate="2015-02-04T07:25:32.930" UserId="8088" />
  <row Id="5322" PostId="5039" Score="0" Text="yeah, if I understand what you're saying. I'm starting with initial_theta = np.array([0,0,0]) and when calculating line by line I get a result of final_wealth = 0.1335, then I tried initial_theta = np.array([-0.1,0,0]) and got final_wealth = 0.0433.  So that part is working properly (and same as my MATLAB solution).  But I also tried changing the initial_theta to non-zero values and running the optimization function and ended up with that same non-zero initial_theta.  So it seems that the optimization isn't running any iterations..." CreationDate="2015-02-04T07:26:39.603" UserId="985" />
  <row Id="5323" PostId="5039" Score="0" Text="Sorry, that should say &quot;inspect&quot; rather than &quot;expect&quot;. It seems like you caught the drift though." CreationDate="2015-02-04T07:28:38.170" UserId="182" />
  <row Id="5324" PostId="5043" Score="1" Text="I have change the description of ambiguous description of Y, actually it is a continuous value from zero to one, not a binary value of zero or one." CreationDate="2015-02-04T07:32:28.260" UserId="7867" />
  <row Id="5325" PostId="5043" Score="0" Text="In that case I would suggest to examine scatter plots between each of your X variables versus y (instead of histograms). Additionally since your target data is bound between 0 and 1, you may still want to consider trying logistic regression." CreationDate="2015-02-04T07:41:43.963" UserId="182" />
  <row Id="5326" PostId="5039" Score="0" Text="fyi, the optimization is exceeding the max iterations, yet the result is not changing at all from the initial_theta" CreationDate="2015-02-04T07:51:36.820" UserId="985" />
  <row Id="5328" PostId="5014" Score="0" Text="Sounds like i should contact one of the authors of the paper." CreationDate="2015-02-04T12:49:27.307" UserId="8063" />
  <row Id="5329" PostId="5043" Score="2" Text="@RyanJ.Smith you can't do logistic regression on real values. You can do beta regression on numbers bound to the [0,1] interval, or you can do linear regression on the logit transform" CreationDate="2015-02-04T13:07:21.407" UserId="1399" />
  <row Id="5330" PostId="5033" Score="0" Text="@Monozygotic thank you for reply. . Euh sincerely i unserstand a little bit.. Sorry but in wich step i include the id_hotel? It is unknown feature for the model" CreationDate="2015-02-04T00:26:12.163" UserId="8088" />
  <row Id="5331" PostId="5030" Score="0" Text="This is effectively a link-only answer and those are discouraged." CreationDate="2015-02-04T15:47:07.533" UserId="21" />
  <row Id="5332" PostId="5016" Score="0" Text="Remember DBNs and CNNs are not the same thing, they sound similar, but they are different architectures. Your input dimension is clearly to big, so you should segment it in small windows and pretrain the CNN with those windows first.&#xA;Then you should do a sweep over each lidar image, augmenting in that way your dataset, from 4500 to way more. &#xA;&#xA;If yoh have a single label per lidar candidate, you probably want to pool the windows for each candidate." CreationDate="2015-02-04T16:06:03.680" UserId="8065" />
  <row Id="5333" PostId="5032" Score="0" Text="To add to this, this resource includes a visual demonstration of how an ROC curve is constructed by varying the thresholds (about 2/3 down the page): http://www.r-bloggers.com/roc-curves-and-classification/" CreationDate="2015-02-04T17:21:33.547" UserId="525" />
  <row Id="5334" PostId="5034" Score="0" Text="Add all of these comments in their respective answers, you are just adding clutter in this way" CreationDate="2015-02-04T17:53:10.530" UserId="8065" />
  <row Id="5335" PostId="5043" Score="0" Text="Good catch, Ben.  I had forgotten that though the output from logistic regression is continuous, the response is originally assumed to be Bernoulli (binary)." CreationDate="2015-02-04T17:53:46.163" UserId="182" />
  <row Id="5336" PostId="111" Score="0" Text="So...Mobile App would write directly to DynamoDB or go via a web layer(REST API) that would write to DynamoDB? Any pros/cons or comments on that?" CreationDate="2015-02-04T18:33:51.763" UserId="8098" />
  <row Id="5337" PostId="5043" Score="0" Text="+1 for &quot;It is also possible that there is not a strong relationship between any of your features and your targets.&quot; This is the first explanation that would come to my mind. Prediction is often *hard*." CreationDate="2015-02-04T20:02:27.120" UserId="2853" />
  <row Id="5338" PostId="5034" Score="0" Text="I'm not allowed to leave comments yet..." CreationDate="2015-02-05T02:26:36.137" UserId="8075" />
  <row Id="5339" PostId="5034" Score="0" Text="Could you maybe tell me which part of my answer should have been placed in a comment instead? This was actually the first answer posted to the question, so I couldn't place anything as a comment to other answers anyway. I don't really see what's wrong with the answer..." CreationDate="2015-02-05T02:45:06.033" UserId="8075" />
  <row Id="5340" PostId="5038" Score="2" Text="What type of data is `y`?  Is it an int or a float?  Do you use `from __future__ import division`?" CreationDate="2015-02-05T09:36:42.253" UserId="8110" />
  <row Id="5341" PostId="5024" Score="0" Text="Hi Sean, I've focused more my question, is it OK?" CreationDate="2015-02-05T11:19:36.340" UserId="8076" />
  <row Id="5342" PostId="5034" Score="0" Text="You have two answers to the same question, your second answer should be a comment to the first, or just edit the first answer" CreationDate="2015-02-05T17:20:13.240" UserId="8065" />
  <row Id="5343" PostId="5058" Score="0" Text="I will be performing linear regression as part of my analysis, but would like to take it further by using ML." CreationDate="2015-02-05T21:32:42.050" UserId="6683" />
  <row Id="5344" PostId="5056" Score="0" Text="Thanks, that makes sense. Would I also use the features `athlete`, `tracktype`, `coach` and `location` as factors (although, in my actual data I have hundreds of athletes, coaches (and other features))?" CreationDate="2015-02-05T21:36:24.430" UserId="6683" />
  <row Id="5346" PostId="5064" Score="0" Text="This is a [spoke-hub](http://en.wikipedia.org/wiki/Spoke-hub_distribution_paradigm) or [star network](http://en.wikipedia.org/wiki/Star_network). Do you have a more specific question?" CreationDate="2015-02-06T06:02:47.590" UserId="381" />
  <row Id="5348" PostId="5038" Score="0" Text="y is an int... changed it to a float and solved the issue! thanks a ton!" CreationDate="2015-02-06T07:14:06.950" UserId="985" />
  <row Id="5349" PostId="5039" Score="1" Text="TheBlackCat solved it, but thanks for your help Ryan!" CreationDate="2015-02-06T07:15:07.750" UserId="985" />
  <row Id="5351" PostId="5039" Score="0" Text="Make sure that the correct answer gets posted as an answer and marked correct, not just left as a comment above." CreationDate="2015-02-06T09:13:21.833" UserId="182" />
  <row Id="5352" PostId="5038" Score="0" Text="Great!  I have added the answer as an answer below, please mark it as correct so others can find it more easily." CreationDate="2015-02-06T09:42:57.257" UserId="8110" />
  <row Id="5353" PostId="5056" Score="0" Text="Of course. Better to use as many features as you have at the beginning and then check bias\variance of the result - i.e split your training data into 2 sets, try to train algorithm on first subset and check the outcome on the second set. If it will 'overlearn' with specified amount of features you may want to think about dropping out one of the features (for example, coach) to check if it improves the performance of your model." CreationDate="2015-02-06T14:46:07.693" UserId="7969" />
  <row Id="5354" PostId="5056" Score="0" Text="You may also want to try to transform your original features in some way (like I mentioned for # of race (first second etc). For example one good idea would be to transform quantitative features (like height or weight) into cathegorical or binary features (&gt; 6 feet and &lt; 6 feet or 'between 60 and 70 KG, between 70 and 80 KG'). General advice is that you could get a really good results with rather simple algorithm (Linear Regression for example) but with a good selection of features." CreationDate="2015-02-06T14:50:30.430" UserId="7969" />
  <row Id="5355" PostId="5053" Score="0" Text="What do you mean by `chi-square difference`? Is this a goodness of fit test using Pearson's chi-squared test? It is not clear from Vik's code because I think he wrote a function to perform that operaton." CreationDate="2015-02-06T20:03:48.263" UserId="4621" />
  <row Id="5356" PostId="5064" Score="0" Text="Yes but a multi-hop star network can have a single node sending info to multiple recipients. Also, in spoke-hub a single node can receive from multiple nodes. I am looking for a model where there are radial lines coming out from a single node. Each radial line comprises cascaded nodes. Also, I want to know if a social network can mimic such network in practice." CreationDate="2015-02-06T20:12:38.660" UserId="8127" />
  <row Id="5357" PostId="5048" Score="0" Text="How do you read line by line in OCR?" CreationDate="2015-02-07T01:22:14.147" UserId="4621" />
  <row Id="5358" PostId="5053" Score="0" Text="@RobertSmith please look at the function `corpora::chisq` and this link for description: http://rpackages.ianhowson.com/rforge/corpora/man/chisq.html" CreationDate="2015-02-07T09:58:12.067" UserId="8059" />
  <row Id="5359" PostId="5056" Score="0" Text="All very useful, thanks. When I start delving deeper into this I'm sure I'll be back with more questions!" CreationDate="2015-02-07T10:51:00.023" UserId="6683" />
  <row Id="5360" PostId="5053" Score="0" Text="Great. Thank you!" CreationDate="2015-02-07T16:47:27.623" UserId="4621" />
  <row Id="5361" PostId="5079" Score="1" Text="Sure. That is a good idea. The general term for this kind of techniques is &quot;model order reduction&quot;. SVD is routinely used for this purpose (also called singular value decomposition.) Keep in mind that is advisable to demean your matrix before using SVD. Also, most metrics have issues in high dimensional spaces, so be careful there." CreationDate="2015-02-08T06:47:25.977" UserId="4621" />
  <row Id="5362" PostId="5082" Score="0" Text="Thanks, looks interesting, but probably not suitable for my needs. First, I don't have a boolean input, and second, I can have a large set of items to base recommendations on, so I can't precalculate frequently used sets of base items." CreationDate="2015-02-08T16:44:16.137" UserId="8158" />
  <row Id="5363" PostId="5079" Score="0" Text="In response to your update, take a look at [this answer](http://stats.stackexchange.com/a/93281/2676) about demeaning your matrix. You're using a 30x30 matrix, right? It shouldn't be slow at all. However, a  20,000x20,000 matrix is probably too much. In the answer I linked above, it talks about an incremental SVD. It is also a good idea to read the approaches used in the Netflix competition to pick up some tricks to deal with huge matrices in this context." CreationDate="2015-02-08T20:10:22.670" UserId="4621" />
  <row Id="5364" PostId="5084" Score="0" Text="I also thought about it in those terms. The problem is that even in chain-of-command structures, there is communication between people within the same rank and there is also communication from one-to-many between different levels of the tree." CreationDate="2015-02-08T20:20:41.830" UserId="4621" />
  <row Id="5365" PostId="5084" Score="0" Text="Yeah, I don't think this kind of graph will occur naturally in a system. Though in a military, officers at the same level cannot issue orders to others on the same level. And officers can't issue orders to juniors that are under a different officer." CreationDate="2015-02-08T20:33:35.193" UserId="90" />
  <row Id="5366" PostId="5084" Score="0" Text="Sure, although in the restrictive case of communicating orders, one officer can issue orders to several juniors." CreationDate="2015-02-08T20:39:30.977" UserId="4621" />
  <row Id="5367" PostId="5084" Score="0" Text="Sure, multiple reciever nodes are still allowed if I understood what you asked correctly - _Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender._" CreationDate="2015-02-08T20:56:00.980" UserId="90" />
  <row Id="5368" PostId="1175" Score="0" Text="Sean, silly, but can I confirm with you, this whole idea is really about making importance weights (as shown in the input format required by VW) work (and not overshoot)?" CreationDate="2015-02-08T20:59:02.630" UserId="1138" />
  <row Id="5369" PostId="1175" Score="1" Text="yes that's right" CreationDate="2015-02-08T21:48:08.620" UserId="1256" />
  <row Id="5370" PostId="5084" Score="0" Text="Well, I didn't ask it but I understood it differently. I thought that the picture in the question disallowed several recipients for a single sender." CreationDate="2015-02-09T01:53:20.803" UserId="4621" />
  <row Id="5371" PostId="5034" Score="0" Text="Ah I see. I'll keep that in mind for next time. :)" CreationDate="2015-02-09T05:23:35.310" UserId="8075" />
  <row Id="5372" PostId="5081" Score="1" Text="Thanks, nice answer! Unfortunately I can not give point to that because I do not have enough reputation." CreationDate="2015-02-09T07:36:58.740" UserId="3346" />
  <row Id="5373" PostId="5081" Score="0" Text="@user115415: You're quite welcome! No problem - feel free to upvote/accept, when you reach reputation of 15 or more (should be pretty soon :-)." CreationDate="2015-02-09T08:23:36.520" UserId="2452" />
  <row Id="5374" PostId="5080" Score="5" Text="I've flagged this to be closed as &quot;unclear what you are asking&quot;, because the answer to your question as posed is obviously &quot;yes&quot;. Of course they do. What are you really asking? Because if that **is** what you are really asking then I dread to think what questions you ask of your data as a data scientist..." CreationDate="2015-02-09T12:15:27.513" UserId="471" />
  <row Id="5377" PostId="5091" Score="0" Text="You may want to change the question title, since you question title doesn't really match what you want to ask." CreationDate="2015-02-10T02:08:09.487" UserId="1003" />
  <row Id="5379" PostId="5094" Score="0" Text="thank you very much for you reply, it helps me, but is it possible  to count the number of people currently working in the same company?" CreationDate="2015-02-10T09:33:35.477" UserId="8088" />
  <row Id="5380" PostId="5094" Score="0" Text="Yes, after you extract the company names for each people, you can reference the hadoop/spark word count examples to count the number of people in the same company." CreationDate="2015-02-10T09:46:02.293" UserId="1003" />
  <row Id="5381" PostId="968" Score="1" Text="I'm voting to close this question as off-topic because its a simple R programming question and should be on http://stackoverflow.com or http://stats.stackexchange.com/" CreationDate="2015-02-10T10:26:30.023" UserId="471" />
  <row Id="5382" PostId="5094" Score="0" Text="Thank you, Do you think that Hive regex, or Pig regex, can be useful to extract company's names?" CreationDate="2015-02-10T12:45:09.037" UserId="8088" />
  <row Id="5383" PostId="5094" Score="0" Text="Of course. If you already know the rules to extract all company names, just use the regex to extract the names." CreationDate="2015-02-10T13:56:17.170" UserId="1003" />
  <row Id="5384" PostId="5094" Score="0" Text="Ok, i'am trying to use Hive Regex to do this, but I have no idea after this how to count the number of people currently working in the same company. Should I use map reduce algorithm ?" CreationDate="2015-02-10T14:01:23.120" UserId="8088" />
  <row Id="5386" PostId="5094" Score="0" Text="Please try to search &quot;hadoop word count&quot; on Google, and you will get ideas." CreationDate="2015-02-10T14:07:01.053" UserId="1003" />
  <row Id="5389" PostId="5084" Score="0" Text="As Mr. Smith pointed out, I am disallowing several recipients for single sender. Anyways I think I should still select this answer. I do not think this question is wellposed and it may not occur in practice. The motivation for asking this question was the following. For such topology, the observation at the bottommost or the leaf nodes are independent given the source symbol. Most of the social networks that I have seen have links going all over the place." CreationDate="2015-02-10T20:47:02.977" UserId="8127" />
  <row Id="5390" PostId="968" Score="1" Text="As R is a primary data science tool, I don't find this off topic IMHO." CreationDate="2015-02-11T02:06:43.697" UserId="21" />
  <row Id="5391" PostId="5108" Score="0" Text="What's the category of the &quot;Advice for new faculty members&quot;?" CreationDate="2015-02-11T08:15:00.033" UserId="587" />
  <row Id="5392" PostId="5108" Score="0" Text="@LauriK: Definitely not the one that has been selected." CreationDate="2015-02-11T08:22:39.830" UserId="2452" />
  <row Id="5393" PostId="5111" Score="1" Text="Thanks a lot; although this is more about finance and statistics, I think it'd come in handy in due time. Also, man, that's a lot of books :-)" CreationDate="2015-02-11T13:01:45.940" UserId="8214" />
  <row Id="5394" PostId="5111" Score="0" Text="@Chiffa: You're very welcome! &quot;Lots of books&quot;: I prefer to under-promise and over-deliver, not vice versa :-). Seriously, though, it is just my initial selection, which is more of a tip of an iceberg. If you noticed, I've included several ML-specific books, as I understand that's your current focus." CreationDate="2015-02-11T13:10:20.860" UserId="2452" />
  <row Id="5395" PostId="5111" Score="0" Text="Indeed; those are the ones I'm going to look into first." CreationDate="2015-02-11T13:12:24.973" UserId="8214" />
  <row Id="5396" PostId="5112" Score="1" Text="Thank you for the answer (+1). I don't think that the hierarchical structure of the classification path necessarily implies the use of hierarchical clustering (HC) approach. They might be using some fancy ontology-based or other technique, different from HC. I'm not sure that they are doing a good job - as I said, I've seen multiple similar errors in the books section, which is one of Amazon's major e-commerce channels." CreationDate="2015-02-11T13:34:45.507" UserId="2452" />
  <row Id="5397" PostId="5113" Score="0" Text="What do you do with that split? Can you give us more context on that?" CreationDate="2015-02-11T13:44:08.033" UserId="108" />
  <row Id="5398" PostId="5114" Score="0" Text="So, rule of thumb it is. Thanks. As for cross-validation, I haven't read that far yet, so it might cover it." CreationDate="2015-02-11T14:15:23.477" UserId="8214" />
  <row Id="5399" PostId="5112" Score="1" Text="@AleksandrBlekh Yep, it's just a thought." CreationDate="2015-02-11T14:16:15.483" UserId="8113" />
  <row Id="5400" PostId="5114" Score="0" Text="As we know, 80/20 rule aka Pareto principle is based on on the [Pareto distribution](http://en.wikipedia.org/wiki/Pareto_distribution). Consequently, the 80/20 split is based on the specific value of the Pareto index ($\alpha \approx$ 1.161). What I'm curious about is whether the two-way and three-way data splits have an **analytical solution**, based on the parameters you've mentioned (&quot;sample size, distributions and relationships between your variables&quot;) as well as Pareto distribution's parameters." CreationDate="2015-02-11T14:16:22.953" UserId="2452" />
  <row Id="5402" PostId="5111" Score="0" Text="@Chiffa: Good luck! Feel free to accept a particular answer, if you are satisfied with it." CreationDate="2015-02-12T00:52:17.167" UserId="2452" />
  <row Id="5403" PostId="5099" Score="0" Text="That's along the lines my train of thought is going, thanks." CreationDate="2015-02-12T02:02:42.473" UserId="6683" />
  <row Id="5404" PostId="5097" Score="0" Text="I think most common ones are `numpy`, `scipy`, `scikit-learn` and `pandas`." CreationDate="2015-02-12T04:47:22.887" UserId="3070" />
  <row Id="5405" PostId="5080" Score="0" Text="Basically, you are asking if someone can find you a job.  That's spam." CreationDate="2015-02-12T04:55:28.853" UserId="8116" />
  <row Id="5406" PostId="5111" Score="0" Text="@AleksandrBlekh wow, thanks for crediting me :)" CreationDate="2015-02-12T09:07:32.143" UserId="8202" />
  <row Id="5407" PostId="5111" Score="0" Text="@AlexanderDidenko: You're welcome. :-) Feel free to suggest other relevant sources - I will be glad to update my answer." CreationDate="2015-02-12T09:21:11.847" UserId="2452" />
  <row Id="5409" PostId="5118" Score="0" Text="@bogatron: speaking of positive and negative examples can be confusing since the dataset has 4 different classes, the subset with weather = sunny just happens to have just two." CreationDate="2015-02-12T13:50:24.163" UserId="8191" />
  <row Id="5410" PostId="5118" Score="0" Text="You are correct - I mistakenly looked at the `Parents` attribute which only has two values (`Yes/No`) and just happened to work out to the same entropy value for `Weather=Sunny`. The answer is correct now." CreationDate="2015-02-12T15:05:15.893" UserId="964" />
  <row Id="5411" PostId="5118" Score="0" Text="bogatron: Thanks for updating !" CreationDate="2015-02-12T15:51:59.077" UserId="8191" />
  <row Id="5412" PostId="5118" Score="0" Text="Unfortunately, I can't upvote you since I don't have enough points." CreationDate="2015-02-12T15:53:19.787" UserId="8191" />
  <row Id="5413" PostId="5135" Score="0" Text="Thanks for the insight. You're right the survival table is just the training data set. I guess what confused me a little is the fact that the example just used the survival table to predict against the test data, without explicitly stating the algorithm used - it basically used the survival table as a lookup table to predict the outcome for each test case  passenger. My question should have been whether this specific approach was an example of using Naive Bayes." CreationDate="2015-02-13T09:19:59.850" UserId="8234" />
  <row Id="5414" PostId="5117" Score="0" Text="Thank you.  Shall up-vote once I clear the up-vote threshold." CreationDate="2015-02-13T09:30:48.003" UserId="8221" />
  <row Id="5415" PostId="5118" Score="0" Text="@ee2Dev Thank you.  Shall up-vote once I clear the up-vote threshold." CreationDate="2015-02-13T09:41:44.280" UserId="8221" />
  <row Id="5416" PostId="5118" Score="0" Text="user1744649: Thanks if my answer was helpful you can also accept my answer." CreationDate="2015-02-13T09:47:08.627" UserId="8191" />
  <row Id="5417" PostId="5135" Score="0" Text="No, it is not. It is even simpler than Naive Bayes; it looks like a simplified version of k-nearest neighbours classification. They group together or cluster the training examples. Each cluster contains the examples in the training set which have the same values in the input variables. Each cluster's class is computed as the rounded mean of the surviving value for the individuals in the cluster, and the results are stored in a lookup table. &#xA;&#xA;The lookup table assigns an output value to each test example depending on its input values." CreationDate="2015-02-13T10:00:59.633" UserId="2576" />
  <row Id="5418" PostId="5135" Score="0" Text="Cool, thanks a lot." CreationDate="2015-02-13T10:52:48.140" UserId="8234" />
  <row Id="5420" PostId="5127" Score="0" Text="Why do you have more &quot;clicked email&quot; observations than &quot;opened email&quot; observations? What is the difference between opening an email and clicking?" CreationDate="2015-02-13T20:58:29.767" UserId="4621" />
  <row Id="5421" PostId="5137" Score="0" Text="I don't think many people are going to start by watching an hour+ long video. How about describing your fears in a bit more detail." CreationDate="2015-02-14T16:36:52.697" UserId="7720" />
  <row Id="5422" PostId="5101" Score="0" Text="Thank you for your interesting suggestions." CreationDate="2015-02-14T20:27:04.030" UserId="3346" />
  <row Id="5423" PostId="5080" Score="0" Text="Basically I asked a question that was clear  enough to somebody and I received two good answers. Of course anybody is free to think what their mind suggest them to think." CreationDate="2015-02-14T20:29:34.690" UserId="3346" />
  <row Id="5424" PostId="5129" Score="0" Text="I think questions that are purely about career and off site learning resources for a tool are off-topic for this site." CreationDate="2015-02-14T22:02:41.447" UserId="21" />
  <row Id="5425" PostId="5137" Score="1" Text="Good news: there are lots more tools like this." CreationDate="2015-02-14T22:03:31.490" UserId="21" />
  <row Id="5427" PostId="5140" Score="0" Text="thank you very much for your answer. In the case of Naive Bayes, don't we have the likelihood p(x,y) over both the examples x and the class labels y? (as in http://cs229.stanford.edu/notes/cs229-notes2.pdf), whereas in logistic regression it is only over y: p(y|x) (http://cs229.stanford.edu/notes/cs229-notes1.pdf)" CreationDate="2015-02-15T13:17:42.890" UserId="8273" />
  <row Id="5429" PostId="5147" Score="0" Text="Can you be a lot more specific? what do you have in mind when you ask about methodologies? modeling, scoring, evaluation?" CreationDate="2015-02-15T17:36:43.477" UserId="21" />
  <row Id="5431" PostId="5148" Score="0" Text="That seems promising, I will have a look at those. As for methodology, I mean a step-by-step phased process that one can use for framing guidance." CreationDate="2015-02-15T17:39:12.710" UserId="7720" />
  <row Id="5432" PostId="5147" Score="0" Text="Specifically I mean a step-by-step phased process that one can use for framing guidance. But I am interested in anything close to that too." CreationDate="2015-02-15T17:40:09.477" UserId="7720" />
  <row Id="5433" PostId="5148" Score="0" Text="Yeah check out the resources I linked, the first one talks about a very high level methodology. But once you have your high level pieces figured out, you need to look for process for each of your sub processes." CreationDate="2015-02-15T17:48:17.760" UserId="90" />
  <row Id="5434" PostId="5147" Score="0" Text="I edited it to clarify what I meant. Can you take me off hold now?" CreationDate="2015-02-15T17:48:54.930" UserId="7720" />
  <row Id="5435" PostId="5147" Score="0" Text="It is clear what a methodology is but the topic is still quite broad. Are you talking about approaches to modeling? Feature selection? visualization?" CreationDate="2015-02-15T17:52:40.743" UserId="21" />
  <row Id="5436" PostId="5140" Score="0" Text="In the case of Naive Bayes, that is the joint probability $p(x,y)$ and they use in page 10 what they call the &quot;joint likelihood&quot;. In the logistic regression example, the notation in page 18 might be confusing because they have $p(y|x;\theta)$. For a moment, forget about $x$ and you have what I wrote above. However, $\theta$ depends on your covariates $x$ and that is why they have $h_{\theta}(x)$. In the same way, you can also write $\pi$ as $\pi(x)$ in the notation I used and you'd have the same description. By the way, notice I fixed a typo a moment ago." CreationDate="2015-02-15T17:53:04.597" UserId="4621" />
  <row Id="5437" PostId="5147" Score="0" Text="No, it is to solve real life business problems, which is of course driving the boom in Data Science. I am not so interested in pure academic applications - although they are fun, they do not require a methodology usually. &#xA;&#xA;Do I need to state that too?" CreationDate="2015-02-15T17:55:37.647" UserId="7720" />
  <row Id="5438" PostId="5140" Score="0" Text="Another change I did was to use Bernoulli distributions to model the binary outcomes in the logistic regression case, since that is what you meant based on your references." CreationDate="2015-02-15T18:01:51.590" UserId="4621" />
  <row Id="5439" PostId="5147" Score="0" Text="To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist." CreationDate="2015-02-15T18:02:51.027" UserId="7720" />
  <row Id="5440" PostId="5156" Score="0" Text="I guess it's really a method of measuring similarity to a cluster that I'm looking for. The clustering is based on a graph of similarities between nodes, so I don't really have any centroid measurements." CreationDate="2015-02-16T14:27:01.477" UserId="474" />
  <row Id="5441" PostId="5145" Score="0" Text="Thanks to both of you.  Can one of you provide me with a URL to free site like wikipedia where I can read up about the methodology?" CreationDate="2015-02-16T15:13:33.030" UserId="8281" />
  <row Id="5442" PostId="5155" Score="0" Text="I'm not sure about type of data you perform clustering on, but maybe [this related answer](http://datascience.stackexchange.com/a/4833/2452) of mine will be helpful in terms of figuring out a method for measuring similarity, optimal for your situation." CreationDate="2015-02-16T15:20:35.737" UserId="2452" />
  <row Id="5443" PostId="5155" Score="0" Text="If your clusters have an exemplar, you could compare the incoming samples against it, otherwise you could compare it against the centroid, which you could keep track of. I'm not sure what the problem is, as this seems obvious." CreationDate="2015-02-16T18:13:04.430" UserId="381" />
  <row Id="5444" PostId="5155" Score="0" Text="@Emre - that's part of the problem, I don't have an exemplar or centroid for each cluster easily available, though perhaps this is something I can calculate (in case it helps, I'm using MCL clustering - http://micans.org/mcl/, so input is a similarity matrix rather than feature vectors)" CreationDate="2015-02-16T18:20:38.513" UserId="474" />
  <row Id="5445" PostId="5155" Score="1" Text="You can always calculate the centroid from the raw features, so I'd give it a try." CreationDate="2015-02-16T18:30:44.193" UserId="381" />
  <row Id="5446" PostId="2504" Score="1" Text="I still don't have enough reputation to put a short comment on your original question and this is not really an answer. In any case, I wanted to say that I think this paper is pretty relevant to this issue: Fernández-Delgado, M., Cernadas, E., Barro, S., &amp; Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems? The Journal of Machine Learning Research, 15, 3133–3181. Retrieved from http://dl.acm.org/citation.cfm?id=2697065" CreationDate="2015-02-16T19:33:26.393" UserId="506" />
  <row Id="5448" PostId="5150" Score="1" Text="I wouldn't call these methodologies though. They are algorithms, that can be applied as part of a methodologie." CreationDate="2015-02-17T05:17:45.623" UserId="7720" />
  <row Id="5451" PostId="128" Score="0" Text="Is HDP supposed to be data-driven in regards to the number of topics it will select? On practical side, I tried to run Blei's HDP implementation and it just ate all memory until I killed the process. I have 16GB RAM and just over 100K short documents to analyze." CreationDate="2015-02-18T09:25:21.440" UserId="7848" />
  <row Id="5452" PostId="5178" Score="0" Text="If you tell me three things I might have an answer! 1. Does your medium size data gets bigger? If so how periodically. 2. Do you use programming languages/frameworks to do pattern matching and data analysis? 3. Does anyone else use this data? If so do they change it?" CreationDate="2015-02-13T10:40:38.030" UserDisplayName="Dave Rose" />
  <row Id="5453" PostId="5178" Score="0" Text="I voted to &quot;Leave Open&quot; when reviewing Close Votes queue because I don't think this question is for specific situation. However, can anybody who is familiar with Software Recommendations SE tell us this question would fit into that site?" CreationDate="2015-02-13T10:52:42.327" UserDisplayName="scaaahu" />
  <row Id="5454" PostId="5178" Score="1" Text="@scaaahu I don't think this is necessarily a software question; an acceptable answer could also describe a workflow or combination of tools and systems. (Anyways, being on topic somewhere else shouldn't play into the decision to close a question here.)" CreationDate="2015-02-13T11:01:58.770" UserDisplayName="ff524" />
  <row Id="5455" PostId="5178" Score="1" Text="Just to protect against data corruption with image data, I periodically run a script that re-computes a checksum file with all files and their md5 checksums. The checksum file is then kept in git. Now I can immediately see with git diff if any of the checksums have changed. And I can also see which files have been removed &amp; added. And if there are e.g. any signs of data corruption, then I can use the regular backups to restore old versions. Not perfect but better than nothing." CreationDate="2015-02-13T11:30:36.463" UserDisplayName="Jukka Suomela" />
  <row Id="5456" PostId="5178" Score="0" Text="@JukkaSuomela I think you should post that as an answer, not a comment." CreationDate="2015-02-13T11:48:49.980" UserDisplayName="ff524" />
  <row Id="5457" PostId="5178" Score="0" Text="@Johann Why not just files, without version control (but with backups)?" CreationDate="2015-02-13T12:22:54.293" UserId="289" />
  <row Id="5458" PostId="5178" Score="0" Text="@PiotrMigdal: Are you seriously asking why people should use version control, instead of just having a bunch of files with backups?-)" CreationDate="2015-02-13T13:01:50.247" UserDisplayName="Jukka Suomela" />
  <row Id="5459" PostId="5178" Score="1" Text="@JukkaSuomela I think it's a reasonable question when you've got very large datasets, if those datasets change frequently... in those cases, backup often *is* what's used as version control." CreationDate="2015-02-13T13:29:18.427" UserDisplayName="jakebeal" />
  <row Id="5460" PostId="5178" Score="1" Text="I'm voting to close this question as off-topic because it deals with data/databases rather than something _specific_ to academia. The questions is great, and (IMHO) should be moved to DataScience.SE or (perhaps) Databases.SE." CreationDate="2015-02-13T14:02:35.853" UserId="289" />
  <row Id="5461" PostId="5178" Score="0" Text="@PiotrMigdal I don't know if this is off-topic, but this question does also not fit well with Databases.SE or DataScience.SE. I would like to know what other researchers/Institute do in practice to deal with this kind of problem - I've updated the question accordingly." CreationDate="2015-02-15T19:22:05.807" UserDisplayName="Johann" />
  <row Id="5462" PostId="5178" Score="0" Text="@DaveRose 1. Yes, I will hopefully add more experimental data and processed images, but not very often (maybe a few iterations); 2. Yes, that is part of my thesis; 3. Yes, others will hopefully use and change the data." CreationDate="2015-02-15T19:32:18.047" UserDisplayName="Johann" />
  <row Id="5463" PostId="5178" Score="0" Text="@JukkaSuomela That actually sounds pretty good (at least much better than anything I've found so far)." CreationDate="2015-02-15T19:33:45.380" UserDisplayName="Johann" />
  <row Id="5464" PostId="5178" Score="0" Text="@PiotrMigdal Going without version control kind of pains me for the reasons I've stated in the question (especially: &quot;did my data change with out me noticing?&quot; and then 2 days of diffing by hand). So I am looking for something smarter." CreationDate="2015-02-15T19:35:56.057" UserDisplayName="Johann" />
  <row Id="5465" PostId="5178" Score="0" Text="@Johann 1. How you store or version control it is in domain of data science (regardless if you use it in academia, industry or for a hobby project). I really want to ensure the best answers and it is good to go where there are many experts in data. 2. My point was only that it might be not for git. (And, all in all, git _is_ a filesystem). Do you want to diff per file, line, or what?" CreationDate="2015-02-15T19:53:37.380" UserId="289" />
  <row Id="5466" PostId="5178" Score="0" Text="@PiotrMigdal 1) You are probably right, though I am still curious how scientist - without a background in data science - handle that situation, 2) No doubt, git cannot handle that kind of data. A diff per file would be enough, just to see if my data has changed (or I changed it inadvertently). The point is to have control/documentation over how my data changed through what action by whom." CreationDate="2015-02-16T05:16:23.600" UserDisplayName="Johann" />
  <row Id="5467" PostId="5178" Score="1" Text="@Johann Data scientist have different backgrounds. Mine is in quantum mechanics, for example. The whole point here is that: 1. StackExchange discourages so-called [boat questions](http://meta.stackexchange.com/questions/14470/what-is-the-boat-programming-meme-about) and 2. its better to get best practices rather than how it is solved by people who had to solve it but had no idea." CreationDate="2015-02-18T12:33:42.243" UserId="289" />
  <row Id="5468" PostId="5179" Score="0" Text="There is a popular open source version of Dropbox, OwnCloud. I haven't tried it, though." CreationDate="2015-02-13T21:35:47.000" UserDisplayName="Davidmh" />
  <row Id="5469" PostId="5152" Score="1" Text="+1 for starting with a simple model and sub-sampling" CreationDate="2015-02-18T13:55:32.467" UserId="8319" />
  <row Id="5470" PostId="5178" Score="0" Text="@PiotrMigdal Thats a good point - thanks!" CreationDate="2015-02-18T13:59:44.580" UserId="8320" />
  <row Id="5476" PostId="5109" Score="0" Text="StackExchange sites should not be a link-dump of increasingly aging and missing resources." CreationDate="2015-02-18T15:25:17.420" UserId="471" />
  <row Id="5484" PostId="2284" Score="0" Text="&quot;Where **he was** born?&quot; is not a question. &quot;Where **was he** born?&quot; is. Are you trying to parse poor English?" CreationDate="2015-02-18T18:02:01.703" UserId="471" />
  <row Id="5485" PostId="3734" Score="0" Text="You can consider to disk based processing instead of whole in memory approach. Just read a batch of instances that fit in your memory update your clusters then read another batch from disk and go on like this. This is slow but the most realiable way. You can conside to use HDF5 like data structures as well, to read structured data from disk." CreationDate="2015-02-18T20:39:44.777" UserId="464" />
  <row Id="5486" PostId="5165" Score="0" Text="Agreed, too broad." CreationDate="2015-02-18T23:07:25.207" UserId="3466" />
  <row Id="5487" PostId="5160" Score="0" Text="Can you please provide some info about your previous question, and link? Perhaps you should just revise that question. Hard to say without any context." CreationDate="2015-02-18T23:08:46.253" UserId="3466" />
  <row Id="5489" PostId="5124" Score="0" Text="You know one for Java? I am not familiar with python and R that well." CreationDate="2015-02-19T07:20:16.607" UserId="8235" />
  <row Id="5492" PostId="5109" Score="0" Text="Yes, I think that this is generally considered off-topic; it's open-ended and opinion-based." CreationDate="2015-02-19T10:06:09.440" UserId="21" />
  <row Id="5494" PostId="5124" Score="0" Text="Check out Weka. I'm not familiar with Java though. Seems like a good question to ask here if it hasn't been asked already" CreationDate="2015-02-19T18:07:02.923" UserId="8236" />
  <row Id="5499" PostId="5181" Score="0" Text="This sounds very promising. Thanks for your reply." CreationDate="2015-02-20T21:52:17.773" UserId="8127" />
  <row Id="5500" PostId="5188" Score="0" Text="Thank you . I understand the first paragraph an dthe second paragraph, but to me, they are saying the opposite things. You say I can't infer that in the first sentence but that I can in the last. Can you explain? Thank you!" CreationDate="2015-02-20T23:00:07.250" UserId="8313" />
  <row Id="5502" PostId="5178" Score="0" Text="@Johann Are you sure you need to version control your dataset? If you are processing a big set of images, you usually want to version control the procedures you followed that led to a modified set of images. Therefore, you usually don't need to keep track the images themselves. If you want to restore this modified set in the future, obviously you only need to take the original dataset and apply the procedures you did according to some commit in your code." CreationDate="2015-02-21T00:50:06.050" UserId="4621" />
  <row Id="5503" PostId="5188" Score="0" Text="what exactly is the goal of your analysis ? Are you interested in saying something about the 'complexity' of condition a in comparison to condition b ? When is a condition complex ? If it's hard to diagnose ? if the symptoms are severe ? can s.o. have both conditions ?" CreationDate="2015-02-21T06:48:01.813" UserId="8191" />
  <row Id="5504" PostId="5199" Score="0" Text="I think that it is impossible to answer this question as it's stated. You may have a better chance of getting a reasonable answer, if you will provide more details about your project(s), goals, specific packages, users (including their number) and requirements for the setup. Keep in mind that, in addition to _local machines_ and _local servers_, you have various **cloud** options, such as _Amazon Web Services (AWS)_." CreationDate="2015-02-21T07:47:30.790" UserId="2452" />
  <row Id="5505" PostId="5199" Score="0" Text="This question is not really Data Science related, as it stands now.  Maybe you can elaborate and make the connection clearer?  Otherwise, you may want to move it to Programmers Stack Exchange." CreationDate="2015-02-21T11:37:13.047" UserId="1367" />
  <row Id="5506" PostId="5201" Score="0" Text="Answered here, but as you can see, there is nothing Data Science specific about the answer.  Hope it helps, anyway." CreationDate="2015-02-21T12:02:18.130" UserId="1367" />
  <row Id="5507" PostId="5206" Score="0" Text="This is extremely helpful, and works. One complication I have which maybe wasn't clear is that each cell in each row df['salary'] is a list, not a string (i.e. `[£26,658, to, £32,547], [Competitive, with, Excellent, Benefits]`, not `[£26,658 to £32,547],[Competitive with Excellent Benefits]` . So when I run your function I will get `TypeError: expected string or buffer`. I tried solving this via `df['salary']=df['salary'].apply(''.join(df['salary']))` just after you define df in your second line of code of the second section, but get `TypeError: sequence item 0: expected string, list found`." CreationDate="2015-02-22T14:04:04.213" UserId="8375" />
  <row Id="5508" PostId="5206" Score="0" Text="first, `apply` takes a function as argument, so you should write `df['salary'].apply(' '.join)`. Note also that I would do `' '.join` as opposed to `''.join` to avoid accidentally concatenating strings that should not be.&#xA;&#xA;Secondly, the only reason you have the lists is because you did `df['salary']=df['salary'].astype(str).str.split()`. My solution doesn't require this step, so you can just work with the un-splitted strings as they are. `re.findall` will scan through an entire string and extract everything that looks like it's a reference to money." CreationDate="2015-02-22T14:29:47.107" UserId="8376" />
  <row Id="5510" PostId="5196" Score="0" Text="I would begin by looking at the time required to make forecasts and interacting with server times. Can they be optimized further? If not, then you should try distributed/parallel computing solutions." CreationDate="2015-02-22T15:29:37.450" UserId="1131" />
  <row Id="5514" PostId="5206" Score="0" Text="I am still getting TypeError: expected string or buffer even when I removed `df['salary']=df['salary'].astype(str).str.split().` When I print df['salary'] it certainly looks now like a column of strings. Even when I did `df['salary']=df['salary'].astype(int32)` based on stackoverflow.com/questions/21841402/… I get `ValueError: invalid literal for long() with base 10: '\xc2\xa336,000 - \xc2\xa340,000 per year plus excellent benefits'.` I read my CSV in as UTF-8 by the way." CreationDate="2015-02-23T13:27:09.893" UserId="8375" />
  <row Id="5515" PostId="5199" Score="0" Text="Thanks for the feedback.  I'll try to add more detail." CreationDate="2015-02-23T17:07:47.243" UserId="8368" />
  <row Id="5516" PostId="5201" Score="0" Text="Thanks.  Which section of Stack Exchange is the correct one for discussing these issues?  I understand that they are only tangentially related to the topic of Data Science." CreationDate="2015-02-23T17:22:14.830" UserId="8368" />
  <row Id="5517" PostId="5201" Score="0" Text="I would move or re-post the question on http://programmers.stackexchange.com . Stack Overflow is for code that is not running as expected, while Programmers SE is for the design stage.  Make it clear you ask for advice on the setup for your team, and not on the setup of Python alone! Otherwise they might suggest to move the question to http://serverfault.com or something ..." CreationDate="2015-02-23T17:25:20.940" UserId="1367" />
  <row Id="5520" PostId="5211" Score="0" Text="Thank you, I'm just looking for different approaches to solve classification problems. The 3 tips that you gave me seems to be the way to go. At first I wanted to see if could translate a single-class classification into a multiple-class classification problem. But knowing that I need instances for each class might be a problem" CreationDate="2015-02-23T21:14:27.530" UserId="8381" />
  <row Id="5521" PostId="5211" Score="0" Text="The animal/human classification is just an example. My goal is to find a way to tweak or improve current malware classification methods for android" CreationDate="2015-02-23T21:15:52.313" UserId="8381" />
  <row Id="5522" PostId="5211" Score="0" Text="Ok, good luck! Feel free to ask again if you get stuck somewhere!" CreationDate="2015-02-23T21:16:56.930" UserId="8191" />
  <row Id="5523" PostId="1151" Score="0" Text="@Air: Unless you can refer to the official rule on DS, which states otherwise, career-related questions are not off-topic and valid. Pointing to several opinions in a Meta discussion doesn't change that." CreationDate="2015-02-24T07:27:08.453" UserId="2452" />
  <row Id="5526" PostId="1151" Score="0" Text="@AleksandrBlekh You do realize you are responding to a comment that is nearly six months old? And that opinions in a Meta discussion *are* as close as you will get to an official rule on topicality?" CreationDate="2015-02-24T14:55:25.263" UserId="322" />
  <row Id="5527" PostId="1151" Score="0" Text="@Air: Does it matter, if that comment is six months old, when your today's one confirms that you still think the same? Fortunately, it's just your opinion, nothing more. As are all other opinions, expressed on Meta." CreationDate="2015-02-24T15:05:09.293" UserId="2452" />
  <row Id="5529" PostId="1151" Score="0" Text="@AleksandrBlekh I'm struggling to find a point to this exchange. If the consensus has changed since I originally commented, all you need do is provide a citation for the new consensus and flag the comment is obsolete. Unless your goal is simply to have an argument (no thanks), there's no reason for me to be involved here." CreationDate="2015-02-24T19:54:05.333" UserId="322" />
  <row Id="5530" PostId="1151" Score="0" Text="@Air: My point is that I believe that your initial comment is formulated too strongly (nothing personal, though). My goal is not to have an argument. However, I don't think that several opinions in Meta establish any consensus, even if they would be the same (which they are not). Especially, since, as many Meta discussions, that  one isn't representative. Generally, I don't see how a Meta discussion could be an accurate measure/indicator of a consensus, unless some minimum level of representativeness is guaranteed. Unless there is a more formal voting process/tool, which I'm unaware of." CreationDate="2015-02-24T20:07:26.970" UserId="2452" />
  <row Id="5531" PostId="435" Score="0" Text="While a good question I notice that it appears on multiple Stack Exchange forums... http://stackoverflow.com/questions/24260299/relational-data-mining-without-ilp  ; Not that I'm a stickler about stuff like that, but I think that we're not supposed to do it" CreationDate="2015-02-25T15:36:26.273" UserId="2723" />
  <row Id="5532" PostId="435" Score="0" Text="Also, it would be very helpful if you could be a little more specific with regards to what you're classifying, the barrier you're encountering and in an ideal world provide us with some sample data to look at" CreationDate="2015-02-25T16:29:08.187" UserId="2723" />
  <row Id="5533" PostId="5235" Score="0" Text="Thanks a lot Alex, the scheduler does have a memory. It takes into account previous allocations in its current decisions.  I will attempt to apply hidden Markov model" CreationDate="2015-02-26T05:20:29.907" UserId="8410" />
  <row Id="5534" PostId="5225" Score="0" Text="The suggested keywords does provide me new search terms to find existing research papers, which I did not find previously. Thanks" CreationDate="2015-02-26T06:24:30.687" UserId="7887" />
  <row Id="5535" PostId="5220" Score="0" Text="Also I've thought the modules you've indicated could possibly generate datas that are suits for predicting. Your answer has been a confirmation for me. I got the gist. Thanks!" CreationDate="2015-02-26T06:58:34.847" UserId="8386" />
  <row Id="5536" PostId="5233" Score="2" Text="Just a little thought here. Why don't you just train one SVM for each response variable?" CreationDate="2015-02-27T09:15:18.167" UserId="2576" />
  <row Id="5537" PostId="5244" Score="1" Text="See [this][1] question on Cross Validated.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/39232/is-r-viable-for-production-deployed-code" CreationDate="2015-02-27T17:16:34.460" UserId="8392" />
  <row Id="5540" PostId="5245" Score="0" Text="OK, I don't see any other questions from you here, but in general, I would not re-post an old question just because it didn't get an answer. It will be closed as duplicate. Instead improve the original question." CreationDate="2015-02-28T04:07:28.117" UserId="21" />
  <row Id="5541" PostId="5212" Score="0" Text="I think this is too broad. You should say more about your requirements or constraints to narrow this down." CreationDate="2015-02-28T04:07:58.203" UserId="21" />
  <row Id="5542" PostId="5238" Score="1" Text="Link-only answers are discouraged. You should add here a summary of the salient information from these resources that answers the question." CreationDate="2015-02-28T04:08:54.120" UserId="21" />
  <row Id="5543" PostId="5243" Score="1" Text="Requests for resources are generally viewed as off-topic. Maybe you can steer this towards asking about a specific problem you'd like to know how to solve?" CreationDate="2015-02-28T04:16:44.690" UserId="21" />
  <row Id="5544" PostId="5215" Score="0" Text="I tried this and now I have gone down a rabbithole of encoding problems after this error when trying to convert to string (I read my CSV as encoding=UTF-8') - `UnicodeEncodeError: 'ascii' codec can't encode character u'\xa3' in position 0: ordinal not in range(128)` (http://stackoverflow.com/questions/3588083/unicodeencodeerror-ascii-codec-cant-encode-character-u-xa3). Then, when I try to encode or decode df['salary'] it says AttributeError: 'Series' object has no attribute 'encode'. Basically what I think is that the pound signs which are now in UTF-8 cannot be converted to strings?" CreationDate="2015-02-28T13:12:33.810" UserId="8375" />
  <row Id="5545" PostId="2633" Score="0" Text="You can always create another google account and use GCE from that." CreationDate="2015-02-28T06:16:06.353" UserId="8466" />
  <row Id="5546" PostId="5248" Score="0" Text="Importing 10^10 rows of data into R just to calculate kendall coefficient is just simply imposible and not smart. That's why I wrote a question here. I am aware on how to specify implementation of kendall coefficient, just simply check the code of `cor` function in R :) I think the best idea would be to implement user defined function to calculate that kendall coefficient. Do you maybe know where could I upload that function later on for public use? By the way, have you used RHive :) do you recommend any good materials for start?" CreationDate="2015-02-28T16:03:05.993" UserId="5224" />
  <row Id="5547" PostId="5251" Score="5" Text="That answer is pure nonsense on so many fronts." CreationDate="2015-02-28T18:46:24.400" UserId="515" />
  <row Id="5548" PostId="5251" Score="1" Text="I don't think this answer contains a single correct statement." CreationDate="2015-02-28T18:55:36.690" UserId="783" />
  <row Id="5549" PostId="5251" Score="0" Text="What exactly is wrong about legal departments being scared of the GPL, and R+CRAN being mostly GPL code? **These are facts, even if you don't like them.**&#xA;Your comments don't contain any *precise* rebuttal..." CreationDate="2015-02-28T19:41:09.777" UserId="924" />
  <row Id="5550" PostId="5248" Score="0" Text="@MarcinKosinski Instead of importing 10^10 rows, why not just sample it? As for UDF's, if you have one or develop one with other people you can put it into a JIRA ticket for addition to Hive via the links in my solution, or become a committer and commit code to the project (or give your code to a committer): https://www.apache.org/dev/committers.html" CreationDate="2015-02-28T20:12:19.093" UserId="2723" />
  <row Id="5551" PostId="5251" Score="0" Text="It's plain and simple FUD and trolling. I have used GPL code in production for the almost decades I have been working.  And as for citing one particular paper: yawn." CreationDate="2015-02-28T21:22:20.950" UserId="515" />
  <row Id="5552" PostId="5251" Score="0" Text="So have I. But I have also seen company polcies that forbid the use of any GPL code. YMMV." CreationDate="2015-02-28T21:27:04.133" UserId="924" />
  <row Id="5553" PostId="5251" Score="0" Text="You may have heard of RedHat, a 12 billion dollar Fortune 500 company selling GPL'ed Linux to just about every corporation out there.  So no, your sample of one does not exactly generalize.  But you're a troll, and I'll stop here now.  For a better answer follow the very first comment above to something I had to say in the matter over on CrossValidated a few years ago." CreationDate="2015-02-28T22:03:40.300" UserId="515" />
  <row Id="5554" PostId="5251" Score="0" Text="I have not been saying it's impossible. I have been saying he needs to check with *his* legal department. I see this as the flaw of legal departments of some companies, not of R - but you have to live with reality... (and they may even be paranoid enough to allow Linux, but no other GPL software...) But I have understood, **it's not what you want to hear**. A pity. But maybe the original poster does have the same problem." CreationDate="2015-02-28T22:22:01.853" UserId="924" />
  <row Id="5555" PostId="5256" Score="4" Text="I would also mention &quot;data analyst&quot;, which is a _more generic_ and, thus, in my opinion, better term than &quot;data miner&quot;." CreationDate="2015-03-01T02:09:19.823" UserId="2452" />
  <row Id="5556" PostId="5259" Score="0" Text="Sven, thanks -- this is brilliant. The key is the order() command, which I had under-appreciated until now. Is this how everyone does it?" CreationDate="2015-03-01T07:32:02.993" UserId="3510" />
  <row Id="5557" PostId="5259" Score="0" Text="@DavidEpstein I cannot tell you how others manage their data. This is how I would do it." CreationDate="2015-03-01T07:48:34.687" UserId="106" />
  <row Id="5558" PostId="5259" Score="0" Text="Well, thanks very much; I appreciate the tip." CreationDate="2015-03-01T07:52:24.180" UserId="3510" />
  <row Id="5560" PostId="5248" Score="0" Text="What's the bias of coefficient calculated on 1000 rows-long sample when population has 10^10 observation?" CreationDate="2015-03-01T17:08:55.447" UserId="5224" />
  <row Id="5561" PostId="5244" Score="0" Text="I saw this post, and still decided to ask here, hoping that this new forum might gather more people with practical know-how than Cross Validated." CreationDate="2015-03-01T21:17:51.110" UserId="8310" />
  <row Id="5562" PostId="5248" Score="0" Text="@MarcinKosinski I'm not sure, but I pull samples of data that are anywhere from 10,000 - 30,000 rows from SQL Server and Hive on a daily basis. Sampling data in R from Hive and other databases has been a standard approach in my team at 2 different companies where I've worked. I expected that even if you had a Kendall Coefficient function in Hive it would be impractical to run it on all 10^10 rows without sampling, at least if it's something you plan to do more than once. That would be very time- and resource-intensive without buying you much." CreationDate="2015-03-02T02:39:41.620" UserId="2723" />
  <row Id="5563" PostId="5264" Score="1" Text="By grid world, do you mean discrete spaces? Please elaborate." CreationDate="2015-03-02T08:08:49.713" UserId="8491" />
  <row Id="5564" PostId="4875" Score="0" Text="Maybe I tend to always want to keep it simple, but why not just use Logistic Regression with some interaction terms?" CreationDate="2015-03-01T03:58:26.763" UserId="8483" />
  <row Id="5566" PostId="5257" Score="0" Text="An upvote is something a person does, so presumably an upvote is a translation of an individual rating. There seem to be only two reasonable choice: either 5-star, or 4- and 5-star, ratings map to an upvote. Is there more to it than this?" CreationDate="2015-03-02T08:55:08.700" UserId="21" />
  <row Id="5567" PostId="5264" Score="1" Text="Yes, discrete spaces" CreationDate="2015-03-02T09:27:20.433" UserId="8013" />
  <row Id="5569" PostId="4991" Score="1" Text="It would be ideal to post a short summary of the article in this answer." CreationDate="2015-03-02T23:53:40.360" UserId="3466" />
  <row Id="5571" PostId="5268" Score="0" Text="maybe cross validate. randomly holdout 1 month of each backtesting year. Or do 1 year(or up to today) forward testing as validation." CreationDate="2015-03-03T13:15:58.133" UserId="7746" />
  <row Id="5572" PostId="5268" Score="0" Text="Unfortunately,  our users got unhappy when we tried to reserve time periods for validation as they (believe) they do their own validation so need all the data :-(.   Statistically speaking, is the screen more likely to be overfit if we randomly pulled months from the full set of results and checked if the screen still performed well?" CreationDate="2015-03-03T16:51:36.540" UserId="8344" />
  <row Id="5574" PostId="1165" Score="1" Text="Our start up is looking for academics to work on big data via stock analysis, if that field interests you, feel free to reach out." CreationDate="2015-03-03T21:28:45.627" UserId="8344" />
  <row Id="5575" PostId="1165" Score="0" Text="Oh, and I know that Universities are very interested in retention rates of students, so creating a model to predict retention rates based on the classes taken might be one proposal you could make to your university..." CreationDate="2015-03-03T21:31:32.303" UserId="8344" />
  <row Id="5576" PostId="2588" Score="0" Text="Nice answer (+1). I'm curious about your opinion (as a comment or an answer) on [this question](http://datascience.stackexchange.com/q/5108/2452) of mine, which I believe is relevant to this discussion." CreationDate="2015-03-03T23:27:18.197" UserId="2452" />
  <row Id="5577" PostId="5275" Score="0" Text="Virtually every job has time and resource constraints. As you said, this has been done before (e.g. mathematicians working on the ENIAC) The fact that data science is widespread doesn't mean it is a new job." CreationDate="2015-03-04T05:10:35.523" UserId="4621" />
  <row Id="5579" PostId="5268" Score="0" Text="if randomly pull, the sample performance tends to have a similar mean but more volatility which considered as a worse performance in most performance measures. Don't think this has anything to do with detecting overfit. I think forward testing might be a good way to go(as Meta trader does), normally traders will do this anyway" CreationDate="2015-03-04T11:34:19.617" UserId="7746" />
  <row Id="5580" PostId="5268" Score="0" Text="I wish all our users were wise enough to do it that way. As you'd think telling people that the developers of the system always trade virtually before investing in a screener would be enough! :-(.  I'm still hoping that someone has a few statistical tricks that will help at least a few people avoid disaster." CreationDate="2015-03-04T15:40:50.973" UserId="8344" />
  <row Id="5581" PostId="1227" Score="0" Text="Have you chosen any particular technology? This could be done with Neo4J and Cypher." CreationDate="2015-03-04T20:22:36.583" UserId="3466" />
  <row Id="5582" PostId="5264" Score="0" Text="Do these answers help you? Or is there something which is still unclear." CreationDate="2015-03-04T21:02:06.673" UserId="8491" />
  <row Id="5584" PostId="5251" Score="0" Text="Despite my respect for Dirk as an incredible contributor to R community, I disagree with his stance here. Even if (and it well may be) this answer is incorrect, partially or completely, I don't think it's a good idea to label and/or generalize people (especially, based on a single answer). Anony-Mousse is a nice contributor to SE community, especially CV and SO, so a bit more respect would nice. But, even regardless of anyone's contributions, I think that being more considerate results in constructive discussion, which, in turn, leads to higher quality professional environment." CreationDate="2015-03-05T03:03:16.783" UserId="2452" />
  <row Id="5585" PostId="5282" Score="0" Text="On StackExchange, such short answers are usually (with rare exceptions) preferred in a form of a comment. Posting answer implies a more detailed/comprehensive treatment of a topic. Just FYI." CreationDate="2015-03-05T03:06:06.607" UserId="2452" />
  <row Id="5586" PostId="5278" Score="0" Text="Thanks so much for your help." CreationDate="2015-03-05T05:38:33.900" UserId="8318" />
  <row Id="5589" PostId="5282" Score="0" Text="@Alex S Kinman multi hidden layer is common in deep learning architecture." CreationDate="2015-03-05T13:40:38.047" UserId="7746" />
  <row Id="5590" PostId="5282" Score="0" Text="@Aleksandr Blekh what if a question doesn't need a long answer at all? do you have a reference where your comment based on, meta, faq etc? thx" CreationDate="2015-03-05T13:43:04.033" UserId="7746" />
  <row Id="5591" PostId="5282" Score="0" Text="@Xin , I don't know enough about deep neural networks. But for traditional neural networks, the universal function approximation theorem states that a multilayer network with a single hidden layer can approximate any continuous decision function. I am curious, are there any results proving that DNN have more representational power than regular ANN?" CreationDate="2015-03-05T14:03:04.347" UserId="8517" />
  <row Id="5592" PostId="5282" Score="0" Text="@Xin: The following relevant SE Meta answers should give you the right impression, hopefully: [on what is considered a good answer](http://meta.stackexchange.com/a/129021/263279) and [on converting a comment to an answer by expanding the former](http://meta.stackexchange.com/a/248125/263279). I'm sure that there many more guidelines on Meta, but that should be enough." CreationDate="2015-03-05T14:07:20.373" UserId="2452" />
  <row Id="5593" PostId="5282" Score="0" Text="This is not a linearly separable problem because a single hyperplane cannot describe the decision boundary. A `4x2x1` network can describe the boundary. The two hidden layer neurons describe the two `AND` surfaces and the output neuron is the logical `OR` of the two `AND` surfaces." CreationDate="2015-03-05T14:55:48.873" UserId="964" />
  <row Id="5594" PostId="5282" Score="0" Text="Also, the statement that &quot;the number of neurons in the hidden layer doesn't need to exceed the number of inputs&quot; is false. Consider the simple problem of class 1 covering the x-y plane with a triangular region cut out for class 2. There are only two inputs for this network but it requires at least three hidden layer neurons to describe the triangular decision surface. One can easily construct examples that require *many* more hidden layer neurons than inputs (e.g., if class 2 lies in multiple triangular regions)." CreationDate="2015-03-05T15:03:07.343" UserId="964" />
  <row Id="5595" PostId="5282" Score="0" Text="@Bogatron : I actually drew the decision boundary for the variables (2 by 2) , and there was a linear decision boundary. What am I doing wrong ?" CreationDate="2015-03-05T17:07:16.317" UserId="8517" />
  <row Id="5596" PostId="5282" Score="0" Text="I don't know what you mean with &quot;2 by 2&quot;. The decision boundary for `y = (x1 ∧ x2) ∨ (x3 ∧ x4)` is in a 4-D space. &quot;Linearly separable&quot; implies that a single 4-D hyperplane can represent the decision boundary but that is not the case. There is no linear decision boundary that can correctly classify all 16 `y` values for this logical function." CreationDate="2015-03-05T18:00:57.187" UserId="964" />
  <row Id="5597" PostId="5282" Score="0" Text="@bogatron (not disputing you, just trying to figure out my error) in the 3-D feature space with dimensions [x1,x2,(x3*x4)] , there is a plane representing the decision boundary. Why won't that translate into a 4-D hyperplane in the [x1,x2,x3,x4] space ?" CreationDate="2015-03-05T23:10:59.267" UserId="8517" />
  <row Id="5599" PostId="2391" Score="0" Text="Take a look at information I've shared in [this related answer](http://datascience.stackexchange.com/a/5200/2452). I hope that it'll be helpful." CreationDate="2015-03-06T04:29:42.800" UserId="2452" />
  <row Id="5601" PostId="5282" Score="1" Text="@AlexSKinman I deleted my previous comment because the 3-D case I gave was a bad example (that one is indeed linearly separable). I tried training a 4x1 network on the `y = (x1 ∧ x2) ∨ (x3 ∧ x4)` problem numerous times and could not get it to learn the function (the best it would do was accuracy of 0.9375 (i.e., one input was always misclassified). Using a 4x2x1 network, the function was easily learned. If you believe you can construct a separating hyperplane, perhaps you could provide its coefficients (i.e., the `a` coefficients for `f(x) = a1*x1 + a2*x2 + a3*x3 + a4*x4 - a0`)." CreationDate="2015-03-06T06:05:28.453" UserId="964" />
  <row Id="5602" PostId="3699" Score="0" Text="as @Sidhha said, grid search is good way to go. I found Michael Nielsen's book([chap3](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters)) is useful to understand how to choose hyper parameter in practice." CreationDate="2015-03-06T07:26:51.637" UserId="7746" />
  <row Id="5603" PostId="5289" Score="0" Text="Could you elaborate further on your answer? I myself *accept* what you said, which seems reasonable to happen, but don't actually understand why it is so." CreationDate="2015-03-06T09:33:35.240" UserId="84" />
  <row Id="5604" PostId="5024" Score="0" Text="I found the glmulti package useful for GA-like fitting.  One of the problems with &quot;best package&quot; questions is that without a good understanding of the nature of the problem, the data, and the goal - the means to get to the answer are unknonw." CreationDate="2015-03-06T16:48:06.033" UserId="8552" />
  <row Id="5605" PostId="5298" Score="0" Text="It's a very strange question idd. Maybe he ment CNNs. I agree with your answer though you *can* make connections like regression if you really need to. But in the end it's just 2 different things." CreationDate="2015-03-09T22:03:59.677" UserId="5316" />
  <row Id="5606" PostId="1185" Score="0" Text="It's not what you are asking. But you can take a look at this link of [Data Science](http://subasish.github.io/pages/data_sc_best_papers/) best papers." CreationDate="2015-03-09T00:24:48.157" UserId="8582" />
  <row Id="5607" PostId="5299" Score="0" Text="Well that does make some sense but doesn't it look like it's a vague connection?" CreationDate="2015-03-10T07:20:19.210" UserId="7960" />
  <row Id="5608" PostId="5298" Score="0" Text="+1 for helping with the approach. I think the clarification question you used would definitely be a good way to go about this" CreationDate="2015-03-10T07:21:34.103" UserId="7960" />
  <row Id="5609" PostId="5298" Score="0" Text="I guess I agree with the strange question thing. Yup, he could've meant CNNs. Idk as I was not asked this in any of my interviews. I read this question somewhere." CreationDate="2015-03-10T07:22:55.437" UserId="7960" />
  <row Id="5610" PostId="5299" Score="0" Text="It is vague but there just is no obvious connection. It's one of those interview questions that try to capture many things at once but just fail" CreationDate="2015-03-10T10:17:55.457" UserId="5316" />
  <row Id="5611" PostId="5261" Score="0" Text="Why do you include the line `sentence &lt;- str_replace_all(sentence, wordsDF[x,1], &quot; &quot;)`? I can't see why it's necessary and it's definitely slowing things down" CreationDate="2015-03-10T15:58:08.337" UserId="1156" />
  <row Id="5613" PostId="5310" Score="0" Text="I agree. I primarily tested with the weights initialized to 1 (I have my reasons) and it's how I first noticed the large variance. Then I initialized the weights using NW to check if the results would be better. There was some improvement but the large variance was still present." CreationDate="2015-03-12T13:28:36.473" UserId="8626" />
  <row Id="5614" PostId="5310" Score="0" Text="If you repeat training with the same set of initial weights, I would expect the same result (unless you have some sort of asynchronous processing going on). One other point: the difference in MSE between 0.5 and 0.5*10^-6 is only about 0.5, which isn't necessarily a large difference, depending on your training set size, number of outputs, and initial MSE." CreationDate="2015-03-12T15:00:01.067" UserId="964" />
  <row Id="5615" PostId="5310" Score="0" Text="The variance was still being observed with weights and biases all initialized to 1, which is why I felt confused when this happened. The data set is also consistent and it contains 10,000 values. There is less variance now even with such initialization, however everynow and then it tends to happen and it seems very strange. Is it possible that the error surface being created is different everytime  and hence the network sometimes gets stuck in local minimum by chance?" CreationDate="2015-03-12T15:44:52.797" UserId="8626" />
  <row Id="5616" PostId="5310" Score="0" Text="What makes you say that 0.5 vs. 0.5*10-6 is a large variance? What is the MSE at the start of training? Also, I'm suspicious about *all* weights being initialized to 1. If all the weights in an MLP are initialized to the same value, then I would expect all final weights for a given layer to converge to a common value." CreationDate="2015-03-12T15:53:26.390" UserId="964" />
  <row Id="5617" PostId="5310" Score="0" Text="I have one output since it's approximating a one dimensional cosine function, 10,000 values for training set size. The initial MSE (after the first epoch using Levenberg Marquardt BP) is 14.6268. The neural network has one hidden layer with 4 neurons and it is approximating cos(Pi). All the weights and biases are initialized to 1. It might be worth noting that the data division is happening at random (I'm calling dividerand in MATLAB) every time. Again, the dataset is consistent, and each point is evenly spaced out (that is the cosine function is being plotted with a fixed step)" CreationDate="2015-03-12T19:12:01.543" UserId="8626" />
  <row Id="5618" PostId="5308" Score="0" Text="Thanks for the reply, I'm going to check them all and update my situation later on." CreationDate="2015-03-13T04:08:40.653" UserId="8609" />
  <row Id="5619" PostId="5308" Score="1" Text="@KevinBelloMedina: You're welcome. Note that development of MALLET has been moved to GitHub: https://github.com/mimno/Mallet." CreationDate="2015-03-13T05:02:56.983" UserId="2452" />
  <row Id="5620" PostId="5311" Score="0" Text="Perhaps, _Cross Validated_ is a better place for this question." CreationDate="2015-03-13T07:32:08.347" UserId="2452" />
  <row Id="5621" PostId="189" Score="0" Text="If you're still interested, the corresponding [CRAN Task View](http://cran.r-project.org/web/views/Optimization.html) contains references to a large number of relevant R packages." CreationDate="2015-03-13T07:37:36.660" UserId="2452" />
  <row Id="5622" PostId="4855" Score="0" Text="Are the vectors where you encounter this error *all* zeroes? If so, and you're not fitting an intercept/bias term, this is your problem" CreationDate="2015-03-13T13:30:24.820" UserId="1399" />
  <row Id="5623" PostId="4855" Score="0" Text="And you could fix it by simply including a bias" CreationDate="2015-03-13T13:30:36.933" UserId="1399" />
  <row Id="5624" PostId="5093" Score="1" Text="Something is wrong here. IQR is the gap between the 25th and 75th percentile of your data. By definition this must contain 50% of your data. Is there a parameter on the filter that regulates what multiple of IQR you should consider extreme? Typical values would be 2-3 * IQR" CreationDate="2015-03-13T15:34:25.067" UserId="1399" />
  <row Id="5626" PostId="5318" Score="0" Text="You could normalize the distribution of elements over clusters for each object, then compare them using the [KLD](http://en.wikipedia.org/wiki/Kullback–Leibler_divergence)" CreationDate="2015-03-14T00:34:27.393" UserId="381" />
  <row Id="5627" PostId="5316" Score="0" Text="Look at Termine, and papers associated with the project: http://www.nactem.ac.uk/software/termine/" CreationDate="2015-03-14T04:37:07.673" UserId="609" />
  <row Id="5628" PostId="3719" Score="1" Text="No, it's not; GraphX is not a database." CreationDate="2015-03-14T04:59:30.633" UserId="381" />
  <row Id="5630" PostId="5305" Score="1" Text="I'm voting to close this question as off-topic because questions about finding data are generally off topic here or belong at OpenData SE. Also I think you would want to define what data you need as there is a single &quot;the Twitter data set&quot;." CreationDate="2015-03-14T22:06:09.250" UserId="21" />
  <row Id="5632" PostId="5313" Score="0" Text="I realize you're not sure where to start, but this is quite broad. Maybe you can at least clarify what you want: a paper, a tool, a use case? what is your level of background?" CreationDate="2015-03-14T22:10:08.497" UserId="21" />
  <row Id="5634" PostId="5301" Score="1" Text="I suggest that we close this question on this site. It is a duplicate, cross-posted on Cross Validated and migrated to StackOverflow, where it already has an accepted answer: http://stackoverflow.com/q/28962097/2872891. Anyone, wanting to add their answer, can do so on SO." CreationDate="2015-03-15T02:16:51.593" UserId="2452" />
  <row Id="5635" PostId="5325" Score="0" Text="thanks for your answer. So in the case of regression, there is no idea of maximising a margin?" CreationDate="2015-03-15T20:43:33.710" UserId="8653" />
  <row Id="5636" PostId="5325" Score="0" Text="You've used the $\epsilon$-insensitive formulation by Vapnik, whose goal is to find the flattest (simplest) solution that minimizes the $\epsilon$-insensitive loss, $\mathcal L(x) = (|x| - \epsilon)^+$. I suppose you could say that decreasing $w$ would increase the margin--distance to the line--for a given $\epsilon$ (think of a straight line with $\epsilon$ margin in the vertical direction and mentally rotate it so its slope tends to zero), but I don't usually think of it this way." CreationDate="2015-03-15T21:27:03.533" UserId="381" />
  <row Id="5637" PostId="4852" Score="0" Text="If you have to use a data store with limitations on the number of columns, you could map an NxN structure onto one of MN X N/M. And then wrap it in a retrieval mechanism that retrieves M rows at a time." CreationDate="2015-03-16T13:15:26.073" UserId="7980" />
  <row Id="5638" PostId="1068" Score="0" Text="So far, it [&quot;parses&quot;](https://spark.apache.org/docs/1.0.2/mllib-decision-tree.html) CSV files by splitting using &quot;,&quot;. This fails [for example](http://en.wikipedia.org/wiki/Comma-separated_values#Example) if there are commas inside some field delimited with quotes. Not a good start, but I will keep checking spark along other libraries." CreationDate="2015-03-16T17:21:12.963" UserId="1281" />
  <row Id="5639" PostId="5324" Score="0" Text="Thanks for researching!" CreationDate="2015-03-16T20:29:51.433" UserId="8191" />
  <row Id="5640" PostId="1128" Score="0" Text="I have no particular expertise in this field, but would not a set of shift and scaling, and perhaps copy,  operators be enough to transform any time series into any other time series ?" CreationDate="2015-03-16T21:30:19.597" UserId="7980" />
  <row Id="5641" PostId="5318" Score="0" Text="First you need to define &quot;similarity&quot;, then define some statistical model in order to define probabilities. Without those, this can go nowhere." CreationDate="2015-03-16T22:58:15.713" UserId="471" />
  <row Id="5642" PostId="5328" Score="0" Text="Are you trying to detect anomalous behavior? It would help if you elaborate what exactly are you trying to achieve here." CreationDate="2015-03-17T06:03:03.197" UserId="1131" />
  <row Id="5643" PostId="5328" Score="0" Text="Yes we can say it anomalous, Actually the idea is to capture some specific malware behaviour." CreationDate="2015-03-17T07:13:25.703" UserId="4993" />
  <row Id="5644" PostId="5328" Score="0" Text="You should be looking for [Outlier analysis](http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html#example-covariance-plot-outlier-detection-py) in that case." CreationDate="2015-03-17T07:18:20.020" UserId="1131" />
  <row Id="5645" PostId="5332" Score="0" Text="Imagine pruning a tree all the way back to the root, would that make it more general ?" CreationDate="2015-03-17T07:19:55.070" UserId="7980" />
  <row Id="5646" PostId="5331" Score="0" Text="Are you asking how to get a list of pixel co-ordinates from an image of a character ?  I'm not sure what language you might be using, but in Mathematica this entire list would be returned by: Position[ImageData@Binarize@myCharacterImage, 0]" CreationDate="2015-03-17T07:32:35.543" UserId="7980" />
  <row Id="5647" PostId="5328" Score="0" Text="The question is not about how to find outliers, The question is how to tune outliers detection system performance in terms of detection." CreationDate="2015-03-17T08:35:37.430" UserId="4993" />
  <row Id="5648" PostId="5328" Score="0" Text="Would that not translate to using different kernels (linear, rbf, sigmoid) and tuning parameters for one-class SVM? Correct me if I understand this wrongly." CreationDate="2015-03-17T08:48:19.377" UserId="1131" />
  <row Id="5649" PostId="5328" Score="0" Text="Yes kernels methods helps in tuning classification, but the problem with malware behavioural detection technique is that these are not adaptable with input data environment. and perform poor if test data is not taken from the same environment which provide training data. this is mainly because of dynamic nature of network traffic." CreationDate="2015-03-17T10:10:53.760" UserId="4993" />
  <row Id="5651" PostId="1053" Score="0" Text="Turns out that since 4 days ago [Spark supports data frames](https://spark.apache.org/releases/spark-release-1-3-0.html) and probably loading from csv files, which is something. I will spend some time checking that." CreationDate="2015-03-17T11:45:25.617" UserId="1281" />
  <row Id="5652" PostId="5328" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/22019/discussion-between-sidhha-and-nahraf)." CreationDate="2015-03-17T14:10:01.300" UserId="1131" />
  <row Id="5653" PostId="5338" Score="1" Text="Thanks a lot for your answer! Maybe my question is not very clear, but I struggle more with the calculation of the Y value than with the diverging color scheme. I have the absolute values of the metric, but that doesn't really help me showing the &quot;speed&quot; of change, and if the &quot;speed&quot; is positive compared to the last &quot;step&quot;/&quot;time bucket&quot;." CreationDate="2015-03-17T14:55:41.753" UserId="8686" />
  <row Id="5654" PostId="5338" Score="0" Text="A line gradient solution with D3 can be found at http://bl.ocks.org/d3noob/3e72cafd95e1834f599b and (better) http://codepen.io/ekrembk/pen/ights/ for example..." CreationDate="2015-03-17T15:03:08.393" UserId="8686" />
  <row Id="5655" PostId="5338" Score="0" Text="Have a look at http://codepen.io/anon/pen/PwxaRy Yet, the problem with the Y values remains... Do you have an idea?" CreationDate="2015-03-17T15:18:51.850" UserId="8686" />
  <row Id="5657" PostId="5338" Score="1" Text="You're very welcome! I couldn't display the result of your most recently referred code, as it's anon. Anyway, I don't see calculation of speed/rate of change as a problem. Rate of chage $R_i$ (for a time delta $\Delta t_i$) = $\frac{Y_{i+1} - Y_i}{\Delta t_i}$. So your $Y$ for the trend chart should be the $R_i$. What is the problem with that?" CreationDate="2015-03-17T15:45:03.317" UserId="2452" />
  <row Id="5658" PostId="5338" Score="0" Text="I already calculate the change rates. That's not the problem. Normally, the change rates (with my metric (except errors)) will be &gt;= 0. What I want to with my graph is the &quot;speed&quot; that the values of the metric change from `t0` to `t1`. If the &quot;speed&quot; slows down, I want to show a negative trend. This is IMHO not possible with just the change rate." CreationDate="2015-03-17T15:50:04.433" UserId="8686" />
  <row Id="5659" PostId="5338" Score="1" Text="@Tobi: Very easy (if I understand you correctly) - you just need to calculate [second derivative](http://en.wikipedia.org/wiki/Second_derivative) (rate of change of rate of change) at your predefined intervals. Those will be your $Y$ values." CreationDate="2015-03-17T16:04:28.490" UserId="2452" />
  <row Id="5660" PostId="5338" Score="0" Text="Sorry for being a total noob at this. You mean as described at http://en.wikibooks.org/wiki/Numerical_Methods/Numerical_Differentiation#Second_Derivative" CreationDate="2015-03-17T16:29:07.390" UserId="8686" />
  <row Id="5661" PostId="5338" Score="1" Text="@Tobi: No need to apologize. Nobody knows everything :-). Yes, I believe that's the _quadratic approximation_. There are lots of resources on the topic. Have a look at [this discussion](http://math.stackexchange.com/q/810454), for example. Good luck and let me know, if I can be of more help!" CreationDate="2015-03-17T16:42:52.530" UserId="2452" />
  <row Id="5662" PostId="1125" Score="0" Text="Check [this relevant answer](http://stats.stackexchange.com/a/136760/31372) of mine on Cross Validated. I hope that it will be of your interest." CreationDate="2015-03-17T17:24:15.250" UserId="2452" />
  <row Id="5663" PostId="5331" Score="0" Text="I've already written a script in python which returns a list of points (x,y) on button-1 click (left click). (As we draw in paint, we keep left click pressed ) ..... I'll have to draw all the characters of the keyboard in order to generate the training data, Which is tedious. So, I am interested if any standard such databse already exists?" CreationDate="2015-03-17T18:23:19.520" UserId="8681" />
  <row Id="5664" PostId="5331" Score="0" Text="For example, you might consider automatically converting an existing dataset, for instance the NMIST digits you mention, using something like the above function applied to every character image file. It would scale to thousands of images quite easily." CreationDate="2015-03-17T19:07:38.503" UserId="7980" />
  <row Id="5665" PostId="5331" Score="0" Text="Do a google image search for handwritten characters and you will get a lot of examples." CreationDate="2015-03-17T20:03:05.083" UserId="7980" />
  <row Id="5667" PostId="5303" Score="1" Text="Please post your PCA matrix which maps your data to PCA vectors.  I'd like to figure out what the two paths mean." CreationDate="2015-03-17T20:56:11.513" UserId="1077" />
  <row Id="5668" PostId="5341" Score="0" Text="You have already posted this question on _Cross Validated_. It is almost always not a good idea to post the same question on multiple _SE_ sites." CreationDate="2015-03-17T21:40:51.227" UserId="2452" />
  <row Id="5669" PostId="1128" Score="0" Text="That sort of set sounds plausible, to me.  I'd like to find, either, an explicit set that is used in some practical situation.... or a specific theoretic set, ideally accompanied by an (informal?) &quot;proof&quot; that this set is &quot;sufficient&quot; - in, erm, any sense that that makes sense." CreationDate="2015-03-17T22:13:20.233" UserId="3328" />
  <row Id="5670" PostId="5331" Score="0" Text="First, I don't know how to sample those data. Even if I do it, my array ( obtained from the image) will contain entries from left to right scanning of the image. I want entries according to user strokes, not those obtained by left to right ( n up-down) scanning of the image sample. Could you suggest a way to get entries as user strokes them?" CreationDate="2015-03-18T03:13:39.353" UserId="8681" />
  <row Id="5671" PostId="5331" Score="0" Text="I.e. Entries as per user hand moves while writing a character." CreationDate="2015-03-18T03:14:24.663" UserId="8681" />
  <row Id="5672" PostId="5343" Score="0" Text="thank you for the reply. Can you recommend any good Aho-Corasick implementation/flavor that can handle extended matching features (* and ? ) and preferably SIMD instructions? Naive implementations I tried were quite slow and didn't have extended matching features." CreationDate="2015-03-18T07:10:24.150" UserId="8597" />
  <row Id="5673" PostId="5343" Score="0" Text="Patterns are to be changed moderately often, so I could recompile the automaton once it happens." CreationDate="2015-03-18T07:28:48.913" UserId="8597" />
  <row Id="5674" PostId="5344" Score="0" Text="What a muddle. Are you aware that your source is discussing gradient descent in the context of [reinforcement learning](http://en.wikipedia.org/wiki/Reinforcement_learning)? This is used when you are facing situations that change over time; esp. in response to actions you have taken. If this is not your case, I refer you to the [illustrated explanation in Wikipedia](http://en.wikipedia.org/wiki/Gradient_descent)." CreationDate="2015-03-18T08:19:22.737" UserId="381" />
  <row Id="5675" PostId="5344" Score="0" Text="cool, good looks on that. but it doesn't really change the nature of my question." CreationDate="2015-03-18T08:22:23.997" UserId="8285" />
  <row Id="5677" PostId="5338" Score="0" Text="Sorry, I'm still having problems with the calculation. Could you please give me an examle with the above provided sample data?" CreationDate="2015-03-18T12:44:43.900" UserId="8686" />
  <row Id="5678" PostId="5338" Score="0" Text="@Tobi: What language are we talking about? JavaScript? Also, does your column `change_rate_delta` contains already pre-calculated values, or `change_rate` should be used?" CreationDate="2015-03-18T20:19:38.400" UserId="2452" />
  <row Id="5679" PostId="5345" Score="0" Text="How about Sense - A Next-Generation Platform for Data Science(http://blog.sense.io/introducing-sense-a-platform-for-data-science/). quote &quot;Sense brings together the most powerful tools — R, Python, Julia, Spark, Impala, Redshift, and more — into a unified platform to accelerate data science from exploration to production.&quot;" CreationDate="2015-03-19T02:32:19.980" UserId="1003" />
  <row Id="5680" PostId="5351" Score="0" Text="Just a clarification: when I said &quot;I don't know why one would prefer other options&quot; that statement implied exclusion of Emacs fans - they have special preferences and obviously gravitate toward Emacs-based R solutions :-)." CreationDate="2015-03-19T06:15:06.457" UserId="2452" />
  <row Id="5681" PostId="5349" Score="0" Text="What's SOM? I don't think I've come across this acronym before." CreationDate="2015-03-19T07:35:31.197" UserId="587" />
  <row Id="5682" PostId="5349" Score="0" Text="Whoops, I probably should have spelled that out somewhere.  Updating the question." CreationDate="2015-03-19T08:25:08.230" UserId="8714" />
  <row Id="5683" PostId="5352" Score="0" Text="Taking into accounjt this fact http://blog.revolutionanalytics.com/2015/01/revolution-acquired.html we can expect further support to R from Microsoft" CreationDate="2015-03-19T09:29:19.500" UserId="97" />
  <row Id="5684" PostId="5331" Score="0" Text="Sorry, I misunderstood your needs. There appears to be some work on stroke order on Chinese characters.  http://commons.wikimedia.org/wiki/Commons:Stroke_Order_Project" CreationDate="2015-03-19T10:39:12.730" UserId="7980" />
  <row Id="5685" PostId="5331" Score="0" Text="@image_doctor : thanks. You are awesome. :)&#xA;Though I'll still need some help for Roman script." CreationDate="2015-03-19T14:45:02.187" UserId="8681" />
  <row Id="5686" PostId="5353" Score="0" Text="thank you for the great links!" CreationDate="2015-03-19T19:52:54.130" UserId="8597" />
  <row Id="5687" PostId="5349" Score="0" Text="Thanks. Good question, hope someone knows the answer to it." CreationDate="2015-03-20T07:10:52.800" UserId="587" />
  <row Id="5688" PostId="5338" Score="0" Text="JavaScript/Node.js is my preferred language. As far as I understood, it should be possible to do the calc based on the raw metric values provided with the sample data. But I think I'm doing something wrong. Thanks again for your effort!" CreationDate="2015-03-20T08:37:33.423" UserId="8686" />
  <row Id="5689" PostId="5344" Score="0" Text="value of feature_j is just the mean of the value of the j-th element in all the input vectors" CreationDate="2015-03-20T09:27:26.953" UserId="2576" />
  <row Id="5690" PostId="5344" Score="0" Text="so for the weight associated with the word 'j', find the corresponding value in each of the training example files, and then take the mean of that, and that's what I multiply the loss function by in the update?" CreationDate="2015-03-20T09:33:38.270" UserId="8285" />
  <row Id="5691" PostId="5348" Score="0" Text="I think you need to define what MOA is. This sounds like you are asking about how to use a particular software tool, and unless it's a very common tool like R that's usually considered off topic here." CreationDate="2015-03-20T14:08:38.800" UserId="21" />
  <row Id="5694" PostId="5330" Score="0" Text="thanks. It turns out the issue was `Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,]` was not doing what I intended. I had thought it would interpret the T/F in that column in the data frame temp and use that as inclusion/exclusion criteria. Changing that to `Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF==&quot;TRUE&quot;,]` works and takes almost no time. Examining the results of the incorrect version, it produced a huge file with all NA's. So my question is now, why didn't it take my temp$FGF values and what was it actually doing in that case? Thanks" CreationDate="2015-03-18T12:01:30.120" UserId="8706" />
  <row Id="5695" PostId="5354" Score="0" Text="Thanks. I will try gwidgets too." CreationDate="2015-03-20T17:02:36.027" UserId="8662" />
  <row Id="5696" PostId="5358" Score="0" Text="This is entirely dependent on the software you are using to manage your documents. What are you using?" CreationDate="2015-03-20T17:26:29.933" UserId="3466" />
  <row Id="5697" PostId="5357" Score="2" Text="This question appears to be primarily opinion based. Please consider rephrasing. Maybe ask what kinds of data science tools are available for C/C++, or what kinds of applications use these languages." CreationDate="2015-03-20T17:30:39.807" UserId="3466" />
  <row Id="5700" PostId="5357" Score="1" Text="@sheldonkreger  That is what I'm asking, I'll make that more clear, thanks" CreationDate="2015-03-20T17:44:09.740" UserId="2723" />
  <row Id="5701" PostId="5357" Score="0" Text="Awesome, thanks." CreationDate="2015-03-20T17:46:43.700" UserId="3466" />
  <row Id="5703" PostId="5360" Score="0" Text="I don't think there is a way to achieve this without having a unified access-check mechanism using an architecture similar to what I described above. You need to find a way to, EVERY time a file is requested - regardless of which system it is on - validate that the user requesting it has the correct permission. How you implement that will depend entirely on what kind of software you are using now." CreationDate="2015-03-21T04:22:01.043" UserId="3466" />
  <row Id="5704" PostId="5345" Score="1" Text="@scyen: Sense and similar products (or, rather, the approach) are indeed interesting, however, they are not &quot;IDE alternatives for R programming&quot;, but large, complex and often expensive **platforms** for data science work. Note that this question is specifically about development environments / IDEs." CreationDate="2015-03-21T23:53:10.237" UserId="2452" />
  <row Id="5705" PostId="5348" Score="0" Text="MOA is a tool for Massive Online Analysis http://moa.cms.waikato.ac.nz/" CreationDate="2015-03-23T13:53:16.593" UserId="8710" />
  <row Id="5706" PostId="5366" Score="0" Text="Links aren't considered valid answers on this site. Please answer the original question in your post and use links to supplement your answer." CreationDate="2015-03-23T17:40:37.060" UserId="3466" />
  <row Id="5712" PostId="5358" Score="0" Text="I don't have vote-to-close karma on this SE yet, but this seems way off-topic." CreationDate="2015-03-24T12:34:52.410" UserId="8798" />
  <row Id="5713" PostId="4949" Score="0" Text="If you feel like you have an answer (as in your EDIT), consider posting it as an actual answer. It's OK to answer your own questions on Stack Exchange, and it makes things clearer." CreationDate="2015-03-24T12:37:08.230" UserId="8798" />
  <row Id="5714" PostId="5358" Score="0" Text="@JoeGermuska: Yeah, you can ask a moderator to close it. I can't delete the question because it has an answer. Would be helpful if the [What topics can I ask about here?](http://datascience.stackexchange.com/help/on-topic) would give a hint what was on topic and what wasn't. Why do you think this was off-topic when it's not defined what is *on* topic?" CreationDate="2015-03-24T12:38:43.823" UserId="8743" />
  <row Id="5715" PostId="5357" Score="1" Text="I've used Waffles (C++) to incorporate machine learning into existing C++ engines." CreationDate="2015-03-24T18:11:02.200" UserId="1077" />
  <row Id="5716" PostId="5365" Score="0" Text="Because languages like R and Python are very slow/inefficient compared to languages like C. Thus when dealing with lots of data and computations, if you can do something in C it's faster than if you can do it in R. I do love and use Hadley's packages tho!" CreationDate="2015-03-24T18:25:56.267" UserId="2723" />
  <row Id="5717" PostId="5357" Score="0" Text="@Pete if you can incorporate that into an answer I'd be likely to mark it as the solution" CreationDate="2015-03-24T18:28:05.220" UserId="2723" />
  <row Id="5718" PostId="5366" Score="0" Text="OK, thanks, will do." CreationDate="2015-03-24T18:36:36.600" UserId="8762" />
  <row Id="5719" PostId="5383" Score="1" Text="Sure it will. If you suddenly get a new group of people on your site with a specific fetish, you can be sure that certain items will spike in their values." CreationDate="2015-03-25T09:59:06.760" UserId="587" />
  <row Id="5722" PostId="5357" Score="1" Text="Meta toolkit is available in C++ : https://meta-toolkit.github.io/meta/. There's a course on Coursera that uses it, it's still in week 1, so you may want to take a look. The course is called &quot;Text Retrieval and Search Engines&quot;." CreationDate="2015-03-25T12:29:46.440" UserId="587" />
  <row Id="5723" PostId="5388" Score="0" Text="Excellent answer (+1). Just one suggestion: if possible, provide literature or, at least, general references for the _shallow NLP technique_ that you've mentioned." CreationDate="2015-03-25T13:17:54.010" UserId="2452" />
  <row Id="5724" PostId="5357" Score="0" Text="@LauriK OK, why are you guys putting the best answers in comments instead of as actual answers?? ;)  Seriously, I upvoted the answers because they were great comments, but the actual answers I was looking for like this are in comments!" CreationDate="2015-03-25T13:25:00.010" UserId="2723" />
  <row Id="5725" PostId="5370" Score="1" Text="Thanks, Andre. I do use Pybrain a lot; for me Python is a middleground in between R and C, but I still wanted to learn C for both speed and wider application of the code. I selected this as the solution because I had not thought of using C/C++ to write R extensions, which is a really wonderful idea that I'm absolutely going to do. Thanks!!" CreationDate="2015-03-25T13:27:40.333" UserId="2723" />
  <row Id="5726" PostId="5388" Score="0" Text="Thank you so much. Two questions, can I do this with nltk?  Could I use tf-idf to do the same, then take the most unique words (highest scores) as my key words?" CreationDate="2015-03-25T13:30:04.140" UserId="8643" />
  <row Id="5729" PostId="5388" Score="0" Text="@ Aleksandr Blekh, thanks. I have added additional reading links for learning more about shallow and deep NLP. Hope this helps" CreationDate="2015-03-25T13:56:20.720" UserId="8465" />
  <row Id="5730" PostId="5388" Score="0" Text="@ William Falcon, thanks. 1) Yes, you can use nltk 2) Absolutely, TF-IDF can be used If you are trying to find the concept or theme at document(s) level." CreationDate="2015-03-25T13:59:47.023" UserId="8465" />
  <row Id="5731" PostId="819" Score="0" Text="PostgreSQL also has HSTORE columns and (recently) JSON which provide an outlet for schemaless data associated with rows which are otherwise typed." CreationDate="2015-03-25T20:07:27.683" UserId="8798" />
  <row Id="5732" PostId="5357" Score="0" Text="@Hack-R thanks :)" CreationDate="2015-03-25T20:11:32.137" UserId="587" />
  <row Id="5734" PostId="739" Score="0" Text="Ask Quora might help." CreationDate="2015-03-26T03:41:01.120" UserId="1048" />
  <row Id="5736" PostId="5384" Score="0" Text="These look like great references. Thanks for pointing them out! After a first glance, I think they'll do great to get me started in studying this." CreationDate="2015-03-26T14:33:20.330" UserId="8692" />
  <row Id="5742" PostId="5383" Score="2" Text="User behavior defines item similarity, or at least that's what I take from your question. how can user behavior not change item similarity?" CreationDate="2015-03-27T11:27:06.387" UserId="21" />
  <row Id="5743" PostId="5393" Score="0" Text="The question isn't actually complete..." CreationDate="2015-03-27T11:27:55.127" UserId="21" />
  <row Id="5744" PostId="5401" Score="0" Text="Requests for book recommendations and the like are generally off topic on StackExchange" CreationDate="2015-03-27T11:28:34.987" UserId="21" />
  <row Id="5745" PostId="5407" Score="0" Text="Thanks. Most of the data I'm analyzing is in multi-dimensional form. NoSQL solutions and other cloud based solutions are not really an option for me. Thus, it doesn't make sense to look towards Azure. However, I'll check out the blog post. Unfortunately, SSAS hasn't really improved all that much over the years and I am on 2008 R2." CreationDate="2015-03-27T11:45:11.783" UserId="8842" />
  <row Id="5746" PostId="5407" Score="0" Text="By the way, can you add more insight on the limited statistical methods and algorithm variety? I assumed you could develop your own methods and algorithms with SSAS just like you could with R? Or do you mean the availability of those packages is not as common as the open source community?" CreationDate="2015-03-27T11:51:04.613" UserId="8842" />
  <row Id="5747" PostId="5407" Score="0" Text="@Fastidious: You're welcome. Re: your first comment (I'll address the second one in the next comment). I don't understand your rationale behind rejecting Azure ML. While I'm not a big fan of Microsoft solutions, but for those who are tied to that technology stack, Azure ML seems like a sensible (while, for some, still might not be the best) option. Azure ML has direct ties with Azure, which, being a general cloud platform, supports whatever you throw at it. Wrap whatever environment and tools you use into (or install them separately on) a virtual machine and launch it. It is not cheap, though." CreationDate="2015-03-27T11:55:06.370" UserId="2452" />
  <row Id="5748" PostId="5407" Score="0" Text="@Fastidious: Absolutely. The richness of R ecosystem in terms of community and packages is unparalleled. Even considering recent Microsoft's acquisition of R-focused company Revolution Analytics, it is not going to change the overall situation (while it might improve some MS offerings, add integrations, etc.). R's foothold is way too solid. So, in theory, you can develop your data science projects on SSAS, but you'll experience, at least, two main obstacles: 1) limited variety of packages; 2) limited options for language and platform interoperability. If your needs are modest, it might be OK." CreationDate="2015-03-27T12:07:21.623" UserId="2452" />
  <row Id="5749" PostId="5407" Score="0" Text="The rational is because of strict policies of passing data outside of our network via API's and of course the fact the data is terabytes of data or gigabytes of data per day. So, storage costs, bandwidth, processing power and so forth are deep considerations depending on the costs. Then of course there is API development costs too. I can't just jump to a cloud solution at the drop of a hat hah. :)" CreationDate="2015-03-27T12:27:38.920" UserId="8842" />
  <row Id="5750" PostId="5407" Score="0" Text="@Fastidious: I see. Well, this situation requires a detailed and careful analysis of all factors involved, which hopefully will produce an optimal solution. Good luck!" CreationDate="2015-03-27T13:20:36.140" UserId="2452" />
  <row Id="5751" PostId="5393" Score="0" Text="my full quesetion doesn't appear to be showing. Is there a page error? This is what I see on edit." CreationDate="2015-03-27T15:25:37.753" UserId="8823" />
  <row Id="5752" PostId="5393" Score="0" Text="I have been looking at Stochastic Single Value Decomposition in Mahout for implementing a distributed LSA algorithm. However, I am having trouble finding the best way to set k and p such that k+p&lt;min(m,n). Is there an optimal way to set k and p? I know that p should not exceed 10% of k and that k is the rank (typically from 20 to 200 according to the documents). Can I relate it to the number of dependent vectors?" CreationDate="2015-03-27T15:25:43.987" UserId="8823" />
  <row Id="5753" PostId="5393" Score="0" Text="Fixed it. You had a &lt; that was not followed by space so became an unclosed tag." CreationDate="2015-03-27T15:37:44.240" UserId="21" />
  <row Id="5754" PostId="5393" Score="0" Text="ok thanks, please let me know if I need more" CreationDate="2015-03-27T15:39:32.450" UserId="8823" />
  <row Id="5755" PostId="5407" Score="1" Text="No worries. We do use R and feed data to it via SQL Server. I just wanted to see if we can still leverage everything in SQL too." CreationDate="2015-03-27T17:55:40.993" UserId="8842" />
  <row Id="5756" PostId="5394" Score="0" Text="thanks, but I'm still wondering does the small contribution of $UV$ imply that, to fit my data, I actually doesn't need this non-linear part?" CreationDate="2015-03-28T07:31:39.480" UserId="7867" />
  <row Id="5757" PostId="5401" Score="0" Text="@SeanOwen: Actually, I'm not sure that book recommendations should be considered off-topic here. My logic and experience of Cross Validated, which is a serious forum, suggests that book recommendations are OK, as long as the requests are related to the topic of discussion. Please let me know, if there exist a corresponding DS Meta discussion, so that I could share my opinion." CreationDate="2015-03-28T13:58:35.710" UserId="2452" />
  <row Id="5758" PostId="5412" Score="0" Text="Your algorithm is incomprehensible from your description so far. I can see you want to label some words as relevant or not relevant, and use that to train a NN from the sentences, in order to find relevant words in other sentences. But I cannot see what representation you are using for the sentence. Could you explain perhaps with a little pseudo-code?" CreationDate="2015-03-28T16:42:41.010" UserId="836" />
  <row Id="5759" PostId="5413" Score="0" Text="also, jpeg images cannot be the same title, so wouldn't the ML program recognize every image as its own class if all titles different?" CreationDate="2015-03-28T22:36:22.733" UserId="8860" />
  <row Id="5760" PostId="5412" Score="0" Text="added some explanation." CreationDate="2015-03-29T03:37:39.607" UserId="7712" />
  <row Id="5761" PostId="5414" Score="0" Text="This is right. I asked someone at Ersatz Labs too. Same answer: group class by folder. Thanks." CreationDate="2015-03-29T20:16:16.590" UserId="8860" />
  <row Id="5762" PostId="5414" Score="0" Text="by the way, for that Kaggle link. That is a TON of classes for only 30K images. Interesting. Thanks for sharing that." CreationDate="2015-03-29T20:18:31.723" UserId="8860" />
  <row Id="5763" PostId="5387" Score="0" Text="Thanks @LauriK for your comment. Would you mind elaborating what language or program that automated scripts are written on? I assume you have scripts written on sql. How do you automate them? what software your dashboards are made of? Please give a detailed example as I am trying to adopt something similar to what you have." CreationDate="2015-03-29T21:26:17.557" UserId="7923" />
  <row Id="5764" PostId="5412" Score="1" Text="I am still not sure that I understand your idea. However, the usual way to test such ideas is to build them and score them against other approaches. A sliding window with offsets is similar to n-grams, and it is definitely possible to train a neural network to predict likely next words from n-grams." CreationDate="2015-03-30T07:24:30.700" UserId="836" />
  <row Id="5765" PostId="5387" Score="0" Text="@JeanVids edited the answer." CreationDate="2015-03-30T11:19:12.430" UserId="587" />
  <row Id="5766" PostId="5412" Score="0" Text="You may want to take a look at a course that is specifically about this: https://www.coursera.org/course/textretrieval" CreationDate="2015-03-30T11:23:37.113" UserId="587" />
  <row Id="5767" PostId="5358" Score="0" Text="Duplicate of [Solr Permissions / Filtering Results depending on Access Rights](http://stackoverflow.com/questions/9222835/solr-permissions-filtering-results-depending-on-access-rights)." CreationDate="2015-03-30T14:23:08.267" UserId="8743" />
  <row Id="5768" PostId="5421" Score="0" Text="Hi, thanks for posting. We are aiming to keep a high quality of content on this site. Can you please clean up your formatting, capitalization, and punctuation? If you need help, let me know." CreationDate="2015-03-30T18:36:42.787" UserId="3466" />
  <row Id="5769" PostId="5358" Score="0" Text="I'm voting to close this question as off-topic because poster requested to close as off-topic." CreationDate="2015-03-31T08:10:38.127" UserId="21" />
  <row Id="5771" PostId="5284" Score="0" Text="http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/" CreationDate="2015-04-01T06:50:37.207" UserId="118" />
  <row Id="5772" PostId="5284" Score="0" Text="http://stackoverflow.com/questions/9162943/how-does-hdfs-with-append-works" CreationDate="2015-04-01T06:51:04.357" UserId="118" />
  <row Id="5773" PostId="2454" Score="0" Text="Take a look at AWS lambda service http://aws.amazon.com/ru/lambda/" CreationDate="2015-04-01T06:53:12.893" UserId="118" />
  <row Id="5775" PostId="5429" Score="0" Text="time to read more on Monte-Carlo and multidimensional statistical analysis...thanks for nice explanation..!!!" CreationDate="2015-04-02T04:58:45.300" UserId="6465" />
  <row Id="5776" PostId="5429" Score="0" Text="@hadooper How about an up-vote?" CreationDate="2015-04-02T06:00:55.527" UserId="609" />
  <row Id="5777" PostId="5429" Score="2" Text="It requires 15 rep points...!!!" CreationDate="2015-04-02T06:07:21.577" UserId="6465" />
  <row Id="5778" PostId="5416" Score="0" Text="This is covering a lot of ground. Can you be more specific and show what you have tried already and what didn't work?" CreationDate="2015-04-02T12:07:13.097" UserId="21" />
  <row Id="5779" PostId="5429" Score="0" Text="@hadooper: Just gave you the vote you needed to vote, cheers!" CreationDate="2015-04-02T14:40:22.390" UserId="158" />
  <row Id="5781" PostId="5435" Score="2" Text="`**` is an old-fashioned symbol (originating from the original Algol and Fortran languages in the 1950's) for exponentiation." CreationDate="2015-04-02T21:55:54.550" UserId="2825" />
  <row Id="5782" PostId="5435" Score="0" Text="So in other words it is simply an exponent, ie. O(N^3)? @whuber" CreationDate="2015-04-02T21:58:22.793" UserId="8936" />
  <row Id="5783" PostId="5435" Score="0" Text="Not quite: &quot;3&quot; is the exponent, &quot;\*\*&quot; is the exponentiation operator.  &quot;N\*\*3&quot; nowadays is often written (in text) as &quot;N^3&quot;, it's read aloud as &quot;N cubed,&quot; and can be computed as N\*N\*N." CreationDate="2015-04-02T21:59:50.127" UserId="2825" />
  <row Id="5784" PostId="5435" Score="0" Text="Right, that's what I meant. :) Thank you! @whuber" CreationDate="2015-04-02T22:16:45.733" UserId="8936" />
  <row Id="5785" PostId="5401" Score="0" Text="@SeanOwen Oh. I didn't know that. This was my first question on StackExchange. I will try to be more careful next time." CreationDate="2015-04-02T23:47:41.343" UserId="8839" />
  <row Id="5786" PostId="5402" Score="0" Text="Thank you for your kind answer." CreationDate="2015-04-02T23:48:45.907" UserId="8839" />
  <row Id="5787" PostId="5437" Score="1" Text="Thanks a lot Aleksandr Blekh. I am looking into your suggestions now." CreationDate="2015-04-02T23:52:36.297" UserId="3314" />
  <row Id="5788" PostId="5437" Score="0" Text="@user62198: You're very welcome. Good luck!" CreationDate="2015-04-02T23:55:17.020" UserId="2452" />
  <row Id="5789" PostId="5440" Score="0" Text="Thanks.  That's definitely useful." CreationDate="2015-04-03T02:48:13.807" UserId="8938" />
  <row Id="5790" PostId="5437" Score="0" Text="By &quot;automatically recode those values&quot;, do you mean that I consider the missing values as a separate factor level ? Could you please comment ? Thanks.." CreationDate="2015-04-03T02:51:48.080" UserId="3314" />
  <row Id="5791" PostId="5441" Score="0" Text="All useful terms, but I think agree that performativity is the most applicable, at least in terms of trying to express the concept in writing.  Obviously, there is a fair bit of heterogeneity of terminology across fields, though.  So, I'm not sure that any of these are really going to help me with any literature research.  But that's probably the term that I'm going to try to use." CreationDate="2015-04-03T02:52:42.213" UserId="8938" />
  <row Id="5792" PostId="5437" Score="0" Text="@user62198: No. I meant the recoding in `R` parlance: `''` =&gt; `NA`." CreationDate="2015-04-03T02:56:38.767" UserId="2452" />
  <row Id="5793" PostId="5440" Score="0" Text="@jsmith54: My pleasure." CreationDate="2015-04-03T02:57:15.770" UserId="2452" />
  <row Id="5794" PostId="5429" Score="0" Text="upvoted...!!!!!" CreationDate="2015-04-03T05:00:15.530" UserId="6465" />
  <row Id="5795" PostId="5438" Score="0" Text="Yes, &quot;feedback loop&quot; is how I would always describe this." CreationDate="2015-04-03T09:28:23.903" UserId="21" />
  <row Id="5797" PostId="5437" Score="0" Text="Thanks much. Your answer was very helpful." CreationDate="2015-04-03T13:18:31.290" UserId="3314" />
  <row Id="5798" PostId="5438" Score="0" Text="I think feedback loop is definitely useful to say, but the literature on performativity and reflexivity seem to address more of the complex interplay that I was trying to discuss.  So I think it's useful to say something like, &quot;A model that can act on the concept to change the concept can thus change itself in a sort of feedback loop.  Let's call this property (performativity/reflexivity).  This is distinct from concept drift, because the model is the cause of the drift.&quot;" CreationDate="2015-04-03T15:08:39.797" UserId="8938" />
  <row Id="5799" PostId="5437" Score="0" Text="@user62198: My pleasure. I'm glad I could help. Feel free to upvote as well :-)." CreationDate="2015-04-03T18:01:10.613" UserId="2452" />
  <row Id="5800" PostId="339" Score="6" Text="&quot;Speed: R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow.&quot;&#xA;&#xA;I'm not an experienced R user, but as far as I know pretty much everything with low-level implementations in R also has a similar low-level implementation in numpy/scipy/pandas/scikit-learn/whatever. Python also has numba and cython. This point should be a tie." CreationDate="2015-04-03T22:05:28.860" UserId="2885" />
  <row Id="5801" PostId="5442" Score="0" Text="Their data is almost certainly not public." CreationDate="2015-04-03T22:06:40.433" UserId="2885" />
  <row Id="5802" PostId="5444" Score="3" Text="I would never fault someone for not knowing Hadoop but even in small data situations I feel as if R is superior.  There are simply a miriad of things you can do with R that you can't do with Excel.  It concerns me this individual has not &quot;discovered&quot; that in his 15+ years" CreationDate="2015-04-03T23:30:42.300" UserId="8944" />
  <row Id="5803" PostId="5444" Score="0" Text="@JHowIX: Are your familiar with the term &quot;good enough&quot;? I'm also a big fan of R and would prefer it to many tools, Excel included, any day. However, the fact that R can do more doesn't imply that Excel (or any other tool suitable for a task) is inferior in a particular work context. So, while your concern is valid (I refer to that by using word &quot;disturbing&quot;), it might be that the person haven't had an opportunity/need to do that. Remember, that you're talking about time, when R existed, but was popular mostly in academia and data science (termed data analysis or such) was not as hot as today." CreationDate="2015-04-03T23:39:50.293" UserId="2452" />
  <row Id="5804" PostId="5442" Score="0" Text="https://datafloq.com/read/big-data-obama-campaign/516" CreationDate="2015-04-03T23:40:22.117" UserId="8860" />
  <row Id="5807" PostId="5420" Score="0" Text="Explain more? Is the server just returning responses from the trained model, or is the app sending data and the server needs to do the machine learning magic as well? Anyhoo, have you looked for web server frameworks written in lua? http://turbolua.org/ for example, lets you hook lua functions into web requests." CreationDate="2015-04-04T08:47:03.057" UserId="471" />
  <row Id="5808" PostId="5443" Score="0" Text="Most data science job ads ask for specific skills, like R, Hadoop, whatever. Did you neglect to mention this in your ad? Unless your new Data Scientist is going to work in a bubble then he or she will have to work with the team, and probably need to work with the standard team software..." CreationDate="2015-04-04T08:50:54.213" UserId="471" />
  <row Id="5809" PostId="5443" Score="1" Text="well if they won't use `\LaTeX{}` then i wouldn't hire 'em.  just kidding..." CreationDate="2015-04-04T17:47:03.633" UserId="8953" />
  <row Id="5811" PostId="5457" Score="0" Text="While active learning (AL) seems like a close enough concept, I think that there is a significant difference between AL and situation, described by the OP - the former implies **intentional supervising efforts**, while the latter implies the **lack** of such efforts and, even, intentions. Therefore, my preference is either _side effect_, or _feedback loop_ terms." CreationDate="2015-04-05T08:45:36.657" UserId="2452" />
  <row Id="5812" PostId="5457" Score="0" Text="I'd say that active learning, while definitely related, tends to refer to a fairly specific technique intentionally deployed, rather than the unintentional consequence of an arbitrary model type acting on the concept.  It strikes me as similar to reinforcement learning.  These techniques are likely useful, once you've realized you have a feedback loop in your system, but I agree with Aleksandr that they express a design intention I was implying does not exist in my example." CreationDate="2015-04-05T18:53:31.557" UserId="8938" />
  <row Id="5814" PostId="5443" Score="1" Text="@Spacedman:  I provided the story for anecdotal context but am really more interested in people's views on excel than I am hiring tips.  Our team is free to use whatever tools we like." CreationDate="2015-04-05T23:15:49.570" UserId="8944" />
  <row Id="5815" PostId="5458" Score="0" Text="Have you already manually assigned these 12 category labels to some or all of your dataset?  If you haven't, then this becomes more a problem of unsupervised clustering than supervised classification." CreationDate="2015-04-06T01:20:22.777" UserId="182" />
  <row Id="5817" PostId="5435" Score="0" Text="@whuber `**` is very much still alive and used in Python, PERL and [PHP](http://stackoverflow.com/questions/1211514/raising-to-power-in-php) for exponentiation. That's because `^` generally denotes bitwise-xor." CreationDate="2015-04-06T11:51:26.427" UserId="8501" />
  <row Id="5818" PostId="5449" Score="0" Text="Hi Glen, thanks for your comments.  Take a look at the following link.  Its from Swami Chandrasekaran who led the Watson team at IBM, so a pretty experienced data scientist in my opinion.  He has programming as basically the third thing a data scientist needs to know, behind &quot;Fundamentals&quot; and Statistics.  According to his roadmap, once you know how to program, you are 15% of the way to being a data scientist.  Based on this, I might disagree slightly with the statement that true data scientists come in a &quot;non-programming&quot; flavor.  http://nirvacana.com/thoughts/becoming-a-data-scientist/" CreationDate="2015-04-06T13:19:53.803" UserId="8944" />
  <row Id="5819" PostId="5449" Score="0" Text="Well, I only say that based on experience. Most statistics and data science courses even don't cover programming outside of what you need for the popular statistical programs. Due to that, most of the guys I run into in the stats world are not good at programming. It's like an afterthought when they enter the real world and realize it helps." CreationDate="2015-04-06T13:47:59.090" UserId="3417" />
  <row Id="5820" PostId="5442" Score="0" Text="@TyrionLannister if anything is public, it could be worth asking on [opendata.se](http://opendata.stackexchange.com/)." CreationDate="2015-04-06T14:04:27.047" UserId="8953" />
  <row Id="5822" PostId="5435" Score="1" Text="@smci That doesn't make it any less old-fashioned!" CreationDate="2015-04-06T14:57:20.900" UserId="2825" />
  <row Id="5823" PostId="5460" Score="0" Text="&lt;if var1 in (1, 2)&gt;&#xA;This is really what I need. THANK YOU! )" CreationDate="2015-04-06T18:55:00.523" UserId="8951" />
  <row Id="5824" PostId="5460" Score="0" Text="@Ted awesome.  if that solved your problem, click the checkmark to accept this answer so everybody knows you're good to go.  and remember there's a wealth of `r` information over at [stackoverflow](http://stackoverflow.com/questions/tagged/r)." CreationDate="2015-04-06T19:00:05.147" UserId="8953" />
  <row Id="5828" PostId="5464" Score="0" Text="Wait, what exactly are you asking? This sounds more like a blog post than stackexchange question tbh." CreationDate="2015-04-07T09:51:16.250" UserId="587" />
  <row Id="5829" PostId="5464" Score="0" Text="@LauriK well 'what constitutes data science/big data' has been asked here before and there doesn't seem to be a black/white answer.  so i'm giving a specific example and asking where it falls in the gray." CreationDate="2015-04-07T10:49:25.250" UserId="8953" />
  <row Id="5830" PostId="5428" Score="0" Text="can you link to the wikipedia articles you are referring to?  you don't mean the canonical form of a graph and how it's related to graph isomorphism? as opposed to isometric, where the scale of 3d axes-&gt;2d are the same/are all separated by 120deg angles." CreationDate="2015-04-07T18:54:29.023" UserId="8953" />
  <row Id="5831" PostId="5428" Score="0" Text="http://en.wikipedia.org/wiki/Graph_canonization" CreationDate="2015-04-07T19:40:45.430" UserId="3466" />
  <row Id="5832" PostId="5428" Score="0" Text="http://en.wikipedia.org/wiki/Graph_isomorphism" CreationDate="2015-04-07T19:41:15.707" UserId="3466" />
  <row Id="5833" PostId="5428" Score="0" Text="I did indeed mean graph isomorphism, thanks for asking. I have updated the question." CreationDate="2015-04-07T19:42:41.330" UserId="3466" />
  <row Id="5834" PostId="5471" Score="0" Text="Not sure that a web development question belongs on this site. R is popular here, but this question does not seem to be about doing data science." CreationDate="2015-04-07T20:07:57.800" UserId="3466" />
  <row Id="5835" PostId="5471" Score="1" Text="The question is linked to reproducible research in data science as one can seamlessly integrate R source code with markdown and produce html output." CreationDate="2015-04-07T20:32:56.700" UserId="8644" />
  <row Id="5836" PostId="5471" Score="0" Text="I agree with the OP - this question is more about reproducible research than web development and has significant data science component. Therefore, it definitely belongs to this site in my book." CreationDate="2015-04-07T21:55:19.187" UserId="2452" />
  <row Id="5840" PostId="5446" Score="0" Text="Thanks Alexsandr. I'll take a look. I think the most fascinating part of the Obama campaign and big data, in general, is the ability to personalize. Because they had such rich data sets about us, the President was able to give you a different campaign than he gave me. A campaign volunteer probably read a different script to my neighbor than to me." CreationDate="2015-04-08T02:32:53.363" UserId="8860" />
  <row Id="5841" PostId="5461" Score="0" Text="Thanks. I'll check out her work." CreationDate="2015-04-08T02:35:57.617" UserId="8860" />
  <row Id="5842" PostId="5446" Score="0" Text="I think the hardest part is to generate or find these data. The analysis is the easy part." CreationDate="2015-04-08T02:36:12.227" UserId="8860" />
  <row Id="5843" PostId="5446" Score="0" Text="@TyrionLannister: You're welcome. While personalization can certainly benefit from data science, I doubt it's used to such level of granularity - IMHO it is simply not feasible. I also don't think that analysis in political informatics is easy (I mean here a **comprehensive** analysis, not some trivial one). I partially agree with you about data collection, though - it is definitely not an easy part, but doable and rather simple (especially, considering big budgets that can be thrown at such task); analysis, on the other hand, is challenging and of critical importance (to the stakeholders)." CreationDate="2015-04-08T02:55:35.977" UserId="2452" />
  <row Id="5845" PostId="5477" Score="1" Text="I was looking for a pure R-based solutions actually. I will explore the ones that you pointed at. Thank you." CreationDate="2015-04-08T07:46:40.917" UserId="8644" />
  <row Id="5846" PostId="5477" Score="0" Text="@MihaTrošt: My pleasure - I'm glad to help." CreationDate="2015-04-08T07:56:30.417" UserId="2452" />
  <row Id="5853" PostId="5481" Score="0" Text="This system wouldn't do any processing itself? Just a repository of data sets like CKAN?" CreationDate="2015-04-08T17:57:40.183" UserId="471" />
  <row Id="5854" PostId="5471" Score="0" Text="If its really about reproducible research it needs more explanation. As it stands its just &quot;How do I build a static web site using R?&quot;. Close." CreationDate="2015-04-08T18:01:18.453" UserId="471" />
  <row Id="5856" PostId="5481" Score="1" Text="You could always spin up a NoSQL Environment that allows analyst and other data guys to drop data in and play with. Then when data products are discovered, it can be transitioned to the production system that either NoSQL or even a RDBMS. We considered Hadoop because it allows less ETL development and has additional components such as HIVE and the works to play with." CreationDate="2015-04-08T22:52:31.737" UserId="3417" />
  <row Id="5857" PostId="5471" Score="0" Text="I think it's on-topic but merely too broad. A better question would identify what has been tried, and what specifically went wrong." CreationDate="2015-04-09T01:00:19.963" UserId="21" />
  <row Id="5859" PostId="5458" Score="0" Text="Yes, I manually assigned 12 category." CreationDate="2015-04-09T11:24:14.117" UserId="8969" />
  <row Id="5860" PostId="5370" Score="1" Text="I second the notion of learning Python. I work with large datasets and data scientist utilizing R to analyze those datasets. Although I learned C at a very young age, Python is the one language that is truly giving me value as a programmer and assisting these data scientist. Therefore, look to compliment the team, not yourself." CreationDate="2015-04-09T15:13:21.893" UserId="3417" />
  <row Id="5861" PostId="5489" Score="0" Text="IMHO, this question better fits _Cross Validated_ or _StackOverflow_ SE sites." CreationDate="2015-04-09T15:29:58.890" UserId="2452" />
  <row Id="5862" PostId="5490" Score="0" Text="given that &#xA;    fit-&lt; glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;what is fitted(fit)? Could it be what i'm looking for?" CreationDate="2015-04-09T17:55:57.983" UserId="9033" />
  <row Id="5863" PostId="5370" Score="0" Text="similarly python is sped up by writing  in cython (again basically C). I have to say I have yet to use it myself. There is very little that cannot be done using existing libraries (eg scikit-learn, pandas  in python[which are written in cython so you don't have to!])." CreationDate="2015-04-09T21:19:11.783" UserId="1256" />
  <row Id="5864" PostId="5490" Score="0" Text="@Ciochi: No. In your example above, a fitted `glm` model object would be the `fit` object. See UPDATE section in my answer." CreationDate="2015-04-10T00:19:10.900" UserId="2452" />
  <row Id="5865" PostId="5490" Score="0" Text="@Ciochi: So, my suggestion is to use standard access functions (see UPDATE) for extracting traditional information and `str()` and low-level (direct) access (via `$`), if you need other information, not accessible via high-level functions. I hope that this clarifies things." CreationDate="2015-04-10T00:29:36.703" UserId="2452" />
  <row Id="5866" PostId="5490" Score="0" Text="thanks for the answers, i appreciate it. I've already read the Update section you've added, but i still dont get it, mostly because i'm pretty noob on statistics, i just use basic stuff like t-test etc. Indeed, i dont get what the fit object is. Is it a new variable?" CreationDate="2015-04-10T00:42:13.957" UserId="9033" />
  <row Id="5867" PostId="5490" Score="0" Text="@Ciochi: You're welcome. Feel free to accept/upvote my answer, if it is helpful. Yes, `fit` is new variable that gets created and initialized with the return value of the `glm()` function (the return value is an object of class `glm`)." CreationDate="2015-04-10T01:49:19.313" UserId="2452" />
  <row Id="5869" PostId="5490" Score="0" Text="I still have some problems in getting the value of this new variable fit. Reading elsewhere it seems to me that this variable fit is a variable of estimated probabilities, thus ranging from 0 to 1, as the values i get when i run fitted(fit)." CreationDate="2015-04-10T10:49:28.203" UserId="9033" />
  <row Id="5870" PostId="5490" Score="0" Text="@Ciochi: Sorry, but I don't quite understand what your current issue is." CreationDate="2015-04-10T10:52:04.373" UserId="2452" />
  <row Id="5871" PostId="5490" Score="0" Text="The problem is that i need to use those values for an upcoming proceeding. Being the new variable fit called Z, i need to to present data as Z: Controls vs Cases, mean ± sd vs mean ± sd, P&lt;0.01, for example." CreationDate="2015-04-10T11:01:59.230" UserId="9033" />
  <row Id="5872" PostId="5490" Score="0" Text="@Ciochi: I think that you might receive more attention and help with this question at the _Cross Validated_ SE site. I can ask local moderator to migrate it, if you want." CreationDate="2015-04-10T11:08:26.110" UserId="2452" />
  <row Id="5873" PostId="5490" Score="0" Text="yeah, thank you." CreationDate="2015-04-10T12:13:00.417" UserId="9033" />
  <row Id="5874" PostId="5408" Score="0" Text="How many attributes are you considering in your dataset? And how many examples?" CreationDate="2015-04-10T13:14:06.380" UserId="2576" />
  <row Id="5877" PostId="5492" Score="1" Text="&quot;Generate insight&quot; is far too broad; until you know what you're asking it's not useful to ask about tools." CreationDate="2015-04-10T21:33:25.563" UserId="21" />
  <row Id="5878" PostId="5493" Score="0" Text="Well, weighted average or majority vote, but are you asking for more than that? Can you clarify what you are referring to in the first part?" CreationDate="2015-04-10T21:34:13.117" UserId="21" />
  <row Id="5881" PostId="5493" Score="0" Text="My question more precisely is: when I find out which labels are for whom my RF classifier uncertain is (by voting), I could train another (say, SVM) classifier exclusively for these uncertain labels. I do this by filtering out the RF-certain labels, and the remaining subset (with the unsure labels) is the training set for my SVM model. But how could I technically combine the RF and the SVM for an unseen new dataset with all the possible labels? In R one could classify (&quot;predict&quot;) a given unlabeled dataset with only one model." CreationDate="2015-04-11T04:28:46.710" UserId="9011" />
  <row Id="5882" PostId="5492" Score="0" Text="Hi Sean, Generating insight by here I mean- finding insights such as certain descriptive statistics- mean payment days, median payment days, default by how many days, creating a delay band i.e. 1-10 days, 11-20 days etc. region wise details, BU wise details." CreationDate="2015-04-11T05:02:45.353" UserId="9052" />
  <row Id="5897" PostId="5443" Score="1" Text="Yes, see [here](http://www.microsoft.com/mac/excel). For the joke impaired, see [here too](https://twitter.com/bigdataborat/status/372350993255518208)." CreationDate="2015-04-11T14:12:35.270" UserId="515" />
  <row Id="5898" PostId="5504" Score="0" Text="I think the question is incomplete (&quot;By looking&quot; ... ?) and it's likely too open ended to ask &quot;what can I learn&quot;. Can you narrow this down specifically, to maybe your second point? what are you hoping to compare and what have you tried?" CreationDate="2015-04-12T10:34:32.737" UserId="21" />
  <row Id="5899" PostId="5506" Score="0" Text="Thanks for your answer, can you explain more about your example of acquiring other features (variables) for the states and then average each feature among objects assigned to each cluster? I didn't completely understand it" CreationDate="2015-04-12T16:05:36.780" UserId="9079" />
  <row Id="5900" PostId="5506" Score="0" Text="Maybe get population size of each state during the given calendar year, percent ethnicity for a variety of race/ethnicity combinations, average education, average household income.  Migration could affect names as well.  Next, average the population sizes of the states within a cluster, do this for each cluster, then show a bar chart with the mean (average) population size for each cluster.   A high bar means large average population size of states in that cluster, whereas small bars reflect clusters having states with low population size.  (each bar represents a cluster)." CreationDate="2015-04-12T16:34:33.040" UserId="9086" />
  <row Id="5901" PostId="310" Score="0" Text="as you probably saw, the [Wikipedia article on P-U learning](http://en.wikipedia.org/wiki/PU_learning) has a reference to a paper where this has been applied to gene identification. Maybe it's worth figuring out / asking the authors what software they used." CreationDate="2015-04-12T20:27:06.180" UserId="462" />
  <row Id="5902" PostId="310" Score="0" Text="There is some discussion on PU learning in scikit learn here: http://stackoverflow.com/questions/25700724/binary-semi-supervised-classification-with-positive-only-and-unlabeled-data-set (using a 'one class' support vector machine)" CreationDate="2015-04-12T20:33:17.840" UserId="462" />
  <row Id="5903" PostId="5481" Score="1" Text="From a _storage perspective_, processed data is no different from unprocessed one, so I think that **any** storage approach is appropriate, as long as it satisfies **other requirements** (compatibility with other systems, software, business processes, APIs, etc.), all focused on your IT environment." CreationDate="2015-04-13T04:05:52.303" UserId="2452" />
  <row Id="5904" PostId="5507" Score="0" Text="I think that the kernel doesn't space out the values. My values stay together even after applying the kernel. I'm going to add a picture of how my values are." CreationDate="2015-04-13T09:52:01.180" UserId="2586" />
  <row Id="5906" PostId="5505" Score="0" Text="Thanks for the answer. &#xA;I have access to some local systems, can you give me a start on how to set up a cluster using these systems without any cloud services ?&#xA;Looks like everywhere AWS is being used." CreationDate="2015-04-13T10:15:11.803" UserId="4947" />
  <row Id="5907" PostId="5505" Score="1" Text="@abhivij: You're welcome. Setting up a cluster is not a rocket science, but might be [not trivial](http://stackoverflow.com/q/17923256/2872891), depending on the requirements and your current skills. You can read [this blog post](http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r) and [this blog post](https://beckmw.wordpress.com/2014/01/21/a-brief-foray-into-parallel-processing-with-r) as a starting point. (to be continued)" CreationDate="2015-04-13T10:37:50.190" UserId="2452" />
  <row Id="5908" PostId="5505" Score="1" Text="@abhivij: (cont'd) Also, you'd have to refer to documentation on multiprocessing `R` packages that you will decide to use, for example [this tutorial](http://cran.r-project.org/web/packages/doMPI/vignettes/doMPI.pdf). A more high-level overview and example of an R-based cluster can be found in [this working paper](http://biostats.bepress.com/cgi/viewcontent.cgi?article=1016&amp;context=uwbiostat). Hope this helps." CreationDate="2015-04-13T10:38:14.633" UserId="2452" />
  <row Id="5909" PostId="1100" Score="0" Text="Interesting problem. Since you posted this over 6 months ago can I get you to confirm that you're still interested in this before I spend time taking a stab at it?" CreationDate="2015-04-13T13:40:19.757" UserId="2723" />
  <row Id="5910" PostId="5261" Score="0" Text="I would like to take a stab at this. Do you have any example data or should I select some built-in dataset to benchmark speed improvement?" CreationDate="2015-04-13T13:42:26.980" UserId="2723" />
  <row Id="5912" PostId="5521" Score="0" Text="Thanks! But one thought about using a cluster centroid, it would require each cluster has similar volume/or radius. if all potential clusters are significantly differ in volume, I do not sure the method can work in the case?" CreationDate="2015-04-14T15:01:49.750" UserId="9097" />
  <row Id="5913" PostId="5525" Score="1" Text="This is almost a link-only answer since it doesn't provide any answer to the question directly here, and are discouraged on SE." CreationDate="2015-04-14T15:22:06.767" UserId="21" />
  <row Id="5915" PostId="5524" Score="0" Text="what package does `rpart` come from?" CreationDate="2015-04-14T15:40:41.600" UserId="471" />
  <row Id="5920" PostId="5527" Score="0" Text="Thanks a lot! your are really inspiriting! are there standard/classical methods to check/quantify if the classes are well separated or overlaps?" CreationDate="2015-04-15T15:47:07.267" UserId="9097" />
  <row Id="5923" PostId="5531" Score="0" Text="Pete, sorry, but I remember now that I've used your solution and ... it didn't worked as expected. Indeed, time elapsed (incl. nr of days, but also other things) between two years is not constant.&#xA;Due to that, when I have applied your solution, in the best case, the minutes/seconds parts were changed, which is not expected." CreationDate="2015-04-15T19:10:02.527" UserId="3024" />
  <row Id="5924" PostId="5531" Score="0" Text="Please add what happened on to your question.  Include a small representation of your data.  When I apply this solution to my data, it only changes the year, leaving the mintues/seconds unchanged." CreationDate="2015-04-15T19:13:40.713" UserId="1077" />
  <row Id="5925" PostId="5530" Score="1" Text="We are striving to keep quality very high here. Can you please clean up the punctuation and capitalization issues in your post? If you need help, let me know. Thanks!" CreationDate="2015-04-15T19:47:38.023" UserId="3466" />
  <row Id="5926" PostId="5530" Score="1" Text="I also want to mention that I think this is a great question for this site. Thanks for posting." CreationDate="2015-04-15T19:48:59.080" UserId="3466" />
  <row Id="5927" PostId="5527" Score="0" Text="I would say classes are well separated if you can train a classifier on them and it gets very high accuracy. But if you can do that you probably don't need to do any sampling for k-NN... Or if when you apply k-NN, the top-k classes are consistently the same... you could try k-NN on a subset of your data, then try the &quot;sampling&quot; way, and try to quantify your loss." CreationDate="2015-04-15T23:55:14.153" UserId="9114" />
  <row Id="5928" PostId="5531" Score="0" Text="Thanks Pete,&#xA;&#xA;I don't know what I did previously, but now it works. I'll create another answer to give the source code if it can help somebody in the future. In fact, the &quot;complex&quot; stuff is around how many days are in a year, for which I'm computing delta between two datetime.date objects." CreationDate="2015-04-16T07:35:23.000" UserId="3024" />
  <row Id="5929" PostId="5536" Score="0" Text="thanks Matic DB.....I will try it out once again...." CreationDate="2015-04-16T13:22:35.447" UserId="3314" />
  <row Id="5931" PostId="5536" Score="0" Text="Thanks. @Matic DB...i was able to get the id ..Below is my solution" CreationDate="2015-04-16T15:53:59.910" UserId="3314" />
  <row Id="5932" PostId="2643" Score="0" Text="What do you want to use for a file system and cluster manager instead of Hadoop?  Options include Cassandra for files, Mesos or Yarn for cluster management, etc." CreationDate="2015-04-16T19:28:39.760" UserId="9146" />
  <row Id="5936" PostId="5549" Score="0" Text="I have answered a **very similar question** here: http://datascience.stackexchange.com/a/5200/2452. My _related answer_ on _Cross Validated_ might also be helpful: http://stats.stackexchange.com/a/142725/31372." CreationDate="2015-04-17T23:29:30.693" UserId="2452" />
  <row Id="5937" PostId="5547" Score="0" Text="What's the value of `y.shape`?" CreationDate="2015-04-19T09:01:42.143" UserId="173" />
  <row Id="5938" PostId="5547" Score="0" Text="And are you sure you're executing the code you're displaying? In the code sample the last line is `net1.fit(X, y)` while the traceback indicates that the problem happens when executing `net1.fit(X[i], y[i])`." CreationDate="2015-04-19T09:10:23.857" UserId="173" />
  <row Id="5939" PostId="5555" Score="0" Text="I don't think anybody is able to answer your question satisfactorily. Firstly, the situation depends on too many factors. Secondly, in order to assess potential improvement (or lack of it) from the two factors you've mentioned, a research study has to be designed and performed. Alternatively, you can try to find similar studies and review their results." CreationDate="2015-04-20T00:00:21.937" UserId="2452" />
  <row Id="5941" PostId="5550" Score="0" Text="Thanks for your comments." CreationDate="2015-04-20T16:52:01.413" UserId="3314" />
  <row Id="5942" PostId="5549" Score="0" Text="Thanks again for your suggestions and comments." CreationDate="2015-04-20T16:52:16.960" UserId="3314" />
  <row Id="5943" PostId="5561" Score="0" Text="Could you provide an example which reproduces the errors (with minimal data samples)? Also, posting full tracebacks is extremely helpful to diagnose a problem. Otherwise we can only guess." CreationDate="2015-04-20T17:12:58.540" UserId="173" />
  <row Id="5944" PostId="5560" Score="0" Text="Thanks walczak." CreationDate="2015-04-20T17:19:24.087" UserId="4993" />
  <row Id="5945" PostId="5549" Score="0" Text="You're very welcome." CreationDate="2015-04-20T21:36:50.910" UserId="2452" />
  <row Id="5946" PostId="5550" Score="0" Text="You are welcome. Hoping this helps..." CreationDate="2015-04-21T07:54:48.467" UserId="3024" />
  <row Id="5947" PostId="19" Score="0" Text="That depends on whether it is just being thrown in as a buzzword." CreationDate="2015-04-21T20:52:25.963" UserId="9237" />
  <row Id="5949" PostId="5574" Score="0" Text="Often you rather want to predict (stock market), which is unrelated to Nash. Also crowds/people do not follow Nash and usually you want to control the crowd, rather than game it. And Nash relies on an abstract model for you simply do not have the numbers in real life. That's my impression, but I'd be interested to see confirmed cases of usage which go beyond an academic paper." CreationDate="2015-04-22T06:23:34.550" UserId="723" />
  <row Id="5950" PostId="5581" Score="0" Text="300 might potentially be a small sample in this context, how were the passengers distributed? Were they all on the same plane, on the same route ? Did they fly in the same week/month? Without sight of the data it's hard to say exactly what you might be able to extract from it. Maybe you could make your question more specific?" CreationDate="2015-04-22T09:29:49.153" UserId="7980" />
  <row Id="5951" PostId="5582" Score="0" Text="You might find taking your first steps in machine learning easier and faster  if you use a specialised graphical environment  specifically for ML, such as WEKA. Most of the popular algorithms present  in WEKA have Python equivalents, so once you have explored how different algorithms behave you can implement them with appropriate parameters using the python libraries. There is an accompanying book for WEKA that introduces ML concepts." CreationDate="2015-04-22T09:39:14.373" UserId="7980" />
  <row Id="5952" PostId="5581" Score="0" Text="Sir, this is the website i scraped [ http://www.airlinequality.com/Forum/af.htm ]  Have a look at it. :)" CreationDate="2015-04-22T12:39:02.897" UserId="9174" />
  <row Id="5953" PostId="5581" Score="0" Text="is it possible to do something ? @image_doctor" CreationDate="2015-04-22T13:05:59.440" UserId="9174" />
  <row Id="5954" PostId="5581" Score="0" Text="Something needs definition, what type of question are you seeking to answer ?   Unless you want to analyse the textual comments of the reviewers, you only have a handful of unspecific features which tell you how a selection of passengers perceive the airline in some very general categories. You might be able to extract the country of origin of the reviewer and see how that affects perception.  Without other data you can't compare this airline to the performance of any other airline. Apart from distributions of scores for things like value for money, etc. what are you searching for ?" CreationDate="2015-04-22T13:14:13.203" UserId="7980" />
  <row Id="5956" PostId="5586" Score="1" Text="&quot;Big Data&quot; is often collected without any thought as to the questions you might ask of it. This is in effect a little big data question." CreationDate="2015-04-22T15:49:05.327" UserId="471" />
  <row Id="5957" PostId="5566" Score="1" Text="So are you currently doing a degree in Comp Biol? How far into it? When did you work at all these research labs? Can we see your CV?" CreationDate="2015-04-22T15:54:03.900" UserId="471" />
  <row Id="5963" PostId="5566" Score="2" Text="You could begin your hunt with [DataKind](http://www.datakind.org/projects/)." CreationDate="2015-04-21T08:46:01.687" UserId="1131" />
  <row Id="5964" PostId="5566" Score="1" Text="You can search for inspirations by browsing the projects carried out within [Data Science for Social Good](http://dssg.io/) initiative." CreationDate="2015-04-21T15:22:00.447" UserId="173" />
  <row Id="5965" PostId="5586" Score="0" Text="@Spacedman, you're right. The difference is in the means of data collection, I suppose." CreationDate="2015-04-22T21:02:52.010" UserId="8152" />
  <row Id="5966" PostId="5574" Score="0" Text="I have done a bit of study on the topic, which is not authoritative, but I think Nash-Equilibrium gives us to choose the best decision when we have an idea about the decisions of our competitors. Secondly it is not an abstract model, all the abstractions are removed by assigning a pay-off function which is quantifiable." CreationDate="2015-04-23T07:09:32.993" UserId="9227" />
  <row Id="5967" PostId="5588" Score="0" Text="Just an observation, this can be cast as a classification problem, if the cost  is binned or the prices  fit into common ranges. :)" CreationDate="2015-04-23T07:47:14.557" UserId="7980" />
  <row Id="5968" PostId="5443" Score="0" Text="I'd say that Excel is usefull to present results, not to do the big data processing in itself: your big data &quot;customers&quot; are expecting to have results presented the way they are used to.&#xA;For example, it's not uncommon for my to export a result pandas data frame to excel and then change the layout of the resulting file only ... to make the &quot;customers&quot; happy." CreationDate="2015-04-23T08:25:43.840" UserId="3024" />
  <row Id="5969" PostId="5580" Score="1" Text="Not really a reply, but there's an interesting free Coursera course about that: [Pattern Discovery in Data Mining](https://www.coursera.org/course/patterndiscovery)" CreationDate="2015-04-23T08:30:33.357" UserId="3024" />
  <row Id="5970" PostId="5574" Score="0" Text="I'm just saying the tasks are different. Stock prediction: We need to predict the future of a very complex system that will not care about an equilibrium. Abstract model: They are almost never known payoff-function - welcome to reality. People: They provable do not follow Nash - in a fixed round Prisoners dilemma they do not all default." CreationDate="2015-04-23T11:04:31.383" UserId="723" />
  <row Id="5971" PostId="5588" Score="0" Text="Thanks Henry, suppose if the attributes of items are location name,  company name and components name. In this case how can I model ?" CreationDate="2015-04-23T12:50:38.387" UserId="5091" />
  <row Id="5972" PostId="5595" Score="0" Text="I don't think this is quite right.  I'm a little unclear about where url_data would be coming from.  From the documentation, k.set_contents_from_string() seems to quite literally set the contents of file 'foobar' to whatever is contained in that string.  I want the content at that url to be pushed directly to s3 without needing to be downloaded locally." CreationDate="2015-04-23T15:37:00.280" UserId="9249" />
  <row Id="5975" PostId="1214" Score="1" Text="I recently took a quick look around the market for products that would help with that, and I want to share my findings.&#xA;&#xA;There are two SAAS products that can help a Data Science team to share analyses done in Python and R. They both have an IPython notebook like IDE, and they both build around it a lot of features for running and sharing jobs. I find them both almost identical: [Domino Data Lab][1] and [Sense.io][2]&#xA;&#xA;  [1]: http://www.dominodatalab.com/&#xA;  [2]: http://sense.io/" CreationDate="2015-04-24T02:36:40.490" UserId="3540" />
  <row Id="5976" PostId="1214" Score="0" Text="There is also a Machine Learning environment for Hadoop, which keeps track of Job runs; [h2o.ai][3]. It is not meant for being a tool for streamlining the work of the data team, but with some careful naming conventions it can help a lot. This one fits most with &#xA;&#xA;  [3]: http://h2o.ai" CreationDate="2015-04-24T02:37:27.593" UserId="3540" />
  <row Id="5977" PostId="5601" Score="0" Text="Please, change the topic of your post." CreationDate="2015-04-24T15:15:26.257" UserId="173" />
  <row Id="5978" PostId="5602" Score="0" Text="What type of documents are you intending to look at? Length wise are you looking at Tweets, Tumblr posts, articles, etc?" CreationDate="2015-04-24T15:43:41.600" UserId="9274" />
  <row Id="5979" PostId="5602" Score="0" Text="I am Looking for articles preferably." CreationDate="2015-04-24T19:03:55.397" UserId="9287" />
  <row Id="5981" PostId="5606" Score="0" Text="What are these numbers? What precisely do you do to obtain them?" CreationDate="2015-04-25T15:38:46.580" UserId="173" />
  <row Id="5982" PostId="5606" Score="0" Text="Thanks for your interest. &#xA;I obtain these numbers by entering : mdclassify(['google','France',None,None],tree). The results are the same as the ones obtained by the book. I simply do not understand them.&#xA;mdclassify() is a function used to classify observations when there are missing attributes, but using a weighting process (so the observation is split around different branches in a unequal manner). NB : When an observation fulfill a condition, it goes on the right branch. So here in some way the observation is split on the 3 branches of the right side. But what are these figures?" CreationDate="2015-04-25T17:40:18.930" UserId="9296" />
  <row Id="5984" PostId="5590" Score="0" Text="Depends on what you mean by scale, but that sounds like the answer to your question. What else are you looking for?" CreationDate="2015-04-25T19:57:03.147" UserId="21" />
  <row Id="5985" PostId="5158" Score="0" Text="In a Sage Worksheet (&quot;.sagews&quot;), just enter `%default_mode r` and evaluate the cell. Then you talk to R directly." CreationDate="2015-04-25T20:41:57.987" UserId="9306" />
  <row Id="5986" PostId="5616" Score="0" Text="Wow ! Thanks ! To check my understanding, I tried to apply the same reasoning to the 3 branches at the left but I don't manage to get appropriate results.&#xA;If I enter `mdclassify(['digg', None, None, None], tree)` I'm getting `{'Basic': 2.4615384615384617, 'None': 0.8653846153846154}`. Can you please explain me how it works ? I thought we would have :&#xA;1) Dividing one unit of weight for page&gt;20: None:3 and Basic1 : 0.75 and 0.25&#xA;2) Then counting the weight for for FAQ attribute &#xA;* Yes to FAQ = 4/5&#xA;* No to FAQ =  1/5&#xA;Thus for `None` I would expect to get 0.75/5=0.15. Where am I wrong ?" CreationDate="2015-04-26T09:34:22.187" UserId="9296" />
  <row Id="5988" PostId="5616" Score="0" Text="Try either running the script in debug mode, or add print statements in mdclassify() function and carefully observe what's going on. In short, in this `digg-None-None-None` case we got: `Basic==1`, `None==3`, thus: `Basic==1*0.25` and `None==3*0.75`. Then we go up to attribute &quot;2:yes&quot; and we got: `Basic==4`, and (`Basic==0.25`, `None==2.25`). The `Basic==4` probability is: 4/6.5==0.61, and 4*0.615==2.46 after multiplication. The probability of the second leaf is: `2.5/6.5==0.38`. So, finally we got: Basic==(4*0.615)+(0.25*0.38)=2.55, and None==2.25*0.38=0.85. I get approx. results like these." CreationDate="2015-04-26T13:41:57.590" UserId="173" />
  <row Id="5989" PostId="5616" Score="0" Text="Ok, it's clear now ! You nailed it ! Many thanks !" CreationDate="2015-04-26T20:11:48.590" UserId="9296" />
  <row Id="5990" PostId="5588" Score="0" Text="Hmm, I can't say for certain that I know exactly what you should do.  This falls under the domain of regression with categorical variables [wiki link here](http://en.wikipedia.org/wiki/Categorical_variable#Categorical_variables_and_regression)." CreationDate="2015-04-27T06:12:27.580" UserId="9246" />
  <row Id="5991" PostId="5588" Score="0" Text="--continuation of last comment--&#xA;in which the bulk of the problem lies in translating categorical variables into numerical ones.  &#xA;&#xA;An easy way to do this would just assign numerical representations to each unique categorical entry.  I'm unsure of how this would affect regression. You should be able to train a neural network [(basic introduction video link)](https://www.youtube.com/watch?v=bxe2T-V8XRs) to do this without a hitch" CreationDate="2015-04-27T06:20:30.303" UserId="9246" />
  <row Id="5992" PostId="874" Score="0" Text="The links provided in the answer seem to be dead ;(" CreationDate="2015-04-27T07:27:48.070" UserId="9325" />
  <row Id="5993" PostId="5603" Score="0" Text="Thank you so much for giving a rough scikit-scheme, that was exactly what I needed!" CreationDate="2015-04-27T07:49:26.153" UserId="9281" />
  <row Id="5996" PostId="5485" Score="0" Text="(A comment.) I think this is a classical manifestation of the [curse of dimensionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)." CreationDate="2015-04-26T11:00:37.970" UserId="6550" />
  <row Id="5997" PostId="5485" Score="0" Text="+1 Looks very much like this" CreationDate="2015-04-27T10:18:03.470" UserId="9085" />
  <row Id="5998" PostId="5574" Score="0" Text="Have a look at Acemoglu (http://economics.mit.edu/files/9789) and Jackson (http://web.stanford.edu/~jacksonm/GamesNetworks.pdf). They write on games on networks, and it may have many practical applications." CreationDate="2015-04-28T17:02:07.333" UserId="5279" />
  <row Id="5999" PostId="5635" Score="0" Text="What are these big problems?" CreationDate="2015-04-28T18:47:30.503" UserId="173" />
  <row Id="6000" PostId="5641" Score="0" Text="Thank you.  I was contemplating something like this, but I wanted to ask around to see if there was an easier way.  Thanks for pointing out all the steps too.  Very helpful" CreationDate="2015-04-29T16:18:32.097" UserId="9249" />
  <row Id="6001" PostId="5639" Score="1" Text="Could you, please, elaborate? How does your data look like? It's hard to imagine unstructured metadata. What programming languages do you consider to use? Have you tried to do anything with these data? Structure it in anyway? Without any details we won't be able to help you." CreationDate="2015-04-29T16:31:12.607" UserId="173" />
  <row Id="6003" PostId="5637" Score="0" Text="Thanks for the Glove paper!" CreationDate="2015-04-30T07:38:10.893" UserId="6523" />
  <row Id="6007" PostId="5651" Score="0" Text="But for $$ #(w_i, c_j) $$ I couldnt figure out what it is because this is loss for a specific pair of words. So why count in there" CreationDate="2015-04-30T17:18:26.663" UserId="6523" />
  <row Id="6008" PostId="155" Score="0" Text="I haven't found any good free comprehensive datasets for typical Business Intelligence applications.  The [Microsoft Contoso BI Demo Dataset for Retail Industry from Official Microsoft Download Center](http://www.microsoft.com/en-us/download/details.aspx?id=18279) download works with some Microsoft products (see [AndyGett on SharePoint and Other Business Software](http://blog.bullseyeconsulting.com/archive/2012/08/14/setting-up-sample-contoso-database-for-performancepoint-and-sharepoint.aspx)), but I don't see any plain sql or csv dumps of it, nor any license info." CreationDate="2015-04-30T17:42:11.410" UserId="9146" />
  <row Id="6009" PostId="5655" Score="0" Text="Thanks for your comments and my apology for not providing more information. The answer to the first three questions of yours is 'yes' and the keyError  is given as KeyError: 0  However, following your remark, I am going to provide you with few lines of the dataset, and what movieGenre is. Also, I will include the traceback but it is kinda large." CreationDate="2015-04-30T18:36:32.797" UserId="3314" />
  <row Id="6010" PostId="5655" Score="1" Text="@user62198 well that's sort of what i'm getting at.  you should be able to minimize traceback length by pulling a small portion of your code out into another script - a script that demonstrates your problem but is standalone/removes most of the code that is not causing the problem." CreationDate="2015-04-30T18:50:48.767" UserId="8953" />
  <row Id="6011" PostId="5655" Score="0" Text="@user62198 i edited to include some potential troubleshooting steps.  can you use that to provide more information?" CreationDate="2015-04-30T19:33:18.420" UserId="8953" />
  <row Id="6012" PostId="5618" Score="0" Text="Do all patients experience the event?" CreationDate="2015-05-01T15:24:03.160" UserId="3347" />
  <row Id="6013" PostId="5534" Score="1" Text="I edited your code a bit but it fails because `rating` isn't defined. If you fix that, you could also add `from BeautifulSoup import BeautifulSoup`, and `import requests`. And why not also show `url=&quot;http://etc&quot;` so we don't have to do that for ourselves?" CreationDate="2015-05-01T15:40:36.713" UserId="471" />
  <row Id="6014" PostId="5669" Score="1" Text="Thanks for the answer, I will take a look there!" CreationDate="2015-05-01T21:45:30.137" UserId="9225" />
  <row Id="6015" PostId="5659" Score="0" Text="@Gred Thatcher, Thanks for the link. This project is part of a learning endeavor on web scraping and hence all these troubles. -:)" CreationDate="2015-05-01T21:47:56.593" UserId="3314" />
  <row Id="6016" PostId="5665" Score="0" Text="Thanks @j.a.gartner" CreationDate="2015-05-01T21:49:35.580" UserId="3314" />
  <row Id="6017" PostId="5655" Score="0" Text="Thanks a lot @aeroNotAuto. I am going to try it out soon and let you know." CreationDate="2015-05-01T21:51:07.253" UserId="3314" />
  <row Id="6018" PostId="5672" Score="0" Text="It's a nice Idea! +1 Thanks!" CreationDate="2015-05-02T10:37:43.103" UserId="9225" />
  <row Id="6020" PostId="5674" Score="0" Text="Thank you for such a clear explanation, and for the terrific links, thinking about the problem as detecting variance vs bias will definely help!" CreationDate="2015-05-03T13:07:09.907" UserId="8344" />
  <row Id="6021" PostId="5679" Score="3" Text="You're asking people to devote their time to explain something to you. Please, before you do so, show these people that you have tried to find the answer by yourself and somehow failed along the way. We do want to help you, but we do not want to be your human googling machines." CreationDate="2015-05-03T15:05:31.360" UserId="173" />
  <row Id="6022" PostId="5680" Score="0" Text="That's not going to help me. If I use an arithmetic mean, the score will be too heavily weighted by b in most cases. I need a change in each to equally be capable of influencing c." CreationDate="2015-05-03T19:20:12.273" UserId="9394" />
  <row Id="6023" PostId="5668" Score="0" Text="My survey has roughly 25 questions, but for any particular classification I only look at two or three specific questions that are designed to indicate a classification. Each of those questions are multiple choice. They could be either select-one or select-any. So I indicate a selected answer with 1 and a non-selected answer with 0. This approach may be incorrect from the get go. I certainly don't have as much data as you indicate." CreationDate="2015-05-04T00:11:28.803" UserId="9410" />
  <row Id="6024" PostId="5668" Score="0" Text="Also, I'm trying to avoid hard coded rules (If question 1 == choice A), because we tend to re-write the questions from time to time, and even change a select-one to a select-any. We add questions and answer choices, and drop questions and answer choices. So I think that any hardcoded rules will change somewhat often.  Thanks for the std err calc, more information, and other options as well!" CreationDate="2015-05-04T00:14:13.937" UserId="9410" />
  <row Id="6025" PostId="5668" Score="0" Text="Without knowing the survey content it's hard to say if the 1-0 scale is appropriate.  You could consider binary classification, i.e. a-&gt;1, b-&gt;2, c-&gt;4, then take their sum, and feed it to a random forest regression model." CreationDate="2015-05-04T04:36:34.547" UserId="8041" />
  <row Id="6026" PostId="5685" Score="0" Text="Thank you for your answer, but now my question is : how does it choose the best model, with which criteria?" CreationDate="2015-05-04T15:03:50.347" UserId="9445" />
  <row Id="6027" PostId="5685" Score="0" Text="The weights that are returned are an expression of the importance of your variable vector.  The training process trains several combinations of different weights, and tests to see which subset gives the best results:&#xA;https://spark.apache.org/docs/latest/mllib-linear-methods.html#mathematical-formulation" CreationDate="2015-05-04T18:45:51.957" UserId="8041" />
  <row Id="6028" PostId="5667" Score="0" Text="I also found my self with a very similar problem, and didn't really find a solution. But what *actually* happens is not clear from this code, because spark has 'lazy evaluation' and is supposedly capable of executing only what it really needs to execute, and also of combining maps, filters and whatever can be done together. So possibly what you describe *may* happen in a single pass. Not familiar enough with the lazy evaluation mechanisms to tell, though. Actually I just noticed the .cache(). Maybe there's a way of only doing one .cache() and getting the full results?" CreationDate="2015-05-04T22:51:41.937" UserId="9114" />
  <row Id="6029" PostId="5689" Score="3" Text="You could give more relevant name to the question... like Models performance or smth..." CreationDate="2015-05-05T08:52:07.107" UserId="97" />
  <row Id="6030" PostId="5689" Score="1" Text="Well, one of them has to be quicker. If it was the other way round, would you still be asking the question? Do you know what these algorithms do?" CreationDate="2015-05-05T09:00:17.697" UserId="471" />
  <row Id="6031" PostId="5686" Score="0" Text="So what you mean is that you create binary features for all these billions of words, web sites, friends etc.? And then the problem becomes bringing this down to a billion or so, so that the problem becomes tractable?" CreationDate="2015-05-05T09:30:37.327" UserId="9385" />
  <row Id="6032" PostId="5689" Score="0" Text="I know what the two algorithms do. I just want to know the reason behind Naive Bayes quicker performance. Thank you." CreationDate="2015-05-05T14:30:45.937" UserId="9436" />
  <row Id="6033" PostId="5676" Score="0" Text="Thanks for taking the time to write such a thorough answer." CreationDate="2015-05-05T16:40:37.650" UserId="3466" />
  <row Id="6034" PostId="5534" Score="1" Text="Just in case: http://opendata.stackexchange.com/questions/1073/where-to-get-imdb-datasets" CreationDate="2015-05-05T19:25:04.780" UserId="5279" />
  <row Id="6035" PostId="5693" Score="1" Text="What do you mean by &quot;unstructured geolocalisation data&quot;? A bunch of lat-long coordinates? A mix of coordinates, postal addresses and IP numbers?" CreationDate="2015-05-05T20:23:40.303" UserId="471" />
  <row Id="6036" PostId="5691" Score="0" Text="Maybe we can calculate the distance of slope." CreationDate="2015-05-06T02:09:09.477" UserId="9457" />
  <row Id="6037" PostId="5694" Score="1" Text="Have you tried Internet search? That should be enough." CreationDate="2015-05-06T04:09:32.220" UserId="2452" />
  <row Id="6038" PostId="5694" Score="1" Text="yes, i had google but that's surely isn't enough, see the updated question." CreationDate="2015-05-06T04:34:01.323" UserId="122" />
  <row Id="6039" PostId="5694" Score="1" Text="I'm not sure that it's such a good idea to seek explanation of complex machine learning concepts &quot;in layman terms&quot;. Also, you should widen your search beyond just Wikipedia." CreationDate="2015-05-06T04:39:13.703" UserId="2452" />
  <row Id="6040" PostId="5676" Score="0" Text="@ sheldonkreger, thanks for the encouraging comment." CreationDate="2015-05-06T06:01:42.837" UserId="8465" />
  <row Id="6041" PostId="5693" Score="0" Text="Sorry I didn't mentioned it, it can be all of those examples." CreationDate="2015-05-06T06:15:23.440" UserId="9461" />
  <row Id="6042" PostId="5607" Score="1" Text="Thank you. I think I have no time to execute these kinds of steps. My solution is to request access to a more powerful machine on my University. I'll mark your answer as the solution anyway." CreationDate="2015-05-06T12:32:01.357" UserId="8152" />
  <row Id="6043" PostId="5693" Score="1" Text="I think you really need to spell out in more detail what you want. Even those four terms you have included could have a zillion possibilities. &quot;Clustering&quot; do you mean grouping location points together for display, for analysis, or is this database clustering (ie a group of database servers)? &quot;Saving/restoring&quot;? Of what? To where? And why?" CreationDate="2015-05-06T14:16:18.547" UserId="471" />
  <row Id="6044" PostId="5699" Score="0" Text="Thanks a lot. But are there any examples of this modification?" CreationDate="2015-05-06T21:16:53.580" UserId="9215" />
  <row Id="6045" PostId="5699" Score="0" Text="I'm not aware of examples of this modification for the reasons I mentioned above - it limits the solution space. Plus, I'm not aware of any benefit to having only non-negative weights. But as I stated, you could always modify the weight update rule of an existing implementation to check if the new value is less than zero and clip it to zero if it is." CreationDate="2015-05-07T03:57:01.463" UserId="964" />
  <row Id="6046" PostId="5704" Score="1" Text="How is dimensionality reduction related to manifold?" CreationDate="2015-05-07T07:00:04.360" UserId="122" />
  <row Id="6047" PostId="5700" Score="0" Text="Are there cases where high dimensionality doesn't uncrinkle to a manifold?" CreationDate="2015-05-07T07:00:56.110" UserId="122" />
  <row Id="6048" PostId="5704" Score="0" Text="It is a way of picking out everything on the manifold and excluding everything else." CreationDate="2015-05-07T14:43:14.963" UserId="9492" />
  <row Id="6049" PostId="5708" Score="2" Text="Need more information.  What is the relationship among the categories? Are the categories mutually exclusive? Is there categorical overlap?" CreationDate="2015-05-07T16:29:29.697" UserId="182" />
  <row Id="6051" PostId="5710" Score="1" Text="I wouldn't call [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) a &quot;simple logistic regression&quot;.  Except, I guess, in the sense that modern libraries make complex techniques very accessible :)" CreationDate="2015-05-07T17:08:35.237" UserId="9146" />
  <row Id="6052" PostId="5708" Score="2" Text="Welcome to Data Science! Currently your question is of very low quality. You can't expect quality answers without asking well described questions. Please, provide more information (better description of the data, of your background, programming languages, researched approaches etc.)." CreationDate="2015-05-07T17:09:38.163" UserId="173" />
  <row Id="6053" PostId="5703" Score="1" Text="There are too many questions in your post. You should try to split it up." CreationDate="2015-05-07T19:46:16.080" UserId="9508" />
  <row Id="6055" PostId="5700" Score="0" Text="Definitely! Sometimes, data already lies in it's intrinsic space. In that case, trying to reduce dimensionality will probably be deleterious for classification performance. In these cases, you should find that the features in the dataset you are using are largely statistically independent from each other." CreationDate="2015-05-08T00:03:40.407" UserId="9483" />
  <row Id="6056" PostId="5720" Score="1" Text="Credit for second image: http://stackoverflow.com/questions/4629505/svm-hard-or-soft-margins" CreationDate="2015-05-08T04:40:20.313" UserId="182" />
  <row Id="6057" PostId="5702" Score="0" Text="Thank you for your answer. These are indeed very good tips, but I think that some automation here would really help me here. I've stuck with Sumatra (see edit of my question) which forces me to make commits before I run experiments and keeps very detailed records." CreationDate="2015-05-08T12:07:52.260" UserId="9486" />
  <row Id="6058" PostId="5716" Score="0" Text="thank you very much,&#xA;&#xA;sorry for so many questions, is the first time I use stack exchange,&#xA;I am confused by my graduation project, I have no degree advisors or directors :( and I'm lost so about information ontologies, I'm trying to structure my graduation project on wetlands and ontologies and knowledge management, but not It is if I'm doing well.&#xA;&#xA;Thanks again for your help.&#xA;sorry for my English, I'm trying to improve it" CreationDate="2015-05-08T14:59:52.610" UserId="9488" />
  <row Id="6059" PostId="5713" Score="0" Text="thank you very much, sorry for so many questions, is the first time I use stack exchange, I am confused by my graduation project, I have no degree advisors or directors :( and I'm lost so about information ontologies, I'm trying to structure my graduation project on wetlands and ontologies and knowledge management, but not It is if I'm doing well. Thanks again for your help. sorry for my English, I'm trying to improve it." CreationDate="2015-05-08T15:05:14.553" UserId="9488" />
  <row Id="6064" PostId="5731" Score="0" Text="i have made changes in my question , can you please have a re-look and suggest me data that belongs to the changes in question. i am sorry for the inconvenience." CreationDate="2015-05-09T09:27:24.763" UserId="9521" />
  <row Id="6065" PostId="5733" Score="0" Text="Thank you for your input. This is a part of an NLP project(text mining/analytics) to be precise. I have edited the tags to data science." CreationDate="2015-05-09T14:26:30.240" UserId="9544" />
  <row Id="6066" PostId="5716" Score="0" Text="@AntonioEdgarMartinez please, accept one of the answers. I suggest accepting jkbkot's answer as it's more comprehensive than mine." CreationDate="2015-05-09T18:19:54.063" UserId="173" />
  <row Id="6068" PostId="5735" Score="2" Text="Take a look at software, referred to in [this answer](http://datascience.stackexchange.com/a/3723/2452) of mine." CreationDate="2015-05-10T02:02:13.227" UserId="2452" />
  <row Id="6069" PostId="1180" Score="0" Text="this answer doesn't address the question, which is how to &quot;allow for *weighting of certain ranges of values* each vector.&quot;" CreationDate="2015-05-10T05:09:46.130" UserId="609" />
  <row Id="6071" PostId="5730" Score="2" Text="I think that your question is too broad to expect comprehensive enough answers. I would recommend to research yourself major high-level ML techniques (Wikipedia set of relevant articles is a decent starting point) and then formulate more narrow question(s). Since you've mentioned NER, you might find helpful my related answers [here](http://datascience.stackexchange.com/a/5270/2452) and (linked within) [here](http://stats.stackexchange.com/a/136760/31372)." CreationDate="2015-05-10T06:35:58.720" UserId="2452" />
  <row Id="6072" PostId="5701" Score="0" Text="You have to distinguish between _approaches_, _frameworks_ and _tools_. **Reproducible research** approach (paradigm) is IMHO the key. Check my relevant comprehensive answer [here](http://datascience.stackexchange.com/a/759/2452)." CreationDate="2015-05-10T06:43:37.420" UserId="2452" />
  <row Id="6073" PostId="5730" Score="2" Text="Thanks @AleksandrBlekh, I should have been more specific that I am looking for high level direction than low level implementation. Your comment helps a lot with that. I use Wikipedia a lot, but it never crossed my mind to look there on this subject. I'll update the question to better reflect what I am after and follow with new questions in the future." CreationDate="2015-05-10T06:53:15.607" UserId="9534" />
  <row Id="6074" PostId="5730" Score="0" Text="You are welcome. Always glad to help." CreationDate="2015-05-10T06:55:59.187" UserId="2452" />
  <row Id="6075" PostId="5731" Score="1" Text="Given your new question, I suggest you to split it in two different questions: one asking for help on the predicition variables and the other asking for the dataset." CreationDate="2015-05-10T07:39:16.607" UserId="278" />
  <row Id="6078" PostId="5705" Score="0" Text="Could you provide some specific examples of the type of maths @Ashkan (and others young enthusiasts) can learn to better prepare?" CreationDate="2015-05-10T17:35:51.703" UserId="182" />
  <row Id="6079" PostId="5704" Score="0" Text="I think @alvas has a point here.  It's not immediately clear how this relates to the original question regarding an explanation of manifolds and dimensions." CreationDate="2015-05-10T17:44:34.557" UserId="182" />
  <row Id="6080" PostId="5741" Score="3" Text="Welcome to Data Science! Please, make your questions more detailed and specific. You're asking people to devote their time to answer your questions. If you're expecting quality answers, please consider asking quality questions (try some googling, read something on the topic, and update your posts with detailed, self-researched content). We will be glad to help you." CreationDate="2015-05-10T17:46:44.720" UserId="173" />
  <row Id="6081" PostId="5620" Score="0" Text="Could you expand upon this answer or provide an example of its application?" CreationDate="2015-05-10T17:48:47.550" UserId="182" />
  <row Id="6082" PostId="5714" Score="0" Text="Could you provide additional details in the body of the question? While someone might be able to answer this for you, the overall question quality could be improved." CreationDate="2015-05-10T17:51:22.940" UserId="182" />
  <row Id="6083" PostId="5715" Score="0" Text="This question seems very broad.  What are the &quot;immediate suspects&quot; and why aren't they adequate for your needs?" CreationDate="2015-05-10T17:52:23.410" UserId="182" />
  <row Id="6084" PostId="5743" Score="0" Text="Interesting information Jake!" CreationDate="2015-05-11T08:09:56.033" UserId="3550" />
  <row Id="6085" PostId="5712" Score="0" Text="I'm not sure it is possible to use feature scaling in R .. I can not find it..&#xA;Do you know how to perform logistic regression with LBFGS using R to compare the 2 models?" CreationDate="2015-05-11T08:13:59.130" UserId="9445" />
  <row Id="6086" PostId="5710" Score="0" Text="You are right. Do you know a way to implement Limited-memory BFGS with R ?" CreationDate="2015-05-11T08:15:47.930" UserId="9445" />
  <row Id="6087" PostId="5729" Score="1" Text="I appreciate your answer and the references here, even if the question is vague. It's really helpful to me and probably a lot more people who are just getting their feet wet as well. Thanks! :)" CreationDate="2015-05-11T11:16:37.983" UserId="9534" />
  <row Id="6088" PostId="5747" Score="1" Text="I'm voting to close this question as off-topic because its a stats question and would get better treatment on http://stats.stackexchange.com" CreationDate="2015-05-11T13:51:04.833" UserId="471" />
  <row Id="6089" PostId="5715" Score="0" Text="My main issue is that I don't know if I'm over caching or under caching. I would like to know if there is a tool that does spark profiling. I think I covered the usual suspects, but still think it's too slow. My question is beyond the usual suspects, just like a standalone JVM has JProfiler etc, what is the equivalent profiling tool for a distributed system such as Spark?" CreationDate="2015-05-11T15:36:38.027" UserId="9510" />
  <row Id="6090" PostId="5148" Score="0" Text="+1. The Field Guide is a nice high-level overview." CreationDate="2015-05-11T15:42:10.440" UserId="9123" />
  <row Id="6091" PostId="5737" Score="1" Text="You should take a look at the emails before coming up with any decision." CreationDate="2015-05-11T15:43:26.633" UserId="9123" />
  <row Id="6092" PostId="5754" Score="0" Text="This is possible and, of course, always dependent on the data.  However, given that the poster has 10^10 (presumably independent) samples, it seems that 10 dimensions wouldn't be too big a problem here." CreationDate="2015-05-11T18:03:15.220" UserId="182" />
  <row Id="6093" PostId="5754" Score="1" Text="Thanks for your comment @RyanJ.Smith. your comment is exactly in the same direction of mine. I just did not see anything regarding this problem in the post. And about the nr of samples; however he has many sample points he still might get stuck in the problem of dimensionality. I think you are arguing the opposite side of _Low Sample Size Problem_ which I think is not valid. If he has a high dimensional data, low sample size will be a problem but I think a large amount of data **does not necessarily** mean anything." CreationDate="2015-05-11T18:52:44.187" UserId="8878" />
  <row Id="6094" PostId="5747" Score="0" Text="How did I not find this? Thanks!" CreationDate="2015-05-11T20:01:45.507" UserId="8452" />
  <row Id="6095" PostId="5747" Score="0" Text="Probably appropriate in both sites. I'll migrate it." CreationDate="2015-05-11T20:47:36.177" UserId="21" />
  <row Id="6096" PostId="5759" Score="0" Text="you can locate this via wayback machine, its your best friend http://web.archive.org/web/20150402165739/http://konect.uni-koblenz.de/networks/" CreationDate="2015-05-11T21:04:02.217" UserId="1266" />
  <row Id="6097" PostId="5759" Score="1" Text=":))) Thanks @albert" CreationDate="2015-05-11T21:07:26.857" UserId="8878" />
  <row Id="6098" PostId="5760" Score="0" Text="Great info, I will look up Blondel. I'm going to leave the question open for awhile longer to see if others have ideas." CreationDate="2015-05-11T21:09:30.393" UserId="3466" />
  <row Id="6099" PostId="5760" Score="0" Text="Welcome :) Sure! If you have more specific question I'd be glad to answer if I can. I'm a community detection guy! It's been my research direction during last 2.5 years." CreationDate="2015-05-11T21:10:16.870" UserId="8878" />
  <row Id="6100" PostId="5726" Score="1" Text=":Your elaboration is spot on. Right now I am implementing a 3 player problem with around 13 strategies each. That will give me an idea. In future the number of strategies as well as the number of players are going to rise and the question was in that context. I will post my interpretations and conclusions once I get some concrete results." CreationDate="2015-05-12T06:33:33.923" UserId="9227" />
  <row Id="6101" PostId="5726" Score="0" Text="@AdiPiratla: I am glad to help. Good luck with your implementation. It would be interesting to read your insights upon completion." CreationDate="2015-05-12T07:19:49.800" UserId="2452" />
  <row Id="6108" PostId="5737" Score="0" Text="ya you right .. i gone through some of them ... most of them are business related topics and some are contentious like a threaded . this data set contain everything .. i mean inbox,deleted,sent items and so on." CreationDate="2015-05-12T10:03:06.693" UserId="9035" />
  <row Id="6110" PostId="5762" Score="0" Text="Thx @NeilSlater :) Correction done!" CreationDate="2015-05-12T11:54:15.377" UserId="8878" />
  <row Id="6111" PostId="5110" Score="0" Text="Check [this answer](http://datascience.stackexchange.com/a/814/2452) of mine. If you need dynamic visualization (for example, of streaming network data), also take a look at [this answer](http://datascience.stackexchange.com/a/3685/2452)." CreationDate="2015-05-12T12:42:28.347" UserId="2452" />
  <row Id="6112" PostId="5764" Score="0" Text="thanks ... i got some interesting thoughts from your comment . specially titles .. because those are the first thing that decide whether i would like to open it or not . i thought something with the email marketing. what sort of email got more attention, kind of analysis . and also your statistical analysis things also good ." CreationDate="2015-05-12T14:06:45.337" UserId="9035" />
  <row Id="6114" PostId="5703" Score="0" Text="Also, you cross-posted this to opendata. Please read: http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu" CreationDate="2015-05-12T15:44:28.173" UserId="471" />
  <row Id="6115" PostId="339" Score="3" Text="For you &quot;Dealing with Big Data&quot; comment, I would add that python is one of the 3 languages supported by apache spark, which has blazing fast speeds.  Your comment about R having a C back end is true, but so does python the scikitlearn library is very fast as well.&#xA;&#xA;I think your post has nice balance, but I contend that speed is at least a tie, and scalability (i.e. handling big data) is certainly in favor of python." CreationDate="2015-05-12T16:31:21.347" UserId="8041" />
  <row Id="6117" PostId="5761" Score="0" Text="The answer will depend on whether you want to parallelize using approximate versions of algorithms or not (and how you would want to parallelize them, example multicore or multinode). On a single machine, without any parallel processing, the complexity can be fairly well tracked." CreationDate="2015-05-12T22:40:05.890" UserId="847" />
  <row Id="6118" PostId="5764" Score="0" Text="I suggest you should go for it. You aren't doing a Google email filter, try something easy and simple." CreationDate="2015-05-12T23:39:29.810" UserId="9123" />
  <row Id="6119" PostId="5580" Score="0" Text="If you can explain your problem and data in details I might be able to help ... &quot;next web page prediction using the existing web data&quot; ... what does &quot;next web page&quot; mean? what does the middle column mean? .... what are &quot;items&quot; in the context of your question? ... &quot;I have a set of frequent sequences&quot; what are frequent sequences? words? visits?" CreationDate="2015-05-12T23:49:22.233" UserId="8878" />
  <row Id="6120" PostId="5765" Score="0" Text="hmmm... i think you are on to something here. i still need to look at it again. Thanks for the hint though. I will mark it as the correct answer (permanently) since I can't vote up yet. But I might comment again to ask for more clarification (if something comes up)" CreationDate="2015-05-13T02:19:23.160" UserId="4803" />
  <row Id="6121" PostId="5754" Score="0" Text="10 dimensions are not a lot yet." CreationDate="2015-05-13T07:51:24.333" UserId="924" />
  <row Id="6122" PostId="5754" Score="0" Text="How do you determine my friend? what I said was the result of an experiment designed to answer such a question however it CAN NOT be answered in general! What is &quot;a lot&quot; in your comment exactly? it depends on many circumstances as I mentioned in my answer. in some situations 10D could be problematic." CreationDate="2015-05-13T08:21:50.060" UserId="8878" />
  <row Id="6125" PostId="5738" Score="0" Text="Thanks for the suggestions. I had hoped to use NLTK entirely, but these tools look promising as well." CreationDate="2015-05-13T16:15:38.470" UserId="9511" />
  <row Id="6126" PostId="5751" Score="0" Text="For clarification, are you trying to calculate the relative importance of a word quarter to quarter, or are you trying to make a prediction about the the raw frequency in the next quarter?" CreationDate="2015-05-13T18:28:20.150" UserId="8041" />
  <row Id="6127" PostId="5751" Score="0" Text="Relative importance primarily, prediction secondary. Does it make a difference?" CreationDate="2015-05-13T18:33:21.863" UserId="378" />
  <row Id="6130" PostId="5788" Score="0" Text="Thanks, this is an interesting idea. Could you expand on what you mean by 'normalise by the range of all scores'?" CreationDate="2015-05-13T20:54:09.223" UserId="378" />
  <row Id="6131" PostId="5788" Score="0" Text="Sure thing.  Your basic idf score will be of the form:&#xA;&#xA;idf = log_n([Number of documents in corpus]/[number of documents containing word])&#xA;&#xA;This number varies with the number of documents that are produced, and as such you'll want to find the maximal sample IDF so that you aren't favoring words based on the fact that they came from a period when more documents are produced.  There are more complex calculations for IDF that attempt to mitigate such factors, but by normalizing by maximum possible score, you fix idf to a 0-1 scale." CreationDate="2015-05-13T23:45:18.993" UserId="8041" />
  <row Id="6132" PostId="5790" Score="2" Text="I didn't think beautiful soup allowed you to iterate over pages, turns out [you can](http://stackoverflow.com/questions/28597041/scraping-multiple-paginated-links-with-beautifulsoup-and-requests). Thanks" CreationDate="2015-05-14T06:24:01.073" UserId="378" />
  <row Id="6133" PostId="5792" Score="0" Text="This is an obvious approach that I hadn't considered, thanks. My only concern is that it would be sensitive to the pre-cleaning approach used, since stopword removal, or removal based on tf-idf weighting, remove the most common words. Small changes to the stopword dictionary could lead to wild changes in rank. It would also be difficult to communicate the magnitude of a change in rank, especially for lower rank terms with similar frequencies that could change rank significantly with a small change in frequency." CreationDate="2015-05-14T06:36:30.493" UserId="378" />
  <row Id="6136" PostId="5746" Score="0" Text="Several algorithms are described in the book &quot;Mining of Massive Datasets&quot;,&#xA;which you can download for free [here](http://www.mmds.org/). Read Chapter 7 &quot;Clustering&quot;." CreationDate="2015-05-11T12:42:05.487" UserId="9085" />
  <row Id="6138" PostId="5792" Score="0" Text="@polyphant -- agreed.  These are complications that would have to be overcome using the ranked frequency analysis.  Doing both types of analysis would probably be a good idea." CreationDate="2015-05-14T15:43:57.430" UserId="609" />
  <row Id="6139" PostId="5480" Score="0" Text="Actually, I was quite surprised myself when I read the book that you could do so much with Excel. And that it had evolutionary and other non-linear solvers built-in!&#xA;&#xA;A nice benefit of Excel is that your work, especially if you're into reproducible code,  is accessible to more people than R or Python code." CreationDate="2015-05-14T23:17:07.703" UserId="9576" />
  <row Id="6140" PostId="5791" Score="0" Text="You could use a simple binary classifier." CreationDate="2015-05-14T23:40:05.563" UserId="115" />
  <row Id="6141" PostId="5780" Score="0" Text="I didn't know about MacQueen's online algorithm! Does it usually get the same results as &quot;classic&quot; K-means? What about using reservoir sampling instead? That way OP has a sample to re-run K-means on in case multiple values of K should be tested." CreationDate="2015-05-15T00:12:32.950" UserId="9576" />
  <row Id="6142" PostId="5784" Score="0" Text="This is extremely helpful.  Thank you." CreationDate="2015-05-15T08:31:24.463" UserId="9608" />
  <row Id="6143" PostId="5798" Score="0" Text="Quoting : &quot;For test data I have similar structure, just the Y column is missing in File 1.&quot; This means that in the test data, I get File 1 without Y column (we have to predict). The objective is to classify the observations in test data File 2, and using that figure out the classification of X1. For example, a candidate in X1=6 might be appearing in File 2 100 times. If in those 100 instances, we can classify 80 to be 1 and 20 to be 0, we can classify X1=1 with 0.80 probability." CreationDate="2015-05-15T08:53:30.850" UserId="9495" />
  <row Id="6144" PostId="5798" Score="0" Text="Are you assigning a Y value in your training set in file1 for each value of X1?" CreationDate="2015-05-15T08:55:58.200" UserId="2576" />
  <row Id="6145" PostId="5798" Score="0" Text="No. In training data set File 1 has X1 classified already. The observations where X1 (both that are classified as 0 or 1) are participating in File 2. For test data I have X1 only in my File 1, i.e. no classification. The range(X1, training_data) is mutually exclusive to range(X1, test_data)." CreationDate="2015-05-15T08:59:17.523" UserId="9495" />
  <row Id="6146" PostId="5798" Score="0" Text="Ok, so I have one additional question. Are there observations in file2 with the same value of X1 but different values of Y?" CreationDate="2015-05-15T09:57:56.313" UserId="2576" />
  <row Id="6147" PostId="5798" Score="0" Text="file2 does not contain any classification. Let me spill the beans here. This question is an obfuscated version of [facebook](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot) problem on kaggle. The X1 is the bidder_id. File1 contains classification info of bidders. File2 contains the bids from the bidders. The bids are not classified for us. Now, I am trying to reformulate this problem as bid-classification leading to bidder-classification. If I can classify bids then i guess classifying bidders is easier." CreationDate="2015-05-15T10:05:00.743" UserId="9495" />
  <row Id="6148" PostId="5761" Score="0" Text="I downvoted your answer, as it doesn't answer your question. Why don't you perform a search on the Internet? I'm sure that a proper comprehensive search will result in significant number (likely, hundreds) of relevant papers." CreationDate="2015-05-15T11:56:46.040" UserId="2452" />
  <row Id="6149" PostId="5729" Score="0" Text="Thanks, I have started working with naive bayes and feature engineering in general.  Any other things apart from naive bayes that I should try?" CreationDate="2015-05-15T14:31:23.923" UserId="9497" />
  <row Id="6150" PostId="5729" Score="0" Text="Well, you still haven't offered very many details about the data itself or the specifics of what you've done, so it's very difficult to give you specific suggestions.  The best I can say is consider incorporating some sequential structure into your model and features either through use of bigrams or markov models / finite state machines." CreationDate="2015-05-15T15:06:13.263" UserId="182" />
  <row Id="6151" PostId="5805" Score="1" Text="Thanks for so many ideas. I have to spend some time on each and will get back to you. :) Highly appreciate you answer." CreationDate="2015-05-16T16:43:45.513" UserId="9003" />
  <row Id="6152" PostId="5812" Score="0" Text="@ ffriend Thanks for your feedback.Product name is important for us because some products have discounts and others have no discounts. Our objective is measure the significance of discounts on product growth. Do you think I can use real commissions/Discounts values in dollars or Zero dollars instead of Yes or NO? How R interpret Date? do I need to convert the data into time series first? Is it Multiple regression is good model for this?" CreationDate="2015-05-16T20:23:54.057" UserId="9663" />
  <row Id="6153" PostId="5812" Score="0" Text="Can you measure significance of discounts _per product_? As for real discount values vs. &quot;yes/no&quot;, try both! It's fairly easy to build several alternative models and estimate how much they fit your data (e.g. using [RSS](http://en.wikipedia.org/wiki/Residual_sum_of_squares)). Regarding date, you probably don't need it per se, but instead convert it into several variables like `month` and `date_of_week` (see 1st paragraph of my answer). And yes, multiple regression looks fine here." CreationDate="2015-05-16T23:38:12.013" UserId="1279" />
  <row Id="6154" PostId="5812" Score="0" Text="Also, please, don't [cross-post](http://stats.stackexchange.com/questions/152628/is-my-sales-growth-dependent-on-commissions-discounts-and-how-do-i-analyze-this) questions on several stackexchange sites. You can always migrate your question if you feel like original site is a bad fit for it, but making cross-posting makes it harder for others to find good answers to the same question, since answers become distributed between several places." CreationDate="2015-05-16T23:42:42.840" UserId="1279" />
  <row Id="6155" PostId="5812" Score="0" Text="@friend Thank you for your help. I am new and I am not aware that all forums are connected. I won't do that next time." CreationDate="2015-05-17T00:41:10.243" UserId="9663" />
  <row Id="6156" PostId="5812" Score="0" Text="No problem. Also note, that on your profile page (e.g. [this](http://datascience.stackexchange.com/users/9663/murali) for current site) you can easily reach all your accounts in StackExchange network." CreationDate="2015-05-17T00:56:46.277" UserId="1279" />
  <row Id="6157" PostId="5818" Score="0" Text="Thank you for your answer @Aleksandr Blekh - really appreciated. Ill digg right into that.&#xA;&#xA;Maybe a stupid question, but please correct me here if I'am wrong here:a correlation analysis, while using one airline as the variable to correlate with. The results were compelling so far, as some airlines espc. those who had codeshare agreements had similar prices. &#xA;&#xA;Would such high correlations e.g.:&#xA;`ColumnUA(LH) 0.90435 &lt;.0001&#xA;ColumnSQ 0.32544 &lt;.0001&#xA;ColumnAF(DL) 0.55336 &lt;.0001`&#xA;&#xA;I assume such results **indicate** similar price patterns.&#xA;With a regression analysis, what would I find out?" CreationDate="2015-05-18T02:55:16.293" UserId="9684" />
  <row Id="6158" PostId="5818" Score="0" Text="@s1x: You're very welcome (feel free to upvote/accept, if you value the answer and when you'll get enough reputation to do so, of course). Now, on to your question. As I said, TS analysis is more sophisticated and comprehensive. In particular TS regression, accounts for so-called [autoregression](http://en.wikipedia.org/wiki/Autoregressive_model) and other TS complexities. Hence, my suggestion to use TS regression analysis instead of simpler traditional one. Also, you should _always_ start with EDA, no matter what data analysis you plan to perform (actually, EDA will often change your plans)." CreationDate="2015-05-18T03:21:38.997" UserId="2452" />
  <row Id="6159" PostId="2258" Score="0" Text="I'm surprised no one has flagged this question as Too Broad" CreationDate="2015-05-18T05:29:36.683" UserId="9670" />
  <row Id="6160" PostId="5810" Score="0" Text="I think this is a very broad question. Can you narrow it down to a particular example problem and how much you know about how control theory applies already?" CreationDate="2015-05-18T07:34:18.473" UserId="21" />
  <row Id="6161" PostId="693" Score="0" Text="You can train n svm's for n classes using a one vs the rest strategy. SciKitLearn has code to do that automatically. Technically you need n-1 classifiers, but i've found having n works better." CreationDate="2015-05-18T19:54:36.913" UserId="1301" />
  <row Id="6163" PostId="5788" Score="0" Text="I have a few issues with this approach. Firstly, as mentioned by @MrMeritology, this approach is only valid when comparing counts for the same term. Secondly, tf-idf would negatively discriminate frequent terms appearing across all documents. If it's these frequent terms that you want to compare between periods then the weighted differences would be very small. It's an interesting idea, with it's own area of application, but I'm looking for something more general purpose, preferably taking in to account the properties of Heap's and Zipf's law that lead to the issue in the first place" CreationDate="2015-05-19T16:47:23.443" UserId="378" />
  <row Id="6164" PostId="5822" Score="0" Text="you need to start by creating some basic stats on individuals do/don't get converted from recipients to actors.  I think you'll have a hard time getting people here to do that for you.  I would suggest tackling the basic stats and see how far you get...if you run into problems then post your code and intent-it'll give folks here something to work with and it will demonstrate that you giving your problem some effort." CreationDate="2015-05-19T17:18:17.507" UserId="3457" />
  <row Id="6165" PostId="1100" Score="0" Text="My initial thought would be to use a random forest of logistic regression on per-server-type models.  Then you have your benchmarks and  you'll find out pretty quick if a neural net will give you more.  Neural nets don't always give the best results." CreationDate="2015-05-19T17:24:41.940" UserId="3457" />
  <row Id="6166" PostId="5788" Score="0" Text="I would contend that if the method can be done for a single word, it's pretty trivial to run it across the entire corpus.   None the less, the method proposed by @MrMeritology does extend more naturally to the entire data set.&#xA;&#xA;For your second point, if you have an idf score that shrinks and grow logrithmically, the linear growth of the tf term will overtake the the weighting for all but the most frequently used words (that would most likely be on a stop word list anyway)." CreationDate="2015-05-19T17:35:03.377" UserId="8041" />
  <row Id="6167" PostId="5836" Score="0" Text="I'm sorry but I can't see how in this case you can even think of trying to train a convnet, it simply can't answer this kind of question for the reasons Neil Slater explained : features vectors do not have in general a &quot;spatial&quot; meaning (except special cases e.g. SIFT). For example, you can permute dimensions. On an image, where you have spatial structure, you scan for patterns and convnets are one of the many possibilities. Anyhow, in this case it just doesn't make any sense and moreover it's wrong. Modulo the fact that the question is a little bit vague." CreationDate="2015-05-19T21:35:43.387" UserId="8040" />
  <row Id="6168" PostId="5836" Score="0" Text="@BertrandR  yes, as I said, I agree with Neil...likely it will not work.  And yes, the question is a bit vague...most likely because nub may not know much about NNs and CNNs.  But (s)he has data and sometimes you learn lessons much better by working with your own data.  Even getting a net to train can be an accomplishment when you're starting out.  I'm not saying to use a NN or CNN because I think it will give the best result...I'm encouraging nub to experiment and break stuff and learn." CreationDate="2015-05-20T06:39:10.653" UserId="3457" />
  <row Id="6169" PostId="5841" Score="0" Text="This comment helped me a bit more to realize how things work. I would vote you up but I need 15 reputation in this stackechange site. I'll do it when I have enough rep. I'll keep reading about this kind of things to see if I can manage what I want to do. Thanks." CreationDate="2015-05-20T10:15:14.173" UserId="9714" />
  <row Id="6170" PostId="5788" Score="0" Text="@ j.a.gartner Even if you run it across all terms, you can't then compare the scores for those terms. Agree with you on the second point though, I ignored the tf part" CreationDate="2015-05-20T10:18:41.960" UserId="378" />
  <row Id="6171" PostId="5809" Score="0" Text="Just because you are doing data science doesn't mean you have to ask your question here.  This question has already been answered in Stackoverflow here and in several other questions (so you could have had your answer imediately): http://stackoverflow.com/questions/27217331/r-subset-dataframe-by-time-only." CreationDate="2015-05-20T13:58:15.283" UserId="9689" />
  <row Id="6172" PostId="5853" Score="0" Text="Thanks for your thorough explanation. I agree with you that such analysis based on prices only are quite limited. This also includes notably fare rules (Refundable tickets, minimum stay etc.) Some of those limitation can be overcome by collecting always same fares to make the comparable. However, a important information - as you mentioned, is missing the amount of seats available (can be != seats in a plane) and the the actually amount of sold tickets." CreationDate="2015-05-21T13:16:38.673" UserId="9684" />
  <row Id="6173" PostId="5838" Score="0" Text="Can you provide more information? what is &quot;categories id of product A&quot; and is &quot;searching keywords id of product A&quot; of the same length for all entries? &quot;the dimensions of the searching keywords id could be more than 10000&quot; why? what are they? How many samples do you have? all questions can be answered if you post a few sample of your data here. Then I could probably suggest you something." CreationDate="2015-05-21T13:17:09.953" UserId="8878" />
  <row Id="6174" PostId="5853" Score="0" Text="Access to such data is very limited and if - outdated (eg. Databank 1B from US DOT). Some research such as Clark R. and Vincent N. (2012) Capacity-contingent pricing [...] [link](http://bit.ly/1HvHViw) includes such data and offer much better insights. &#xA;&#xA;I'am aware of the limitations (hopefully ;-) ) and as you mentioned as there are much more information influencing prices. Still when observing a specific market you can get a _feeling_ of what happens. You can see if there is any compeitive behaviour and different pricing strategy approachs. However, you would never be able to find the cause." CreationDate="2015-05-21T13:19:26.397" UserId="9684" />
  <row Id="6175" PostId="5844" Score="1" Text="Does the data include the amount of funding? Are you trying to compute the probability of going to round N+1 given funding X at round N? Do you have any other info about the company (turnover, size, profit)?" CreationDate="2015-05-21T13:22:37.810" UserId="471" />
  <row Id="6176" PostId="5853" Score="1" Text="@s1x - I agree and I wish I had a solid alternative to offer, but, as you've learned yourself, detailed revenue data is the most jealously guarded secret at any airline. Just wanted to make sure you're aware of that and what goes into the data generation process. Beyond, that, I like what you're trying to do and I think the other answer is a step in the right direction, technique-wise. If I might suggest, you could also take a look at using cross-correlation between your various TS during your data exploration, as it is often valuable for discerning patterns between linked TS." CreationDate="2015-05-21T13:24:43.460" UserId="9448" />
  <row Id="6177" PostId="5838" Score="0" Text="Are product A and product B two products that the user bought?  The wording seems to suggest that products A and B are different for each user, since the keywords can vary.  Is this so? And last comment, do you want to classify or cluster? Those are quite different techniques :)" CreationDate="2015-05-21T14:28:46.533" UserId="1367" />
  <row Id="6178" PostId="2492" Score="0" Text="As pointed above, trying to merge those multiple variables into one becomes more complicated as the number of dummy variables rises. For modelling purposes, it is often useful to keep those variables as dummy variables. If some combinations of those dummies are very common, then you can explore those effect by using interactions in your model. Whether you want to reduce those variables into a composite will depend on the type of analysis you plan on conducting on the resulting data." CreationDate="2015-05-21T14:52:43.857" UserId="9646" />
  <row Id="6179" PostId="5855" Score="0" Text="Regarding your last sentence, it depends on your definition of &quot;really weird&quot;. The [origin of the word](http://en.wikipedia.org/wiki/Gerrymandering#Etymology) refers to the salamander-like shape of the resulting district. So you will have to balance optimization with constraints because the more geometric constraints you impose, the less optimized the result will be." CreationDate="2015-05-21T16:24:39.467" UserId="964" />
  <row Id="6180" PostId="5855" Score="0" Text="You are right @bogatron. But I know that there are certain statistical formulas to measure the compactness of a shape. Ideally this could be involved also in the optimization process." CreationDate="2015-05-21T16:32:13.417" UserId="5211" />
  <row Id="6181" PostId="5838" Score="0" Text="Thank you @kasramsh so much for your replies. I updated the description and also attached a sample data. Hope to get some suggestions from you!" CreationDate="2015-05-21T18:09:33.180" UserId="9724" />
  <row Id="6182" PostId="5838" Score="0" Text="@logc yes, product( i said product A earlier) and website( i said product B earlier) are different from each user. Each product has a few keywords and each website has a few keywords too.  Either clustering or classification is fine, as long as I can make an user profile, such as &quot;male young gamer&quot;; &quot;stay at home mom&quot;. I think clustering is more preferable .  Thank you!!" CreationDate="2015-05-21T18:09:49.113" UserId="9724" />
  <row Id="6183" PostId="5852" Score="0" Text="Thank you @Lennart Kloppenburg for your reply. How to perform feature selection if the attribute (keword_id) are ordered number? I updated a sample data above. Could you please take a look and give me some suggestions? Thank you!" CreationDate="2015-05-21T18:13:42.633" UserId="9724" />
  <row Id="6185" PostId="5857" Score="0" Text="Thank you so so much! It's very helpful. I will start from the mapping. I really appreciate it!" CreationDate="2015-05-21T20:19:48.177" UserId="9724" />
  <row Id="6186" PostId="5855" Score="0" Text="try http://gis.stackexchange.com/ people there may have a better idea or some experience with redistricting." CreationDate="2015-05-21T20:53:28.460" UserId="471" />
  <row Id="6187" PostId="5857" Score="0" Text="Happy to help. :)" CreationDate="2015-05-21T21:44:37.090" UserId="1367" />
  <row Id="6189" PostId="5854" Score="0" Text="Yes, that is what I am aiming at. I have data on funding volume and timing on each round. I'll look into the links you posted and I'll try running the code. Thanks a lot for your help." CreationDate="2015-05-22T14:57:04.613" UserId="10171" />
  <row Id="6191" PostId="5844" Score="0" Text="I am trying to predict how many companies I will have to invest in if I raise a US$ x m fund. This depends on the likelihood of companies advancing through financing stages, and how much I will invest in a Seed, Series A, B, C... round. That way I know that I can invest in less companies if I go from investing 12% to 25% of the total amounted in each Series A a participated in rounds. The stochastic modelling im trying here is to calibrate the survival rates in the model." CreationDate="2015-05-22T15:00:39.960" UserId="10171" />
  <row Id="6192" PostId="5860" Score="3" Text="Not sure if a similar question exists already, but this would be a good question for a community wiki answer." CreationDate="2015-05-22T15:41:27.183" UserId="182" />
  <row Id="6193" PostId="5428" Score="0" Text="for completeness, i edited my answer to include a graphical example." CreationDate="2015-05-22T17:06:33.990" UserId="8953" />
  <row Id="6194" PostId="5541" Score="1" Text="... and it may be a useless result, as the Gini is usually applied to data that has two categorial labelings, while AUROC is applied to numerical ranking data + a binary label. They *may* **coincide *only* if your ranking is binary?** in which case it would not make much sense to use AUROC at all because it is a 3-point curve with only 2 degrees of freedom... (I have not checked that result, too much paper spam on Wikipedia these days.)" CreationDate="2015-05-22T21:34:51.397" UserId="924" />
  <row Id="6195" PostId="3751" Score="0" Text="@zen I would assume Sean meant for you to edit/expand on the question, not post details as a comment.  I think it would be more helpful to future users that way, if they end up here for the first time through an online search." CreationDate="2015-05-23T01:54:55.237" UserId="8953" />
  <row Id="6196" PostId="3751" Score="0" Text="Except of already mentioned great coursera stuff by Geoff Hinton, also this series by Nando de Freitas is worth a look: https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu" CreationDate="2015-05-23T10:33:02.757" UserId="173" />
  <row Id="6197" PostId="5864" Score="0" Text="Thanks so much, I'll take a look on these books." CreationDate="2015-05-23T20:02:45.400" UserId="9771" />
  <row Id="6198" PostId="5878" Score="0" Text="Thanks, I'm already watching the first lecture." CreationDate="2015-05-23T20:55:25.817" UserId="9771" />
  <row Id="6199" PostId="5789" Score="2" Text="Web scraping LinkedIn is against their terms of service. See [LinkedIn “DOs” and “DON’Ts”](https://www.linkedin.com/legal/user-agreement)- DON'T:&quot;Use manual or automated software, devices, scripts robots, other means or processes to access, “scrape,” “crawl” or “spider” the Services or any related data or information;&quot;" CreationDate="2015-05-23T21:03:07.677" UserId="1330" />
  <row Id="6200" PostId="5878" Score="0" Text="A good companion to his web series is [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). It has a &quot;physical&quot; book also in addition to the free online PDF." CreationDate="2015-05-23T21:08:41.933" UserId="8354" />
  <row Id="6201" PostId="5883" Score="0" Text="Thanks for the links. That looks great. I have plenty of time at the moment and I'll work on this." CreationDate="2015-05-24T01:50:08.913" UserId="9771" />
  <row Id="6203" PostId="5886" Score="1" Text="When you say &quot;divide by the difference between the largest and smallest number&quot; is the smallest number the smallest in the updated array (updated as in the array where every absolute value has been added) or is the smallest number still negative?" CreationDate="2015-05-24T03:56:17.383" UserId="9802" />
  <row Id="6204" PostId="5877" Score="0" Text="Thank you for the advice." CreationDate="2015-05-24T07:15:33.400" UserId="9793" />
  <row Id="6205" PostId="5861" Score="0" Text="where is that image to be found in original size?" CreationDate="2015-05-24T13:41:44.707" UserId="9810" />
  <row Id="6206" PostId="5861" Score="0" Text="@WalterTross, here https://www.google.com/search?q=data+science+road+map" CreationDate="2015-05-24T14:33:45.417" UserId="97" />
  <row Id="6207" PostId="5886" Score="0" Text="@Jonathan: it doesn't matter as long as both values are from the same array, original or updated. Since the same number is added to every value, the difference between the minimum and maximum stays the same." CreationDate="2015-05-24T21:00:55.447" UserId="9819" />
  <row Id="6208" PostId="5859" Score="0" Text="+1 for you! good answer!" CreationDate="2015-05-24T23:39:13.500" UserId="8878" />
  <row Id="6209" PostId="5864" Score="0" Text="Russel is the starting point usually" CreationDate="2015-05-24T23:41:07.907" UserId="8878" />
  <row Id="6210" PostId="5877" Score="2" Text="Also use all of them and reduce dimensionality. e.g. PCA extracts meaningful features according to the variance." CreationDate="2015-05-24T23:42:52.493" UserId="8878" />
  <row Id="6211" PostId="5893" Score="1" Text="Python NLTK module provides stopwords data and if it did not help you better provide more info about your dataset. Why it was not helpful in your case?" CreationDate="2015-05-25T01:25:41.937" UserId="8878" />
  <row Id="6212" PostId="335" Score="0" Text="I agree. The course material is good to get you started but it is mostly entry level." CreationDate="2015-05-25T07:27:09.663" UserId="75" />
  <row Id="6214" PostId="5893" Score="0" Text="@kasramsh: When i filtered for these SWs i had the impression that this did not filter significantly out the spam. I think the reason is, that these list are generated on natural texts (not sure) and therefore are not usable for searchwords and site queries. Like when you cluster ( based on the search string similarity) i had the impression that the spam has a strong effect at entropy level and thereby is mixing up the end result :-/." CreationDate="2015-05-25T09:53:01.813" UserId="4676" />
  <row Id="6215" PostId="5899" Score="1" Text="what do you mean by 'top 5 Id2 matches of each attribute'. Can you modify your output so that match values are correct too" CreationDate="2015-05-25T12:58:05.340" UserId="9801" />
  <row Id="6216" PostId="5899" Score="0" Text="@device_exec output is edited. top 5 attributes are preferences by each attribute." CreationDate="2015-05-25T13:14:42.673" UserId="8657" />
  <row Id="6217" PostId="5893" Score="0" Text="I think @PlagTag don't understand what is `stop words`. Stop-wrods is a list of most common words in some language, for example `I`, `the`, `a` and so on. You will just remove this words from your text before start train your algorithm which try identify which text is spam or not. It didn't help you identify which text is spam or not, it can give your learning algorithm some improve." CreationDate="2015-05-25T14:18:51.500" UserId="9634" />
  <row Id="6218" PostId="5370" Score="0" Text="Some other useful python libraries include : pandas, numpy, scipy etc. Adding this in support of learning python :)" CreationDate="2015-05-25T14:45:49.237" UserId="75" />
  <row Id="6219" PostId="5888" Score="1" Text="This approach is also known as min-max normalization (as we are using min and max values)" CreationDate="2015-05-25T14:50:32.343" UserId="75" />
  <row Id="6220" PostId="5856" Score="0" Text="Probably this question should exist as community wiki." CreationDate="2015-05-25T14:54:08.057" UserId="75" />
  <row Id="6221" PostId="313" Score="0" Text="I think this question should be marked as community wiki." CreationDate="2015-05-25T15:04:39.207" UserId="75" />
  <row Id="6222" PostId="5907" Score="0" Text="Not a bad idea, although in that particular case I think I'd want separation. Things like &quot;software developer&quot; and &quot;software engineer&quot; might be close enough to merge (in our case, maybe not others)." CreationDate="2015-05-25T18:33:22.760" UserId="9807" />
  <row Id="6223" PostId="5897" Score="1" Text="This would be a better question if you could specify for what specific purpose you think you need linear algebra." CreationDate="2015-05-26T06:36:57.857" UserId="21" />
  <row Id="6224" PostId="5137" Score="0" Text="A related discussion on [reddit](https://www.reddit.com/r/datascience/comments/373lpg/data_science_and_automation/)" CreationDate="2015-05-26T08:38:59.127" UserId="75" />
  <row Id="6225" PostId="5902" Score="0" Text="thx a lot, i will try out this one and report here!" CreationDate="2015-05-26T08:46:17.480" UserId="4676" />
  <row Id="6226" PostId="5893" Score="0" Text="@itdxer, thanks for your comment. I used the term stopwords here in an broader extend (as i thought it might be ok for the purpose). Thank you for clearing up the issue ;-)" CreationDate="2015-05-26T08:49:23.637" UserId="4676" />
  <row Id="6227" PostId="5693" Score="0" Text="You are right @Spacedman, I have edited my post. I hope it is now clearer." CreationDate="2015-05-26T09:03:58.997" UserId="9461" />
  <row Id="6228" PostId="5737" Score="2" Text="If you write a paper using the ideas collected here, will you list StackOverflow as one of your collaborators? :)" CreationDate="2015-05-26T09:43:15.897" UserId="1367" />
  <row Id="6230" PostId="5910" Score="1" Text="I think there are no alternatives to your method!" CreationDate="2015-05-26T13:58:05.330" UserId="9766" />
  <row Id="6231" PostId="5806" Score="1" Text="Thanks for the suggestion! I've used dimensionality reduction in the past (PCA), but wasn't able to figure out how to determine how much each variable contributed to the orthogonal components. I'll look through the Burgess tutorial, perhaps that will give me more ideas." CreationDate="2015-05-26T15:00:02.393" UserId="7922" />
  <row Id="6233" PostId="5877" Score="1" Text="Exactly. You can even consider it as a small research project. You assume that there are some features that will work well based on your intuition, and pick those. Or, you assume nothing and input &quot;everything&quot; into a feature selection algorithm. The output for that will be the machine's way of telling you &quot;These N features work the best for me to discriminate between the possible labels&quot;. Good luck!" CreationDate="2015-05-26T15:59:07.330" UserId="8152" />
  <row Id="6237" PostId="5806" Score="0" Text="@NavaneethanSanthanam: You're very welcome!" CreationDate="2015-05-26T17:07:28.127" UserId="2452" />
  <row Id="6239" PostId="5137" Score="0" Text="How can we tell this Q isn't simply advertising for that product?" CreationDate="2015-05-26T21:53:49.950" UserId="471" />
  <row Id="6240" PostId="5913" Score="4" Text="This question is too broad and needs to be framed as a question rather than a seed for a discussion." CreationDate="2015-05-26T23:34:16.273" UserId="847" />
  <row Id="6241" PostId="5916" Score="0" Text="Could you explain the &quot;path&quot; part of data or attach a sample dataset if possible" CreationDate="2015-05-27T05:45:35.840" UserId="75" />
  <row Id="6243" PostId="5916" Score="0" Text="I'm gonna have a csv file like this&#xA;DeviceID,Screen,Time_spent_on_Screen,Transition. &#xA;ABC,Screen1, 3s, 1-&gt;2.&#xA;ABC,Screen2, 2s, 2-&gt;4.&#xA;ABC,Screen4, 1s, 4-&gt;2.&#xA;So the &quot;path&quot; here is 1-&gt;2-&gt;4-&gt;2, 1,2,3,4 are screens" CreationDate="2015-05-27T05:53:12.337" UserId="9852" />
  <row Id="6244" PostId="5916" Score="0" Text="Looking at it. Meanwhile could you add this to question itself?" CreationDate="2015-05-27T05:55:09.507" UserId="75" />
  <row Id="6245" PostId="5916" Score="0" Text="Ohk now how I see it is : screens are basically pages. So you have the data about how much time the a user (identified by device id) spending on each page (identified by screen) and the data about how the user hops from one page to another. Is this right?" CreationDate="2015-05-27T05:58:38.400" UserId="75" />
  <row Id="6246" PostId="5916" Score="0" Text="Yes, I want to cluster users based on the path they use to browse the app." CreationDate="2015-05-27T06:03:05.040" UserId="9852" />
  <row Id="6249" PostId="5918" Score="0" Text="Hey could you add some more details. Also it seems incomplete as of now." CreationDate="2015-05-27T07:55:31.740" UserId="75" />
  <row Id="6252" PostId="5919" Score="0" Text="&gt;how do I go about clustering the users.&#xA;&#xA;It is indeed about clustering the users no?" CreationDate="2015-05-27T08:08:53.417" UserId="75" />
  <row Id="6253" PostId="5917" Score="1" Text="this clusters the pages, not the users." CreationDate="2015-05-27T08:13:41.863" UserId="6550" />
  <row Id="6254" PostId="5919" Score="0" Text="yes yes. actually you use these features that i mentioned to cluster user behaviors (or users as you say :) )" CreationDate="2015-05-27T08:26:29.823" UserId="8878" />
  <row Id="6256" PostId="5917" Score="0" Text="Edited the answer. Thanks for pointing it out." CreationDate="2015-05-27T09:24:12.517" UserId="75" />
  <row Id="6257" PostId="5909" Score="0" Text="Thank you for sharing @Shagun, I will consider this option." CreationDate="2015-05-27T09:54:41.113" UserId="9461" />
  <row Id="6258" PostId="5908" Score="0" Text="Thanks a lot thilak for the explanation" CreationDate="2015-05-27T10:12:22.907" UserId="9793" />
  <row Id="6259" PostId="5909" Score="0" Text="Sure. Do as share your experience." CreationDate="2015-05-27T10:28:02.330" UserId="75" />
  <row Id="6260" PostId="5909" Score="0" Text="What if, instead of using Elastic on top, I use geohashing as an index in Cassandra ?" CreationDate="2015-05-27T13:31:47.127" UserId="9461" />
  <row Id="6261" PostId="5909" Score="0" Text="[Geohash](http://en.wikipedia.org/wiki/Geohash) is not a spatial index. I assume you were referring to some kind of spatial index. So yeah you can do that. My intent of mentioning Elastic (or any external index) was to take care of cases where the indexing offered by the database itself does not take care of spatial datatypes." CreationDate="2015-05-27T13:35:50.253" UserId="75" />
  <row Id="6262" PostId="5909" Score="0" Text="Ok I read too fast, thank you for your explanations @Shagun" CreationDate="2015-05-27T14:02:03.710" UserId="9461" />
  <row Id="6263" PostId="5860" Score="0" Text="This question is clearly more suited for a community wiki answer, since **it doesn't have a unique answer**.  The format of Q&amp;A sites is more suited to questions with a &quot;best&quot; answer." CreationDate="2015-05-27T14:16:13.913" UserId="1367" />
  <row Id="6264" PostId="5925" Score="0" Text="Thanks Shagun, I will try this." CreationDate="2015-05-27T14:42:13.513" UserId="9793" />
  <row Id="6265" PostId="5925" Score="0" Text="any suggestion on how to extra weight to the recent data?" CreationDate="2015-05-27T14:56:53.847" UserId="9793" />
  <row Id="6266" PostId="5916" Score="0" Text="To me, the mixture of Markov chains model calibrated by maximum likelihood methods seems more serious than graph heuristics. For example, this paper: http://link.springer.com/article/10.1023/A:1024992613384." CreationDate="2015-05-27T15:04:30.043" UserId="6550" />
  <row Id="6267" PostId="5925" Score="0" Text="Time based exponential decay is used commonly with data having a temporal aspect. Assuming you have only 2 years data, you can go for something simpler based on your intuition and requirement of data/end product." CreationDate="2015-05-27T16:08:22.920" UserId="75" />
  <row Id="6268" PostId="5857" Score="0" Text="Hi @logc, I applied LDA for selecting the features. I considered each user_id as a &quot;document&quot; and the keywords are the &quot;words&quot; in the &quot;document&quot;, then by applying LDA I got a few topics of keywords. However, i don't know why most of my topics are consist of the same keywords. Does that mean LDA is not the right method for my case or there are some mistakes? Thank you so much!" CreationDate="2015-05-28T00:33:05.980" UserId="9724" />
  <row Id="6270" PostId="5932" Score="1" Text="Flagged the question as it has nothing to do with data science. Probably [Stackoverflow](http://stackoverflow.com/) would be the right place for this." CreationDate="2015-05-28T08:22:04.493" UserId="75" />
  <row Id="6271" PostId="5930" Score="1" Text="You are right! I would like to know what techniques could be used to tackle the problem. Specifically, I was asking whether there is a way to minimize the correlation between X1 and Y without using linear regression. Thank you for the clarification :)" CreationDate="2015-05-28T08:25:50.033" UserId="9838" />
  <row Id="6272" PostId="5857" Score="0" Text="@sylvia : I would suggest that you turn that question into a new question on this site.  Otherwise, we might end up writing a ton of comments, and that is not the best format for Q&amp;A. :)" CreationDate="2015-05-28T10:54:10.410" UserId="1367" />
  <row Id="6273" PostId="5933" Score="0" Text="When I read the question's title, I thought you expected some commercially viable product ideas -- something probably outside the scope of this site.  But you seem to be looking for some interesting analysis.  Why do you call it a product? What is it you expect as an outcome?" CreationDate="2015-05-28T11:04:40.330" UserId="1367" />
  <row Id="6274" PostId="5932" Score="0" Text="A simple Google search of the first error line would take you to a [StackOverflow thread where they discuss this issue](http://stackoverflow.com/questions/14024756/slf4j-class-path-contains-multiple-slf4j-bindings) or [another SO thread where they solve it again](http://stackoverflow.com/questions/22896243/maven-slf4j-class-path-contains-multiple-slf4j-bindings)" CreationDate="2015-05-28T11:08:35.790" UserId="1367" />
  <row Id="6275" PostId="5933" Score="0" Text="I call it a product in the sense that I do not want some simple or trivial analysis. And as you correctly mentioned - I am not looking for any commercially viable idea - just some cool fun idea :)" CreationDate="2015-05-28T12:12:42.793" UserId="75" />
  <row Id="6277" PostId="5937" Score="0" Text="Thank you...I will read the example." CreationDate="2015-05-28T15:56:31.993" UserId="9501" />
  <row Id="6278" PostId="5880" Score="0" Text="Thanks for the recommendation, but the first link seems not working to me." CreationDate="2015-05-28T18:35:27.500" UserId="9771" />
  <row Id="6279" PostId="5857" Score="0" Text="Thanks for the suggestion. Here is the link I posted if you have time to take a look http://datascience.stackexchange.com/questions/5941/need-help-with-lda-for-selecting-features Thanks!" CreationDate="2015-05-28T22:10:02.293" UserId="9724" />
  <row Id="6280" PostId="5943" Score="0" Text="This question does not ask for any form of source code. Please consider including only the relevant portion of code (if at all). Also, mathematically, the logistic loss function (or the cross entropy function) is well defined for binary classification (mapping classes to -1 and 1). What is the loss function mathematically for cross-entropy for multinomial case (in mathematical form)? For example, one way to solve multinomial problems is to solve K-1 binary problems (for K classes) which is not really a loss function for the multinomial case." CreationDate="2015-05-29T03:38:56.487" UserId="847" />
  <row Id="6281" PostId="5946" Score="0" Text="With 2D features you can visualize your data, correct? Everything will be much clearer then. And why  &quot;nor can I use hierarchical clustering&quot;?" CreationDate="2015-05-29T08:08:17.297" UserId="9085" />
  <row Id="6282" PostId="5946" Score="0" Text="Yes I can visualize the data.... Hierarchical clustering is going to give me l clusters and subclusters ..... Oh so are you suggesting that even if I get subclusters , in order to obtain my clusters I should repeatedly subtract a subcluster from its parent cluster?" CreationDate="2015-05-29T08:11:36.060" UserId="8013" />
  <row Id="6283" PostId="5946" Score="0" Text="I just tried to visualize it, but identification of the separate clusters is not possible...." CreationDate="2015-05-29T08:15:11.850" UserId="8013" />
  <row Id="6284" PostId="5946" Score="1" Text="With hierarchical clustering you make a dendogram. Then you cut it at specific level, which you estimate reasonable from the dendogram.  The precedure is very clear and gives you clusters.  No &quot;subclusters&quot;, no substraction.  If you post your visualization (plot of your actual data that you are trying to cluster), it will be easier to tell more." CreationDate="2015-05-29T08:18:26.837" UserId="9085" />
  <row Id="6285" PostId="5944" Score="0" Text="(This question belongs to math.stackexchange..) You can get an approximate solution by simply computing your probability for all n for which L1 distance between x P^n and the stationary distribution x* is more than epsilon. This assumes your chain is also aperiodic." CreationDate="2015-05-29T08:43:39.023" UserId="6550" />
  <row Id="6286" PostId="5910" Score="0" Text="Why don't you just see which input has the least pearson correlation(since the relationship is linear) to the output, and then ignore it ?" CreationDate="2015-05-29T08:49:19.273" UserId="9460" />
  <row Id="6287" PostId="170" Score="0" Text="Is this entropy? I am confused." CreationDate="2015-05-29T12:41:01.670" UserId="6550" />
  <row Id="6288" PostId="455" Score="0" Text="A more appropriate place for this question is http://opendata.stackexchange.com/" CreationDate="2015-05-29T17:38:53.977" UserId="3466" />
  <row Id="6289" PostId="155" Score="0" Text="A great place to find public data sets is http://opendata.stackexchange.com/" CreationDate="2015-05-29T18:00:14.140" UserId="3466" />
  <row Id="6290" PostId="5880" Score="0" Text="In case the link for Intro to Artificial Intelligence doesn't work, try this: go to udacity.com, click &quot;catalog&quot; on the upper right corner, then type &quot;artificial intelligence&quot; in upper right search box. A list of classes will show up. I just tried this. The second class is &quot;Intro to Artificial Intelligence&quot;." CreationDate="2015-05-29T18:51:02.840" UserId="9593" />
  <row Id="6293" PostId="5730" Score="0" Text="Interesting project. The free Udacity class - Intro to Machine Learning (https://www.udacity.com/course/intro-to-machine-learning--ud120) also explores Enron Corpus. To discover which authors might have affinity, maybe one can start with examining the number of emails between two people, assuming the number of emails would indicate that they cared about each other enough so they didn't avoid emailing each other as much as possible. What's your learning goal in this project?" CreationDate="2015-05-29T20:50:07.590" UserId="9593" />
  <row Id="6294" PostId="5730" Score="1" Text="Suppose you find that some people exchanged emails a lot (say 100 email exchanges or above). Can these people be further divided into two groups according to certain features, such as the average time to reply an email (within three days or longer than that)? In addition to clustering, you can do supervised learning - search newspaper stories to find some people who cared about each other and other people who didn't care that much, then check the characteristics of emails from these two groups." CreationDate="2015-05-29T21:30:49.603" UserId="9593" />
  <row Id="6297" PostId="5958" Score="1" Text="Please note the language. Your question is totally unclear. What does &quot;the sample data have 1 observation per case and are not timed&quot; mean?" CreationDate="2015-05-30T13:31:24.033" UserId="8878" />
  <row Id="6298" PostId="5880" Score="0" Text="All right. Thanks again." CreationDate="2015-05-30T19:00:12.117" UserId="9771" />
  <row Id="6299" PostId="5959" Score="1" Text="Thanks for a great answer. My goal, in fact, is to do &quot;segmentation of time series&quot; for each vehicle in my data set." CreationDate="2015-05-30T20:03:43.590" UserId="4933" />
  <row Id="6300" PostId="5958" Score="0" Text="I was talking about the examples I found in online tutorials. The sample data they use have only 1 observation per case / individual (e.g. customer, country, etc). And those data are not time series." CreationDate="2015-05-30T20:07:32.847" UserId="4933" />
  <row Id="6301" PostId="5959" Score="1" Text="I am studying tutorials on time series decomposition. I found that there are ways to decompose them into trend, seasonal and cyclical components. My time series, however, are few seconds of vehicle trajectories. Is it possible to decompose them into different driving behavior components based on the trends in accelerations, speeds, lead vehicle speeds &amp; accelerations within an observed trajectory?" CreationDate="2015-05-30T20:38:09.290" UserId="4933" />
  <row Id="6302" PostId="5959" Score="1" Text="Maybe! for this better to consider both &quot;decomposition&quot; and &quot;segmentation&quot;. For instance if your time series show significant clusters in PC space you can relate them to driving behavior. Segmentation is also to detect different driving behaviors within a time series. The long story short is that you can use segmentation for different driving behavior segments for one vehicle and decomposition techniques for detecting global driving behaviors over all vehicles." CreationDate="2015-05-30T23:52:53.677" UserId="8878" />
  <row Id="6304" PostId="5963" Score="1" Text="btw - a million entries for 50 person sounds like you have many key card doors in this office or somehow the job requires that. Approx 25 entry/exit pairs per person per day for 365 days of year." CreationDate="2015-05-31T11:53:44.037" UserId="9901" />
  <row Id="6305" PostId="5845" Score="0" Text="Turns out that there are various methods for doing this, most of which rely on assumptions about w or the data. I think I've derived a really simple way of converting the problem into a k nearest neighbor search. I'll post about it after I've tested it and found out whether it's actually something well known." CreationDate="2015-05-31T23:37:00.517" UserId="1283" />
  <row Id="6306" PostId="5712" Score="0" Text="Here, check the documentation of function `glmnet` from `glmnet` package and look and parameter `standardize` [http://cran.r-project.org/web/packages/glmnet/glmnet.pdf](http://cran.r-project.org/web/packages/glmnet/glmnet.pdf) - there also is possibleto use regularization" CreationDate="2015-06-01T10:03:17.397" UserId="5224" />
  <row Id="6307" PostId="5966" Score="0" Text="I just read mean shift clustering , and I seem to like it...I will read more and let you know.. thanks a lot!" CreationDate="2015-06-01T11:45:46.800" UserId="8013" />
  <row Id="6308" PostId="5973" Score="0" Text="It seems that least squares estimator and stepwise regression may fail if the variables are almost collinear." CreationDate="2015-06-01T13:23:20.503" UserId="5091" />
  <row Id="6309" PostId="5973" Score="0" Text="You did not include a fundamental piece of information: how many observations do you have? :)" CreationDate="2015-06-01T16:37:38.663" UserId="9766" />
  <row Id="6310" PostId="5969" Score="0" Text="Are your categories being used as inputs to some other model, or are they the output?" CreationDate="2015-06-01T17:01:15.677" UserId="8041" />
  <row Id="6311" PostId="5947" Score="0" Text="Thank you for your reply!" CreationDate="2015-06-01T17:46:27.627" UserId="9724" />
  <row Id="6312" PostId="5982" Score="0" Text="Can you elaborate on your last paragraph (&quot;transfer learning&quot;)? Could you provide any links or names?" CreationDate="2015-06-01T20:49:31.190" UserId="6550" />
  <row Id="6314" PostId="5881" Score="0" Text="With clarity, this question could be made much better. Can you provide more details on what domain would you like to find interesting algorithms and from what time? Some of us here could be able to provide you with a detailed list of breakthrough results/ algorithms developed in a given era." CreationDate="2015-06-01T21:07:30.223" UserId="847" />
  <row Id="6315" PostId="5933" Score="0" Text="@Shagun This is an open ended question that is not suited for this format. What seems trivial to you, might be involved for someone else and high return generating (for a product, app or an organization). The answers to this question only encourage meandering discussions not suitable here (it also seems unlikely that you will ever accept an answer as its going to be hard to judge if this question has been appropriately answered). Please consider moving this question to another forum." CreationDate="2015-06-01T21:19:50.170" UserId="847" />
  <row Id="6316" PostId="5973" Score="0" Text="What is meant by &quot;unknown parameters&quot;? Is your question about how to choose the final set of features using which you would carry out &quot;ordinary least squares regression&quot;? Consider editing your question description with this information as well as updating the title of the question." CreationDate="2015-06-01T21:25:37.343" UserId="847" />
  <row Id="6317" PostId="5981" Score="0" Text="In the set up of your classification problem does the binary feature (the response/ label) that you are trying to predict has categories that roughly mean: &quot;feasible&quot; and &quot;not-feasible&quot;?" CreationDate="2015-06-01T21:49:39.333" UserId="847" />
  <row Id="6318" PostId="5982" Score="1" Text="In conventional machine learning, we have some data that comes from a particular probability distribution. Then we learn some kind of model on that data, hoping that the model will generalize to examples not seen during training. This will only work if these unseen samples come from the same probability distribution, so we assume this is the case. In transfer learning, we don't make that assumption. [Here's](http://www1.i2r.a-star.edu.sg/~jspan/publications/TLsurvey_0822.pdf) a survey paper on the field." CreationDate="2015-06-01T23:24:50.183" UserId="9483" />
  <row Id="6319" PostId="5933" Score="0" Text="@Nitesh You are right about accepting any of the answers as it will be difficult to judge. But as I stated in the question itself, I am not looking for a revenue generating product. I am just looking for what possible stuff can be done with a typical dataset like this. You are correct when you say this is open-ended but then I am not expecting people to explain all of their approach. If the answer mentions a relevant keyword or a pointer towards the answer, I would be upvoting it and would consider it a valid answer. I would move it to another forum if you feel the points are not valid :)" CreationDate="2015-06-02T04:10:01.683" UserId="75" />
  <row Id="6320" PostId="5947" Score="0" Text="You are welcome, Please provide more feedback. Did this way improve the result in your case? What did you get?" CreationDate="2015-06-02T04:15:26.867" UserId="9085" />
  <row Id="6321" PostId="5985" Score="1" Text="There is no &quot;best&quot; here because you haven't clearly defined your criteria, and only you can do that. Why isn't it simply the total number of SEP_11 computers? If a site has 100 SEP_11 why does it matter if it has 0 SEP_12 or 1000 SEP_12? Do you want the ratio of SEP_11/SEP_12? I'm not even sure this is Data Science..." CreationDate="2015-06-02T16:25:10.520" UserId="471" />
  <row Id="6322" PostId="3719" Score="0" Text="Since this was couple of months back, I assume you made some progress. Why not add your own answer (here or elsewhere?)" CreationDate="2015-06-02T16:59:31.707" UserId="9951" />
  <row Id="6323" PostId="5985" Score="0" Text="@Spacedman I wasn't sure if this was the best forum to post, I was looking more for Data Analysis forum .... I think ratio might be a solution ... thinking ......" CreationDate="2015-06-02T17:12:00.467" UserId="9947" />
  <row Id="6324" PostId="5933" Score="0" Text="Unfortunately, this question-answer format does not encourage idea generating discussions (for example, the possible things that can be done with this dataset) even though I personally like to indulge in such discussions :(" CreationDate="2015-06-02T18:42:07.767" UserId="847" />
  <row Id="6325" PostId="5881" Score="0" Text="@Nitesh I made some editing." CreationDate="2015-06-02T21:47:05.827" UserId="9799" />
  <row Id="6326" PostId="5973" Score="0" Text="Sorry- this should be a comment. With prediction we don't need worry about collinearity. Ideally, you should cut out the predictors that measure the same thing. For prediction I would say OLS will do the trick if your response is continuous. As far as what predictors to include in the model, well, you can put them all if you want but you run the risk of overfitting. But generally with prediction, the more predictors the better the prediction." CreationDate="2015-06-01T14:51:56.563" UserId="9698" />
  <row Id="6328" PostId="5981" Score="0" Text="Yes,  indeed, those are exactly the categories I want to &quot;predict&quot;.  I've been furthering my research (will edit OP later): I want to extract rules on the feature space (&quot;minimum&quot; complexity linear combination of features values is enough). Is there something like a tree of linear combinations ?" CreationDate="2015-06-02T23:00:26.640" UserId="9928" />
  <row Id="6329" PostId="1165" Score="0" Text="Hello @EmilyCrutcher. I apologize for the delay, it has been ages since I didn't check this post. I am very excited to start working with you in this project !" CreationDate="2015-06-03T02:47:00.543" UserId="3433" />
  <row Id="6330" PostId="3723" Score="0" Text="@AleksandrBlekh No one up voted my answer :( But your answer is very satisfying. I am going to up vote it!" CreationDate="2015-06-03T03:05:35.083" UserId="847" />
  <row Id="6331" PostId="3723" Score="0" Text="@Nitesh: Thank you for kind words and upvoting. Don't worry too much about your particular answer - it is not bad, just rather limited in scope. Nevertheless, I'll upvote it for the content and to cheer you up a bit :-)." CreationDate="2015-06-03T03:45:13.970" UserId="2452" />
  <row Id="6332" PostId="5990" Score="0" Text="Excellent first question! Can you add some more information about what is your goal to carry out this specific feature transformation? Do you intend to use this transformed feature as an input to a supervised learning problem? If so, please consider adding that information as it may help others answer this question better." CreationDate="2015-06-03T06:52:48.670" UserId="847" />
  <row Id="6333" PostId="5990" Score="1" Text="@Nitesh, Please see update" CreationDate="2015-06-03T06:57:42.743" UserId="8338" />
  <row Id="6334" PostId="5988" Score="0" Text="`R`? This is new to me, seems interesting!" CreationDate="2015-06-03T12:26:03.407" UserId="9947" />
  <row Id="6335" PostId="5933" Score="0" Text="Hmm I get your point. Should I delete it or let it be closed?" CreationDate="2015-06-03T14:01:06.490" UserId="75" />
  <row Id="6336" PostId="5988" Score="0" Text="@SohniMahiwal If you like the answer, please consider up voting/ accepting it. If you'd like, I could send you the script to produce this image." CreationDate="2015-06-03T15:30:09.600" UserId="847" />
  <row Id="6337" PostId="5991" Score="1" Text="I believe, asking here http://opendata.stackexchange.com/ is better. Users on Open Data SE are more familiar with open data sets." CreationDate="2015-06-03T15:37:33.983" UserId="201" />
  <row Id="6338" PostId="5988" Score="0" Text="I need 15 reputation or more to vote up" CreationDate="2015-06-03T15:51:04.437" UserId="9947" />
  <row Id="6339" PostId="5947" Score="0" Text="There are still the same words shown in different topics. Each word only presents once in each document, but may present in different documents. I updated a new graph of the frequency of the words. The few words with the highest frequency each take only 14% of the documents.  I also wonder how to deal with words I removed. Could they be considered as features or too common to exist?" CreationDate="2015-06-03T18:45:27.710" UserId="9724" />
  <row Id="6340" PostId="5990" Score="0" Text="You can find answers here: http://datascience.stackexchange.com/questions/4967/quasi-categorical-variables-any-ideas" CreationDate="2015-06-04T02:13:34.443" UserId="609" />
  <row Id="6341" PostId="5991" Score="0" Text="Thanks a lot @AnastasiosVentouris. Moving my question there." CreationDate="2015-06-04T08:32:14.090" UserId="9958" />
  <row Id="6342" PostId="5984" Score="0" Text="For regression model, if node impurity is variance, what will be node purity?" CreationDate="2015-06-04T16:36:42.223" UserId="9941" />
  <row Id="6343" PostId="6005" Score="0" Text="Welcome to the community :) As there are &quot;r&quot; and &quot;data visualization&quot; tags this question is not irrelevant but maybe stackoverflow is a better place to get a helpful answer" CreationDate="2015-06-04T19:38:04.417" UserId="8878" />
  <row Id="6344" PostId="6007" Score="0" Text="What is cls? What does the acronym and values represent?" CreationDate="2015-06-04T19:55:28.373" UserId="9947" />
  <row Id="6345" PostId="6005" Score="0" Text="@kasramsh: Okay, Thanks!" CreationDate="2015-06-04T20:14:12.180" UserId="9982" />
  <row Id="6346" PostId="6005" Score="0" Text="I found one solution to the problem here: &lt;br/&gt; http://stackoverflow.com/questions/19731187/r-plotting-multiple-groups-of-data-in-a-single-3d-plot &lt;br/&gt; which requires creating a data frame before doing this. I was wondering if there is a simpler, quicker way of doing it." CreationDate="2015-06-04T20:25:06.987" UserId="9982" />
  <row Id="6347" PostId="6008" Score="0" Text="@kylethecreator, yes that is what I exactly need to do - build a classifier based on the data that I have so that it can categorize questions.  [Here](https://drive.google.com/file/d/0B8NiXBOrlRu5cUhscUwxa0tkeHc/view?usp=sharing) is data set I had created manually by categorizing 1000 questions in one of the 10 categories. Please suggest the future course of action." CreationDate="2015-06-05T04:12:48.463" UserId="9966" />
  <row Id="6349" PostId="6014" Score="0" Text="Sorry: ***STANFORD NLP" CreationDate="2015-06-05T05:20:36.170" UserId="9988" />
  <row Id="6351" PostId="6019" Score="0" Text="prior? likelihood?" CreationDate="2015-06-05T12:01:15.473" UserId="9123" />
  <row Id="6352" PostId="5986" Score="0" Text="Sorry for this. My mistake. I expected it to be a comment. But after all, I decided to describe my test with LDA." CreationDate="2015-06-05T12:49:49.887" UserId="9314" />
  <row Id="6353" PostId="6023" Score="0" Text="Neural Networks was one of my options, since I have made a few other projects with them. However, I wanted to extract a formula which can be sent to a third party to use it for predictions based on the input. Since I find the prediction model I will not be able to run it every time to find the probabilities for A, B and C." CreationDate="2015-06-05T15:29:11.960" UserId="201" />
  <row Id="6354" PostId="6023" Score="1" Text="I think at the best case you can provide a third party by probabilities but not a model. If internal functions are random what would a model mean? Maybe better to run many experiment and try to fit a distribution to your empirical distribution (using maximum likelihood for instance). Then provide the third party with that distributions." CreationDate="2015-06-05T15:41:26.837" UserId="8878" />
  <row Id="6355" PostId="6007" Score="0" Text="`cls` is the column with the [climate zones](http://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification)." CreationDate="2015-06-05T16:08:54.853" UserId="178" />
  <row Id="6356" PostId="6007" Score="0" Text="Now I understand, thanks!" CreationDate="2015-06-05T16:20:22.497" UserId="9947" />
  <row Id="6357" PostId="6019" Score="1" Text="Is this not more of a discussion question than a question with an actual answer?" CreationDate="2015-06-05T19:52:57.123" UserId="8152" />
  <row Id="6358" PostId="6019" Score="0" Text="One cannot do science without a clear definition of terms, so I would be very surprised if there is no official, clear definition of this kind of knowledge" CreationDate="2015-06-05T23:35:55.840" UserId="9960" />
  <row Id="6359" PostId="5986" Score="0" Text="Thank you for your detailed reply. I am going to try TF-IDF then!  Look forward to your update." CreationDate="2015-06-05T23:46:22.013" UserId="9724" />
  <row Id="6361" PostId="5730" Score="0" Text="Thanks @hostjc, those are good ideas. I think you're right that I need to think in terms of features that can be accessed by the clusterer, time to respond is a perfect example of a non-obvious feature that would have a lot of very interesting applications." CreationDate="2015-06-06T09:04:08.643" UserId="9534" />
  <row Id="6362" PostId="6014" Score="0" Text="Hmm, thanks! I've been working with Epic, a tool written in Scala from http://www.scalanlp.org...." CreationDate="2015-06-06T09:06:03.843" UserId="9534" />
  <row Id="6363" PostId="6027" Score="0" Text="Sounds like you have some code, data and visualizations already. Can you share some of them? It would be easier to help then." CreationDate="2015-06-06T11:54:30.327" UserId="7720" />
  <row Id="6364" PostId="6017" Score="0" Text="Is it valid to make such assumptions? From my training set, the distribution is definitely not equal as in your assumption (btw this variable is a response from survey, so a lot of subjectivity would come into the answers). So does it mean I should use the distribution of the training set to create a weighted distance measure?" CreationDate="2015-06-06T15:38:52.847" UserId="1133" />
  <row Id="6365" PostId="6027" Score="2" Text="How about [Dimensionality Reduction with Spherical Constraints](http://www.cs.utah.edu/~suresh/6160/spring09/projects/final/arvind.pdf) or [Spherical Laplacian Information Maps for dimensionality reduction](http://web.eecs.umich.edu/~hero/Preprints/carter_ssp09.pdf)?" CreationDate="2015-06-07T01:09:00.290" UserId="381" />
  <row Id="6367" PostId="3719" Score="0" Text="Hi @Jayan as per our use-case we were initially thinking of storing entire data on Neo4j, but finally we choose MongoDB as the central database and stick with Neo4j  for analyzing relationships only (not as central DB). And also we are exploring with Spark and Graphlab" CreationDate="2015-06-07T11:39:32.940" UserId="5091" />
  <row Id="6369" PostId="5865" Score="0" Text="It is not just that squaring the errors has the effect of treating positive and negative errors equally, because abs( ) would have the same effect. ***Squaring the errors puts more weight on large errors***, compared to linear or log functions. Therefore, minimizing sum-of-squares error serves this decision criteria: &quot;Favor the model that has the least aggregate large errors.&quot;" CreationDate="2015-06-07T20:13:51.580" UserId="609" />
  <row Id="6370" PostId="5882" Score="0" Text="I don't believe that *computational expense* is the main reason why square root is not taken over sum-of-squared errors.  (I don't have any evidence one way or the other.)  After all, you only need to perform the square root operation once for all error values.  Instead, I believe that sum-of-squared errors serves a different decision criterion. ***Squaring the errors puts more weight on large errors***, compared to linear or log functions. Therefore, minimizing sum-of-squares error serves this decision criterion: &quot;Favor the model that has the *least aggregate large errors*.&quot;" CreationDate="2015-06-07T20:28:45.447" UserId="609" />
  <row Id="6371" PostId="6036" Score="1" Text="That looks like MS Excel -- one of the predefined line graphs. No fancy software or programming required." CreationDate="2015-06-07T20:36:20.810" UserId="609" />
  <row Id="6372" PostId="6036" Score="0" Text="Thank you @MrMeritology" CreationDate="2015-06-07T20:39:39.340" UserId="10029" />
  <row Id="6373" PostId="6029" Score="0" Text="In most recent benchmarks, PostgreSQL totally owned MongoDB..." CreationDate="2015-06-07T21:38:16.317" UserId="924" />
  <row Id="6374" PostId="6027" Score="0" Text="@Emre The first one looks interesting, I think I will just code up t-sne with the norm constraint and geodesic distance.." CreationDate="2015-06-07T22:44:48.530" UserId="10001" />
  <row Id="6375" PostId="5865" Score="0" Text="Yes, using squared errors puts greater emphasis on large errors but `abs()` has other issues, like the fact that its derivative is discontinuous at 0. And regarding your stated decision criterion, wouldn't that be even better met by $E^4$ or $E^6$? I don't have a citation but I suspect the real reason squared errors are used is related to the quote in the original question: errors *tend* to be Gaussian and minimizing the squared errors provides the maximum likelihood estimate in that case. They also have the benefit of being easily differentiable, which makes them simple to use in practice." CreationDate="2015-06-07T23:14:20.793" UserId="964" />
  <row Id="6376" PostId="5865" Score="0" Text="Good points.  Yes, abs( ) is discontinuous at 0, but I'm not sure what difference that makes in empirical analysis.  Yes, higher ordered error functions would weight large errors even more, and would support the decision criterion mentioned.  If minimizing squared errors leads to max. likelihood estimate (assuming Gaussian), then I agree with that line of reasoning." CreationDate="2015-06-08T01:28:45.933" UserId="609" />
  <row Id="6377" PostId="6015" Score="0" Text="Check [this][1] out. The explanation provided by the guy is very lucid. &#xA;&#xA;&#xA;  [1]: http://stats.stackexchange.com/questions/59124/random-forest-assumptions" CreationDate="2015-06-08T04:10:10.900" UserId="9966" />
  <row Id="6378" PostId="6043" Score="0" Text="please can we talk private ? i have some questions about what you said, can i give you my email please?" CreationDate="2015-06-08T14:34:12.307" UserId="10036" />
  <row Id="6379" PostId="6043" Score="0" Text="to be clear, I do have other data sets like a transation and items. but i thought that is for the next question, the first question is to analysi the client profile," CreationDate="2015-06-08T14:35:03.310" UserId="10036" />
  <row Id="6381" PostId="6043" Score="0" Text="if you have more questions please comment here and I'll modify my answer so that others also can use it. This is the philosophy behind this website :)" CreationDate="2015-06-08T15:20:10.183" UserId="8878" />
  <row Id="6382" PostId="6036" Score="3" Text="How much like that do you want it? Lines, points, grid lines, axes, labels, legend? There's so much in that plot. You are asking a very vague question." CreationDate="2015-06-08T16:37:51.900" UserId="471" />
  <row Id="6383" PostId="6043" Score="0" Text="Okay, I have updated my question containing a description of the datasets. Could you help me please? Thanks in advance" CreationDate="2015-06-08T16:52:08.327" UserId="10036" />
  <row Id="6384" PostId="6014" Score="0" Text="What language do you use?" CreationDate="2015-06-08T19:44:58.630" UserId="9988" />
  <row Id="6385" PostId="6014" Score="0" Text="I'm using Scala, but I didn't want to make that a predicate on the high level technique that would be good to pursue on a project like this." CreationDate="2015-06-09T06:01:31.427" UserId="9534" />
  <row Id="6386" PostId="6015" Score="0" Text="Thanks, I have got my answers. and seems like my question is a repeated one." CreationDate="2015-06-09T06:29:26.423" UserId="9941" />
  <row Id="6387" PostId="6017" Score="0" Text="Of course. If it's really skewed, I would pick different margins, just make sure they are empirically grounded. Mine were example margins :)" CreationDate="2015-06-09T09:14:21.913" UserId="8152" />
  <row Id="6388" PostId="5882" Score="0" Text="if we are only looking to minimize large aggregate errors, a cubic error is better than a squared error is it not. I know I read it somewhere mentioning the reason as 'optimization technique' but can't recall where" CreationDate="2015-06-09T13:47:44.827" UserId="9801" />
  <row Id="6390" PostId="6051" Score="0" Text="Define AI first." CreationDate="2015-06-09T14:51:48.817" UserId="5316" />
  <row Id="6392" PostId="2651" Score="2" Text="Why is this upvoted. It doesnt show any effort whatsoever and its a duplicate of a dupe" CreationDate="2015-06-09T14:56:12.073" UserId="5316" />
  <row Id="6393" PostId="6056" Score="0" Text="No. the point is that there are terms like &quot;str1 str2&quot; and &quot;str1str2&quot;. When I want to draw histogram of course I need to count the occurrences of each term. So I need to filter the data so that all ambiguous versions of a specific term become the same. Then I start counting." CreationDate="2015-06-09T16:31:59.813" UserId="8878" />
  <row Id="6394" PostId="6054" Score="1" Text="An answer to a similar question (for neural networks) is [here](http://stackoverflow.com/a/10357067/1361822)." CreationDate="2015-06-09T16:39:24.793" UserId="964" />
  <row Id="6395" PostId="6048" Score="0" Text="Can you add more details like the number of rows, number of columns (also how many categorical/ continuous)?" CreationDate="2015-06-09T17:21:02.783" UserId="847" />
  <row Id="6396" PostId="6059" Score="0" Text="Exactly @Victor." CreationDate="2015-06-09T17:22:28.520" UserId="9966" />
  <row Id="6397" PostId="5986" Score="0" Text="I tried TF-IDF and it did work better. Thanks for sharing again. but do u know how to deal with the terms that you removed based on TF-IDF? Are those not the important information for your recommendation system?" CreationDate="2015-06-09T20:46:21.467" UserId="9724" />
  <row Id="6398" PostId="6050" Score="0" Text="Sure: _hashing_ comes to mind." CreationDate="2015-06-10T03:22:54.740" UserId="2452" />
  <row Id="6399" PostId="6050" Score="0" Text="Could you please explain a bit more that how hashing would help me here? Do we make a hash of k-vector?" CreationDate="2015-06-10T07:00:43.013" UserId="8338" />
  <row Id="6400" PostId="6060" Score="0" Text="So we will still use k-vector but this time k is the number of groups and hence less than number of unique users?" CreationDate="2015-06-10T07:01:43.967" UserId="8338" />
  <row Id="6401" PostId="6050" Score="0" Text="What I meant is that [hashing](http://en.wikipedia.org/wiki/Hash_function) allows to _compress_ data of arbitrary size to a smaller data set. However, the side effect of that is the presence of _collisions_. I would research potential options of dealing with collisions, in general and in particular context of your data types, so that you can see, if this approach is _feasible_ (I'm not stating that). Hope this helps." CreationDate="2015-06-10T07:25:04.587" UserId="2452" />
  <row Id="6402" PostId="6050" Score="0" Text="Hmm... thanks I would think about it!" CreationDate="2015-06-10T07:36:01.260" UserId="8338" />
  <row Id="6403" PostId="6064" Score="2" Text="Drug? Is milk a drug where you live?" CreationDate="2015-06-10T09:53:39.213" UserId="471" />
  <row Id="6404" PostId="6060" Score="1" Text="This is a little redundant though.. the information you add from this clustering is just the info contained in the other features, which are already inputs." CreationDate="2015-06-10T11:42:37.627" UserId="21" />
  <row Id="6405" PostId="6050" Score="1" Text="I don't think you literally mean more compact, like, taking fewer bytes right? but instead taking on fewer than 30K distinct values? hashing won't help unless there are lots of collisions then. Although this is a well-known technique for very high-dimensional data I'm not sure it's appropriate here." CreationDate="2015-06-10T11:44:11.767" UserId="21" />
  <row Id="6406" PostId="6050" Score="0" Text="Also are you sure you want to predict based on user ID? you normally predict based on qualities of a person, not their actual identity." CreationDate="2015-06-10T11:44:31.247" UserId="21" />
  <row Id="6408" PostId="6050" Score="0" Text="@SeanOwen You are right, but my problem is little different. There are huge number of operations are done on the users. I have to predict the future performance of these actions per user basis." CreationDate="2015-06-10T12:56:13.683" UserId="8338" />
  <row Id="6409" PostId="6050" Score="0" Text="I think I got your point. These performance totally depend on userIds but in real I could replace these ids with some properties which contribute to the performance impact and make the model agnostic to the user. But the problem here is that sometimes we have limited domain knowledge, so we club all this feature under a group and give them id(say user id)." CreationDate="2015-06-10T13:17:18.703" UserId="8338" />
  <row Id="6410" PostId="6008" Score="0" Text="@kylethecreator: Are you still on this?" CreationDate="2015-06-10T14:58:47.137" UserId="9966" />
  <row Id="6411" PostId="6008" Score="1" Text="@untitledprogrammer: you could definitely apply the methods I described in my answer toward this dataset. Since you have full-text questions in each cell you could tokenize each of these and use them as unigram features for classifiers. Tokenization is fairly straightforward, you could just split on whitespace, or use a more specialized library depending on your programming language. I'm biased toward Python, so the NLTK library is a natural choice for many of these ideas." CreationDate="2015-06-10T15:36:01.220" UserId="4897" />
  <row Id="6412" PostId="6008" Score="0" Text="Thanks for the clarification." CreationDate="2015-06-10T15:43:22.440" UserId="9966" />
  <row Id="6413" PostId="6064" Score="0" Text="Sorry for typo. My real data has Drug name Brand 1, Drug name-Brand 2 etc (Example: Tylenol is brand 1 and Ibrofen is brand 2)...I corrected original post." CreationDate="2015-06-10T16:24:50.287" UserId="9663" />
  <row Id="6414" PostId="6061" Score="0" Text="+1 Will! I'll look at it deeper but a quick look told me it probably doesnt solve my problem. I mean what if &quot;big data&quot; does not happen too often? But thanks after all. Probably a good starting point!" CreationDate="2015-06-10T16:50:05.627" UserId="8878" />
  <row Id="6415" PostId="6067" Score="0" Text="I hope this helps you out with your text. I have worked with text data and I know the pain. Numeric and or factual data is much better to be mined." CreationDate="2015-06-10T17:02:35.200" UserId="9966" />
  <row Id="6416" PostId="6048" Score="0" Text="Hi @Nitesh, I have 32 input variables + 1 target variable. Records are close to 2.5 lakh for training data and say around 1 lakh testing data. Testing data is out of time data." CreationDate="2015-06-10T18:36:26.770" UserId="9793" />
  <row Id="6417" PostId="6058" Score="0" Text="Thanks for the response" CreationDate="2015-06-10T18:39:50.473" UserId="9793" />
  <row Id="6418" PostId="6068" Score="0" Text="Don't be intimidated by the position of the authors of the papers.  The best resources are usually written by people whose goal is to make the topic understandable. The most important thing is to keep at it until you get it and reach out in forums like this when you need help." CreationDate="2015-06-10T18:42:22.807" UserId="178" />
  <row Id="6419" PostId="6059" Score="0" Text="@Victor Thanks a lot for a very detailed explaination." CreationDate="2015-06-10T18:42:52.540" UserId="9793" />
  <row Id="6420" PostId="6054" Score="0" Text="@bogatron - I agree with you completely. But my ones just a **SVM specific** answer." CreationDate="2015-06-10T19:43:54.307" UserId="9966" />
  <row Id="6421" PostId="6054" Score="1" Text="Except it isn't. Your answer is correct but there is nothing about it that is specific to SVMs (nor should there be). $w^{T}x=b$ is simply a vector equation that defines a hyperplane." CreationDate="2015-06-10T22:01:22.693" UserId="964" />
  <row Id="6422" PostId="6061" Score="1" Text="It might be worth trying. It should detect at least the most common examples, and the others may not make a big difference. Also, are all of the examples you are looking for misspellings, like &quot;bigdata&quot;? In that case, you could compare your documents to a list of known words, and then compare them to the list of pairs of adjacent words in the text." CreationDate="2015-06-11T01:44:15.173" UserId="8275" />
  <row Id="6423" PostId="6073" Score="0" Text="Thanks. I belatedly found out also that the answer to the portion &quot;but I have no clue how to view the rpart2 tree.&quot; is to view the `finalModel` element in the output of `rpart2` trained with `caret`." CreationDate="2015-06-11T07:21:15.543" UserId="1133" />
  <row Id="6425" PostId="6074" Score="0" Text="Even with random they will be qualitative. Numbers would be just labels...!" CreationDate="2015-06-11T08:10:42.060" UserId="8338" />
  <row Id="6426" PostId="6067" Score="0" Text="Thanks alot. It helped. :)" CreationDate="2015-06-11T10:10:33.153" UserId="6514" />
  <row Id="6427" PostId="6067" Score="0" Text="Good to know that!" CreationDate="2015-06-11T13:09:20.147" UserId="9966" />
  <row Id="6428" PostId="6061" Score="0" Text="I actually have a huge list of human generated tags and I want to make a ground-truth for them. Some people enter &quot;big data&quot; some enter &quot;bigdata&quot; and currently in my ground-truth they are two different tags but they should be identical." CreationDate="2015-06-11T15:03:25.410" UserId="8878" />
  <row Id="6429" PostId="6061" Score="1" Text="So these are not actual sentences, just phrases without context? Another option you could try would be to use an edit distance/Levenshtein distance to group similar tags: http://en.wikipedia.org/wiki/Levenshtein_distance" CreationDate="2015-06-11T17:01:49.793" UserId="8275" />
  <row Id="6430" PostId="6083" Score="0" Text="Good but How can I make software column?" CreationDate="2015-06-11T17:11:43.873" UserId="3151" />
  <row Id="6432" PostId="6083" Score="0" Text="That is something you are going to have to research yourself. I am not sure what your data set looks like-- the melt function may be of use to you." CreationDate="2015-06-11T17:27:11.387" UserId="9698" />
  <row Id="6433" PostId="6061" Score="0" Text="I was thinking about Levenshtein distance at the beginning but I thought maybe someone here has a better solution. Thanks for answers anyway and I'll keep this question open for a few more days to see if someone has a better idea :)" CreationDate="2015-06-11T17:44:50.090" UserId="8878" />
  <row Id="6434" PostId="6074" Score="0" Text="Obviously they are unordered random numbers and are still categorical data; however, decision trees don't have a linearity presumption. This means that the splits will sort out the different effects of each category. Imagine only one category is different from the rest. A decision tree will take only two splits to identify this category. If there are 30,000 categories, very likely many of them will have similar effects. A decision tree takes advantage of this when partitioning the data." CreationDate="2015-06-11T20:14:00.517" UserId="10086" />
  <row Id="6435" PostId="6084" Score="0" Text="Can you give an example of a dataset that you are trying to visualize? Currently, your question is vague. Providing an example dataset and a corresponding plot you would like to see would help. Also, providing external links (specifically from transient websites like twitter) is discouraged, so try describing it as best as you can in the question itself." CreationDate="2015-06-11T21:05:29.050" UserId="847" />
  <row Id="6437" PostId="5351" Score="1" Text="I found this plugin for R in IntelliJ: https://plugins.jetbrains.com/plugin/6632?pr= ." CreationDate="2015-06-11T23:26:21.860" UserId="5279" />
  <row Id="6438" PostId="6083" Score="0" Text="@LaurenGoodwin My answer to this question uses gridExtra to plot a barplot and boxplot side by side to produce the visualization requested." CreationDate="2015-06-11T23:52:13.277" UserId="847" />
  <row Id="6439" PostId="6075" Score="0" Text="Can you add some detail around what X is? Also, what data is available to you? For example, user attributes, their past buying behaviour etc.?" CreationDate="2015-06-12T00:16:28.923" UserId="847" />
  <row Id="6440" PostId="5351" Score="0" Text="@Anton: Thanks for the information. Either that plug-in info wasn't published as of time of my post, or (more likely) I have simply missed it. However, in general, I would definitely prefer a manufacturer's embedded support, especially, considering the prominence of R in academia, science and industry." CreationDate="2015-06-12T00:48:37.123" UserId="2452" />
  <row Id="6444" PostId="6090" Score="0" Text="My description is not very clear, but you had already got my mind. Really appreciate !" CreationDate="2015-06-12T06:43:46.130" UserId="9728" />
  <row Id="6446" PostId="6070" Score="0" Text="Check [my answer](http://stats.stackexchange.com/a/131284/31372) on _DTW clustering_ of time series." CreationDate="2015-06-12T07:59:24.580" UserId="2452" />
  <row Id="6447" PostId="6070" Score="0" Text="Basically, I have a time series of time series. Let's assume that I use at time $t$ a DTW (but I would rather use $\phi = \arccos \langle p,q \rangle$), how to extend it to the whole time series? This is really my point." CreationDate="2015-06-12T08:36:28.710" UserId="7966" />
  <row Id="6448" PostId="6017" Score="0" Text="what you choose as the margin may even depend on what errors matter more and which matter less as per your requirements. eg say we have an item X for which if label is B or BV, we would be discarding it else keeping it. So in this case the distinction between B and BV is less important then distinction between G and B. So your mismatch score between B and BV should be less than that between B and G" CreationDate="2015-06-12T08:39:12.560" UserId="75" />
  <row Id="6449" PostId="6070" Score="0" Text="You're welcome. My advice does not imply that I think that TDW is universal solution. I just thought that papers, referenced in my linked answer, _potentially_ might contain some _ideas_, useful to your case. I don't have an answer for your &quot;time series of time series&quot; case. As for analyzing distortions, you could consider applying time series _anomaly detection and analysis_ approaches." CreationDate="2015-06-12T09:05:50.133" UserId="2452" />
  <row Id="6450" PostId="6094" Score="0" Text="Thanks for the help. This is some kind of online _online learning_ as I expected, but I am really not clear about the implementation part. Could you throw in some hints for the same. Say I have a model and I have 5 parameters, and corresponding 5 weights, for example the model be f = (0.1*a + 0.8*b + 0.4*c + 0.2*d + 0.5*e) and I use this to recommend initially. Now how do I update the model using the implicit feedback?" CreationDate="2015-06-12T12:17:27.403" UserId="10127" />
  <row Id="6451" PostId="6094" Score="0" Text="If you are planning to apply online learning to update the weights, then you should take a look to the Stochastic Gradient Descent entry in Wikipedia. It includes a very simple example, similar to the one in your comment." CreationDate="2015-06-12T15:49:35.953" UserId="2576" />
  <row Id="6452" PostId="6101" Score="1" Text="I am going to edit the title to be more descriptive. Good question, though." CreationDate="2015-06-12T19:48:17.940" UserId="3466" />
  <row Id="6453" PostId="6100" Score="0" Text="Few clarifications: Are you having problems with the stems that are generated, just asking if their is a lemmatisation module that handles Hungarian, or are you asking which algorithm you should use to classify and build a wordcloud after stem creation? Or am I completely missing your question?" CreationDate="2015-06-12T21:25:01.447" UserId="10019" />
  <row Id="6455" PostId="6100" Score="0" Text="Stems are good to identify different forms of the same word.But stems are not actual words usually just word chunks(at least in Hungarian).  I want to somehow translate stems back to real words that I can use later in my wordcloud. Lemmatisation would be nice,it would handle the problem, but I did not find a Hungarian lemmatisation module (and because of the complexity of the Hungarian language I think it is very difficult to create one). So I want a solution to replace the stems with one of the real word occurrences in the text. Or any other workflow is welcome that would solve my problem." CreationDate="2015-06-12T22:56:32.640" UserId="5211" />
  <row Id="6456" PostId="2464" Score="0" Text="You don't have to represent each word as a vector. You get the new representation for the entire *document* by applying the LDA transformation you learned *to the corpus*. For an example with LSI, see this link: http://radimrehurek.com/gensim/tut2.html The key part is where they apply the learned LSI transformation to the entire corpus with lsi[doc_bow]" CreationDate="2015-06-13T02:29:44.743" UserId="8275" />
  <row Id="6457" PostId="6070" Score="0" Text="Have you tried mutual information?" CreationDate="2015-06-12T06:05:08.283" UserId="10094" />
  <row Id="6458" PostId="6070" Score="0" Text="For measuring dependency between variables, I prefer using copulae, [though mutual information and copula are very much the same]{http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077935}! Yet, in my case, dependency is not the only information I care about / in this kind of time series. In fact, I wish I could obtain [a result similar to this one]{http://arxiv.org/pdf/1506.00976v1.pdf}." CreationDate="2015-06-12T06:33:56.677" UserId="7966" />
  <row Id="6459" PostId="6098" Score="0" Text="You haven't specified what you are doing or what the problem is." CreationDate="2015-06-13T07:21:18.423" UserId="21" />
  <row Id="6460" PostId="6092" Score="0" Text="What is your problem? do you want to validate your predicted ratings? then you need rating data." CreationDate="2015-06-13T07:22:17.023" UserId="21" />
  <row Id="6462" PostId="6059" Score="0" Text="+1 for this detailed answer. Arun you might want to accept one of the answers." CreationDate="2015-06-13T08:35:03.013" UserId="9123" />
  <row Id="6463" PostId="6092" Score="0" Text="Validation is one problem. Here I am more concerned about building the model itself. I am not very comfortable with unsupervised models." CreationDate="2015-06-13T13:29:26.020" UserId="10127" />
  <row Id="6465" PostId="2432" Score="0" Text="MapReduce is not really a technology for joining. Do you mean Hive? Impala is the closest analog to Redshift and things like Teradata" CreationDate="2015-06-14T09:39:43.943" UserId="21" />
  <row Id="6466" PostId="2432" Score="0" Text="consider [monetdb](http://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html)" CreationDate="2015-06-14T12:44:33.113" UserId="9924" />
  <row Id="6468" PostId="6113" Score="0" Text="The quality of question should be improved to get a proper answer e.g. do all sequences always change at the same point (like how you illustrated in the example)?" CreationDate="2015-06-14T17:42:03.373" UserId="8878" />
  <row Id="6469" PostId="6113" Score="0" Text="My real data is more complicated. It is list of 9-dimension vectors. I will add picture to the main section." CreationDate="2015-06-14T21:30:31.857" UserId="9637" />
  <row Id="6470" PostId="6117" Score="0" Text="Thanks for detailed response. As you can see above, I can not use thresholds for my real data sequence, I think, it is too complicated for it. I am trying to modify k-means algorithm, it will consider the condition of sequence (element can only belong to one of two neighboring clusters). I hope, that I do not reinvent the wheel. :)" CreationDate="2015-06-14T21:42:53.727" UserId="9637" />
  <row Id="6471" PostId="6117" Score="0" Text="I think your data is not so noisy (i.e. complicated) and you can go for threshold stuff. the point is that you have an impression of the data so u can use kinda supervised algorithm i.e. trying to learn thresholds (and hope it generalizes well!) . I also update my answer for a nice solution :)" CreationDate="2015-06-14T22:13:58.960" UserId="8878" />
  <row Id="6472" PostId="6084" Score="0" Text="Excel is the best (visually most beautiful one)! you can find implementations in python or other languages but they are not as great as excel. I tried a month ago!" CreationDate="2015-06-15T01:43:38.460" UserId="8878" />
  <row Id="6473" PostId="6118" Score="0" Text="There's another old question about this same error with lots of other possibilities, in case the Encoding trick doesn't help: http://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs" CreationDate="2015-06-15T03:49:48.297" UserId="8275" />
  <row Id="6474" PostId="6116" Score="1" Text="Nice question! But should be a community wiki I suppose" CreationDate="2015-06-15T12:00:39.540" UserId="816" />
  <row Id="6475" PostId="6124" Score="0" Text="In the scikit learn tutorial, to implement this method, they have used the two np.arrays with one having a 2d list and one with 1d list. I tried to replicate the pattern without using numpy and got the error mentioned above. Don't know how else i should approach the implementation" CreationDate="2015-06-15T17:51:48.220" UserId="5043" />
  <row Id="6476" PostId="6124" Score="1" Text="When it says, &quot;Unknown label type&quot; it looks like your &quot;y&quot; is actually a numeric array, not an array of labels. If you are trying to predict &quot;ph&quot; from &quot;fixed acid&quot;, you should use a Regressor, not a Classifier." CreationDate="2015-06-15T18:13:08.803" UserId="8275" />
  <row Id="6477" PostId="6124" Score="0" Text="That's what I thought. I am not 100% clear about what the sgdclassifier example at scikit learn website means. They have 2 lists and using the model and fit to get some output. Could you please explain me what can i do with this data set to work with the sgdclassifier ?" CreationDate="2015-06-15T18:27:19.853" UserId="5043" />
  <row Id="6478" PostId="6124" Score="1" Text="Well, if you really want to use sgd*classifier*, you could try using &quot;quality&quot; as the &quot;y&quot; variable. But that might not be too appropriate, because I assume that quality is really an ordered variable. Why don't you try [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)?" CreationDate="2015-06-15T18:30:16.023" UserId="8275" />
  <row Id="6479" PostId="6124" Score="0" Text="I have to use both SGDClassifier and SGDRegressor, with this data set, for my project. Not sure what I can do with this dataset to implement SGDClassifier model." CreationDate="2015-06-15T18:32:25.413" UserId="5043" />
  <row Id="6480" PostId="6124" Score="0" Text="Could you tell me a good place to find a tutorial that explains these models properly. I am finding the concept hard to digest." CreationDate="2015-06-15T18:34:30.193" UserId="5043" />
  <row Id="6481" PostId="6124" Score="0" Text="So did you have some trouble understanding the scikit tutorial? I don't know right off the top of my head about a tutorial that is so much better than that one. Did you do the tutorial from the [beginning](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) of the Quick Start? If I were you, I would use the SGDClassifier to predict &quot;quality&quot; and SGDRegressor to predict something else." CreationDate="2015-06-15T18:39:07.217" UserId="8275" />
  <row Id="6483" PostId="6124" Score="0" Text="okay. I did not read the tutorial from the beginning. Probably, should do that now. Thanks for your help Will" CreationDate="2015-06-15T18:48:30.820" UserId="5043" />
  <row Id="6484" PostId="6117" Score="0" Text="Thank you for the interesting links, I think, it can be used for my purpose, but for now, I use k-means with my modifications, which gives me acceptable results (pic. in question)." CreationDate="2015-06-15T20:28:55.453" UserId="9637" />
  <row Id="6485" PostId="6117" Score="0" Text="very nice results! smart move. I'm proud of you :D Good Luck!" CreationDate="2015-06-15T21:58:00.217" UserId="8878" />
  <row Id="6486" PostId="6122" Score="0" Text="That's up to you. If you don't care about mac vs. windows, you could just use the fraction v11/(v11+v12). If you think the volume (v11+v12) is also significant, you could encode that in the saturation." CreationDate="2015-06-16T00:19:52.440" UserId="381" />
  <row Id="6487" PostId="6126" Score="0" Text="My view is that data science is dominated by machine learning and computer scientists and that statisticians take a back seat. One index of this in the States is that the Amer Stats Assoc (ASA) has several blogs on its website about concerns that statistical grant requests are being underfunded, esp relative to CS grants. Not to mention that network analysis is a big thing in data science and obtaining deep, good experience in that would make your instantly employable." CreationDate="2015-06-16T11:03:33.170" UserId="9974" />
  <row Id="6488" PostId="6126" Score="0" Text="I don't want to start a religion war on this subject, but in my opinion machine learning IS statistics in a broader sense: it is statistical learning. Then, of course a statistician who wants to become a data scientist needs to develop some CS skills." CreationDate="2015-06-16T11:07:02.323" UserId="9766" />
  <row Id="6489" PostId="6126" Score="0" Text="There you go! You've confirmed my point. But I do agree that opinions can run high on this issue. Again, by pointing to the ASA's own statements, one can get a sense of what is really happening out there and which side is winning the war right now." CreationDate="2015-06-16T11:42:09.353" UserId="9974" />
  <row Id="6491" PostId="6099" Score="0" Text="I'm voting to close this question as off-topic because this is a stats question, migrate to stats.stackexchange.com" CreationDate="2015-06-16T16:44:49.737" UserId="471" />
  <row Id="6492" PostId="6099" Score="1" Text="@Spacedman Thank you for your opinion. Initially, stats.stackexchange.com wanted to do the same for being off-topic because this was considered a stat-software question." CreationDate="2015-06-16T18:43:59.353" UserId="7966" />
  <row Id="6493" PostId="6088" Score="0" Text="How many samples do you have of each? An alternative could be to try some outlier detection approach and test it against your fraud data." CreationDate="2015-06-16T19:36:54.587" UserId="2621" />
  <row Id="6494" PostId="6132" Score="1" Text="Nicely done. Your answer kind of demarcates expectation from reality. Bravo!" CreationDate="2015-06-16T19:45:37.997" UserId="9966" />
  <row Id="6495" PostId="6132" Score="0" Text="Oddly the upvote here did not register in my points." CreationDate="2015-06-17T06:08:45.433" UserId="7720" />
  <row Id="6496" PostId="6099" Score="0" Text="I think it's close enough for this site since it's more about doing this is in software, and DS is more of the overlap between stats and engineering." CreationDate="2015-06-17T07:33:47.720" UserId="21" />
  <row Id="6497" PostId="6098" Score="0" Text="@sean-owen If you unhold this I can answer." CreationDate="2015-06-17T11:19:09.633" UserId="5303" />
  <row Id="6498" PostId="6134" Score="0" Text="By &quot;Time series can be assigned to clusters based on their fit to the cluster's pdf. &quot; you mean the mixture modelling EM-based clustering? I am not sure to understand your point. Basically, pdfs are my object of study, i.e. my dataset consists in $N \times T$ pdfs. Any of these N series can be viewed as a pdf which is evolving &quot;smoothly&quot; through time, and we have T regularly spaced snapshot of it. So, I want to capture its distorition dynamics AND I want to capture how it relates to other pdfs time series both in pdfs similarity and dynamics." CreationDate="2015-06-17T11:41:45.167" UserId="7966" />
  <row Id="6499" PostId="6133" Score="0" Text="You can try [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) for fitting your hyper-parameters. It might not help that much in your case." CreationDate="2015-06-17T12:10:54.400" UserId="7966" />
  <row Id="6500" PostId="6133" Score="1" Text="When you say good performance for training data, what do you mean?Predicting the majority class all the time  will get you 95% accuracy. But you may have a cost matrix which makes this  an unattractive choice  ?" CreationDate="2015-06-17T14:00:39.610" UserId="7980" />
  <row Id="6501" PostId="6068" Score="0" Text="Try to pick something manageable. Decide what a minimum viable model would be for the data you are interested in. Go at it hammer and tongs until done. Start writing your paper based on what you learned. If you get to this stage and still have time left, add more sophistication if you want to. Version control is your friend with experimental software." CreationDate="2015-06-17T16:09:44.760" UserId="5303" />
  <row Id="6502" PostId="6133" Score="0" Text="@image_doctor, For training data. i get close to 75% accuracy for both majority class and minortiy class. However when i bring in some new data to test for the performance, there it is not giving good results. I get accuracy close to 40% in the new data. Simply put, my model is not robust or it is over-fitting." CreationDate="2015-06-17T18:21:49.197" UserId="9793" />
  <row Id="6503" PostId="6133" Score="0" Text="@mic , thanks for the suggestion. I tried k fold cross validation for rpart. However i dont know how to use the same in C5.0 algorithm in R." CreationDate="2015-06-17T18:23:22.850" UserId="9793" />
  <row Id="6504" PostId="6133" Score="0" Text="@image_doctor, what you are referring as cost matrix? is that a technique ?" CreationDate="2015-06-17T18:24:15.480" UserId="9793" />
  <row Id="6505" PostId="6137" Score="0" Text="It appears you are mis-applying PCA with regard to this data. It is not as simple as making the association that `PC2--&gt;stage`. When you look at the data for your known stages, they do not align with the y (PC2) axis. And your samples appear to be extreme outliers with respect to at least one of your input variables. What is the dimensionality of your inputs? You might consider generating some scatter plots to figure out which input feature is blowing up your variance in the first PC." CreationDate="2015-06-17T20:39:00.477" UserId="964" />
  <row Id="6508" PostId="6140" Score="1" Text="Needs to go on StackOverflow, not here." CreationDate="2015-06-18T10:25:44.007" UserId="7720" />
  <row Id="6509" PostId="5352" Score="0" Text="I did not see anything like this there. Am I blind or did it get taken down?" CreationDate="2015-06-18T11:39:02.277" UserId="7720" />
  <row Id="6510" PostId="6141" Score="0" Text="Why don't you use Python? That would be so easy. If you want Python recipe I can answer." CreationDate="2015-06-18T12:12:22.873" UserId="8878" />
  <row Id="6511" PostId="5352" Score="0" Text="Second to last paragraph mentioned it. Or do you mean in Visual Studio itself?" CreationDate="2015-06-18T14:04:45.950" UserId="587" />
  <row Id="6513" PostId="6134" Score="0" Text="@mic think my reading of your post oversimplified the question. Do we have any simplifying assumptions, e.g. could the distribution at time $t$ have a parametric form?" CreationDate="2015-06-18T14:53:52.793" UserId="5303" />
  <row Id="6514" PostId="6142" Score="0" Text="Thanks, kasramsh. I update my questions, adding more info about the input dataset. You are right. It is possible there are other factors leading to the difference on PC1. Generally, that difference is due to two sources of data, not limiting to protocol itself." CreationDate="2015-06-18T14:57:21.120" UserId="10201" />
  <row Id="6515" PostId="6142" Score="0" Text="What I am interested in is the difference on PC2. Based on the public datasets (points on the right), the difference on PC2 is due to time. What upsets me is whether I can project my samples (points on the left) to public data (points on the right) to conclude my samples are on stages between D12 and D19. I tried to find the answer by learning PCA algorithm but I'm still not sure. Thanks!" CreationDate="2015-06-18T14:57:34.183" UserId="10201" />
  <row Id="6516" PostId="6137" Score="0" Text="Thanks, bogatron. I updated the question with more details about the data. The differences between my 18 samples and 24 published samples are expected. Some points overlap on the PCA plot. On the left, there are 18 points. Sorry, this might mislead you." CreationDate="2015-06-18T15:07:11.690" UserId="10201" />
  <row Id="6517" PostId="6134" Score="0" Text="No. But, approximation is allowed if it can lead to a reasonable solution." CreationDate="2015-06-18T15:40:25.380" UserId="7966" />
  <row Id="6518" PostId="6098" Score="0" Text="@ahmad-tay it would help future users if you used math notation, check out: http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" CreationDate="2015-06-18T15:50:20.863" UserId="5303" />
  <row Id="6519" PostId="30" Score="0" Text="Others describe 4 V's of big data [IBM](http://www.ibmbigdatahub.com/infographic/four-vs-big-data) or even 5 V's [DAVE BEULKE 2011](http://davebeulke.com/big-data-impacts-data-management-the-five-vs-of-big-data/)" CreationDate="2015-06-18T15:57:30.660" UserId="10220" />
  <row Id="6520" PostId="6134" Score="1" Text="@mic what are your samples? Do you actually have the distributions at all times for each series or do you have a sample(s) from the distribution at time $t$ in series $n$?" CreationDate="2015-06-18T15:59:59.043" UserId="5303" />
  <row Id="6521" PostId="30" Score="0" Text="The original 3 V's were set out in 2001 by Doug Laney  [3D Data Management: Controlling Data Volume, Velocity, and Variety](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)." CreationDate="2015-06-18T16:18:20.830" UserId="10220" />
  <row Id="6522" PostId="6068" Score="0" Text="@ChristopherLouden: When it comes to deep learning, sadly, there hardly any people who have been working to make the topic understandable." CreationDate="2015-06-18T17:08:17.960" UserId="10085" />
  <row Id="6523" PostId="6142" Score="0" Text="Regarding your update I'd say be careful about interpretation! you do not know the procedure of getting those data and it would be problematic for interpretation. The point is that those datasets are from different stages but also from different sources! so we can not make decision here. The other point is about using prcomp function. I've never used it but usually in machine learning tools the setting is like samples in rows and features in columns so I recommend to have a quick look at it again." CreationDate="2015-06-18T18:34:10.583" UserId="8878" />
  <row Id="6524" PostId="6137" Score="0" Text="If I understand your update to the question, you have a data set with 17436 variables (gene expressions) and 42 samples. So I'm curious how you're even doing PCA because your covariance matrix is definitely singular. I wonder if you have your data matrix transposed (inadvertently treating it as 42 variables with 17436 samples). Someone could help you better if you post the relevant portion of your code." CreationDate="2015-06-18T20:16:22.547" UserId="964" />
  <row Id="6525" PostId="6139" Score="1" Text="Can you please clarify what your problem is exactly? Is it poor utilization of server resources? Are you unable to process the data?" CreationDate="2015-06-18T20:48:32.777" UserId="3466" />
  <row Id="6526" PostId="6145" Score="0" Text="That is exactly what you wanna do. Cheers!" CreationDate="2015-06-18T21:07:45.607" UserId="9966" />
  <row Id="6528" PostId="2360" Score="0" Text="@xtian x_t is in the transform scale while x_i is in the current scale. Both x_{max} and x_{min} are also in the current scale." CreationDate="2015-06-18T21:19:26.000" UserId="847" />
  <row Id="6530" PostId="6141" Score="0" Text="Just let the search function be a conditional in an if...then statement." CreationDate="2015-06-18T23:30:48.443" UserId="8005" />
  <row Id="6531" PostId="6137" Score="0" Text="I transposed the matrix. The code for PCA is pasted in the update. Thanks." CreationDate="2015-06-19T02:22:31.723" UserId="10201" />
  <row Id="6532" PostId="6142" Score="0" Text="I transposed the matrix, thus rows are for samples and columns for genes. The code I use is in the 2nd update. Thanks!" CreationDate="2015-06-19T02:28:08.547" UserId="10201" />
  <row Id="6533" PostId="6088" Score="0" Text="bias–variance tradeoff should be there in training data." CreationDate="2015-06-19T07:19:03.470" UserId="5091" />
  <row Id="6534" PostId="6141" Score="0" Text="I wrote a command to do it in bash but for exploration excel is something far better." CreationDate="2015-06-19T07:33:30.907" UserId="3151" />
  <row Id="6535" PostId="6150" Score="0" Text="Could you edit your question and include what command have you tried in Matlab, and what is the specific error that Matlab is throwing? Otherwise, it's difficult to say more than &quot;`tdfread` is the command listed in the documentation for this kind of task&quot;" CreationDate="2015-06-19T07:37:32.037" UserId="1367" />
  <row Id="6536" PostId="6145" Score="0" Text="It worked. Thanks. The first match would suffice for me because the text is short (an abstract of a paper) and a word with a specific root will most probably happen once in the text." CreationDate="2015-06-19T07:45:47.720" UserId="3151" />
  <row Id="6537" PostId="6149" Score="0" Text="subset did not solve the problem." CreationDate="2015-06-19T08:13:25.170" UserId="3151" />
  <row Id="6538" PostId="6148" Score="1" Text="yes it might be possible in programming, but for R, the way it process data  is quite differently. I am using the library twitteR from R and the min it an retrieve is one day. I am not sure how am I going to continue from the point I stop tweet if I ran the searchTweets func again." CreationDate="2015-06-19T08:34:56.430" UserId="10225" />
  <row Id="6539" PostId="6133" Score="0" Text="@Arun It's a way of expressing the different values placed on correctly and incorrectly classifying instances of your data. Perhaps misclassifying the minority class has a great cost, perhaps it doesn't , using a cost matrix accounts for this." CreationDate="2015-06-19T12:21:02.233" UserId="7980" />
  <row Id="6543" PostId="6149" Score="0" Text="Well, it worked for me! so try `Sys.setlocale` and see if it helps." CreationDate="2015-06-19T15:56:02.300" UserId="5055" />
  <row Id="6545" PostId="6148" Score="0" Text="Wish I knew more about R to help you out. Sorry!" CreationDate="2015-06-19T22:07:12.613" UserId="3466" />
  <row Id="6548" PostId="6154" Score="1" Text="Is it possible to summarise the content of any one of those links, in a short paragraph? The links might be useful for further research, but ideally a stack exchange answer should have enough text to address the basic question without needing to go off site." CreationDate="2015-06-20T07:01:29.390" UserId="836" />
  <row Id="6549" PostId="6154" Score="0" Text="I am sorry but the content of these pages is too large to be summarized in a short paragraph." CreationDate="2015-06-20T09:11:39.483" UserId="9943" />
  <row Id="6551" PostId="6154" Score="1" Text="A full summary is not required, just a headline - e.g. &quot;A deconvolutional neural network is similar to a CNN, but is trained so that features in any hidden layer can be used to reconstruct the previous layer (and by repetition across layers, eventually the input could be reconstructed from the output). This allows it to be trained unsupervised in order to learn generic high-level features in a problem domain - usually image processing&quot; (note I am not even sure if that is correct, hence not writing my own answer)." CreationDate="2015-06-20T11:08:49.030" UserId="836" />
  <row Id="6552" PostId="6162" Score="0" Text="yes preparing model with CRF is ok. But I would like to know is there any other packages/libraries available? like Stanford NER for normal text message." CreationDate="2015-06-20T16:57:30.433" UserId="5091" />
  <row Id="6553" PostId="6162" Score="0" Text="Well the thing is that for open texts the standard CRF models might work poorly. The advantage of having your own is that you can make corrections and re-train." CreationDate="2015-06-20T16:58:55.340" UserId="7848" />
  <row Id="6554" PostId="6166" Score="0" Text="You may be correct it looks like naive Bayes only has the imbalance problem with text classification. Can you explain what smoothing is?" CreationDate="2015-06-21T15:34:14.737" UserId="10255" />
  <row Id="6555" PostId="6166" Score="0" Text="Have you had a look at the link? It's explained there. In short, when estimating $P(x \mid y)$ sometimes $x$ is never seen during training and  smoothing ensures that it doesn't ruin the classifier performance by adding some extra count (in case of laplace or +1 smoothing) to all the features." CreationDate="2015-06-21T17:35:16.417" UserId="816" />
  <row Id="6556" PostId="6146" Score="1" Text="Why don't you use Python or just command line tools to retrieve Twitter data, save it into chunks (even `gzipped`), and then get back to R for analysis?" CreationDate="2015-06-21T18:56:10.850" UserId="5279" />
  <row Id="6557" PostId="5371" Score="0" Text="Include in your comparison [Revolution R](http://www.revolutionanalytics.com/get-revolution-r), which is an optimized proprietary version of R with some libraries released for free." CreationDate="2015-06-21T18:58:08.923" UserId="5279" />
  <row Id="6558" PostId="155" Score="1" Text="Have you joined the Open Data Stack Exchange? http://opendata.stackexchange.com" CreationDate="2015-06-21T21:00:42.177" UserId="10133" />
  <row Id="6559" PostId="6139" Score="0" Text="Poor utilization of resource so cloud solution is needed. &#xA;But I don't know if I can use Big Data Analytics I can use in this case or not ." CreationDate="2015-06-22T04:11:22.663" UserId="10210" />
  <row Id="6560" PostId="6143" Score="0" Text="Sir, thank you for your reply. But my question is how to write this  \alpha which depends on Z and W." CreationDate="2015-06-22T07:57:15.493" UserId="10131" />
  <row Id="6561" PostId="6143" Score="0" Text="@AhmadTay, unfortunately your question suggested you had misread the book as some of your matrices had incorrect dimensions and your distribution for $f$ was off. In the model above, $\alpha$ is given as a parameter." CreationDate="2015-06-22T08:52:56.483" UserId="5303" />
  <row Id="6562" PostId="6173" Score="0" Text="and I think@mike-wise is right about the correct place for this question." CreationDate="2015-06-22T13:01:22.803" UserId="10270" />
  <row Id="6563" PostId="6175" Score="0" Text="I suspect the answer to this question will depend somewhat on the analysis method you're hoping to use; so more detail there would be helpful." CreationDate="2015-06-22T15:07:42.637" UserId="5303" />
  <row Id="6564" PostId="6175" Score="0" Text="Ok I gave some examples, thank you for your interest @conjectures" CreationDate="2015-06-22T15:27:25.147" UserId="10275" />
  <row Id="6565" PostId="6175" Score="0" Text="out of interest, with mongodb did you try using ensureIndex: http://www.tutorialspoint.com/mongodb/mongodb_indexing.htm (the indexing process itself takes a while, but speeds up queries afterwards)" CreationDate="2015-06-22T15:43:28.980" UserId="5303" />
  <row Id="6566" PostId="6175" Score="0" Text="also the second bullet is totally possible via 1 line at a time reading of file and adding a number to whichever groups the line belongs to and corresponding group counts." CreationDate="2015-06-22T15:45:31.873" UserId="5303" />
  <row Id="6567" PostId="6175" Score="0" Text="no I didn't try ensureIndex, to be honnest it was my almost first experience with mongoDB so it may be possible to optimize. It's probable that I can do every request I want with mongo but I am worried about the time it will take and if it is not the proper solution for my purpose, I am ready to change. Thank you for the link !" CreationDate="2015-06-22T16:03:33.887" UserId="10275" />
  <row Id="6568" PostId="6159" Score="0" Text="Thank you very much. Will try this and let you know." CreationDate="2015-06-23T02:29:21.323" UserId="9039" />
  <row Id="6569" PostId="6159" Score="0" Text="@logc Thank you for replying. I am going to work on it again and will let you know and if it does not work well I will go ahead and edit the question with more detail" CreationDate="2015-06-23T02:30:06.517" UserId="9039" />
  <row Id="6570" PostId="6178" Score="0" Text="sixth, [you could use sql](http://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html)" CreationDate="2015-06-23T02:35:29.223" UserId="9924" />
  <row Id="6571" PostId="6174" Score="0" Text="How much time is &quot;too much time&quot;? 10 seconds? 10 minutes? 10 hours? What sort of &quot;data file&quot;? CSV? How many rows, columns? What is the rest of the spec of your machine? Are you sharing this server with other people? Does your server sysadmin limit your resources? What version of R? Post the sessionInfo() output, because if its a 32 bit R you need to switch. What are you going to do with it when you have got it?" CreationDate="2015-06-23T07:31:13.677" UserId="471" />
  <row Id="6572" PostId="6178" Score="1" Text="I'm not sure how any of this addresses the questioners extremely vague question. This Q should be voted down and clarification asked for in comments." CreationDate="2015-06-23T07:35:06.810" UserId="471" />
  <row Id="6574" PostId="6179" Score="0" Text="Thank you for your answer, I will check that. I hope I am not that silly, I am a statistician and still have a lot to learn about computers on a hardware point of view (it is not clear for me why RAM is necessary when you don't need to have access to all the data at the same time, but maybe speed of writing and reading is essentially based on RAM..). I have limited ressources at work and at home, so I am wondering : how do people who participate to challenges like kaggle as a hobby cope with these issues ? Are they necessarily very well equipped ?" CreationDate="2015-06-23T09:10:16.143" UserId="10275" />
  <row Id="6576" PostId="6183" Score="1" Text="In the context of text mining I've seen &quot;annotated data&quot; vs. &quot;unannotated data&quot; or &quot;raw data&quot;." CreationDate="2015-06-23T13:55:09.790" UserId="2673" />
  <row Id="6577" PostId="6183" Score="0" Text="@Suzana_K, that already seems a bit better. Thanks for the suggestion! More suggestions are very welcome." CreationDate="2015-06-23T13:56:38.377" UserId="9486" />
  <row Id="6578" PostId="432" Score="0" Text="The first two links are dead." CreationDate="2015-06-23T14:30:02.550" UserId="2673" />
  <row Id="6579" PostId="2651" Score="1" Text="You should at least accept an answer." CreationDate="2015-06-23T14:51:23.123" UserId="2673" />
  <row Id="6580" PostId="432" Score="0" Text="@Suzana_K Thanks for reporting -- updated." CreationDate="2015-06-23T15:12:43.107" UserId="241" />
  <row Id="6581" PostId="6181" Score="0" Text="Let me know if there's a better StackExchange for this..." CreationDate="2015-06-23T16:41:07.827" UserId="10293" />
  <row Id="6582" PostId="6190" Score="0" Text="Are the y values of the baseline and the elevated parts known beforehand? Because then its just a case of looking for runs in over- or under-threshold values." CreationDate="2015-06-23T21:40:47.407" UserId="471" />
  <row Id="6583" PostId="6184" Score="0" Text="In `r`, `data.table` and `dplyr` packages will do that." CreationDate="2015-06-23T23:36:32.363" UserId="10307" />
  <row Id="6584" PostId="6189" Score="1" Text="[Using SQL, pandas, and Python for data analysis](http://johnbeieler.org/blog/2013/06/06/using-sql/) &amp; [Reading Files in HDFS with Pandas framework](http://stackoverflow.com/questions/16598043/reading-files-in-hdfs-hadoop-filesystem-directories-into-a-pandas-dataframe) &amp; [Large Data Work with pandas](http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas)" CreationDate="2015-06-24T03:14:57.380" UserId="10019" />
  <row Id="6585" PostId="6191" Score="0" Text="Actually to be as clear as possibleI  the 1st paragrpah is an intro and the 2nd specifically describes the problem I'd like to solve through machine learning . Anyway, what do you mean &quot;highest overlap&quot; please ?" CreationDate="2015-06-24T07:09:34.467" UserId="10102" />
  <row Id="6586" PostId="6175" Score="1" Text="Why not buy more RAM?" CreationDate="2015-06-24T09:37:21.043" UserId="7925" />
  <row Id="6588" PostId="6175" Score="0" Text="I tried Monetdb which failed to load my big dataset but I managed to load it with Rsqlite in 2 hours." CreationDate="2015-06-24T09:30:11.707" UserId="10275" />
  <row Id="6589" PostId="6191" Score="0" Text="It's not clear what the assumptions are.  For example, it's not clear where the tags come from, where you are drawing recommendations from, or whether your set that you want to draw recommendations from is tagged with the same tagset." CreationDate="2015-06-24T15:14:53.687" UserId="10303" />
  <row Id="6590" PostId="6191" Score="0" Text="&quot;Overlap&quot; means &quot;in common&quot;,  There are many similarity metrics based on overlap, for example [overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient)" CreationDate="2015-06-24T15:15:54.553" UserId="10303" />
  <row Id="6591" PostId="6175" Score="0" Text="@reinierpost : I will not buy extra RAM for my professionnal computer obviously but I am thinking about improving my personnal hardware. Actually before I struggled with these data, I had no idea that 4GB RAM was very little. It is not clear for beginners what you can do or not do depending on softwares or hardware..." CreationDate="2015-06-24T16:19:37.247" UserId="10275" />
  <row Id="6592" PostId="6179" Score="0" Text="I tried Monetdb with no success, but I succeeded in loading at least with rSQLite, thank you for answering though." CreationDate="2015-06-24T16:21:48.517" UserId="10275" />
  <row Id="6594" PostId="6150" Score="0" Text="@logc I edited the question and I have included the command and the error message I get. Would you please be able to help." CreationDate="2015-06-24T17:47:01.017" UserId="9039" />
  <row Id="6595" PostId="6209" Score="0" Text="Do you need to join texts that are separated by new lines?" CreationDate="2015-06-24T20:41:27.160" UserId="9966" />
  <row Id="6596" PostId="6209" Score="0" Text="Yes but only in certain cases. Some lines are perfectly ok" CreationDate="2015-06-24T20:46:04.903" UserId="10322" />
  <row Id="6597" PostId="6209" Score="0" Text="Sounds good. My solution should work then. Could you please show us a glimpse of your data?" CreationDate="2015-06-24T20:48:06.330" UserId="9966" />
  <row Id="6598" PostId="6209" Score="0" Text="Sure, here you go:" CreationDate="2015-06-24T20:58:46.333" UserId="10322" />
  <row Id="6599" PostId="6209" Score="0" Text="Employers' Liability Assurance&#xA;Corporation (Limited) of London, England, Lawfford &amp; McK3m, General&#xA;Agents, 19 and 21 Chamber of Commerce&#xA;Fidelity and Casualty Company of&#xA;New York , BLrekhead &amp; Son,&#xA;Agents-,. 306 Water  &#xA;Fidelity and Casualty Oo of New York&#xA;,. Robt Schaefer,, res mngr, 22 s&#xA;Holliday&#xA;Frankfort Marine Accident and  Pl ate-Glass Insurance Company  of&#xA;Frankfort-on-the-Main, Germany, Spear&#xA;&amp; Burbank, General Agents, 10-12 s Holliday	.&#xA;General Accident Assurance Corporation&#xA;of Scotland S03 Merchants* Nat’l Bank Bldg&#xA;GREAT EASTERN CASUALTY and Indemnity Company of NewYork," CreationDate="2015-06-24T20:59:59.260" UserId="10322" />
  <row Id="6600" PostId="6209" Score="0" Text="If you want it in another format let me know" CreationDate="2015-06-24T21:00:17.147" UserId="10322" />
  <row Id="6601" PostId="6209" Score="0" Text="This won't help much cause the formatting goes away when you comment. Attach screen shots of the data in hand and how you want it to be processed." CreationDate="2015-06-24T21:02:48.830" UserId="9966" />
  <row Id="6602" PostId="6211" Score="0" Text="Firstly, thanks for the answer and explanation (+1), @Azrael. So, according to you, shouldn't I care about the number of observation? Could you provide some reference about that?" CreationDate="2015-06-24T21:05:13.830" UserId="9225" />
  <row Id="6603" PostId="6209" Score="0" Text="http://imgur.com/ztYCaDU,BoZYHo6#0    The first pic is what it looks like now, the second is what I'd like it to look like" CreationDate="2015-06-24T21:11:42.230" UserId="10322" />
  <row Id="6604" PostId="6211" Score="1" Text="The data points mean the observations." CreationDate="2015-06-24T21:17:52.873" UserId="9943" />
  <row Id="6605" PostId="6211" Score="0" Text="Ok, thanks for the explanation again!" CreationDate="2015-06-24T21:22:12.917" UserId="9225" />
  <row Id="6606" PostId="6211" Score="0" Text="No problem. Happy to help." CreationDate="2015-06-24T21:22:47.287" UserId="9943" />
  <row Id="6607" PostId="6211" Score="0" Text="Maybe someone else knows better, but I&quot;m not entirely sure this is true. You aren't trying to solve the equations of the neural network in a classical sense. You could quite easily have a complex network that was trained to recognise the difference between only two data points." CreationDate="2015-06-24T21:33:52.300" UserId="7980" />
  <row Id="6608" PostId="6211" Score="0" Text="Yes, you are right. For example, you can keep giving a neural network 0 as an input, and 1 as an output multiple times and it will start learning to become a NOT gate. But what the question has demanded is a rule of thumb. The number of parameters that are required can not be correctly predicted without knowing more details." CreationDate="2015-06-24T21:38:27.727" UserId="9943" />
  <row Id="6610" PostId="677" Score="0" Text="Cross-posted with http://stackoverflow.com/q/24583249/2954547" CreationDate="2015-06-24T22:59:00.503" UserId="1156" />
  <row Id="6611" PostId="6211" Score="0" Text="I think that demonstrates that your opening paragraph is not true ?" CreationDate="2015-06-24T23:40:55.280" UserId="7980" />
  <row Id="6612" PostId="6211" Score="0" Text="Again, the question asks for a thumb rule." CreationDate="2015-06-24T23:43:07.173" UserId="9943" />
  <row Id="6613" PostId="6211" Score="0" Text="And your first para presents a rule, not a rule of thumb ?" CreationDate="2015-06-25T08:04:17.000" UserId="7980" />
  <row Id="6614" PostId="6211" Score="0" Text="This edit should address your concerns." CreationDate="2015-06-25T10:17:30.757" UserId="9943" />
  <row Id="6615" PostId="6175" Score="0" Text="@Stéphanie C: I don't know who is paying for your computer, but if they are also paying for your time, getting them to add more RAM may be actually be the most cost-effective solution for them - unless you plan to work with much larger data sets later on." CreationDate="2015-06-25T15:37:55.773" UserId="7925" />
  <row Id="6616" PostId="6179" Score="0" Text="You are correct: the reason to add RAM is access speed, primarily for writing." CreationDate="2015-06-25T15:39:33.640" UserId="7925" />
  <row Id="6617" PostId="6201" Score="0" Text="Check http://opendata.stackexchange.com/ and https://www.reddit.com/r/datasets" CreationDate="2015-06-25T16:11:14.307" UserId="816" />
  <row Id="6618" PostId="6213" Score="0" Text="Have you read about item-based collaborative filtering or matrix factorization for recommender systems? Your problem is very much like Amazon's &quot;customers who bought this also bought that.&quot;" CreationDate="2015-06-25T16:34:09.020" UserId="9956" />
  <row Id="6622" PostId="1165" Score="0" Text="teaches me not to check my stackoverflow email :-).  If you are still interested my direct email emily at equitieslab dot com" CreationDate="2015-06-25T18:51:39.887" UserId="8344" />
  <row Id="6624" PostId="6216" Score="0" Text="Can you explain why it doesn't seem like the right way? If you provide clear information on what's wrong, its easier for people to help you." CreationDate="2015-06-25T20:51:29.997" UserId="41" />
  <row Id="6625" PostId="6217" Score="2" Text="If I understand correctly, the concept you are looking for is _embedding_. Look up _kernel methods_, and _kernel PCA_ in particular." CreationDate="2015-06-26T00:22:35.050" UserId="381" />
  <row Id="6626" PostId="6146" Score="0" Text="Can python retrieve old tweets using the method you mentioned?" CreationDate="2015-06-26T01:44:51.267" UserId="10225" />
  <row Id="6627" PostId="6192" Score="0" Text="I think  it is possible to consider 'click only data' same as purchase data. From 'purchase data' you know keywords for each link. right ? can't you try using this data for 'click only data' also? I mean group 'purchase data' keywords with 'click data' will that help ?" CreationDate="2015-06-26T04:08:54.240" UserId="5091" />
  <row Id="6628" PostId="6200" Score="0" Text="cross validation" CreationDate="2015-06-26T04:16:13.223" UserId="5091" />
  <row Id="6629" PostId="6204" Score="0" Text="What do you mean by swm?" CreationDate="2015-06-26T05:00:48.350" UserId="2643" />
  <row Id="6630" PostId="6217" Score="0" Text="I am not sure about this, so I am not posting it as an answer. In a neural network type of model, you can keep the hidden layer dimensionality &gt; input layer dimensionality. Then you can use the hidden layer as input to another network/model. But doing so requires lots of data." CreationDate="2015-06-26T09:01:10.423" UserId="9943" />
  <row Id="6631" PostId="6216" Score="0" Text="So I initialised all the values in the first iteration.. computed the V(S)t and then I used all the V(S)t already computed to computed alpha trial-by-trial.. first, I really don't know if it is the right way of proceeding ..second I've got a alpha vector of all 1 and NaN values..." CreationDate="2015-06-26T09:29:57.800" UserId="10344" />
  <row Id="6632" PostId="6225" Score="0" Text="How would you &quot;cluster&quot; them into more generic groups? By means of a linguistic distance?" CreationDate="2015-06-26T12:31:31.587" UserId="982" />
  <row Id="6633" PostId="6146" Score="0" Text="It can, but you'll need to buy the API access from Twitter or another seller of Twitter history." CreationDate="2015-06-26T13:15:10.223" UserId="5279" />
  <row Id="6635" PostId="6225" Score="0" Text="In this case since number of FB categories are less, you can manually cluster them or go for similarity matching algorithms such as BOW/cosine similarity/ wordvec etc" CreationDate="2015-06-26T16:45:34.130" UserId="5091" />
  <row Id="6636" PostId="6233" Score="0" Text="You already asked this on CrossValidated:http://stats.stackexchange.com/questions/158897/simple-path-to-route-algorithm" CreationDate="2015-06-27T02:22:48.227" UserId="9424" />
  <row Id="6637" PostId="6225" Score="0" Text="@Sreejithc321 Your observation about the number of categories simplified the problem greatly." CreationDate="2015-06-27T08:26:21.873" UserId="75" />
  <row Id="6638" PostId="6234" Score="0" Text="Thanks for the answer. Would you mind giving a R code to do it  with the example I gave in order to have the answer to be perfect?" CreationDate="2015-06-27T14:24:14.200" UserId="6527" />
  <row Id="6639" PostId="5252" Score="0" Text="Thanks M.Dax a lot ! Yes you are right, thank you very much for the advice." CreationDate="2015-06-27T15:13:37.987" UserId="3433" />
  <row Id="6640" PostId="6175" Score="0" Text="A friend of mine is being hired as a datascientist in private company and get a 16 GB ram computer, is it a benchmark of the acceptable minimum for datascience do you think ? @reinierpost" CreationDate="2015-06-27T15:31:18.117" UserId="10275" />
  <row Id="6641" PostId="2258" Score="0" Text="@Sean Owen, Why are there so many questions on how to get started on neural networks? Shouldn't they be marked duplicate?" CreationDate="2015-06-27T16:20:01.383" UserId="9943" />
  <row Id="6642" PostId="6175" Score="0" Text="@Stéphanie C: I don't know, it entirely depends on the amounts of data you need to process and the complexity of the processing (e.g. selecting is usually cheap, joining expensive)." CreationDate="2015-06-27T19:04:05.067" UserId="7925" />
  <row Id="6643" PostId="2258" Score="0" Text="Heh don't ask me, but please flag duplicates as you see them." CreationDate="2015-06-27T21:19:36.917" UserId="21" />
  <row Id="6644" PostId="6240" Score="0" Text="The problem I have with the Pearson is that I am not sure how to join my data and take care of categorical variable (maybe could be done with dummy variable like bey suggested but not sure it would work with it)" CreationDate="2015-06-27T22:00:47.417" UserId="6527" />
  <row Id="6645" PostId="6208" Score="0" Text="you asked this [on Stack Overflow](http://stackoverflow.com/questions/30996952/working-with-inaccurate-incorrect-dataset/31003163#31003163) and @MaximHaytovich gave you a good answer" CreationDate="2015-06-28T03:12:17.343" UserId="10019" />
  <row Id="6646" PostId="6234" Score="1" Text="@zipp here you go. I had it calculate the normalized vectors, I also created a normalized vectors for each person, by normalizing the vector sum of the rows that correspond to that person. Finally, I take the dot product of each person against every other person using the $XX^T$ formula" CreationDate="2015-06-28T04:47:08.893" UserId="9424" />
  <row Id="6647" PostId="6224" Score="0" Text="Is the network being used for classification or regression ?" CreationDate="2015-06-28T11:04:41.577" UserId="7980" />
  <row Id="6648" PostId="6235" Score="0" Text="You might want to consider asking this on OpenData StackExchange http://opendata.stackexchange.com/" CreationDate="2015-06-28T11:06:27.840" UserId="7980" />
  <row Id="6649" PostId="6217" Score="0" Text="When you say 2 dimensional data, defined by at least three variables, in what sense do you use the term 'variable'? Would classes be a suitable substitution ? It's worth noting that PCA extracts maximally variant dimensions from data, this is not necessarily the most discriminative transform to apply. Have you looked at clustering ?" CreationDate="2015-06-28T11:20:08.813" UserId="7980" />
  <row Id="6650" PostId="6234" Score="0" Text="Thank you very much" CreationDate="2015-06-28T13:16:33.127" UserId="6527" />
  <row Id="6651" PostId="6204" Score="0" Text="Support vector machine. Just as an example, this algorithm offers an ability to assign different weights to classes manually" CreationDate="2015-06-28T15:49:45.353" UserId="7969" />
  <row Id="6653" PostId="6242" Score="0" Text="Are you talking about [that GATE](https://gate.ac.uk/family/process.html)? The answer depends on what level of integration you mean. For a basic direction, I suggest you to search for Java-Python interoperability software modules." CreationDate="2015-06-28T22:51:15.643" UserId="2452" />
  <row Id="6654" PostId="5248" Score="1" Text="I've found a spare.cor functions for sparse matrixes in R that calculates correlation for any dimension for sparse matrix. @Hack-R think about this bias. By the way sampling coefficients should have also something-like bootstrap confidence intervals: see this: http://stats.stackexchange.com/questions/126176/whats-the-bias-of-calculating-the-kendall-coefficient-of-correlation-on-a-sampl&#xA;&#xA;And the sparse.cor function for sparse matrixes is here: &#xA;http://stackoverflow.com/questions/5888287/running-cor-or-any-variant-over-a-sparse-matrix-in-r" CreationDate="2015-06-29T09:44:16.767" UserId="5224" />
  <row Id="6655" PostId="6224" Score="0" Text="It is being used for regression!" CreationDate="2015-06-29T12:23:32.627" UserId="10353" />
  <row Id="6656" PostId="6224" Score="0" Text="Are you happy with both those levels of error for estimating your target function ?" CreationDate="2015-06-29T12:32:50.320" UserId="7980" />
  <row Id="6657" PostId="6224" Score="0" Text="Thats the problem! I do not know the significance of the term 'relative sum of squares error'. Are these errors out of 1? Meaning, does 0.9 mean 90% error? Or is it some other way? Please tell me the significance of the term 'relative sum of squares error'." CreationDate="2015-06-29T13:12:42.593" UserId="10353" />
  <row Id="6658" PostId="6224" Score="0" Text="Also, is there some way to calculate RMSE in SPSS Neural Network analysis? Like instead of relative sum of squares error i want to calculate the RMSE of the network." CreationDate="2015-06-29T13:15:48.470" UserId="10353" />
  <row Id="6659" PostId="6243" Score="0" Text="Does anyone have a clue?" CreationDate="2015-06-29T15:01:07.867" UserId="10381" />
  <row Id="6660" PostId="5982" Score="0" Text="And how do you come up with `d` variable? Is it a fixed number that is chosen by a scientist?" CreationDate="2015-06-29T15:29:27.583" UserId="10404" />
  <row Id="6661" PostId="6255" Score="0" Text="Is this what you are looking for? http://math.stackexchange.com/questions/647395/why-root-mean-square-error" CreationDate="2015-06-29T15:45:25.667" UserId="10404" />
  <row Id="6662" PostId="6257" Score="0" Text="Is there a real-life problem behind your problem? Maybe you could use some algorithm that solves &quot;Market Basket Analysis&quot; (explained here for example: http://www.albionresearch.com/data_mining/market_basket.php)" CreationDate="2015-06-29T15:53:57.800" UserId="10404" />
  <row Id="6663" PostId="6257" Score="0" Text="I am trying to induce tags in blog post articles. This is exactly what I was looking for thank you https://en.wikipedia.org/wiki/Association_rule_learning. Could you post both links as an answer so I can accept it?" CreationDate="2015-06-29T15:57:33.363" UserId="9202" />
  <row Id="6664" PostId="6257" Score="0" Text="I'm glad I could help. :)" CreationDate="2015-06-29T16:02:54.327" UserId="10404" />
  <row Id="6665" PostId="6214" Score="0" Text="I've followed this before. Not working" CreationDate="2015-06-29T16:25:09.573" UserId="5043" />
  <row Id="6666" PostId="5982" Score="0" Text="A common approach is to have $d$ words, and each of the $d$ elements in the vector represent the frequency with which that word occurs in the text. There are only so many unique words used in all of the samples you're considering, so there's a definite upper-bound on $d$. Researchers usually also remove certain kinds of words that they don't think will be useful for classification, like &quot;the,&quot; &quot;and,&quot; &quot;it,&quot; etc." CreationDate="2015-06-29T16:28:22.523" UserId="9483" />
  <row Id="6667" PostId="6243" Score="0" Text="This is too vague...are you interested in the Shannon Entropy specifically?" CreationDate="2015-06-29T16:34:39.893" UserId="9424" />
  <row Id="6668" PostId="6229" Score="0" Text="This has worked. Thanks a lot" CreationDate="2015-06-29T16:56:35.813" UserId="9793" />
  <row Id="6669" PostId="6192" Score="0" Text="@Sreejithc321 If for Kmeans, there isn't the numeric value of the click data keywords, as there isn't any purchasing event occurred, so there isn't any sales amount that i could enter for the value of the keywords. I am trying the topic modeling for grouping the purchase data and click data right now, but the purchase data should have more weights than the click data and i don't know how to allocate the weights for them is appropriate.  I am trying purely 2:1 right now." CreationDate="2015-06-29T17:00:37.733" UserId="9724" />
  <row Id="6670" PostId="6186" Score="1" Text="Interesting, I haven't read the Bishop book, but I've heard online learning and sequential learning treated as different concepts. I understand online learning to mean a learning problem in which a model is able to use new incoming data as a way to &quot;grow&quot; the training set and update the model dynamically. I've heard sequential learning applied to problems where the goal is to predict a sequence instead of individual instances, as in predicting part-of-speech sequences in POS tagging." CreationDate="2015-06-29T19:22:13.487" UserId="4897" />
  <row Id="6671" PostId="5233" Score="0" Text="I second @PabloSuau's thought. Many packages have the capability to train multi-class classifiers, but often what's happening under the hood is that n binary classifiers are trained, and these are used individually to make predictions for each class. The SO question here also seems relevant: http://stackoverflow.com/questions/22009871/how-to-perform-multi-class-classification-using-svm-of-e1071-package-in-r" CreationDate="2015-06-29T19:32:29.603" UserId="4897" />
  <row Id="6675" PostId="6243" Score="0" Text="@Bey Yes, let's say Shannon Entropy." CreationDate="2015-06-29T20:56:02.983" UserId="10381" />
  <row Id="6676" PostId="6224" Score="0" Text="Have you seen ..http://stats.stackexchange.com/questions/71315/whats-relative-error-in-a-neural-network-model and this http://www-01.ibm.com/support/knowledgecenter/SSLVMB_20.0.0/com.ibm.spss.statistics.help/idh_idd_mlp_output.htm" CreationDate="2015-06-29T21:23:59.767" UserId="7980" />
  <row Id="6677" PostId="6243" Score="0" Text="Ok, well unlike least squares, the Shannon Entropy requires that you be able to assign a probability or density value to each error. Do you want to assume a gaussian error with mean 0 and a pre-specified stdev? Also, do you really want to maximize entropy or minimize entropy?" CreationDate="2015-06-30T03:23:40.490" UserId="9424" />
  <row Id="6679" PostId="6270" Score="0" Text="Plain text file, Word document, PDF, RTF, HTML, LaTeX source, PostScript file, document scan in image format?" CreationDate="2015-06-30T08:25:33.737" UserId="471" />
  <row Id="6681" PostId="6259" Score="0" Text="Have you read the &quot;Web Technologies&quot; task view on CRAN? Tells you all about web scraping in R." CreationDate="2015-06-30T08:30:04.243" UserId="471" />
  <row Id="6682" PostId="6274" Score="0" Text="Is time really a categorical attribute ?  Perhaps day of the week or month might be?" CreationDate="2015-06-30T10:46:02.823" UserId="7980" />
  <row Id="6683" PostId="6274" Score="0" Text="Value under time attribute appearing as  1/11/2011  11.54  and showing as Factor/W 20823 labels" CreationDate="2015-06-30T12:03:47.883" UserId="10416" />
  <row Id="6684" PostId="6275" Score="0" Text="yeah...got it. Thanks." CreationDate="2015-06-30T12:04:21.970" UserId="10416" />
  <row Id="6685" PostId="6274" Score="0" Text="My meaning was to suggest that interpreting time as a category, is probably not useful in a machine learning context." CreationDate="2015-06-30T12:37:42.687" UserId="7980" />
  <row Id="6686" PostId="6243" Score="0" Text="@Bey You are right a pdf is needed. But since we get a sequence of error, i.e. {epsilon_i}, can we use emperical distribution? I think it is the simplist and roughest way but please tell me if I am wrong (like using a Gaussian actually help). And I want to maximize entropy." CreationDate="2015-06-30T13:28:23.157" UserId="10381" />
  <row Id="6687" PostId="6243" Score="0" Text="@Bey And I am also interested in general if there is any other theory or tool to support self-desinged objective, other than least-square, for multiple linear regression. If a general method or tool is available, I believe I can apply my particular case (just make the objective Shannon entropy)." CreationDate="2015-06-30T13:34:13.610" UserId="10381" />
  <row Id="6689" PostId="6243" Score="0" Text="@Bey Actually, the underlying reason of why I am trying this is because I try to use a linear-regression kind of &quot;filter&quot; to process the sequences {x_i} and try to make the &quot;filtered&quot; sequence have maximum entropy (i.e. In fact I try to use x_i as y_i). If you have any other comments about how I can do this or whether this makes sense, your comments are welcome. I hope I am not confusing you." CreationDate="2015-06-30T13:34:29.787" UserId="10381" />
  <row Id="6691" PostId="6279" Score="1" Text="Machine learning classification algorithms are designed to seek features that usefully predict the  instances class, as you note a highly predictive feature is a very attractive target for them.  You might make more progress if you had training and test data which had a significant percentage of language learner decisions which differed from the true class of the sentence, forcing the classifier to look elsewhere for discriminative information.  The logical conclusion of this process is to remove the  learner decision from the features, which places you back at the start. Maybe combine both?" CreationDate="2015-06-30T14:08:56.323" UserId="7980" />
  <row Id="6692" PostId="6279" Score="0" Text="Unfortunately, the learner data is not annotated so I have no knowledge of their errors as of now. I could manually add a bias towards the existing class with probabilty (so if initial guess + 0.5 &gt;0.95 or something, then assign the error), but it is more elegant if the classifier does all this implicitly." CreationDate="2015-06-30T14:50:00.540" UserId="8152" />
  <row Id="6693" PostId="6279" Score="0" Text="I may be a little confused, I thought you had the learner decision and had included it, but I may have misunderstood ?" CreationDate="2015-06-30T15:13:38.797" UserId="7980" />
  <row Id="6694" PostId="6272" Score="0" Text="Hey, could you please explain to me what this block does? I ran my script with this code added and I got  [6, 1, 4, 3, 5, 7, 8, 0, 2] as the output. I want to substitute numerical values to the work class content using the values in the dictionary." CreationDate="2015-06-30T15:43:15.680" UserId="5043" />
  <row Id="6695" PostId="6279" Score="0" Text="I do, but the 'true class' is not known, so the differences between those cannot be included unfortunately." CreationDate="2015-06-30T16:08:32.220" UserId="8152" />
  <row Id="6696" PostId="6279" Score="0" Text="So you have two sets of data, labelled sentences, not from language learners and unlabelled data from language learners ?" CreationDate="2015-06-30T16:20:36.663" UserId="7980" />
  <row Id="6697" PostId="6272" Score="0" Text="Hi, The mapr function will return numerical value associated with the category value. eg : 6 for 'Self-emp-not-inc', python dictionaries are unordered. If you want an ordered dictionary, try collections.OrderedDict." CreationDate="2015-06-30T16:35:19.633" UserId="5091" />
  <row Id="6698" PostId="6280" Score="0" Text="You've had two hours to look at this and see how badly formatted it is. Please try and improve the formatting of your questions. There is a preview that shows you what it will look like. Learn a bit about marking up questions." CreationDate="2015-06-30T17:03:59.993" UserId="471" />
  <row Id="6699" PostId="6203" Score="0" Text="Mahout in Action is a great book if you are comfortable with Java. I would highly suggest it." CreationDate="2015-06-30T17:09:24.360" UserId="3466" />
  <row Id="6700" PostId="6280" Score="0" Text="@Spacedman, I am sorry for this. Its my bad. Thanks for pointing." CreationDate="2015-06-30T17:38:50.230" UserId="9793" />
  <row Id="6701" PostId="6259" Score="0" Text="Use https://www.kimonolabs.com/ for scraping. Then just use its `csv` output." CreationDate="2015-06-30T18:20:37.223" UserId="5279" />
  <row Id="6702" PostId="6272" Score="0" Text="Okay, now I understand the function. The thing is, I have a CSV with several thousand rows and there is a column named Workclass which contains any one of the value mentioned in the dictionary. So, for each row, I need to change the text in that column to a number by comparing the text with the dictionary and substitute the corresponding number. How do I use a function to parse the column by rows and compare the values with the dictionary?" CreationDate="2015-06-30T21:16:01.960" UserId="5043" />
  <row Id="6703" PostId="6279" Score="0" Text="That is correct! I train on the labelled ones, and I evaluate shortly on those as well, but the actual data it is created for, is the unlabelled data." CreationDate="2015-06-30T23:05:08.647" UserId="8152" />
  <row Id="6704" PostId="6243" Score="0" Text="I really don't follow what you are trying to do. Perhaps a small example? In theory, any increasing function of $\epsilon$ can be used, but not all of these generate sensible fits. The problem with entropy is that if you want to maximize it, you will be generating horrible fits." CreationDate="2015-07-01T01:33:14.853" UserId="9424" />
  <row Id="6706" PostId="6287" Score="0" Text="thank you. This has worked." CreationDate="2015-07-01T04:53:41.733" UserId="9793" />
  <row Id="6713" PostId="6243" Score="0" Text="@Bey If to maximize entropy, I think you are probably right about getting horrible fits. But my purpose is not fitting. I am sorry I confuse you but the following might be clear. I just want to get a sequence (1) having linear relationship (like a transformation) with the original one (2) having maximum entropy." CreationDate="2015-07-01T13:29:42.680" UserId="10381" />
  <row Id="6714" PostId="6243" Score="0" Text="Here's the problem: the empirical distribution puts $\frac{1}{n}$ probability  on each residual, so it will always result in the minimum entropy (uniform)" CreationDate="2015-07-01T13:43:34.627" UserId="9424" />
  <row Id="6715" PostId="6294" Score="0" Text="Look up Markov models and sequential machine learning." CreationDate="2015-07-01T14:18:46.747" UserId="381" />
  <row Id="6716" PostId="6287" Score="0" Text="Always happy to help! If it works for you, please accept the answer! Thank You!" CreationDate="2015-07-01T14:43:44.873" UserId="10299" />
  <row Id="6720" PostId="6287" Score="0" Text="Hi Shiva, I have accepted the answer. Thank you" CreationDate="2015-07-01T19:58:56.327" UserId="9793" />
  <row Id="6721" PostId="6305" Score="0" Text="The fields I'm extracting will be the same for a certain type of document, of which many types come in. I'm now thinking something along the lines of using a classifier to detect the document type and then some kind of business logic per document class to extract the information. Thanks for your answer and making me aware of regularization, leaving this open for now for more feedback." CreationDate="2015-07-01T21:22:09.747" UserId="10429" />
  <row Id="6722" PostId="6294" Score="0" Text="I have a hunch that Bayesian Analysis might be able to address this problem. But couldn't find reliable literature for it.&#xA;@Emre, can you provide references where Markov models are used for this type of Prediction/Modelling?" CreationDate="2015-07-02T07:18:27.340" UserId="10125" />
  <row Id="6725" PostId="6314" Score="0" Text="What do you know about the error distribution? Is it unbiased for example? Stable in time? etc..." CreationDate="2015-07-02T14:35:05.590" UserId="7720" />
  <row Id="6731" PostId="6325" Score="0" Text="Have you thought about forming cluster centres using sets comprised of a minimum number of points from each dataset and then force the clusters only to grow ?" CreationDate="2015-07-03T06:36:40.873" UserId="7980" />
  <row Id="6732" PostId="6293" Score="0" Text="The analysis aIgos are in batch. I mostly use R for the the modelling where as in live the code is mostly C++. I am exploring  parallelization as the problem seems embarrisingly parallel. Thanks" CreationDate="2015-07-03T08:51:53.410" UserId="10409" />
  <row Id="6735" PostId="6325" Score="0" Text="Thanks! I think the problem with this would lie in defining the initial clusters centers which I would like to be determined by some variation minimising scheme." CreationDate="2015-07-03T12:26:22.910" UserId="10463" />
  <row Id="6736" PostId="6291" Score="0" Text="The max and min of the training dataset are not necessary the same for the predictive dataset. Neither the average and the ST. Dev. So, the assumption to use the same statistics is not right." CreationDate="2015-07-03T13:51:55.823" UserId="201" />
  <row Id="6737" PostId="6294" Score="0" Text="Markov models are the simplest correlated sequence models, so they are widely used. A good example is in modeling language. You can predict the next word in a sequence given the past few words. Many spelling checkers work this way." CreationDate="2015-07-03T14:17:26.657" UserId="381" />
  <row Id="6738" PostId="6291" Score="1" Text="Like I said, I agree that the min and max are not good scale factors. They are too sensitive to outliers. However, if the robust statistics of mean and standard deviation the two datasets are that much different than why are you training with that dataset? It would seem that you should not be using a training set that is so far away from the prediction set." CreationDate="2015-07-03T17:31:59.777" UserId="2728" />
  <row Id="6740" PostId="6325" Score="0" Text="The implication seems to be that whatever distance metric you use, should not be derived solely from the feature space distance, but have a component dependent on the class of the data, or some data dependent variable weighting of feature distance and class. Without sight of representative data this may prove challenging ?" CreationDate="2015-07-03T19:58:34.043" UserId="7980" />
  <row Id="6741" PostId="6068" Score="0" Text="possible duplicate of [Deep learning basics](http://datascience.stackexchange.com/questions/2651/deep-learning-basics)" CreationDate="2015-07-03T21:40:24.750" UserId="9943" />
  <row Id="6743" PostId="6272" Score="0" Text="You can create an additional column, say 'workclass_num'  which store numerical values corresponding to the categorical value. Check Python Pandas library." CreationDate="2015-07-05T08:32:26.057" UserId="5091" />
  <row Id="6744" PostId="6335" Score="0" Text="Brilliant way to handle it. It would give 0 and 23 hrs similar scores but won't it make am/pm time similar too? Which is in fact separated by 12hr window." CreationDate="2015-07-05T12:34:18.123" UserId="8338" />
  <row Id="6745" PostId="6343" Score="1" Text="All these clusters are *convex*, and this doesn't answer the question." CreationDate="2015-07-05T13:21:12.920" UserId="924" />
  <row Id="6746" PostId="6335" Score="0" Text="12 hour (AM/PM) time doesn't work, just convert it to 24 hour time." CreationDate="2015-07-05T16:41:36.560" UserId="9420" />
  <row Id="6747" PostId="6335" Score="0" Text="I just noticed that you are dividing by 24. When you gave analogy to clock, I thought it is a standard 12-hour clock. However you are taking 24-hr clock. It seems to be the best way for me to transform. Thank You!" CreationDate="2015-07-06T07:28:18.883" UserId="8338" />
  <row Id="6748" PostId="6348" Score="0" Text="Thanks @twalbaum! Exactly what I needed!" CreationDate="2015-07-06T07:42:01.357" UserId="9225" />
  <row Id="6750" PostId="6360" Score="1" Text="Are you affiliated with this company @henry.oswald?" CreationDate="2015-07-06T21:28:24.317" UserId="21" />
  <row Id="6752" PostId="6369" Score="0" Text="SE discourages link-only answers -- summarize the content that is relevant to the answer?" CreationDate="2015-07-07T08:23:10.430" UserId="21" />
  <row Id="6753" PostId="6360" Score="0" Text="Yes, sorry I should have made that clear. Have now." CreationDate="2015-07-07T10:33:04.977" UserId="10509" />
  <row Id="6755" PostId="6374" Score="0" Text="What's not available in Orange?&#xA;I actually tried around other libs with Apriori and FP-Growth and indeed you're right, FPG is way faster but the results look a bit different (maybe just because of the lib I used) but still good. I'd go that way I guess." CreationDate="2015-07-07T15:29:02.187" UserId="10513" />
  <row Id="6756" PostId="6133" Score="0" Text="@image_doctor, thanks for the explanation. Could you please advice me whether cost matrix possible in random forest? I use R. Thanks in advance." CreationDate="2015-07-07T17:07:22.577" UserId="9793" />
  <row Id="6758" PostId="694" Score="2" Text="And now there's a new contender - [Scikit Neuralnetwork](http://scikit-neuralnetwork.readthedocs.org/en/latest/): Has anyone had experience with this yet? How does it compare with Pylearn2 or Theano?" CreationDate="2015-07-07T12:20:57.617" UserId="10519" />
  <row Id="6759" PostId="6144" Score="0" Text="Why would you want to work with data that hasn't been standardized?  I'm not familiar enough with hidden markov, but there are so many pitfalls to not scaling, it is so easy to do, and it is reversible, so why would you not just scale the data and move on?" CreationDate="2015-07-07T21:30:21.050" UserId="9420" />
  <row Id="6760" PostId="6377" Score="0" Text="Thanks for the tip, I'll check it out! It looks like it'll help. The difficulty is going to be in defining a good metric for &quot;similarity of response to inputs&quot; in any case." CreationDate="2015-07-07T23:02:37.607" UserId="10427" />
  <row Id="6761" PostId="6363" Score="0" Text="Thanks, what you have said makes sense. The questions are looking for me to fake a data set by taking random numbers off relevant distributions with real/related data from real data sets. I had a hard time figuring out what the end result should be, but a dataset makes sense." CreationDate="2015-07-07T23:11:57.310" UserId="10510" />
  <row Id="6762" PostId="6379" Score="0" Text="This has been asked before in StackOverflow. Your answer is here: http://stackoverflow.com/questions/11053899/how-to-get-a-reversed-log10-scale-in-ggplot2" CreationDate="2015-07-08T05:07:40.883" UserId="609" />
  <row Id="6763" PostId="6374" Score="0" Text="for all I know FP-Growth is not available in Orange. The results should be the same for same input data when you use same min-support / min-confidence, but like you said, different libs are using different variants of apriori / FP-growth for better performance." CreationDate="2015-07-08T08:31:47.937" UserId="10521" />
  <row Id="6764" PostId="6361" Score="0" Text="I suggest that you read books and tutorials on process modeling (deterministic and stochastic).  Too many people who are new to data science assume that they can produce good/working models without knowing anything about the underlying process.  Sometimes this is so, but many times it is not. The *last* thing you should think about his how to implement in R." CreationDate="2015-07-08T08:55:17.250" UserId="609" />
  <row Id="6765" PostId="6361" Score="0" Text="By analogy, it is though you have learned optics and now you assume that you can do astronomy using optics alone. No, you can't. *PLEASE* learn how to model processes: queues (FIFO), stacks (LIFO), serial, parallel, branching, critical path, and associated decision processes." CreationDate="2015-07-08T09:00:19.807" UserId="609" />
  <row Id="6767" PostId="6370" Score="1" Text="You are confusing two completely different types of systems.  A REST API (or API of any sort) applies to a software library that you call inside your code (Java, C++, Python, or whatever).  Separately, there are systems that perform Map-Reduce on data stored in HDFS (e.g. Hadoop).  Typically, there are intermediary systems (e.g. Hive) that interpret your query and execute it on the designated infrastructure (e.g. Hadoop).  I suggest that you read/watch tutorials on Hadoop/Hive (or similar), and don't think about REST API in that context." CreationDate="2015-07-08T09:10:40.023" UserId="609" />
  <row Id="6769" PostId="6361" Score="0" Text="Thanks. What you are saying makes sense. R is designed to be used by data analysts/scientists, exclusively, and therefore it is important to know these topics before considering learning r." CreationDate="2015-07-08T13:31:48.927" UserId="10510" />
  <row Id="6770" PostId="6361" Score="0" Text="Thanks for the process modeling tip. I agree with your sentiment that learning data science is quite involved." CreationDate="2015-07-08T13:46:42.840" UserId="10510" />
  <row Id="6772" PostId="6385" Score="0" Text="Thanks for the specific references.  I learned a lot from them." CreationDate="2015-07-08T15:06:40.703" UserId="10531" />
  <row Id="6773" PostId="6390" Score="0" Text="Thank you for addressing the &quot;output&quot; question.  One thing though:  *&quot;You could decide to project the observations into the first two principal components&quot;*  Is that the same thing as step 5 in your quoted section?  The eigenvectors of the covariance matrix and the principal components are the same, are they not?" CreationDate="2015-07-08T15:08:57.303" UserId="10531" />
  <row Id="6774" PostId="6390" Score="0" Text="Also, that step by step example article you linked is *really* excellent." CreationDate="2015-07-08T15:10:23.720" UserId="10531" />
  <row Id="6775" PostId="6144" Score="1" Text="Upon further reflection of my question, scaling the data seemed like the right thing to do.  Thank you for confirming my thought process." CreationDate="2015-07-08T15:42:52.877" UserId="10187" />
  <row Id="6776" PostId="6390" Score="1" Text="Thanks for pointing that out, I failed to realize my suggestion did overlap with the given example. I've edited my answer to include further steps or alternatives to the example." CreationDate="2015-07-08T16:23:59.703" UserId="10536" />
  <row Id="6779" PostId="253" Score="0" Text="It is not absolutely necessary. It is just one of the tools. What is necessary is an understanding of statistics and linear algebra. The choice of tool is secondary." CreationDate="2015-07-09T02:01:40.770" UserId="10522" />
  <row Id="6780" PostId="6379" Score="0" Text="And so it has.  Thanks." CreationDate="2015-07-09T05:49:38.633" UserId="916" />
  <row Id="6781" PostId="694" Score="0" Text="There is also Keras - https://github.com/fchollet/keras - which is relatively recent. The problems with tracking &quot;best&quot; by any measure, and keeping the Q&amp;A valid over time is why this sort of question is usually off topic in other Stack Exchange networks." CreationDate="2015-07-09T08:37:25.873" UserId="836" />
  <row Id="6782" PostId="6408" Score="0" Text="I edited my findings into the question, I'm not sure if it notifies you of that or not." CreationDate="2015-07-09T12:50:07.247" UserId="10560" />
  <row Id="6783" PostId="6408" Score="1" Text="Maybe use some &quot;optimism bias&quot;. Initialize all Q-values above any reasonable estimate of their long run value. Then states that haven't been tried will always have Q-values above those that have and this encourages exploration." CreationDate="2015-07-09T13:11:34.297" UserId="10568" />
  <row Id="6784" PostId="6375" Score="0" Text="what are your feature vectors right now?" CreationDate="2015-07-09T17:38:14.180" UserId="10587" />
  <row Id="6785" PostId="6401" Score="0" Text="mrjob seems to be the one I'm looking for.Thanks!" CreationDate="2015-07-09T20:22:24.340" UserId="10327" />
  <row Id="6786" PostId="6408" Score="1" Text="I tried initializing the Q-values a couple different ways with no success. Let me see if I'm understanding the update process correctly: ∀ $i≤ t ∈ T$, ∀ $x_i$, $a_i$ Update all Q-Values according to their eligibility traces&#xA;&#xA;$Q_t^{k+1}(x_i, a_i) ← Q_i^k(x_i, a_i) + α(x_i^k,a_i^k)δ_t^ke_t^k(x_i,a_i)$" CreationDate="2015-07-10T01:06:29.527" UserId="10560" />
  <row Id="6787" PostId="6408" Score="0" Text="So essentially what this is saying is that the Q value for the time period that I'm in during the next episode will be equal to the current Q value + learning rate * TD error * eligibility trace for all state-action pairs that I've visited during this episode, correct?" CreationDate="2015-07-10T01:13:11.537" UserId="10560" />
  <row Id="6788" PostId="6375" Score="0" Text="My feature vectors are composed of numerical values acquired by dense optical flow (from OpenCV). Basically what happens is that there is a 17 frame video, and each frame is composed of 136 points used in dense optical flow. For each frame, the 136 point vector constantly concatenates with each other until the 17th frame to form a single 2312 sized vector. If there were to be a 2nd video, there will be another 2312 sized vector in the 2nd row." CreationDate="2015-07-10T02:56:48.723" UserId="10523" />
  <row Id="6789" PostId="6391" Score="1" Text="Here is an interesting article that relates to your question. http://labs.eeb.utoronto.ca/jackson/ecol.%20modelling%20ANN.pdf" CreationDate="2015-07-10T04:06:19.367" UserId="609" />
  <row Id="6790" PostId="6412" Score="0" Text="Thanks for your attention. I will try what you suggested and will get back to you." CreationDate="2015-07-10T06:11:04.710" UserId="9323" />
  <row Id="6791" PostId="694" Score="0" Text="Does any of this packages scale like h2o deep learning?&#xA;As far as I know lasagne doesn't.Theano does support GPU so as any library basing on it,but does any of them support mapreduce or spark." CreationDate="2015-07-09T21:20:08.200" UserId="10327" />
  <row Id="6792" PostId="6225" Score="0" Text="By the way, I am finding more than a thousand categories from Facebook." CreationDate="2015-07-10T09:36:09.710" UserId="982" />
  <row Id="6793" PostId="6366" Score="0" Text="I need an suggestion, if we use this logs processing as web service for end user like my employee wants to give 100-200 gb for processing so Can I provide web service through which he/she can access this ??" CreationDate="2015-07-10T10:13:55.187" UserId="10210" />
  <row Id="6794" PostId="6391" Score="0" Text="Thanks for the comment, @MrMeritology! I found that really useful!" CreationDate="2015-07-10T11:25:26.440" UserId="9225" />
  <row Id="6795" PostId="6421" Score="0" Text="I'm mainly interested in the area of deep learning.My company can't put their data on EC2 due to security reason hence GPU isn't an option for me.I use h2o deep learning on clusters,however it lacks some feature I think,such as setting learning rate for different layers.Since it is written in Java and my java is bit rusty  I was looking for any open source scalable deep learning written in python.Yes pyspark gives access to clusters,but even if I build something on it'll work on top of scala layer and not on native python code.Hence I am on a search for an ongoing python project." CreationDate="2015-07-10T11:36:24.000" UserId="10327" />
  <row Id="6796" PostId="6374" Score="0" Text="Yes, I see. I tried several libs now, FP-Growth as well and it's indeed faster. Still playing around with different libs - Orange seems not that awesome to me after I wrapped my head more around it." CreationDate="2015-07-10T13:23:54.443" UserId="10513" />
  <row Id="6797" PostId="6375" Score="0" Text="each &quot;point&quot; is and (x, y), yes? so your dimensionality is 2312*2 = 4624? what classification algos are you using now?" CreationDate="2015-07-10T15:09:00.657" UserId="10587" />
  <row Id="6798" PostId="6412" Score="0" Text="Great. If you think that has answered your question, please mark at as such. In any case, best of luck." CreationDate="2015-07-10T15:09:50.777" UserId="10587" />
  <row Id="6799" PostId="6424" Score="0" Text="Looks like what I'm trying to do. Thanks for the tip!" CreationDate="2015-07-10T15:53:44.027" UserId="10596" />
  <row Id="6800" PostId="6412" Score="0" Text="I think, I should ensemble SMO with CART according to your suggestion. I edited my question." CreationDate="2015-07-10T19:48:35.977" UserId="9323" />
  <row Id="6801" PostId="6412" Score="0" Text="Few things: 1) Why not try each combination of three algorithms you have?, 2) for that matter, why aren't you adding other algorithms? Random Forest, KNN come to mind as strong for difficult problems, 3) At some point stacking will not help - you'll need more examples, better features, and hyperparameter tuning. That's going beyond scope of this post and more than someone can answer in a comment. Stacking, unfortunately, is not an solution in itself - merely an augmentation to the basics." CreationDate="2015-07-10T22:29:31.323" UserId="10587" />
  <row Id="6802" PostId="6412" Score="0" Text="Actually these are the algorithms which I had the best results and I tried each combination of them. I didn't want to just say I tried the everything and these are the best results. I wanted to add some logical explanations, like I select Logistic Regression because it have ... properties, which SMO doesn't have, to improve SMO. Thanks for your answer." CreationDate="2015-07-10T22:42:03.117" UserId="9323" />
  <row Id="6803" PostId="6414" Score="0" Text="Define scalable. What exact requirements do you have? Do you only care about feeding data into models to get predictions or is training them a concern as well? I also wouldn't make the assertion that sci-kit isn't scalable, it most certainly is for quite a few (if not most) cases." CreationDate="2015-07-10T23:52:41.367" UserId="947" />
  <row Id="6804" PostId="6391" Score="0" Text="While I'm sure you you'll be able to understand this (pretty simple) neural network, if interpretability is a relatively large concern then you probably shouldn't be using a neural network in the first place. Is there a specific reason you selected one over other algorithms?" CreationDate="2015-07-11T00:06:31.173" UserId="947" />
  <row Id="6805" PostId="6398" Score="0" Text="I suggest taking a MOOC to learn a little about model tuning and assessment. Two options are Andrew Ng's ML Coursera course or the tutorials on Kaggle.  But this question is too green and vague to answer with anything other than a tutorial on the basics of ML." CreationDate="2015-07-11T00:33:34.890" UserId="9420" />
  <row Id="6806" PostId="6414" Score="0" Text="@David well, some of the scikit learn models such as random forest can run in parallel and can use all the cores in a node,but can't extend beyond that.To me scalability means the capacity to run on multiple nodes.Also I think it's the training that matters as that consumes most memory(specially in the case of text mining)." CreationDate="2015-07-11T02:10:29.980" UserId="10327" />
  <row Id="6807" PostId="694" Score="0" Text="h2o doesn't even use the GPU yet so it's hardly scalable." CreationDate="2015-07-11T06:51:22.540" UserId="381" />
  <row Id="6808" PostId="6391" Score="0" Text="Yes, @David! I would like to learn using this kind of model. I never used them in my job and I'm studying that just for fun. Do you have any idea about interpreting the plot?" CreationDate="2015-07-11T09:27:26.707" UserId="9225" />
  <row Id="6809" PostId="6433" Score="0" Text="Please be a little more specific about what exactly you mean by &quot;connect&quot;.  Do you mean that you want to train all 3 NNs separately and then combine their result on any input data to get the result ? If so take a look at [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)." CreationDate="2015-07-11T14:38:32.473" UserId="10446" />
  <row Id="6810" PostId="6381" Score="0" Text="Thank you for this considered reply - especially the 'Stepping back...' paragraph.  I'm aware that various operations on time series constitute groups. The rub is &quot;finding other time series&quot;. Rather than rely upon serendipity, I'd like to establish how to find these &quot;other time series&quot;  by constructing candidates systematically.  I expect the effective strategies to be domain-specific.  I am focused on time domain over frequency domain approaches because the time series which interest me most, right now, are finite length and non-cyclical." CreationDate="2015-07-11T15:47:36.983" UserId="3328" />
  <row Id="6811" PostId="6414" Score="1" Text="Ok, that's helpful information. Scalable typically refers to being able to handle a large amount of input data for making predictions, since training is generally a one time cost and not performed on that much data (relatively speaking). It seems like you're really interested in using a natively distributed system, which spark makes a lot of sense for." CreationDate="2015-07-11T16:04:29.477" UserId="947" />
  <row Id="6813" PostId="6431" Score="0" Text="Thanks for your reply and suggestions. I will try to provide a good sample in near future." CreationDate="2015-07-11T19:36:54.190" UserId="10318" />
  <row Id="6814" PostId="6433" Score="0" Text="yes, that is what I mean" CreationDate="2015-07-11T19:48:24.643" UserId="10584" />
  <row Id="6815" PostId="6434" Score="0" Text="Neither GD or direct solve is feasible as it would be take too many computational resources when training a convolutional neural network (CNN). While it therefore did not solve my problem, your answer provided a nice insight to the issue." CreationDate="2015-07-11T22:18:16.937" UserId="3044" />
  <row Id="6816" PostId="6434" Score="0" Text="Wouldn't GD also be affected by outliers, although to a lesser degree?" CreationDate="2015-07-11T22:22:06.347" UserId="3044" />
  <row Id="6817" PostId="6366" Score="0" Text="It sounds like you need to develop some sort of web service by yourself. Otherwise if you have budget, try some SaaS based data platform like [Databricks](https://www.databricks.com/), they are the new kid in town but I have heard great things about them." CreationDate="2015-07-11T22:27:26.303" UserId="708" />
  <row Id="6818" PostId="6434" Score="0" Text="Affected by, yes, but to a lesser degree i.e. less stochastic.  In addition to just turning down the learning coefficient, you can prime lots of descenders and use the most common minima." CreationDate="2015-07-12T00:32:55.410" UserId="9420" />
  <row Id="6819" PostId="6434" Score="0" Text="How can you simultaneously have a sparse dataset for which removing outliers isn't feasible and a problem with so much data that resources are tight and SGD is needed? Is it that you have a non normal dataset? Then try some normalizing transformations like log, square root, or inverse hyperbolic tangent." CreationDate="2015-07-12T02:15:30.597" UserId="9420" />
  <row Id="6820" PostId="6432" Score="0" Text="thanks that explanation makes sense. Yes I am aware that machine learning algorithms do not always produce output which is in line with actual probabilities which is why I use a calibration plot on an independent data set to assess the quality of the output &quot;probabilities&quot; versus observed events." CreationDate="2015-07-12T06:09:06.797" UserId="2817" />
  <row Id="6821" PostId="6434" Score="0" Text="The ML course doesn't cover mini-batches, which would help here as a compromised between fully online SGD and full batch learning. Also using dropout for regularisation may help." CreationDate="2015-07-12T19:35:12.177" UserId="836" />
  <row Id="6822" PostId="5375" Score="0" Text="Related: [How hard is debugging with Theano?](https://www.quora.com/How-hard-is-debugging-with-Theano)" CreationDate="2015-07-12T19:49:09.137" UserId="843" />
  <row Id="6823" PostId="6378" Score="0" Text="you need to give input on how did you do it and what are the number of data elements to help others offer help." CreationDate="2015-07-13T01:48:34.840" UserId="10638" />
  <row Id="6824" PostId="253" Score="0" Text="Look at this free e-book and it tries to answer your question.http://www.oreilly.com/data/free/files/analyzing-the-analyzers.pdf" CreationDate="2015-07-13T01:49:48.203" UserId="10638" />
  <row Id="6825" PostId="6446" Score="1" Text="does this not belong on stack overflow?" CreationDate="2015-07-13T13:37:51.517" UserId="8953" />
  <row Id="6826" PostId="694" Score="1" Text="@Emre: Scalable is different to high performance. It typically means you can solve larger problems by adding more resources of the same type you have already. Scalability still wins out, when you have 100 machines available, even if your software is 20 times slower on each of them . . . (although I'd rather pay the price for 5 machines and have benefits of both GPU and multi-machine scale)." CreationDate="2015-07-13T15:25:45.183" UserId="836" />
  <row Id="6827" PostId="6095" Score="0" Text="This is true for your test set, but not training. Oversampling is necessary for problems like these since the vast majority of instances will not be fraud." CreationDate="2015-07-13T15:32:04.297" UserId="947" />
  <row Id="6829" PostId="694" Score="0" Text="So use multiple GPUs...nobody uses CPUs for serious work in neural networks. If you can get Google-level performance out of a good GPU or two, just what are you going to do with a thousand CPUs?" CreationDate="2015-07-13T16:11:09.433" UserId="381" />
  <row Id="6830" PostId="6217" Score="0" Text="Do you know anything about the nonlinearity of the model?  Though it may be too complex to simulate, knowing that it is at most made up of degree 3 polynomials restricts the feature engineering significantly e.g. you could add all 3rd degree polys and then PCA it back down to 3D." CreationDate="2015-07-13T16:20:07.693" UserId="9420" />
  <row Id="6831" PostId="6438" Score="0" Text="I think the question needs a bit of clarification. Do I understand correctly, that you give the user a set of polygons that can be transformed with a simple transformation. Now, is for each of the polygons only a set of outputs possible, or is there a global space defined as legal transformation outcomes?" CreationDate="2015-07-13T17:25:00.237" UserId="10655" />
  <row Id="6832" PostId="6447" Score="0" Text="That should work! are you sure there's no typo, say in `newdf`?" CreationDate="2015-07-13T17:43:10.307" UserId="5055" />
  <row Id="6833" PostId="6447" Score="0" Text="@EhsanM.Kermani - Thanks for the reply, My code is similar one, as i could not copy that from my office network, i pasted this one. I did similar to this." CreationDate="2015-07-13T17:50:56.920" UserId="9793" />
  <row Id="6834" PostId="6447" Score="0" Text="This is strange, b/c when you have the coefficients in this particular case then prediction is a simple algebra, but returning 'NA's should indicate a problem with new data not the loaded model!" CreationDate="2015-07-13T18:04:10.193" UserId="5055" />
  <row Id="6835" PostId="6447" Score="0" Text="@EhsanM.Kermani - Thanks again. The model works if i use directly in the same programme. (means, build model and predict immediately for a new data). Where as I get into this problem only when i save the model, load again and then use it for prediction." CreationDate="2015-07-13T18:48:20.063" UserId="9793" />
  <row Id="6836" PostId="6217" Score="0" Text="I have discussed with a statistician friend of mine who suggested using kernel PCA on the **derivative** of my data, since I'm looking for slopes. Would taking the derivative count as &quot;feature engineering&quot;?" CreationDate="2015-07-13T18:49:26.167" UserId="10346" />
  <row Id="6837" PostId="6446" Score="0" Text="@aeroNotAuto - I apologize if i had posted in wrong place. I thought confusionMatrix is something related to data science and hence posted here." CreationDate="2015-07-13T19:21:52.870" UserId="9793" />
  <row Id="6838" PostId="6414" Score="0" Text="While checking on python xgboost I found the existence of this open source project https://github.com/dmlc/rabit that helps create scalable machine learning program.Should be worth exploring." CreationDate="2015-07-14T06:12:25.237" UserId="10327" />
  <row Id="6839" PostId="6436" Score="0" Text="Multipliers are given or you can choose them? All rows are taken or only some of them?" CreationDate="2015-07-14T07:08:58.127" UserId="108" />
  <row Id="6840" PostId="6447" Score="0" Text="@EhsanM.Kermani - as you said, this has worked. The issue which was preventing from working is, one of the old models which was still in the R's Cache memory. I have removed that using rm=list(ls)) which has solved the problem. Thanks you very much for your help." CreationDate="2015-07-14T08:41:55.277" UserId="9793" />
  <row Id="6841" PostId="6088" Score="0" Text="Have you seen this question:https://datascience.stackexchange.com/questions/6200/what-are-the-basic-approaches-for-balancing-a-dataset-for-machine-learning/6249#6249" CreationDate="2015-07-14T08:44:52.227" UserId="7980" />
  <row Id="6842" PostId="6088" Score="0" Text="You might also consider layering a cost-matrix over your classification algorithm, as there is an imbalance in the penalty for making different errors in classification." CreationDate="2015-07-14T08:47:24.423" UserId="7980" />
  <row Id="6843" PostId="6088" Score="0" Text="Is there a particular reason you want to use Adaboost in this context ?" CreationDate="2015-07-14T08:48:42.773" UserId="7980" />
  <row Id="6844" PostId="6436" Score="0" Text="@rapaio multipliers are not given and they are to be calculated. Arbitrary numbers of rows can be chosen. The algorithm would ideally generate an indefinite number of solutions: some may have 4 rows, others may have 200, and it's even feasible that the algorithm finds a solution using all rows (probably with very small multipliers!). Thanks!" CreationDate="2015-07-14T11:24:27.310" UserId="10624" />
  <row Id="6845" PostId="6453" Score="0" Text="Have you considered using machine learning to calculate the variable directly from the observables ?" CreationDate="2015-07-14T14:12:24.287" UserId="7980" />
  <row Id="6846" PostId="6359" Score="0" Text="Outliers would be points that in deed don't have correspondence in all datasets. Still I was looking for a way to constrain the clustering for enforcing memberships from all datasets, which in case of the outliers would be too costly. I agree that using a fuzzy clustering might be a good idea, because defining cluster membership could then be assigned using some trade off criterium which on this postprocessing level might be easier to formulate. Thanks!" CreationDate="2015-07-14T14:17:08.993" UserId="10463" />
  <row Id="6847" PostId="6447" Score="0" Text="Glad you solved the issue :) cheers" CreationDate="2015-07-14T14:17:26.613" UserId="5055" />
  <row Id="6848" PostId="6453" Score="0" Text="I haven't but it is also not possible as I don't have the possibility to change the calculation." CreationDate="2015-07-14T14:19:57.587" UserId="10669" />
  <row Id="6849" PostId="6453" Score="0" Text="So is it the case you have , observables, true output and error available? You could play around and see if you can get a better result with smaller errors  than the existing  &quot;calculation&quot;. Failing that you can use machine learning to build a mapping directly between observables and error. Without any idea of the form of the data and errors it's a challenge to be more precise." CreationDate="2015-07-14T15:02:20.557" UserId="7980" />
  <row Id="6850" PostId="6436" Score="0" Text="the a trivial random algorithm would be like the following: peek some columns at random, peek some rows at random, compute the sum for each column for specified rows, find factors to put each column sum in the desired range; do other iteration. On the other side I agree that this question does not belong to data science." CreationDate="2015-07-14T16:06:14.307" UserId="108" />
  <row Id="6851" PostId="6208" Score="0" Text="Add a noise term, and model it to account for the types of errors you observe." CreationDate="2015-07-14T21:19:03.333" UserId="381" />
  <row Id="6852" PostId="6459" Score="3" Text="I think you are *grossly* underestimating the challenge of AI music composition, and you are trying to take on music recognition at the same time.  I also think you (and many other people) have an inflated opinion of what ML (including recurrent neural networks) can do." CreationDate="2015-07-15T05:27:45.877" UserId="609" />
  <row Id="6854" PostId="6459" Score="0" Text="Have you considered how you will deal with instruments that produce cords, such as piano, guitar and other stringed instruments?" CreationDate="2015-07-15T10:10:26.687" UserId="7980" />
  <row Id="6856" PostId="6456" Score="0" Text="I note that your probabilities do not sum to one, is that the case ?" CreationDate="2015-07-15T10:21:55.870" UserId="7980" />
  <row Id="6857" PostId="6462" Score="0" Text="create a regression where a, b, and c predict x for every state, then apply it to every zip code.  but your results will surely be biased by whatever differences there are in the relationship between these variables at the state vs local levels." CreationDate="2015-07-15T11:31:48.000" UserId="9924" />
  <row Id="6858" PostId="6453" Score="0" Text="I have the observables. Then the output is calculated in a &quot;black box&quot;. The output is not exact. It has an error. I have training data which has true error data. Now I want to find the relation between the input observables and the error of the output. It's really not about the &quot;output&quot;, it's about the error." CreationDate="2015-07-15T12:30:52.273" UserId="10669" />
  <row Id="6859" PostId="6459" Score="0" Text="[Deep Dreaming](https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB) for audio?" CreationDate="2015-07-15T12:39:43.553" UserId="836" />
  <row Id="6860" PostId="6464" Score="0" Text="Can you clarify what you're asking? what have you tried and what is the result? what's the problem?" CreationDate="2015-07-15T14:57:37.437" UserId="21" />
  <row Id="6861" PostId="6464" Score="0" Text="There is no problem except this is my first task in machine learning. I just want to ensure that I have choosen right algorithm and, for example, should I normalize features? Also I am still confused whether it is correct to combine categorical and continuous features." CreationDate="2015-07-15T15:12:14.460" UserId="10694" />
  <row Id="6862" PostId="6453" Score="0" Text="Just to be clear, if you could make another black box, without error, that wouldn't be of use ?" CreationDate="2015-07-15T15:42:32.453" UserId="7980" />
  <row Id="6863" PostId="6456" Score="0" Text="yes, maybe probabilities isn't the word I should use, frequencies is a better description of the numbers." CreationDate="2015-07-15T16:09:57.470" UserId="10584" />
  <row Id="6864" PostId="6456" Score="0" Text="I wasn't certain if you had automated the generation of the training data by sampling from a distribution based on the relative frequencies of your data. That approach is nicely general in that it will work with any machine learning algorithm , some of which may perform better than a neural network. You could build your own neural network code which used relative frequencies, but that seems like hard work when resampling will do the job for you. Of course the number of training epochs affects how many times samples are seen, but normally they apply equally to all samples." CreationDate="2015-07-15T16:34:34.737" UserId="7980" />
  <row Id="6866" PostId="6459" Score="1" Text="This is such a tough problem that you'll have to do a lot of paper sighting. I don't think it's possible to sum everything up that you need to know in a few paragraphs (since there are no established best practices)" CreationDate="2015-07-15T17:22:22.137" UserId="5316" />
  <row Id="6870" PostId="6468" Score="0" Text="What you say is true: In the second case I am more confident that the classification is C1 or C2 however I am less confident that is C1 (because it is almost equally probable that it is C2).  I have added another example into the main question to elaborate this situation more clearly." CreationDate="2015-07-15T18:55:35.497" UserId="10701" />
  <row Id="6874" PostId="6472" Score="0" Text="Try shift+enter." CreationDate="2015-07-15T22:17:11.193" UserId="381" />
  <row Id="6875" PostId="6467" Score="0" Text="[Confidence intervals for cross-validated statistics](http://stats.stackexchange.com/questions/69831/confidence-intervals-for-cross-validated-statistics)" CreationDate="2015-07-15T22:19:45.947" UserId="381" />
  <row Id="6876" PostId="6257" Score="1" Text="This is a studied problem: [tag recommendation](https://scholar.google.com/scholar?q=tag+recommendation)." CreationDate="2015-07-15T22:31:28.813" UserId="381" />
  <row Id="6878" PostId="6473" Score="0" Text="That is great , tnks!" CreationDate="2015-07-16T07:26:45.977" UserId="10094" />
  <row Id="6879" PostId="6479" Score="0" Text="I looked in the tag-list for something a little more descriptive - surely part of data-science includes finding and making use of publically (or commercially available) data sources - sharing and collaborating on how/where to find such sources, at least to me, would seem to be a common type of question - befitting a tag of its own!" CreationDate="2015-07-16T12:36:24.750" UserId="10399" />
  <row Id="6880" PostId="6479" Score="0" Text="Your question should probably be migrated to  http://opendata.stackexchange.com/ which is designed for exactly  what you need." CreationDate="2015-07-16T12:48:29.543" UserId="7980" />
  <row Id="6886" PostId="6476" Score="0" Text="I'd link to the paper and elaborate what you are asking for here." CreationDate="2015-07-16T20:56:37.950" UserId="21" />
  <row Id="6887" PostId="6462" Score="0" Text="The method you have suggested seems basically right, but you are likely loosing a lot of information during the aggregation and gaining a lot of noise as you scale back down to the zip level.  Since projecting from state down to zip is a big jump, maybe you should try simply leaving one zip out.  e.g. For `n` zip codes per state regress on `n-1` zip codes and on `n` zip codes. Then the difference in the `n` and `n-1` target variable will be the target variable for the zip code that was left out.  This way the regression will be closer to the region it was trained on." CreationDate="2015-07-16T21:45:25.930" UserId="9420" />
  <row Id="6888" PostId="6462" Score="0" Text="This will also ensure that the aggregate totals are equal to the state totals, so effectively constrains the problem." CreationDate="2015-07-16T21:55:43.623" UserId="9420" />
  <row Id="6889" PostId="6471" Score="0" Text="Yep, I think this the correct direction.  The information gain is a nice measure of &quot;purity&quot; of the choice, which is what I was looking for in &quot;confidence&quot;.  Thanks." CreationDate="2015-07-16T23:03:18.977" UserId="10701" />
  <row Id="6890" PostId="266" Score="0" Text="In addition to learning Machine Learning by Andrew Ng you can try with some courses in data science signature track in kaggle.Also a quick way to learn practical machine learning is to take part in following machine learning competition at kaggle,as that has nice guide material on how to do feature selection,data munging and building final model in R and in Python.https://www.kaggle.com/c/titanic/details/getting-started-with-python" CreationDate="2015-07-17T00:49:46.610" UserId="10327" />
  <row Id="6891" PostId="6479" Score="0" Text="Thanks, yes looking at it, opendata is probably the place to go - what's the protocol for doing this, do I take this one down and post it back up in the other place, or wait for a mod to do it?" CreationDate="2015-07-17T07:54:45.103" UserId="10399" />
  <row Id="6892" PostId="6479" Score="0" Text="Linked to [here](http://opendata.stackexchange.com/questions/5640/where-can-i-get-a-list-of-atm-identifiers-that-i-can-map-to-geographic-location)" CreationDate="2015-07-17T08:00:23.097" UserId="10399" />
  <row Id="6893" PostId="6479" Score="0" Text="If you are lucky a nice moderator will migrate it :)" CreationDate="2015-07-17T10:01:54.850" UserId="7980" />
  <row Id="6894" PostId="6484" Score="0" Text="I see that they are trying to get rules out of minimal episodes but if given a event stream like 'G,E,A,A,B,D,H,G,F,D,G,F,C,F,D,H,H,H,G,F,D,G,E,B,D,H,H,H,G,F,C'  where each event occurs with 1 time unit gap then what are the possible episodes in this stream?" CreationDate="2015-07-17T10:55:36.107" UserId="10717" />
  <row Id="6896" PostId="6484" Score="0" Text="Then this doesn't fit within the definition of finding episodes as the events in an episode do not occur at regular frequency e.g. that is the definition of an episode.  The paper you referenced specifically has inhomogeneous event frequency.  It looks like a sequencing problem and you should employ sequencing methods." CreationDate="2015-07-17T17:12:00.150" UserId="9420" />
  <row Id="6897" PostId="6493" Score="0" Text="this is such a great answer. thank you" CreationDate="2015-07-17T19:48:53.047" UserId="10761" />
  <row Id="6898" PostId="6495" Score="0" Text="Can you please describe how this question relates specifically to data-mining or data science ?" CreationDate="2015-07-18T15:21:16.570" UserId="7980" />
  <row Id="6899" PostId="6495" Score="0" Text="Hi image_doctor, I could not find the best &quot;community&quot; to ask this question so I chose Data Science because one has to use tools to perform Data Science in the first place. Would you be able to suggest other places I could post this question? That would be very helpful given that I'm new to this place. Thank you." CreationDate="2015-07-19T04:37:41.117" UserId="10767" />
  <row Id="6900" PostId="337" Score="1" Text="&quot;R has a better community for [...] learning&quot; - I guess this highly depends on the type of learning. How much is going on with neural networks (arbitrary feed-forward architectures, CNNs, RNNs) in R?" CreationDate="2015-07-19T14:41:19.090" UserId="8820" />
  <row Id="6901" PostId="6498" Score="0" Text="Can you give an example showing how using XML would affect the annotations?" CreationDate="2015-07-20T05:22:12.170" UserId="843" />
  <row Id="6902" PostId="6495" Score="0" Text="@Raptor StackOverflow is the best place." CreationDate="2015-07-20T05:23:16.260" UserId="843" />
  <row Id="6903" PostId="6495" Score="0" Text="Tough call on questions like this. StackExchange sites overlap at the edges, and I think this could be considered a simple programming question. Matlab is also on-topic for data science. I don't think it's wrong to post here. That said I suspect SO will get you answers faster, and there isn't one here yet. In this case I'd migrate, but not obviously off topic." CreationDate="2015-07-20T07:31:17.120" UserId="21" />
  <row Id="6904" PostId="6508" Score="0" Text="To answer your question about the knee in the variance case, it looks like it's around 6 or 7, you can imagine it as the break point between two linear approximating segments to the curve . The shape of the graph is not unusual, % variance will often asymptotically approach 100%. I'd put k in your BIC graph as a little lower , around 5." CreationDate="2015-07-20T15:12:26.903" UserId="7980" />
  <row Id="6905" PostId="6510" Score="0" Text="Questions explicitly seeking opinions are generally discouraged on stack exchange sites, http://datascience.stackexchange.com/help/dont-ask , perhaps you could rephrase the question to require exemplars in support of users experience ? Or seek a theoretical basis for one position or the other." CreationDate="2015-07-20T15:18:49.560" UserId="7980" />
  <row Id="6906" PostId="6510" Score="1" Text="Random Forests are less likely to overfit the other ML algorithms, but cross-validation (or some alternatively hold-out form of evaluation) should still be recommended." CreationDate="2015-07-20T15:53:07.157" UserId="947" />
  <row Id="6907" PostId="6510" Score="0" Text="I think you sholud ask that question on statistician SO: http://stats.stackexchange.com/" CreationDate="2015-07-20T16:01:12.860" UserId="5224" />
  <row Id="6908" PostId="6511" Score="0" Text="What is cool or boring for you ?" CreationDate="2015-07-20T16:02:12.503" UserId="7980" />
  <row Id="6909" PostId="6518" Score="0" Text="Thanks. I was thinking of R. Any reason for python over R?" CreationDate="2015-07-20T17:42:12.913" UserId="10522" />
  <row Id="6910" PostId="6518" Score="1" Text="No specific reason for python. R vs. Python is a hotly debated topic.  R has been around for a long time so has tons of packages.  Python is more user friendly and is the future of data science.  Python will make you happier and will help you get things done faster.  If you really need R then you can call R from within Python.  Someone will probably respond and tell you why R is best, but I like python.  Take a look at all of the following: pandas, scikit-learn, numpy, matplotlib, ipython notebook.  There's a great book &quot;Python for Data Analysis&quot; and the scikit learn user guide is fantastic." CreationDate="2015-07-20T17:50:59.883" UserId="9420" />
  <row Id="6911" PostId="799" Score="0" Text="Took me an year, but I decided to accept this answer. I still don't know what that bitstream is, but I probably won't find out. It does have a nice pattern, though!" CreationDate="2015-07-20T20:12:03.117" UserId="2604" />
  <row Id="6912" PostId="6510" Score="0" Text="I would like to second @David...one way or another, you're going to be doing cross validation." CreationDate="2015-07-20T20:37:23.120" UserId="9424" />
  <row Id="6914" PostId="6522" Score="0" Text="Thank you. That's exactly what I was looking for." CreationDate="2015-07-20T21:18:11.153" UserId="10799" />
  <row Id="6915" PostId="6508" Score="0" Text="but I should have (more or less) the same results in all the methods, right?" CreationDate="2015-07-21T08:38:30.263" UserId="989" />
  <row Id="6916" PostId="6508" Score="0" Text="I don't think I know enough to say. I doubt very much that  the three methods are mathematically equivalent with all data, otherwise they wouldn't exist as distinct techniques, so the comparative results are data dependent. Two of the methods give numbers of clusters that are close, the third is higher but not enormously so. Do you have a priori information about the true number of clusters ?" CreationDate="2015-07-21T08:59:10.210" UserId="7980" />
  <row Id="6917" PostId="6529" Score="0" Text="Thanks image_doctor! This is quite a help." CreationDate="2015-07-21T10:02:12.903" UserId="10810" />
  <row Id="6918" PostId="6508" Score="0" Text="I'm not 100% sure but I expect to have from 8 to 10 clusters" CreationDate="2015-07-21T10:29:22.837" UserId="989" />
  <row Id="6919" PostId="6518" Score="0" Text="And I thought Julia was the future of data science ;)" CreationDate="2015-07-21T12:44:16.980" UserId="7980" />
  <row Id="6920" PostId="4876" Score="0" Text="This is a good way to show your results.&#xA;But sometimes, if you want to calculate many things it's better to store the table in &quot;long format&quot;.  Google it, long and wide format.  You can use cast, melt, reshape..." CreationDate="2015-07-21T15:55:01.177" UserId="10815" />
  <row Id="6921" PostId="6531" Score="0" Text="Please add some additional information to your question in order to elicit a better response and avoid being down voted.  In particular 1) how big is your dataset? 2) How many features do you have? 3) How many test cases do you have?  Have you searched for other sources for this information?  Simply googling &quot;k-means clustering python&quot; returns the result that most people will steer you towards initially: `mini-batch k-means` using `scikit-learn`.  If you need better parallel scalability then we can steer you toward another package based on the answer to 1,2, and 3." CreationDate="2015-07-21T17:44:29.127" UserId="9420" />
  <row Id="6922" PostId="6529" Score="0" Text="Implementation of newton's method also exists in [scipy](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.newton.html) and the source is [here](https://github.com/scipy/scipy/blob/v0.14.0/scipy/optimize/zeros.py#L45)" CreationDate="2015-07-21T17:50:39.420" UserId="10446" />
  <row Id="6923" PostId="6528" Score="0" Text="Newton's method can apply in a lot of contexts, and EM is really a whole class of algorithms. I think you need to narrow down your purpose." CreationDate="2015-07-21T18:05:22.000" UserId="21" />
  <row Id="6924" PostId="6536" Score="0" Text="but how would you store intermediate calculations in long format? (such as averages, cumsums or lagged differencies)" CreationDate="2015-07-21T19:34:33.463" UserId="10815" />
  <row Id="6925" PostId="6536" Score="0" Text="Indeed. You have to provide some more information with what data you want to do which calculations and how many individuals you excpect as well as how sparse you except an individual's features to be." CreationDate="2015-07-21T20:52:01.573" UserId="10314" />
  <row Id="6926" PostId="6533" Score="0" Text="Lists, as inspired by Lisp,  are a very powerful data structure. They can store arbitrarily deep and ragged data structures of mixed types. Take a look at Mathematica or  perhaps JSON for some inspiration." CreationDate="2015-07-21T22:23:46.713" UserId="7980" />
  <row Id="6927" PostId="6528" Score="0" Text="Hi Sean, I got the answer I was looking for. I know it might be too broad but this is exactly the thing I was looking for." CreationDate="2015-07-22T05:37:58.867" UserId="10810" />
  <row Id="6928" PostId="6539" Score="0" Text="You might be better off asking this question on another stack exchange site, this is  not the best place for purely database related queries." CreationDate="2015-07-22T05:49:05.973" UserId="7980" />
  <row Id="6931" PostId="6536" Score="0" Text="This week my data is a table with 2400 medical variables (such as diastolic blood pressure, results from biochemical analysis...)  most of them real numbers, some missings, subdived in groups (and repeated several times through time), and almost 30000 individuals." CreationDate="2015-07-22T09:20:43.470" UserId="10815" />
  <row Id="6932" PostId="6546" Score="0" Text="Have you checked the [scaladoc](https://spark.apache.org/docs/1.4.1/api/scala/index.html#org.apache.spark.sql.DataFrame)? It has an example for average and max: `.agg(avg(people(&quot;salary&quot;)), max(people(&quot;age&quot;)))`. With sorting you can probably find (using `skip` and `take`) the percentiles, but there might be faster options." CreationDate="2015-07-22T16:47:07.370" UserId="1359" />
  <row Id="6933" PostId="6546" Score="0" Text="I had seen this previously in the scaladocs.  When I try to use them like the example I receive and error `not found: value avg` and `not found: value max`" CreationDate="2015-07-22T18:46:14.193" UserId="10832" />
  <row Id="6934" PostId="6546" Score="0" Text="What are your imports? It might be easier to help if there is an example and you describe what were the problem." CreationDate="2015-07-22T18:48:26.920" UserId="1359" />
  <row Id="6935" PostId="6536" Score="0" Text="It sounds like space shouldn't be an issue - even in long form, we're talking max, 72 MM rows which a DB on a reasonably beefy server should handle. You probably want to draw timeseries for repeated measurements which should definitely be &quot;long&quot; form - a field for the timestamp and a field that tells you &quot;what&quot;, e.g., &quot;diastolic BP&quot;, and a field for the value. Not sure what other analysis you're going to want to do. If you want to do some kind of regression, you'll want to transform your data such that one row represents one patient." CreationDate="2015-07-22T18:56:55.960" UserId="5010" />
  <row Id="6936" PostId="6546" Score="0" Text="`import org.apache.spark.rdd.RDD`  &#xA;`import org.apache.spark.sql.SQLContext`  &#xA;`import org.apache.spark.{SparkConf, SparkContext}`  &#xA;`import org.joda.time.format.DateTimeFormat`" CreationDate="2015-07-22T19:02:33.493" UserId="10832" />
  <row Id="6937" PostId="6546" Score="0" Text="The following [test](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala) might help start using DataFrame functions. It seems you have to import the `org.apache.spark.sql.functions._` too. (BTW.: I think the additional information is better added to the question itself and it is enough to add a comment after edit.)" CreationDate="2015-07-22T19:11:27.673" UserId="1359" />
  <row Id="6938" PostId="6555" Score="0" Text="Could it be, that the following would work? `$ ipython --profile=pyspark notebook`? It might be that the problem was only with the order of the arguments." CreationDate="2015-07-23T06:17:06.870" UserId="1359" />
  <row Id="6939" PostId="6553" Score="1" Text="Are the books you are using solely on the topic of democracy, if not, might not your distance metric get swamped by larger differences between the books contents? This is a side effect of your problem being in a very high dimensional space and being touched by the hand of the curse of dimensionality.  Perhaps taking only a small region of text around the word of interest would help, but it is still a problem with significant dimension." CreationDate="2015-07-23T06:41:46.460" UserId="7980" />
  <row Id="6940" PostId="6553" Score="0" Text="@image_doctor That's a excellent question. Technically, I expected &quot;democracy&quot; to be mentioned in different contexts across books. But you mean that the other words will &quot;move away&quot; the mentions of democracy because they themselves are not well-placed relative to the content of other books?" CreationDate="2015-07-23T08:08:00.993" UserId="5279" />
  <row Id="6941" PostId="6553" Score="1" Text="Yes that's the essence of that. here goes with a probably ill thought out metaphor. Imagine chapters of books being represented by colours. And a book a a whole represented as the mixture of all the colours of the chapters. A book on democracy in western europe would likely end up with an overall reddish hue as the sum of it's chapters. If we represent tourism by blue, a book on Tourism in Cuba, with a sole chapter on democracy and it's influence on economic development, would have a strong blue hue. So the two books would appear very different when viewed as a whole." CreationDate="2015-07-23T08:38:22.250" UserId="7980" />
  <row Id="6942" PostId="6553" Score="1" Text="That's the more accessible way of saying  what a data scientist would phrase as the  vectors for the two books will be a long way apart in feature space and so will appear quite dissimilar. It's really hard to quantify beforehand how many examples you will need without playing with the data, but language is subtle and layered so you will probably want as many as you can get .... and maybe more. Ultimately you won't know until you try. It's not a concrete answer, but unless someone direct experience of doing a similar thing, it's probably the best you will get." CreationDate="2015-07-23T08:43:59.097" UserId="7980" />
  <row Id="6943" PostId="6551" Score="0" Text="Thanks for your time, but please see my first bullet of &quot;Resources&quot;; I have reviewed this option, and looking for something that meets my &quot;Constraints&quot;." CreationDate="2015-07-23T13:41:22.267" UserId="1406" />
  <row Id="6944" PostId="6549" Score="0" Text="Thank you for your efforts on the deep discussion. Though programming this doesn't seem too bad (quite interesting, I may say, to deep dive into the algorithms), I am curious of packages that already are available. Do you know of anything that exists that is simple to install? Note this is not the same as simple to implement, which I understand cannot be guaranteed. If I can get my environment functional, I believe I can finesse it based on examples for my task." CreationDate="2015-07-23T13:47:01.377" UserId="1406" />
  <row Id="6945" PostId="6552" Score="0" Text="Thanks! I haven't considered this package yet - I will add it to the list of candidates. To clarify, when you say &quot;beyond version 3 it has similar module available in python as well&quot;, do you know if h2o's anomaly detection module (beyond ver 3) is available in Python, or some other module?" CreationDate="2015-07-23T13:52:16.183" UserId="1406" />
  <row Id="6946" PostId="6552" Score="1" Text="@ximik Well,I revisited the python documentation of their latest version 3.0.0.26(http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/26/docs-website/h2o-py/docs/index.html) and it seems like h2o.anomaly is not yet available unlike its R api.I've raised the question in their google group(https://groups.google.com/forum/#!topic/h2ostream/uma3UdpanEI) and you can follow that." CreationDate="2015-07-23T15:48:47.820" UserId="10327" />
  <row Id="6947" PostId="6552" Score="1" Text="Well,h2o support group has answered the question and anomaly is available in python as well.An example is available here.&#xA;https://github.com/h2oai/h2o-3/blob/master/h2o-py/tests/testdir_algos/deeplearning/pyunit_anomaly_largeDeepLearning.py" CreationDate="2015-07-23T16:07:13.250" UserId="10327" />
  <row Id="6948" PostId="6552" Score="0" Text="Perfect! thank you for investigating. i'll update this post with results." CreationDate="2015-07-23T16:58:11.390" UserId="1406" />
  <row Id="6949" PostId="5257" Score="0" Text="Might be useful: [Deriving the Reddit Formula](http://www.evanmiller.org/deriving-the-reddit-formula.html)" CreationDate="2015-07-23T17:36:26.543" UserId="381" />
  <row Id="6951" PostId="6491" Score="0" Text="I totally agree with this answer. I just wanted to add that another advantage of random hyperparameter optimization is the possibility of exploring interesting areas of the hyperparameter space that may be ignored by the grid search (take a look to Fig. 1 in the paper linked by @AN6U5)." CreationDate="2015-07-24T07:43:49.900" UserId="2576" />
  <row Id="6952" PostId="6567" Score="0" Text="In some months I'll have to apply survival and logistic regression analysis on a file with 12000 rows (people) and 45000 variables. before that I want to practice and plan a general strategy." CreationDate="2015-07-24T09:41:25.450" UserId="10815" />
  <row Id="6953" PostId="6572" Score="1" Text="Great answer @AN6U5. Appreciate your comments. Thanks..." CreationDate="2015-07-24T15:25:35.157" UserId="3314" />
  <row Id="6954" PostId="6574" Score="0" Text="Thanks. I haven't decided if I'm going to do the larger project with R, Python or both so this is helpful" CreationDate="2015-07-24T17:19:12.903" UserId="10799" />
  <row Id="6955" PostId="6027" Score="0" Text="@Pratik, what was the disposition of your investigation of dimensionality reduction with spherical constraints?  Can you now answer your own question based on Emre's suggestions?  It would be great if you could post an update since this is a pretty fascinating question." CreationDate="2015-07-24T17:31:59.110" UserId="9420" />
  <row Id="6956" PostId="6574" Score="0" Text="I was thinking you were asking how to actually reconcile the inhomogeneity (e.g. fill in the missing values) rather than how to concatenate dataframes with missing values, but it looks like you just needed the append and/or smartbind methods?" CreationDate="2015-07-24T18:32:18.947" UserId="9420" />
  <row Id="6957" PostId="6558" Score="0" Text="This is a tough problem.  The closest thing I have personally heard of is the IBM Watson project.  They digested Wikipedia and created such semantic relationships in order to answer Jeaopardy questions.  Torsten Bittner gave a great talk on this in Seattle.  It looks like [another version is on YouTube](https://www.youtube.com/watch?v=tlontoyWX70).  One can imagine a method that consists of 1) finding the wikipedia heading for each subject, 2) cross searching the subject for the other word, 3) downselecting cases with a clear object-subject pairing, 4) choosing the most common case." CreationDate="2015-07-24T18:46:07.663" UserId="9420" />
  <row Id="6959" PostId="6545" Score="0" Text="Nice, good explanation." CreationDate="2015-07-24T22:09:49.037" UserId="10826" />
  <row Id="6960" PostId="6578" Score="0" Text="If you're looking for a search term, the problem is called &quot;online document classification&quot;. The biggest challenge in your scenario is that the classes are dynamic." CreationDate="2015-07-25T03:07:38.777" UserId="381" />
  <row Id="6961" PostId="253" Score="0" Text="I have a similar question on IBM Watson Analytics,Google's Bigquery and other cloud based analytics are this technologies better then Hadoop and spark .....I am just starting to learn Hadoop and spark and do I really need to learn Hadoop and spark to do big data analytics" CreationDate="2015-07-24T21:29:47.357" UserId="10883" />
  <row Id="6962" PostId="6582" Score="0" Text="Thanks for your answer. How did you learn?" CreationDate="2015-07-25T07:19:22.530" UserId="10879" />
  <row Id="6963" PostId="6582" Score="1" Text="Books, tutorials online and a lot of hands on code related to play with data. Try the kaggle.com and try thru competitions. Is great in starting to learn ML." CreationDate="2015-07-25T07:30:58.487" UserId="8752" />
  <row Id="6964" PostId="6582" Score="0" Text="and ultimately try to find a community of data scientists  and participate in the projects, you will gain so much experience shared in the projects what no books can teach." CreationDate="2015-07-25T07:42:53.463" UserId="8752" />
  <row Id="6965" PostId="6577" Score="0" Text="Yes a worked example would be useful, from you, to tell us how much a consumption of 625 would cost. You could do this very easily." CreationDate="2015-07-25T07:47:41.743" UserId="471" />
  <row Id="6966" PostId="6582" Score="0" Text="But I am not good at theory like stats, Maths etc. I did study them in Uni days" CreationDate="2015-07-25T10:28:52.427" UserId="10879" />
  <row Id="6967" PostId="6582" Score="0" Text="I'n my particular case I did considered returning back to school and move to Ph.D program in Analytics and Data Science ... requiring calculus 1,2, Linear algebra, numerical linear algebra, SAS, R, math for big data, graph theory and much more ..." CreationDate="2015-07-25T15:17:41.173" UserId="8752" />
  <row Id="6968" PostId="6585" Score="0" Text="I am not exactly sure what you want, but [ODASE](http://www.missioncriticalit.com/odase.html) looks something similar. (You might find publications if you search for `mercury lang ontology`.)" CreationDate="2015-07-25T19:47:52.993" UserId="1359" />
  <row Id="6969" PostId="6584" Score="0" Text="This answer is the same as the previous answer.  It also doesn't do a very good job of explaining the solution, uses poor formatting, and lots of abbreviations, emoticons, and text message style vernacular.  If you have content not contained in the previous answer, then please edit your answer with a proper explanation and proper copy editing.  If not, then consider deleting this answer." CreationDate="2015-07-25T20:30:11.260" UserId="9420" />
  <row Id="6970" PostId="6355" Score="1" Text="This is a great question.  My initial thought was R-squared, which tells you how much of the variation is explained by the regression for a given set of features.  Since the Bayes error rate gives a statistical lower bound on the error achievable for a given classification problem **AND** associated choice of features.  Though the Bayes Error Rate is difficult to calculate (estimate), it has great universal utility for any classifier as you point out.  So I started thinking about Bayesian Regression and it almost seems like you are looking for the Bayes Loss." CreationDate="2015-07-25T21:29:49.480" UserId="9420" />
  <row Id="6971" PostId="759" Score="0" Text="A link about [literate programming here](http://infohost.nmt.edu/~shipman/soft/litprog/): basically, it's about commenting the code enough so that the code becomes a standalone documentation." CreationDate="2015-07-26T01:07:59.163" UserId="2544" />
  <row Id="6972" PostId="759" Score="0" Text="@gaborous: I am aware about the literate programming's meaning and have not included any links to the paradigm, as there are many sources for that and they are very easy to find. Nevertheless, thank you for your comment." CreationDate="2015-07-26T01:16:20.780" UserId="2452" />
  <row Id="6973" PostId="759" Score="1" Text="I guessed it, that's why I added this info as a comment for the interested reader :)" CreationDate="2015-07-26T01:28:38.933" UserId="2544" />
  <row Id="6974" PostId="759" Score="0" Text="@gaborous: Good, thanks again :-)." CreationDate="2015-07-26T01:30:07.517" UserId="2452" />
  <row Id="6975" PostId="6583" Score="0" Text="Is there some specific regarding large number of cases?" CreationDate="2015-07-26T13:56:39.463" UserId="10882" />
  <row Id="6977" PostId="6349" Score="0" Text="In addition to answer of @lollercoaster. I found the paper of LUDMILA I. KUNCHEVA and CHRISTOPHER J. WHITAKE which title is &quot;Measures of Diversity in Classiﬁer Ensembles and Their Relationship with the Ensemble Accuracy&quot;. I found it very explanatory about diversity." CreationDate="2015-07-26T15:37:31.013" UserId="9323" />
  <row Id="6981" PostId="6582" Score="0" Text="good starting point for statistics will be: Statistics 1: Introduction to ANOVA, Regression, and Logistic Regression. https://support.sas.com/edu/schedules.html?ctry=us&amp;id=1979 next for learning ML https://www.dataquest.io/course/kaggle-competitions &amp; https://www.kaggle.com/c/titanic/details/getting-started-with-python all this ones will help you get started with data science. (try to do all tutorials to can get used to data) I prefer more python versus Excel and R but hence you try to be a Data scientist than will need broad knowledge in more tools/language. R is easy to learn too." CreationDate="2015-07-26T19:30:17.013" UserId="8752" />
  <row Id="6982" PostId="6590" Score="1" Text="Feature selection is always going to help unless your initial features happen to super high quality to begin with. Sklearn offers a lot of different feature selection libraries (http://scikit-learn.org/stable/modules/feature_selection.html) I'm partial to RFE myself." CreationDate="2015-07-26T19:58:06.537" UserId="947" />
  <row Id="6983" PostId="6588" Score="0" Text="This is quite broad, and generally open-ended career discussion questions are off-topic on SE." CreationDate="2015-07-26T21:49:15.190" UserId="21" />
  <row Id="6987" PostId="6590" Score="0" Text="Thanks David, shall let you know how it goes!" CreationDate="2015-07-27T05:41:10.303" UserId="9061" />
  <row Id="6988" PostId="6591" Score="0" Text="&quot;...the training data I have is just for the move that was made, and whether that had good or bad results.&quot;  So you don't have the state of the game?  You can't train your network with just the move and its effect on the error.  You have to connect those moves to the game." CreationDate="2015-07-27T05:58:03.803" UserId="9420" />
  <row Id="6992" PostId="6355" Score="1" Text="Thank you for your answer. The computation of R-squared requires predictions, so I am wondering whether a theoretical bound of R-squared can be estimated. I read a paper on the estimation of the Bayes error rate by  means of an ensemble of classifiers; maybe something similar can be applied to R-squared (just a random thought here).&#xA;&#xA;I am not familiar with Bayesian regression. I will check that out." CreationDate="2015-07-27T09:04:25.463" UserId="2576" />
  <row Id="6993" PostId="6227" Score="0" Text="Can you share the table structure? What database are you using? Have you looked into recursive CTEs (common table expressions)?" CreationDate="2015-07-27T10:44:03.677" UserId="4766" />
  <row Id="6994" PostId="2448" Score="0" Text="ETL tools have fuzzy logic built into them that can match like terms. Also [postgresql 9.0 (and greater) has Levenshtein algorithm](http://www.postgresql.org/docs/9.0/static/fuzzystrmatch.html) in the `fuzzystrmatch` function" CreationDate="2015-07-27T10:52:35.230" UserId="4766" />
  <row Id="6995" PostId="6558" Score="0" Text="As @AN6U5 said, it is a really tough problem. From my experience, it is even more difficult to find an ontology that can cover everything. At the moment, ontologies are focused on a specific field. For example, you can find an ontology for Pathology or Algebra in Mathematics. Imagine that an ontology for Medicine is too broad based on the Pathology one." CreationDate="2015-07-27T12:28:05.563" UserId="201" />
  <row Id="6996" PostId="6591" Score="0" Text="Ow yeah, sorry, of course I have that. But I don't have results for any other possible moves." CreationDate="2015-07-27T19:25:52.617" UserId="10907" />
  <row Id="6997" PostId="6591" Score="2" Text="&quot;Reinforcement Learning&quot; might be a good topic to Google whilst waiting for an answer, although there are a lot of variants that won't apply to your specific game. Also, for a small network, you could look into combining neural networks with genetic algorithms to search for good NN weights as opposed to backpropagation of error terms (where precise error values may not be known, or are significantly delayed)" CreationDate="2015-07-27T19:49:11.860" UserId="836" />
  <row Id="6998" PostId="6591" Score="0" Text="Good that you have the state.  2048 has 1. initial game state, 2. move then resulting game state, 3. resulting game state + stochastic addition of extra tiles.  If you are just connecting 1 to 2, then this piece is deterministic.  Why use a learning algorithm at all?  Really you want to use the learning algo for what is the best state before adding random 2 tiles.  This seems better because there is invarriance in the system under rotation, so left, right, up, down loose meaning for different game states.  e.g. don't have your ANN choose between 4 moves, have it choose between 4 final states." CreationDate="2015-07-27T21:52:20.007" UserId="9420" />
  <row Id="6999" PostId="6602" Score="0" Text="Oh, believe me the RAM is by far not the slowest component in the system!" CreationDate="2015-07-28T01:32:34.867" UserId="7848" />
  <row Id="7000" PostId="6600" Score="0" Text="No mathematical model unfortunately. The data is from a survey, so a lot have to do with subjective perspective.&#xA;&#xA;I am considering regression-based feature eliminations for the numeric models. But I'm not sure how well it will work combined with the other pure categorical predictors. I believe in a regression, each level in a category will be split into its own yes/no variable, so lasso may eliminate one level but retain others. My aim is to identify the category as a whole to retain or not.&#xA;&#xA;Hence my thought of doing those I can represent numerically separately from the pure categories." CreationDate="2015-07-28T01:49:07.600" UserId="1133" />
  <row Id="7003" PostId="6588" Score="1" Text="Go through some answers on [How can I become a data scientist?](https://www.quora.com/How-can-I-become-a-data-scientist) Start with William Chen's answer [here](https://www.quora.com/How-can-I-become-a-data-scientist/answers/4451343?srid=3joE&amp;share=1). Hope it helps.&#xA;&#xA;PS : As suggested by NeilSlater, I have added the links in form of comment and deleted my original answer." CreationDate="2015-07-28T03:17:39.623" UserId="75" />
  <row Id="7004" PostId="6605" Score="0" Text="What does `Ip` refer to?" CreationDate="2015-07-28T09:44:24.463" UserId="122" />
  <row Id="7005" PostId="6605" Score="0" Text="I believe it's the priors, but not exactly sure what the formulation using `Ip` refers to." CreationDate="2015-07-28T09:45:37.917" UserId="7980" />
  <row Id="7006" PostId="6602" Score="0" Text="Are you saying the Xeon processor might be the slowest component ? ;)" CreationDate="2015-07-28T09:48:56.237" UserId="7980" />
  <row Id="7007" PostId="6602" Score="0" Text="This is a hardware related question - you should ask it in http://superuser.com instead." CreationDate="2015-07-28T12:42:20.190" UserId="10937" />
  <row Id="7008" PostId="5966" Score="0" Text="Be aware that in the case of Mean Shift Clustering you are replacing one parameter (number of clusters) by another one (kernel radius) that in fact will indirectly determine the final number of clusters. Therefore, model selection is still required." CreationDate="2015-07-28T13:05:41.813" UserId="2576" />
  <row Id="7009" PostId="6610" Score="0" Text="How much data do you expect now, in 1 year and in 5 years? What kind of analysis do you plan to apply to your data?" CreationDate="2015-07-28T15:31:41.003" UserId="1279" />
  <row Id="7010" PostId="6608" Score="0" Text="Most Hadoop tools, including standard MapReduce and Spark, treat single file and directory with files the same way. E.g. if on HDFS you have files`/data/jobresult/part-00001`, `/data/jobresult/part-00002`, `/data/jobresult/part-00003`, etc., you can read them all from Spark using `sc.textFile(&quot;/data/jobresult&quot;)`. Note, that nested directories are not supported - only plain files or flat directories with such files." CreationDate="2015-07-28T15:43:41.173" UserId="1279" />
  <row Id="7011" PostId="6609" Score="0" Text="Thanks @kpb. My problem is classification. And you are right, randomForest broke because I had too many possible values over the range of the categorical predictors.&#xA;&#xA;Would converting the categorical predictors to binary sparse matrix myself, rather than letting caret do the grunt work behind the scene on a data frame, allow randomForest to handle more possible levels? If so, I can give it a try.&#xA;&#xA;And I agree about issue with centering; I may resort to normalizing the numerics into a 0-1 range instead." CreationDate="2015-07-28T15:47:53.537" UserId="1133" />
  <row Id="7012" PostId="6612" Score="0" Text="what method of word embedding are you using?" CreationDate="2015-07-28T20:23:26.923" UserId="10587" />
  <row Id="7013" PostId="6612" Score="0" Text="@lollercoaster word2vec and GloVe." CreationDate="2015-07-28T20:26:37.597" UserId="843" />
  <row Id="7014" PostId="6506" Score="0" Text="Use the cosine similarity because the Euclidean distance behaves counter-intuitively due to the [concentration of distance](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions) in high-dimensional spaces." CreationDate="2015-07-28T20:51:17.687" UserId="381" />
  <row Id="7015" PostId="6591" Score="1" Text="@Mark Please, read this answer on StackOverFlow http://stackoverflow.com/a/22498940/2309097...You will love it :)" CreationDate="2015-07-28T21:22:27.090" UserId="201" />
  <row Id="7016" PostId="6576" Score="0" Text="Please be specific about what you want to &quot;get into&quot;.  Not only the field, but also at what level.  For example-- &quot;professional medical text miner&quot; or &quot;amateur astrophysical universe examiner&quot;" CreationDate="2015-07-28T22:01:28.717" UserId="1077" />
  <row Id="7017" PostId="6617" Score="0" Text="Can you elaborate on these terms?" CreationDate="2015-07-29T01:06:35.087" UserId="3466" />
  <row Id="7018" PostId="6617" Score="1" Text="@sheldonkreger the operations that the algorithm does need to be independent of how you order or group your data...this minimizes the need for cross-talk in the algorithm and leads to more efficiency." CreationDate="2015-07-29T02:05:27.273" UserId="9424" />
  <row Id="7019" PostId="6576" Score="0" Text="I am willing to become something that could work as a consultant or an employee that could be contact for companies to dug into their data and get insights of it." CreationDate="2015-07-29T04:26:30.353" UserId="10879" />
  <row Id="7020" PostId="6610" Score="0" Text="Hard to say as we are more building to cater for possible growth. Right now we have around 100 gigs of data for 30 entities over 3 years.&#xA;&#xA;So that is approximately 10 gigs per entity per year. We would like to be able to support thousands as we foresee major growth in the coming year or two. e.g. could break the 1T mark per year. &#xA;&#xA;The analysis currently is to predict the ETAs of these coordinates coming in. The final algorithm hasn't been decided on, but it will be a combination of machine learning from historical data and path matching." CreationDate="2015-07-29T08:03:18.347" UserId="10940" />
  <row Id="7022" PostId="6615" Score="0" Text="Thanks for the response. Originally thought the &quot;Big Data&quot; eco-system was an all or nothing. Good to know that we can utilise some of the frameworks such as Storm without committing to the full stack.&#xA;&#xA;Want to incrementally implement this as oppose to building a full-fledged system straight away. The reason for this, is that initially we won't have any demand and only building this system to offer a service and scale it when the demand warrants it. &#xA;&#xA;Don't doubt that, should that service becomes popular, we will need Hadoop but wondering if the first implementation could do without it." CreationDate="2015-07-29T08:29:13.810" UserId="10940" />
  <row Id="7023" PostId="6615" Score="0" Text="I may very well be over-estimating the investment needed for Hadoop and the reason ATS would be an easier route is because we have implemented it before so it is a familiar technology. &#xA;&#xA;Also, being extra cautious as we do have an impending deadline on this service, so I want to make sure we aren't over-engineering our first release, while still ensuring that it is future proof.&#xA;&#xA;Last question, is it relatively easy to migrate from ATS to Hadoop?" CreationDate="2015-07-29T08:38:16.797" UserId="10940" />
  <row Id="7025" PostId="6615" Score="1" Text="Yes the ecosystem is getting as broad as the umbrella term &quot;Linux&quot; -- hundreds of relevant projects. You'd never use more than a fraction. I don't think demand for your service determines how many things you put in your architecture; it determines how big your cluster is. You may add architecture as you add features. If by &quot;Hadoop&quot; you mean &quot;more than one machine&quot; I think you clearly need that from the start. There is no such thing as migrating from ATS to Hadoop; there are analogs (like HBase) but totally different API. Architecture is reusable; code is not." CreationDate="2015-07-29T09:05:28.870" UserId="21" />
  <row Id="7029" PostId="6609" Score="0" Text="1. converting to sparse format (yourself or caret, doesn't matter really) will help, but randomForest is not super efficient - you want tree-based methods, i would suggest xgboost&#xA;2. normalizing to 0-1 (i am guessing by min/max scaling and subtraction) has exact same problem. if you insist on normalizing, just divide by standard deviation. BTW, this is only important for logistic regression, svm and the like - trees are invariant under monotone transforms, so for xgboost / randomForest normalization is not necessary" CreationDate="2015-07-29T09:07:57.250" UserId="10936" />
  <row Id="7032" PostId="6618" Score="0" Text="Can you clarify a little, if you have 50 test scores for each individual, the  test scores do not differentiate in any way whether an individual has {disA} , {disC},..,{disA,disB}, {disB,disC},...,{disA,disB,disC} ?" CreationDate="2015-07-29T11:24:52.883" UserId="7980" />
  <row Id="7034" PostId="6618" Score="0" Text="yes. each individual has only one specific test score. But can have {disA} or {disB} or {disC} or {disA,disB} or {disB,disC} or {disA,disB,disC}. Actually, test scores are according to individuals instead of diseases." CreationDate="2015-07-29T11:42:22.393" UserId="10951" />
  <row Id="7035" PostId="6618" Score="0" Text="But having one test score per individual  is not quite the same thing  as it not differentiating which disease(s) they have ?" CreationDate="2015-07-29T11:45:16.150" UserId="7980" />
  <row Id="7036" PostId="6618" Score="0" Text="These diseases are neurodevelopmental disorders which are closely linked with each other. Here, I am trying to find out phenotypic difference among them. Imagine Communication is common problem in all diseases and I haves Test for communication skills. Now, I want to check in which disease or disorder higher values of communication test scores are associated or vice versa." CreationDate="2015-07-29T12:30:41.660" UserId="10951" />
  <row Id="7037" PostId="6618" Score="0" Text="Thank you for the background, I'm not sure that helped clarify how f(dis{A,B,C}) -&gt; g(testscore). If the test score   is not different for different combinations of diseases then you cannot separate them with machine learning, but perhaps that is not the case here ?" CreationDate="2015-07-29T12:38:31.243" UserId="7980" />
  <row Id="7038" PostId="6620" Score="0" Text="It appears the migration to &quot;cross validated&quot; was rejected by their users (http://stats.stackexchange.com/questions/163548/which-language-best-to-use-for-machine-learning-library).  Would you like me to attempt to migrate it to &quot;data science&quot;?" CreationDate="2015-07-29T03:52:12.783" UserDisplayName="Lev Reyzin" />
  <row Id="7039" PostId="6620" Score="0" Text="Weird -- seemed like a relevant question for TCS also; people certainly ask mush &quot;softer&quot; ones on a regular basis. But yeah, sure -- feel free to migrate it to whatever forum won't ban it..." CreationDate="2015-07-29T13:00:57.150" UserDisplayName="Aryeh" />
  <row Id="7040" PostId="6621" Score="1" Text="This might be better asked on http://opendata.stackexchange.com/" CreationDate="2015-07-29T13:36:11.810" UserId="7980" />
  <row Id="7041" PostId="6621" Score="0" Text="I did not know of this one, data science seemed the closest. I will duplicate the question there, but will leave it hanging here for a short while before I delete it, just in case." CreationDate="2015-07-29T13:39:16.067" UserId="10962" />
  <row Id="7042" PostId="6621" Score="0" Text="Yes, its quite new, I hope you get a good response there :)" CreationDate="2015-07-29T13:53:33.180" UserId="7980" />
  <row Id="7043" PostId="6618" Score="0" Text="Unfortunately, I have this f(dis{A,B,C}) -&gt; g(testscore) case." CreationDate="2015-07-29T14:07:40.823" UserId="10951" />
  <row Id="7044" PostId="6618" Score="0" Text="It's not clear from your answer if you have a usable discriminative feature set, but if you want a mapping between  f(dis{A,B,C}) -&gt; g(testscore) and its inverse, try merging your three disease features into one and run it through a random forest and see how that performs and let us know how it goes." CreationDate="2015-07-29T14:24:28.980" UserId="7980" />
  <row Id="7045" PostId="6618" Score="0" Text="I will follow your suggestion.Thanks you for your answer." CreationDate="2015-07-29T14:58:34.943" UserId="10951" />
  <row Id="7046" PostId="6620" Score="2" Text="You will get a range of opinions on this. Several languages have good ML libraries. Stack Exchange sites Q&amp;A format doesn't really do polls or popularity contests well - votes are mainly for finding answers useful, they don't work for garnering support, at least not on the main sites. As an opinion I would propose you create a reference implementation in C, with a documented library/API. Many languages have support for creating bindings to a C library, so if the idea takes off you may find collaborators willing to make those bindings in e.g. Python" CreationDate="2015-07-29T15:21:42.853" UserId="836" />
  <row Id="7047" PostId="5637" Score="0" Text="And the GloVe vectors are significantly faster to compute than word2vec" CreationDate="2015-07-29T15:25:17.123" UserId="7848" />
  <row Id="7048" PostId="6591" Score="0" Text="@TaVen: A heuristic-based search will often beat NNs, and that was a fun read. I believe that the OP's question isn't about finding best 2048 player though, but gaining skills in teaching NNs to play games." CreationDate="2015-07-29T15:35:25.343" UserId="836" />
  <row Id="7049" PostId="6551" Score="0" Text="To reiterate, and perhaps be more blunt, using Twitter's AnomalyDetection package is NOT an option here: Please read the &quot;Constraints&quot; section more carefully. I do not mean to denounce any sincere attempts to help on this, but the question is strictly for Python-based packages. Therefore, future voters, PLEASE do not upvote this answer because it is not usable option. I would recommending clearing the current 2 votes for this via downvoting but perhaps this is unethical within the Stackexchange community and do not want to catch any flack." CreationDate="2015-07-29T16:30:41.523" UserId="1406" />
  <row Id="7050" PostId="6551" Score="0" Text="Again, I apologize to harp on this, but I am simply trying to make this question very clear and usable for others encountering a similar problem, and don't want them to go on a wild goose chase." CreationDate="2015-07-29T16:31:24.387" UserId="1406" />
  <row Id="7051" PostId="6620" Score="1" Text="I agree with Neil, ultimately most people would probably use something in Python but that something is probably just a binding to optimized C." CreationDate="2015-07-29T17:12:39.180" UserId="947" />
  <row Id="7052" PostId="6591" Score="0" Text="@Neil It's true. I found a YouTube video about 2048 and evolving neural networks, but couldn't find a source for the code." CreationDate="2015-07-29T18:30:23.300" UserId="201" />
  <row Id="7053" PostId="6627" Score="1" Text="Yeah, we are in a very initial phase of project so sorry for the very broad overview.&#xA;&#xA;We are also converting non-Iot devices to IoT along with user profiling. So these non-IoT devices will have ordinal values (sometimes probably on or off only) but the new actual IoT devices will run on continuous values. It will be a mix of both, probably thresholding continuous values can determine states for converted devices.&#xA;&#xA;Last section was an interesting look at the problem, but i am skeptical if we can determine that based on others preferences in same area/conditions.&#xA;&#xA;Thanks for the idea and links." CreationDate="2015-07-30T04:10:23.053" UserId="10967" />
  <row Id="7054" PostId="6626" Score="0" Text="None of the libraries will auto-select a network architecture for you. At the least, you will be wanting to test with variations in number of layers, types of layers (convolutional, pooling, dropout etc), hidden layer sizes, and choosing between several other hyper-params and/or variations on training. The easiest way to &quot;auto-pick&quot; those kinds of things is to follow a tutorial on a related problem to the one you want to solve, and keep things as similar as possible so you don't need to think about those things for now . . ." CreationDate="2015-07-30T06:53:46.407" UserId="836" />
  <row Id="7055" PostId="6626" Score="0" Text=". . . so it might be worth giving a brief summary of what kind of problem you are trying to solve (e.g. image classifying). A library with good tutorials and sample code related to your problem may be the best choice." CreationDate="2015-07-30T06:56:32.497" UserId="836" />
  <row Id="7056" PostId="6591" Score="0" Text="@TaVen Yeah that answer is great for both AI and CS, and it made me realize that looking 6 turns into the future is probably enough, so now the score of a move is based on the points gained in the next 6 turns. I found two Youtube videos, here's the code for one https://github.com/anubisthejackle/2048-Deep-Learning" CreationDate="2015-07-30T09:34:06.360" UserId="10907" />
  <row Id="7058" PostId="6591" Score="0" Text="@AN6U5 Taking the symmetry into account is a good idea. But using transitions would make the output encoding more difficult right? Seems easier to align the boards, maybe using image moments. Does that seem okay?" CreationDate="2015-07-30T09:52:03.297" UserId="10907" />
  <row Id="7059" PostId="6626" Score="0" Text="@NeilSlater I am exploring the possibility of using DL on futures contracts in the financial markets. I will be using minute by minute price data, ratios, etc. All the tutorials I can find are for text recognition or image classifying, as you mentioned. I haven't been able to find one related to financial &quot;prediction&quot;..." CreationDate="2015-07-30T13:17:16.670" UserId="10949" />
  <row Id="7060" PostId="6626" Score="0" Text="CNN is not synonymous with Deep Learning, it is just one of the more successful designs. Do you have a plan for the convolution parts? If not, an RNN (deep or not) may be better suited. Not that this changes your question much, just you are more likely to find what you want looking up RNN and time-series than CNN with anything" CreationDate="2015-07-30T13:25:55.227" UserId="836" />
  <row Id="7061" PostId="6626" Score="0" Text="@NeilSlater Sorry if my question was poorly formulated. I have been planning to use RNN, specifically a variant with LTSM. Thanks for all the insights." CreationDate="2015-07-30T13:30:27.270" UserId="10949" />
  <row Id="7062" PostId="6591" Score="0" Text="@AN6U5 I made a new question about that https://datascience.stackexchange.com/questions/6631/alignment-of-square-nonorientable-images-data" CreationDate="2015-07-30T14:07:59.307" UserId="10907" />
  <row Id="7064" PostId="6630" Score="0" Text="This might receive more attention and be resolved quicker if it were moved to [Cross Validated](http://stats.stackexchange.com/).  What are your assumptions?  Is the process birth rate uniform? Is the population sufficiently large?  If so then the solution is pretty straightforward." CreationDate="2015-07-30T16:49:55.687" UserId="9420" />
  <row Id="7065" PostId="6627" Score="1" Text="As a beginner, would you recommend me to use libraries directly or should I stick with naive but my own simple implementations which i understand thoroughly. Before your advice I had been working on simple online reinforcement learning for adaptive weight adjustment of features.&#xA;&#xA;And I did upvote, but stackexchange says it will accept my vote only after I have earned certain amount of 'reputation'." CreationDate="2015-07-30T17:19:15.357" UserId="10967" />
  <row Id="7066" PostId="6627" Score="0" Text="As a learning tool, I think its really useful to code up your own implementation of the basics like (stochastic) gradient decent, linear regression, k-means, SVMs, decision trees.  But you can probably do this as part of a MOOC separate from your actual work.  I absolutely suggest using a well known library for your problem.  Scikit-Learn is great for the reasons mentioned above and for prototyping, but doesn't scale particularly well.  [H20](http://h2o.ai/) and [Mahout](http://mahout.apache.org/) are high quality scalable libraries that I would recommend for a big data production system." CreationDate="2015-07-30T17:26:45.673" UserId="9420" />
  <row Id="7067" PostId="6633" Score="0" Text="This might be better suited for [Stack Overflow](http://stackoverflow.com/) since it is just a coding nuance.  It will certainly get an answer much quicker given the userbase." CreationDate="2015-07-30T20:48:48.503" UserId="9420" />
  <row Id="7069" PostId="6291" Score="0" Text="This is good advice...  @dpmcmlxxvi is right that 1) standard deviation tends to be more robust than min and max and 2) the validity of your model depends on the mean and standard deviation of your training data set being similar to the mean and standard deviation of your test data set." CreationDate="2015-07-31T02:01:23.080" UserId="9420" />
  <row Id="7070" PostId="6611" Score="0" Text="Are you planning on using distributed computation? If so, have you considered Apache Spark...it's much faster in many cases than map reduce." CreationDate="2015-07-31T02:07:37.277" UserId="9424" />
  <row Id="7071" PostId="6592" Score="0" Text="Why are you using lists instead of numerical vectors?" CreationDate="2015-07-31T02:26:35.003" UserId="9424" />
  <row Id="7072" PostId="6635" Score="0" Text="Also, see the _&quot;Embedding Fine-Tuning&quot;_ section on the page 5 of [this paper](http://lebret.ch/wp-content/uploads/2013/12/nips2013.pdf) - I know, it's not software, but I thought it might give you some useful ideas." CreationDate="2015-07-31T05:23:35.197" UserId="2452" />
  <row Id="7073" PostId="6545" Score="0" Text="The one part about this that is unclear to me, is - how does this account for the variance in the denominator?  It was  my understanding that NCC is normalized by the variance of the function.  Is that true?  I'm not seeing this in your explanation." CreationDate="2015-07-31T09:33:36.263" UserId="10826" />
  <row Id="7074" PostId="6638" Score="1" Text="Yeah it's helpful, thanks! But I still need a way to map all theses states to one in a consistent way... But now I know exactly which states and operations there are :-)" CreationDate="2015-07-31T11:35:06.033" UserId="10907" />
  <row Id="7075" PostId="6545" Score="0" Text="The distinction between standardizing the cohort and normalizing the individual vectors is confusing when NCC is put in functional form.  I'll try to edit my answer over the weekend to hash this out." CreationDate="2015-07-31T15:54:01.107" UserId="9420" />
  <row Id="7076" PostId="6638" Score="0" Text="I tried to answer this with the edit I just made.  Hope it helps!" CreationDate="2015-07-31T16:20:00.683" UserId="9420" />
  <row Id="7077" PostId="6643" Score="0" Text="Can you add some additional information to help clarify your question?  Is the algorithm you are running written by you in CUDA or are you using a package written by someone else?  Does that package or code only run on OSX or are you wondering about connectivity between AWS and OSX?  You can certainly connect a Mac to AWS and run your code/package on AWS in a Linux or Windows environment.  Further, OSX is built on a Unix environment, so most small code packages that run in OSX will run in Linux." CreationDate="2015-07-31T16:38:54.557" UserId="9420" />
  <row Id="7078" PostId="6645" Score="0" Text="It would be very useful if you would provide the R package that you are using  and a sample of the data.  My guess is that there are some sort of hidden differences in the features.  Often R data comes in as factors or character strings instead of numbers, so even though they look like numbers, they aren't.  There are several ways to convert to numeric data, but `as.numeric()` is the most popular.  Check out [this stack overflow post](http://stackoverflow.com/questions/2288485/how-to-convert-a-data-frame-column-to-numeric-type)" CreationDate="2015-07-31T16:54:22.150" UserId="9420" />
  <row Id="7079" PostId="6645" Score="0" Text="@AN6U5- Thanks for the reply. I am using randomForest package. The error i got during the predict statmenet.  I will try to provide some sample data by tomorrow. However I could see that both the training data and the test data contains same factors for categorical data and the few numerical variables." CreationDate="2015-07-31T18:42:48.107" UserId="9793" />
  <row Id="7081" PostId="6611" Score="0" Text="I'm using spark. columnSimilarity is implemented in scala for spark, I'm using python so I implemented it myself in the python bindings. Either way, doing all pairs similarity brute force is infeasible even on huge spark clusters. DIMSUM offers a smart trick for sampling only column similarities that have a high probability of being over a certain threshold, the issue is that DIMSUM works only when you have more rows than columns. I have many more columns than rows. see the linked paper for details" CreationDate="2015-07-31T20:44:15.750" UserId="10943" />
  <row Id="7083" PostId="6645" Score="0" Text="I'm still skeptical that either the types are different, the number of features are different, or you are giving it the transpose of your data so the shape is different.  Those are the things I would investigate with that error." CreationDate="2015-07-31T21:40:55.240" UserId="9420" />
  <row Id="7084" PostId="6645" Score="0" Text="@AN6U5 - Thank you again. So do you advice to try all the numeric fie lds to be declared as.numeric explicitly ( and the factor fields as as.factor() respectively) and give a try?" CreationDate="2015-07-31T21:52:19.893" UserId="9793" />
  <row Id="7085" PostId="6645" Score="0" Text="I just want you to run `sapply(data, mode)` and `sapply(data, class)` and `dim(data)` on your training data and testing data to see if everything matches." CreationDate="2015-07-31T21:58:13.987" UserId="9420" />
  <row Id="7086" PostId="6645" Score="0" Text="@AN6U5 - Thank you. I will try this and get back to you." CreationDate="2015-07-31T22:54:08.683" UserId="9793" />
  <row Id="7087" PostId="6645" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/26481/discussion-between-arun-and-an6u5)." CreationDate="2015-07-31T23:17:18.823" UserId="9793" />
  <row Id="7088" PostId="6645" Score="0" Text="@AN6U5 - As per your suggestion. I tried with sapply functions mentioned above. Everything looks ok for me.                                          But one clue I got from your earlier reply - &quot;or you are giving it the transpose of your data so the shape is different&quot;- Could you please let me know what you mean by this? Actually i started getting this problem only after applying a filter with in the program.   I apply something like this testing_data &lt;- subset(testing_data, var1 != &quot;xyz&quot;).. I will explain you a little bit more in the next comment." CreationDate="2015-08-01T07:08:41.753" UserId="9793" />
  <row Id="7089" PostId="6645" Score="0" Text="@AN6U5 - I have two R programmes. Let us call it as A &amp; B. Program A does the preprocessing, puts the data into a .csv and saves the file in a location. Program B picks the .csv, builds the model and does the prediction job. Everything was working fine for me, until i applied the above said filter to the data, that is, testing_data &lt;- subset(testing_data, var1 != &quot;xyz&quot;). As you said that &quot;transposing  could have reshaped the data&quot;, I suspect that this subset filter could be causing some issue. Could you please advice me how can i find what is making this problem please?" CreationDate="2015-08-01T07:20:10.400" UserId="9793" />
  <row Id="7091" PostId="6629" Score="0" Text="Not really clear what you want since if you make it generally reversible, it has not been anonymized . You probably have a user/role model and a consumption model in mind, but you need to explicitly describe this in the question. As well as taking Franck's answer into consideration." CreationDate="2015-08-01T20:14:55.703" UserId="7720" />
  <row Id="7092" PostId="6545" Score="0" Text="Okay.  I'll wait.  Thanks." CreationDate="2015-08-02T01:18:39.470" UserId="10826" />
  <row Id="7093" PostId="6643" Score="0" Text="Thank you AN6U5! I want to use lasagne to train my neural network. And yes, I wrote that code, and will try convolutional NN later. I want to use the AWS instances to accelerate the computation. My doubt came from the AWS website saying that new users have 750h / month of Linux /Windows micro instances usage for free, Mac OS is not included." CreationDate="2015-08-02T06:47:53.220" UserId="10994" />
  <row Id="7094" PostId="6650" Score="0" Text="Hi Emre, thank you for your reply! Actually I am thinking about buying a new laptop since my Macpro is really old. I likeMac, but it seems they don't come with any NVIDIA card. Could you recommand me some laptops suitable for machine learning tasks? (Sorry I know almost nothing about hardware)" CreationDate="2015-08-02T07:09:31.780" UserId="10994" />
  <row Id="7095" PostId="6657" Score="1" Text="What have you tried? what was different from the result you expected?" CreationDate="2015-08-02T07:33:57.557" UserId="21" />
  <row Id="7096" PostId="6650" Score="0" Text="The [Macbook Pro comes with a GeForce GT 750M](https://www.apple.com/macbook-pro/performance-retina/) with [384 cores](http://www.gpuzoo.com/GPU-NVIDIA/GeForce_GT_750M.html). Outside of Apple, you will probably want a gaming laptop, [such as this Alienware with an Nvidia GTX 980](http://www.laptopmag.com/gaming-laptops), [which has 1536 cores](http://www.geforce.com/hardware/notebook-gpus/geforce-gtx-980m/specifications). [Here are some others](http://www.ultrabookreview.com/5729-gaming-laptops-nvidia-970m-980m/). Note that machine learning algorithms outside of deep learning don't need GPUs." CreationDate="2015-08-02T07:41:57.957" UserId="381" />
  <row Id="7097" PostId="6650" Score="0" Text="Ok! Thank you very much for the answer, it's more clear for me now!" CreationDate="2015-08-02T08:21:53.440" UserId="10994" />
  <row Id="7099" PostId="6660" Score="1" Text="you are most of the way there,  nearest neighbour is not too hard to implement yourself. As you observe, you just need to compute the distance between your candidate who doesn't have a score for your potential recommendation and the other users who have a score for that item and then select the smallest distance using whatever metric empirically works best. Testing will show whether that might be euclidean, manhattan, jaccard or something else." CreationDate="2015-08-02T12:09:05.290" UserId="7980" />
  <row Id="7100" PostId="6644" Score="0" Text="Thank you. I couldn't quite follow this part, would you mind elaborating further please? - &#xA;&#xA;&quot;I might have weights going from activation of old output i=3, AOld3 to logit of new outputs ZNewj, where ZNewj=Σi=9i=0Wij∗AOldi as follows:&#xA;&#xA;W3,0=−10&#xA;W3,1=−10&#xA;W3,2=+10&#xA;W3,3=+10&quot;" CreationDate="2015-08-02T12:10:38.450" UserId="10990" />
  <row Id="7101" PostId="6657" Score="0" Text="i want to do the same as in article &quot;Predicting the drivers of behavioral intention to use mobile learning: A hybrid SEM-Neural Networks approach&quot; so in Neural Network part i use Neural Networks spss and the report gives SSE and relative error but i want RMSE." CreationDate="2015-08-02T08:50:33.383" UserId="11026" />
  <row Id="7102" PostId="6660" Score="0" Text="Thanks image_doctor, is there any resource you could direct me to where I can learn about implementing custom functions in R? I am familiar with the theory for Jaccard and I have some experience implementing this in Excel but I don't know how to &quot;tell&quot; R to loop through each pair of users to compute similarity or how to store the results?" CreationDate="2015-08-02T13:03:09.710" UserId="11029" />
  <row Id="7103" PostId="6660" Score="0" Text="You'll probably find functional programming useful here, it will be faster than loops, here is a reference: http://adv-r.had.co.nz/Functionals.html" CreationDate="2015-08-02T14:23:39.143" UserId="7980" />
  <row Id="7104" PostId="6644" Score="0" Text="@VictorYip: The equation is just the normal feed-forward network equation, but to use it I had to define my terms carefully (since you have no reference maths in your question). The &quot;logit&quot; Z value is the value calculated at the neuron *before* activation functions have been applied (and generally $A_i = f( Z_i )$ where $f$ is e.g. sigmoid function). The example weights are the values I would use for connecting new output layer neurons to old ones, but just the ones that connect the 4 neurons in the new output layer to one of the neurons in  old output layer (the one for output &quot;3&quot;)" CreationDate="2015-08-02T15:20:18.047" UserId="836" />
  <row Id="7107" PostId="6662" Score="0" Text="@Wajdo Ben Saad, thanks for the response, but A) I'm already well aware of regex's and various string dif distance algorithms B) To suggest creating 200k+ regex's is insane and Levenshtein distance calculations are computationally prohibitive C) neither of these techniques leverage machine learning of any sorts nor do they improve their accuracy over time." CreationDate="2015-08-02T18:24:53.783" UserId="11021" />
  <row Id="7108" PostId="6662" Score="0" Text="The machine learning part will be included in your model. You need to have a subset of your data to use for building the model with whatever technique you see fit, and test the accuracy of your model on a test subset. I know it is complicated to give a satisfying answer to your question. As I said, you might post a simple problem and the goal you need to accomplish, it would be easier to solve :)" CreationDate="2015-08-02T18:33:26.673" UserId="7714" />
  <row Id="7109" PostId="6645" Score="0" Text="@AN6U5 - Do you have any other thoughts on this please?" CreationDate="2015-08-03T05:18:55.950" UserId="9793" />
  <row Id="7110" PostId="6666" Score="1" Text="Interesting comments in your second paragraph.  Are there any general rules of thumb in this regard?  Or guidelines in how certain parameter choices or data cleaning will affect your results?" CreationDate="2015-08-03T14:28:20.290" UserId="1097" />
  <row Id="7111" PostId="6662" Score="0" Text="@scribbles Thought, maybe the 200K+ reg exs  could be condensed into some type of tree structure with common parts ..." CreationDate="2015-08-03T14:36:28.200" UserId="7980" />
  <row Id="7112" PostId="6666" Score="0" Text="@Matt: I'd like to know that, too. I just took some data I understand already pretty well and experimented with them, with the results above. I found that adjusting the stopwords to the actual corpus helps a lot in clearer topic definition." CreationDate="2015-08-03T14:36:35.620" UserId="10169" />
  <row Id="7113" PostId="6645" Score="0" Text="You haven't provided enough information yet for people to help you. Please provide some data and your code." CreationDate="2015-08-03T18:05:43.870" UserId="9420" />
  <row Id="7114" PostId="6645" Score="0" Text="@AN6U5 - Sorry for the delay. I have now the added the required infoarmtion." CreationDate="2015-08-03T18:47:27.477" UserId="9793" />
  <row Id="7115" PostId="6676" Score="0" Text="A question and an observation: how stable is your accuracy of SGD on repeated runs?   the two algorithms are not equivalent and will not necessarily produce the same accuracy given the same data. Practically you could try changing the epochs and or the learning rate for SGD. Beyond that you could try normalising the features for SGD." CreationDate="2015-08-04T10:29:42.717" UserId="7980" />
  <row Id="7116" PostId="6672" Score="0" Text="In my case I think the cohort split makes sense. If for instance after a special communication a lot of new customers are acquired I can at least better predict now the impact it will have in the coming months, especially if they don't spend immediatly." CreationDate="2015-08-04T14:17:37.427" UserId="10983" />
  <row Id="7120" PostId="6676" Score="0" Text="So, I didn't test the SGD on repeated runs because the above uses 10 fold cross validation; for me this sufficed." CreationDate="2015-08-04T16:21:46.450" UserId="8774" />
  <row Id="7121" PostId="6676" Score="0" Text="Can you explain to me how come these algorithms are not equivalent? If I look at the SGDClassifier here, it mentions &quot;The ‘log’ loss gives logistic regression, a probabilistic classifier.&quot; I believe there is a gap in my machine learning knowledge." CreationDate="2015-08-04T16:23:21.147" UserId="8774" />
  <row Id="7122" PostId="6638" Score="0" Text="Ignore my previous comment, there was a code mistake. Operations can be applied in any order even though they don't commute. It seems to work well now!" CreationDate="2015-08-04T16:27:55.207" UserId="10907" />
  <row Id="7123" PostId="6638" Score="1" Text="For future users: I used image moments instead of highest value tile, which generalizes to things other than 2048 boards. For \ mirroring, I check the sum of the upper triangle vs that of the lower." CreationDate="2015-08-04T16:29:23.577" UserId="10907" />
  <row Id="7124" PostId="6638" Score="1" Text="Note that there is a loss of information in summing the diagonal region since multiple different sums and orientations can result in the same sum.  I try to stay away from sums or averages when trying to determine absolute differences.  Image moments also have a loss of information.  For large images, you wouldn't expect much conflict, but the 2048 board would have a significant number of conflicts." CreationDate="2015-08-04T16:36:39.037" UserId="9420" />
  <row Id="7125" PostId="695" Score="0" Text="Note that nolearn is a wrapper that makes other libraries easier to use and compatible with sklearn. It's not of itself a neural network library, but nonetheless recommended. At the time of writing it's mostly for Lasagne but there's some Caffe code and maybe others." CreationDate="2015-08-04T16:36:51.790" UserId="10907" />
  <row Id="7126" PostId="6673" Score="1" Text="Not sure I understand what you're saying. I never said that knowing &quot;applied statistics&quot; isn't important - I simply made the distinction that gaining experience applying methods is more important than gaining theoretical knowledge about the methods itself." CreationDate="2015-08-04T16:57:07.137" UserId="947" />
  <row Id="7128" PostId="6684" Score="0" Text="Thanks David. Any insight on how to choose the threshold above which features are useful? (put aside from removing the least useful feature, running the RF again and see how it impacts the prediction performance)" CreationDate="2015-08-04T18:02:24.060" UserId="843" />
  <row Id="7129" PostId="6684" Score="1" Text="As with most automated feature selection I'd say most people use a tuning grid. But using domain expertise when selecting (and engineering) features is probably the most valuable -- but isn't really automatable." CreationDate="2015-08-04T18:05:32.373" UserId="947" />
  <row Id="7130" PostId="6673" Score="0" Text="David, that was exactly my point of disagreement. Without having theoretical knowledge of the methods themselves we are simply just script kiddies. Experience is important, but it is a by-product of theoretical knowledge, not the other way around." CreationDate="2015-08-04T18:31:36.300" UserId="11054" />
  <row Id="7132" PostId="6673" Score="0" Text="I see, well then we are in extreme disagreement. I think knowing how to correctly apply something (when to, how to, assumptions, etc...) is far more important than knowing its underlying proof." CreationDate="2015-08-04T18:37:00.090" UserId="947" />
  <row Id="7133" PostId="6673" Score="0" Text="Not really, what you call &quot;experience&quot; is actually theoretical knowledge - the two are inseparable. What you advocate is a person who knows how to drive a car, but has no clue how it runs. Companies hiring data scientists would prefer to have a person who can do both. The script kiddie reading kaggle tutorials without any background in statistics won't be in the game for very long." CreationDate="2015-08-04T18:43:56.830" UserId="11054" />
  <row Id="7134" PostId="6673" Score="1" Text="No, it isn't. There is a vast difference between applied experience and theoretical knowledge, it is frequently the difference between what is gained in industry vs in the classroom. For example, it's more valuable to know how to effectively verify that a model has not overfit using an applied method like cross validation than it is to know the theoretical underpinnings of regularization. Also, please stop mentioning &quot;script kidies&quot; -- no one is advocating using kaggle's new and horrible one-click-to-submit functionality." CreationDate="2015-08-04T18:47:42.433" UserId="947" />
  <row Id="7135" PostId="6673" Score="1" Text="If what you are saying is true, then why do companies prefer PhD's and people with Masters degrees over people with simply Bachelors? It is because they have theoretical knowledge of the techniques which drive the algorithms. They are the engine builders per se. Theoretical knowledge is deeper knowledge. Kaggle is a holding tank for script kiddies." CreationDate="2015-08-04T18:53:00.963" UserId="11054" />
  <row Id="7136" PostId="6673" Score="0" Text="If the trade-off is between someone with a BA and 5 years of successful industry experience or a PhD with no industry experience then they wouldn't. If you think that someone needs a PhD to succeed in the field then you are wrong. Masters degrees are nice -- and provide an applied, not theoretical, education -- but are not required. Speaking as one myself, I'm pretty sure you don't understand what employers are looking for but I know that I cannot say is anything to convince you otherwise so I'll stop here." CreationDate="2015-08-04T19:04:12.603" UserId="947" />
  <row Id="7137" PostId="6673" Score="0" Text="Your position is a false dichotomy - one doesn't study for 1 year or 5 years and then suddenly start getting experience. Experience is learned along the way. Data Science &gt; Kaggle Competitions.&#xA;&#xA;Kaggle is a holding tank for script kiddies, whose models are either useless, overtly complex, or overfitted. This why they had to fire 1/3 of their staff." CreationDate="2015-08-04T19:05:10.387" UserId="11054" />
  <row Id="7138" PostId="6673" Score="0" Text="David, generalizations about Master's degrees are never good. Some are theoretical and some are applied. Some are both. To be successful, a PhD is not necessary, but a Masters degree most certainly is. Most of the people on Kaggle only have a Bachelors degree. &#xA;&#xA;&quot;.. insiders say that they haven’t been lucrative enough for Kaggle. And the algorithms that win them aren’t always general enough to be useful to the company sponsoring the competition.&quot;&#xA;&#xA;Source: http://www.wired.com/2015/02/data-science-darling-kaggle-cuts-one-third-staff/" CreationDate="2015-08-04T19:05:23.117" UserId="11054" />
  <row Id="7139" PostId="6545" Score="0" Text="Have you figured this out?  Thanks." CreationDate="2015-08-04T19:57:43.857" UserId="10826" />
  <row Id="7140" PostId="6675" Score="2" Text="The Neural Networks (and other ML methods) are designed to use *when* you don't know the hidden logic. You want the machine to learn what output to provide given the input. In other words, you know that there is some hidden process which given your input get that specific output." CreationDate="2015-08-04T22:38:25.267" UserId="7848" />
  <row Id="7141" PostId="6680" Score="0" Text="The format and data sample shown is likely the LIBSVM format. The first is the label (+1 or -1), followed by dimension/value pairs. This format is convenient for sparse vector storage. From the example, there might be a problem with format (I see the commas at the end). You can use LIBSVM software package on this data." CreationDate="2015-08-04T23:26:04.747" UserId="7848" />
  <row Id="7142" PostId="6692" Score="0" Text="Have you looked at geotools?" CreationDate="2015-08-05T03:11:55.703" UserId="11088" />
  <row Id="7143" PostId="6692" Score="0" Text="Not yet. I'll take a look. Did you used?" CreationDate="2015-08-05T03:32:14.947" UserId="3164" />
  <row Id="7144" PostId="6692" Score="0" Text="No, but colleagues of mine do." CreationDate="2015-08-05T03:33:18.470" UserId="11088" />
  <row Id="7145" PostId="6676" Score="0" Text="Without a detailed study of the implementations I don't think I can be specific about why they are not equivalent, but a good clue that they are not equivalent is that the results for each method are significantly different. My guess would be that it has to do with the convergence properties of the estimation methods used in each." CreationDate="2015-08-05T06:51:20.053" UserId="7980" />
  <row Id="7146" PostId="6696" Score="0" Text="Worth mentioning that a grid world problem is presented as part of that course." CreationDate="2015-08-05T07:26:37.860" UserId="836" />
  <row Id="7148" PostId="6696" Score="0" Text="yes, have seen that, but not enough to code the same" CreationDate="2015-08-05T07:51:40.553" UserId="8013" />
  <row Id="7149" PostId="6689" Score="0" Text="Dirk thank you for the answer. The thing that is giving me (lots of) problems is the definition of a churn event, because this is not a subscription-based service. Isn't survival analysis only useful for model where I have a &quot;death event&quot; (a user who cancels his subscription, a patient who dies, etc.)?" CreationDate="2015-08-05T07:54:22.430" UserId="11066" />
  <row Id="7150" PostId="6295" Score="0" Text="You could use the sqldf package." CreationDate="2015-08-04T01:08:09.050" UserId="11054" />
  <row Id="7151" PostId="6685" Score="0" Text="They're the same length. Thanks!" CreationDate="2015-08-05T06:08:10.840" UserId="11064" />
  <row Id="7152" PostId="6698" Score="0" Text="SimHash and MinHash do not use these similarity functions. I think a better way to say it would be that they create digests which approximate these functions." CreationDate="2015-08-05T09:15:04.923" UserId="816" />
  <row Id="7153" PostId="6430" Score="0" Text="Thank you very much for the reply and advice. Just want to know how different is using a caret package for feature selection when compared to selecting important variables using the randomForest algorithm?" CreationDate="2015-08-05T10:39:40.660" UserId="9793" />
  <row Id="7154" PostId="6430" Score="0" Text="That sounds like a different question to post -- the differences are vast given that you run things like recursive feature selection using any algorithm of your choice in caret." CreationDate="2015-08-05T14:38:33.157" UserId="947" />
  <row Id="7155" PostId="6676" Score="1" Text="These algorithms are different because logistic regression uses gradient descent where as stochastic gradient descent uses stochastic gradient descent.  The convergence of the former will be more efficient and will yield better results. However, as the size of the data set increases, SGDC should approach the accuracy of logistic regression.  The parameters for GD mean different things than the parameters for SGD, so you should try adjusting them slightly.  I would suggest playing with (decreasing) learning rates of SGD a bit to try to get better convergence as it may be thrashing around a bit." CreationDate="2015-08-05T16:17:22.207" UserId="9420" />
  <row Id="7156" PostId="6545" Score="0" Text="I think I get it @an6u5." CreationDate="2015-08-05T17:30:41.507" UserId="10826" />
  <row Id="7157" PostId="6698" Score="0" Text="@AlexeyGrigorev I am a little confused. I looked into the following implementation for minHash 'computeSimilarityFromSignatures' @ [link](https://mymagnadata.wordpress.com/2011/01/04/minhash-java-implementation/). It uses a |HashedArray(A) &amp; HashedArray(B)|/ (total number of entries)" CreationDate="2015-08-05T19:15:24.313" UserId="5179" />
  <row Id="7160" PostId="6707" Score="0" Text="how do I set a threshold... What i am doing id the update the value of each grid with respect to the grids that the control can go to from the present grid.. What do you mean by saying V is a function" CreationDate="2015-08-06T04:00:21.690" UserId="8013" />
  <row Id="7161" PostId="6705" Score="0" Text="actually i wanted to see a grid world problem being solved by calculating on pen and paper, because that would help me understand the concept, unless i can understand the concept I cannot code(specially value iteration)" CreationDate="2015-08-06T04:01:29.510" UserId="8013" />
  <row Id="7163" PostId="6707" Score="0" Text="$V(s)$ is a function that returns the utility of that state. In a computer program, where you have enumerated the states, you may well end up modelling $V$ as a simple array and treat it as an array lookup" CreationDate="2015-08-06T08:53:37.033" UserId="836" />
  <row Id="7164" PostId="6689" Score="0" Text="Survival doesn't have to model death. You can define it, make sure it's definition is accepted in the business." CreationDate="2015-08-06T10:07:55.240" UserId="11080" />
  <row Id="7167" PostId="6399" Score="0" Text="Yeah, I would currently recommend blocks today over pylearn2 if you're ok putting in a bit of time to understand Theano." CreationDate="2015-08-06T19:50:16.547" UserId="684" />
  <row Id="7168" PostId="6399" Score="0" Text="Great library built by great people." CreationDate="2015-08-06T19:50:22.947" UserId="684" />
  <row Id="7170" PostId="6720" Score="0" Text="another question, in most of the grid world problem one state has a high valued number written on it,like +100. Is that the reward or the value of that state" CreationDate="2015-08-07T08:48:39.850" UserId="8013" />
  <row Id="7171" PostId="6710" Score="0" Text="Thanks for your answer. But I have to further ask that, in the case $p&gt;&gt;n$, the complexity is $O(n^2p+n^3)$?" CreationDate="2015-08-07T09:24:58.777" UserId="9893" />
  <row Id="7172" PostId="6714" Score="0" Text="Thanks. Could you please kindly give some link of the related publications?" CreationDate="2015-08-07T09:26:10.847" UserId="9893" />
  <row Id="7173" PostId="6710" Score="0" Text="@aaronyxt, Yes, in that case you can additionally take into consideration all fast growing terms with p." CreationDate="2015-08-07T11:03:42.057" UserId="10989" />
  <row Id="7174" PostId="6723" Score="0" Text="Please don't cross-post. Of the two posts of yours that I stumbled across, I think the question is probably better in dsp.stackexchange, as it is mainly about the device to use and how to get accuracy. You could also look at http://gis.stackexchange.com/questions/tagged/gps as your question is directly about technologies used for mapping." CreationDate="2015-08-07T11:23:33.793" UserId="836" />
  <row Id="7175" PostId="6720" Score="0" Text="It is the reward, which is fixed for the problem. The value is learned, it is an estimate of accumulated future rewards going from that state." CreationDate="2015-08-07T14:48:13.530" UserId="9814" />
  <row Id="7177" PostId="6715" Score="0" Text="Thank you for the detailed responses!                                                           As a follow-up to all of your answers:  I understand that if the features are on different scales, this could present a problem.  However, if the distance metric is normalized to the variance, does this achieve the same result as standard scaling before clustering?  i.e. I usually use a normalized euclidean distance [related](http://stackoverflow.com/questions/31869799/how-to-implement-callable-distance-metric-in-scikit-learn) - does this also mitigate scaling effects?" CreationDate="2015-08-07T16:38:04.743" UserId="11124" />
  <row Id="7178" PostId="6722" Score="0" Text="But, then couldn't you just include that as an additional feature that you cluster on?  i.e. the price becomes a feature to cluster on?" CreationDate="2015-08-07T16:47:28.703" UserId="11124" />
  <row Id="7181" PostId="6710" Score="0" Text="Thanks for your help and reply." CreationDate="2015-08-08T01:44:28.203" UserId="9893" />
  <row Id="7182" PostId="6714" Score="0" Text="Thanks for your reply and  references." CreationDate="2015-08-08T01:45:50.533" UserId="9893" />
  <row Id="7183" PostId="6673" Score="1" Text="While I can see points both of you are trying to make, I think it's perhaps out of context. The original question was 'how can a programmer transition into a job in data science ?'&#xA;&#xA;If the response is 'drop everything, spend some years getting a PH.D in statistics, then do some projects on your own and then start applying', that's a pretty onerous obstacle and you may as well tell them not to bother in a practical sense.  Conversely, given the number of Stats PHD (or even Masters) and the number of people looking, employers may consider people who can demonstrate experience without a degree." CreationDate="2015-08-08T07:49:38.170" UserId="11146" />
  <row Id="7184" PostId="6716" Score="0" Text="Please see my comment to my question.  Thanks" CreationDate="2015-08-08T09:17:20.130" UserId="11124" />
  <row Id="7185" PostId="6726" Score="1" Text="Context matters. I would suspect a bug or misunderstanding in your script (e.g. a mistake when constructing features of test data, or using the wrong/untrained model when predicting). If it's a 101 competition,  you may be better off asking in the Kaggle forum for that competition, or in the Getting Started forum there. To get answers here, you may need to show some of your code and explain it before someone could offer advice." CreationDate="2015-08-08T09:33:50.633" UserId="836" />
  <row Id="7186" PostId="6693" Score="0" Text="There may be better implementations, and your support may be set too high. Also, there are plenty of alternative algorithms..." CreationDate="2015-08-08T13:19:08.697" UserId="924" />
  <row Id="7187" PostId="6729" Score="0" Text="thank you Tasos , i'm still a newbie for python . if you can please support me with the code ." CreationDate="2015-08-08T13:27:55.673" UserId="9035" />
  <row Id="7188" PostId="6729" Score="0" Text="@Miller Check my edited answer." CreationDate="2015-08-08T14:24:14.633" UserId="201" />
  <row Id="7189" PostId="6728" Score="0" Text="I'd suggest you plot the clusters at least to get an idea of how good they are. [This](http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) is a good one for hierarchical clustering, with code examples of nice plots in the links, although unfortunately is in python. Hope it helps anyway." CreationDate="2015-08-08T19:02:26.327" UserId="9584" />
  <row Id="7190" PostId="6726" Score="0" Text="Thank you. I have some things to look at as a result of posting there and I may come back here if I have more questions, The code itself is very simple. I think It's either the formatting of the data or the options I am using that are at issue here" CreationDate="2015-08-08T20:41:09.967" UserId="11146" />
  <row Id="7191" PostId="6728" Score="0" Text="Thank you for your answer Irnzcig. I have already plotted dendrograms for all the agglomeration methods. I'm looking for a method like Silhouette, where I can get a measure for the goodness" CreationDate="2015-08-08T22:51:20.533" UserId="11063" />
  <row Id="7192" PostId="6728" Score="0" Text="You're welcome. I was trying to say that you have plots similar to Silhouette in the links coming from the link I posted, and in particular [this one](http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#example-cluster-plot-agglomerative-clustering-metrics-py). Maybe it was not so easy to find, and it is in python, but if you take a look to the `for loop` that plots the distances you might get an idea. Hope it is more helpful this time." CreationDate="2015-08-09T11:44:17.933" UserId="9584" />
  <row Id="7193" PostId="5302" Score="0" Text="What software can you use to solve this? Are you limited to SAS?" CreationDate="2015-08-09T13:30:35.230" UserId="8021" />
  <row Id="7194" PostId="1128" Score="0" Text="It is interesting that time series form an abelian group. I am looking for an example of binary time series that form a group. Let A be the set of all binary time series. (for example, all spike trains between time t_1 and t_2) I am looking for an operation *, such that (A,*) form a group. Is the set of all spike trains form an abelian group?" CreationDate="2015-08-09T10:44:16.053" UserId="12157" />
  <row Id="7195" PostId="6734" Score="0" Text="This may be more of a math question, not a data science question." CreationDate="2015-08-09T15:53:24.803" UserId="21" />
  <row Id="7196" PostId="6704" Score="0" Text="Thank you very much for the answer. I implemented this with 2 methods WavDistance and diss.DWT but I got different values (I assume both these methods does the same thong where diss.DWT in TSclust is a wrapper for waveDistance in TSDist). The issue was my data was in columns. Once I took the transpose it was fine. Sorry about the delayed reply." CreationDate="2015-08-10T01:23:55.903" UserId="11063" />
  <row Id="7197" PostId="6726" Score="1" Text="What happens when you use `predict_proba()`? If &quot;1&quot; is a rare outcome then it's pretty likely that your predictions are different but all less than 0.5. If that's the case then you can do a number of things, such as downsampling your training set or using the `class_weights` argument when building your RF." CreationDate="2015-08-10T01:39:32.667" UserId="947" />
  <row Id="7198" PostId="6734" Score="0" Text="@SeanOwen: I agree - this question should be migrated to _Math.SE_." CreationDate="2015-08-10T04:37:40.320" UserId="2452" />
  <row Id="7199" PostId="6735" Score="1" Text="I suggest you to read about [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)." CreationDate="2015-08-10T04:50:12.297" UserId="2452" />
  <row Id="7200" PostId="6707" Score="0" Text="how do I set the threshold" CreationDate="2015-08-10T05:08:08.020" UserId="8013" />
  <row Id="7201" PostId="6735" Score="0" Text="What type of analysis do you intend to apply to the variables, this will affect the answer, as will the size of your data set. In some circumstance highly correlated variables may contain useful discriminative information. Ultimately run your analysis with the variable removed and with it left in and see how the results differ." CreationDate="2015-08-10T08:24:15.340" UserId="7980" />
  <row Id="7202" PostId="6742" Score="0" Text="This does sound a little like a &quot;fix my code&quot; problem, you might get more responses if you narrowed down the problem a little and perhaps provided a smaller example that demonstrates the unexpected behaviour. :)" CreationDate="2015-08-10T10:00:32.480" UserId="7980" />
  <row Id="7203" PostId="6742" Score="0" Text="i have described it, i provided the code for better understanding of what i had done, i wanted to know when to stop updating values of the states" CreationDate="2015-08-10T10:33:31.727" UserId="8013" />
  <row Id="7204" PostId="6735" Score="0" Text="@AleksandrBlekh Thanks for your suggestion" CreationDate="2015-08-10T10:33:59.843" UserId="9793" />
  <row Id="7205" PostId="6735" Score="0" Text="@image_doctor - Thank you very for your advice. My intention of doing chisquare test is to check for the collinearity between the variables. I got confused between the purposes of collinearity and chisquare test. Hence i raised the question. Thanks again." CreationDate="2015-08-10T10:36:31.123" UserId="9793" />
  <row Id="7206" PostId="6742" Score="0" Text="If you have provided everything that you feel someone needs to solve the problem easily, I&quot;m sure you will get a solution very quickly." CreationDate="2015-08-10T10:37:26.767" UserId="7980" />
  <row Id="7207" PostId="6735" Score="0" Text="@Arun: You're welcome." CreationDate="2015-08-10T10:49:34.543" UserId="2452" />
  <row Id="7208" PostId="6748" Score="0" Text="I tried to plot it as heat map, but it looks different. Could you please give me any hits to start? Thank you." CreationDate="2015-08-10T18:46:09.120" UserId="12177" />
  <row Id="7209" PostId="6707" Score="0" Text="Make some test to what is best for you. Typically 0 is the optimal solution. That means that there is no better solution than this one. Since it's an hyperparam, you can learn it via a neural network." CreationDate="2015-08-10T20:55:57.640" UserId="11113" />
  <row Id="7210" PostId="6749" Score="0" Text="Yes, it is quite straightforward. What have you tried so far? Do you have code to work with?" CreationDate="2015-08-10T22:03:51.397" UserId="947" />
  <row Id="7211" PostId="6748" Score="0" Text="IMHO, this question fits the _Cross Validated_ SE site the best. I suggest to consider migrating it there or the _DSP_ SE site, as @maj noted." CreationDate="2015-08-11T03:10:14.830" UserId="2452" />
  <row Id="7212" PostId="6749" Score="0" Text="I suggest migrating this question to _StackOverflow_, as this is a purely R programming question (_data visualization_)." CreationDate="2015-08-11T03:28:42.997" UserId="2452" />
  <row Id="7213" PostId="6707" Score="0" Text="@Dref360 i want to learn it via dynamic programming , I dont want to learn it via neural," CreationDate="2015-08-11T03:53:12.397" UserId="8013" />
  <row Id="7214" PostId="6707" Score="0" Text="@Dref360 what is hyperparam, i googled, i got the term hyperparameter, i that the short form of hyperparam ?" CreationDate="2015-08-11T03:54:34.170" UserId="8013" />
  <row Id="7215" PostId="6707" Score="0" Text="@Dref360 can I stop learning when I notice no new updation in any of the states ??" CreationDate="2015-08-11T04:19:40.790" UserId="8013" />
  <row Id="7216" PostId="6673" Score="0" Text="Chrisfs, this is actually a straw man argument.. Re-read what I wrote, &quot;To be successful, a PhD is not necessary, but a Masters degree most certainly is&quot; and compare it to what you posited. They are quite different." CreationDate="2015-08-11T06:32:47.547" UserId="11054" />
  <row Id="7217" PostId="6748" Score="0" Text="ok. I will do that, thank you" CreationDate="2015-08-11T12:57:16.717" UserId="12177" />
  <row Id="7218" PostId="6760" Score="0" Text="What kind of topic model are you using? Have you tried varying the number of topics?" CreationDate="2015-08-11T15:03:03.807" UserId="11136" />
  <row Id="7220" PostId="6760" Score="0" Text="What sort of analysis are you doing? Is it supervised? Is it exploratory? The issue may not even be the topics themselves but the data you are feeding into forming topics." CreationDate="2015-08-11T17:11:06.830" UserId="947" />
  <row Id="7221" PostId="6638" Score="0" Text="That's true, though on the other hand I think it's good to use &quot;meaningful&quot; indicators of orientation. Calculating a hash would probably be unique, but two very similar states would be mapped completely differently." CreationDate="2015-08-11T19:39:57.133" UserId="10907" />
  <row Id="7222" PostId="6707" Score="0" Text="@Rishika HyperParam == HyperParameter for exemple in neural network : number of layer, number of hidden neuron. Yes you can stop learning when there is not update in the state. That mean there is no better solution." CreationDate="2015-08-11T21:19:23.810" UserId="11113" />
  <row Id="7223" PostId="6766" Score="0" Text="You will probably want to look at 10-fold cross validation, not a single 75/25% hold out,  to assess the accuracy of your model. Then you can train your final model on all the available data before deployment." CreationDate="2015-08-11T22:21:13.427" UserId="7980" />
  <row Id="7224" PostId="6756" Score="0" Text="Interesting answer! That's an excellent idea.Thanks" CreationDate="2015-08-11T22:39:03.580" UserId="8037" />
  <row Id="7226" PostId="6767" Score="0" Text="Thank you! So if I have this right, my probability for a white male, age 45-64, for 2010-2012 would be &#xA;$$ &#xA;P = {7.0 * 12.0 * 6.7 \over 7.3^2}/1000 = 0.01056108087 &#xA;$$&#xA;Meaning there is about a 1.056% chance of diagnosis for an individual with that profile." CreationDate="2015-08-11T23:47:55.327" UserId="12203" />
  <row Id="7228" PostId="6767" Score="0" Text="Almost. You forgot the normalization step. You need to also compute the value for not getting diabetes:&#xA;$$ \frac{993.0 * 988.0 * 993.3}{992.7^2}/1000 = 0.98889591936 $$&#xA;And then normalize so that these two numbers add to one (as probabilities should):&#xA;$$ P = \frac{0.01056108087}{0.01056108087 + 0.9888959194} = 0.01056681865 $$&#xA;For a **1.057%** chance. With these particular sets of numbers, the normalization step doesn't make much of a difference. With other numbers, though, leaving off the normalization step can lead to rather nonsensical results." CreationDate="2015-08-12T00:33:20.407" UserId="11142" />
  <row Id="7229" PostId="6707" Score="0" Text="okay, got it :)" CreationDate="2015-08-12T03:45:20.677" UserId="8013" />
  <row Id="7230" PostId="6768" Score="1" Text="What did the data look like in 2 or 3 dimensions after you applied PCA, were there noticeable clusters ? What examples are being misclassified, is there a pattern ?" CreationDate="2015-08-12T07:04:40.567" UserId="7980" />
  <row Id="7231" PostId="6768" Score="0" Text="What do the power spectra of the traces look like?  if you plot the mean spectra for each class, do they look different, if so how and can you optimise a classifier to capture that difference ?" CreationDate="2015-08-12T07:12:40.623" UserId="7980" />
  <row Id="7232" PostId="6748" Score="0" Text="Tough one -- I'd consider 'how to plot in R' on-topic for this SE, although then the question should contain more detail about what is being plotted and what's been tried so far." CreationDate="2015-08-12T09:32:36.350" UserId="21" />
  <row Id="7233" PostId="6753" Score="0" Text="Did you mean `attach(try1)`? The `data` function gets data sets from packages. No idea where the `Error in names(StCol)` is coming from, you don't give us any clues. Run one line at a time and tell is where it fails. And  you've got `tyr1.seq` instead of `try1.seq` in your `pstree` line. Read code carefully." CreationDate="2015-08-12T10:20:07.640" UserId="471" />
  <row Id="7235" PostId="6774" Score="0" Text="Have you tried multinominal? It gives you finite states which is what you want." CreationDate="2015-08-12T14:38:08.227" UserId="9123" />
  <row Id="7236" PostId="6775" Score="0" Text="The most likely answer is yes, though if can you clarify what you mean as &quot;just as an image&quot; and &quot;object recognition&quot; ( segmentation + classification ? ) the differences will become clearer." CreationDate="2015-08-12T15:15:48.667" UserId="7980" />
  <row Id="7237" PostId="6775" Score="0" Text="Like counting people in a cam video and counting people in image. Is there anything thing I should consider for video?" CreationDate="2015-08-12T15:25:54.070" UserId="11141" />
  <row Id="7238" PostId="6775" Score="0" Text="For video you have a time component, so methods that involve tracking as a component would become relevant. You might develop a model of human movement which would help increase accuracy with video sequences. Depending on frame rate and object velocity, deformation or blurring of the object in the video frame may be an issue or not." CreationDate="2015-08-12T16:07:36.460" UserId="7980" />
  <row Id="7239" PostId="6768" Score="0" Text="1) Can you show us the PCA cluster plot?, 2) Have you tried decision trees? If the original features are somewhat human-scrutinizable, you might be able to make sense of where it is going wrong. Otherwise (barring some silly bug on your part) it would seem your features are simply not discriminative enough." CreationDate="2015-08-12T16:20:58.547" UserId="10587" />
  <row Id="7240" PostId="6773" Score="0" Text="Just use `table(Data$ID)` or `as.data.frame(table(Data$ID))` if you want a `data.frame` back." CreationDate="2015-08-13T06:12:13.583" UserId="8479" />
  <row Id="7241" PostId="6783" Score="0" Text="Interestingly combining the binning and the bearing/distance has increased my overall accuracy to 0.82." CreationDate="2015-08-13T07:54:57.957" UserId="10998" />
  <row Id="7242" PostId="6783" Score="0" Text="Is the price your target variable or a feature ?" CreationDate="2015-08-13T09:14:27.173" UserId="7980" />
  <row Id="7243" PostId="6783" Score="0" Text="Price is my label I am attempting to classify. In my training data I have binned it into $50 buckets" CreationDate="2015-08-13T09:17:52.380" UserId="10998" />
  <row Id="7244" PostId="6783" Score="0" Text="The geospatial element definitely has an impact on the prices that are returned - so I can't ignore it at all." CreationDate="2015-08-13T10:29:14.940" UserId="10998" />
  <row Id="7245" PostId="6766" Score="0" Text="Are you able to provide a sample of the data so that we can see if our suggestion works?" CreationDate="2015-08-13T10:37:19.900" UserId="10814" />
  <row Id="7246" PostId="6788" Score="0" Text="Well there is a valid use as a visualisation/introspection tool to find patterns that your network has learned. In this link http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html see the discussion about the dumbbell classifier. Not sure if that counts as a true purpose, since it is self-referential" CreationDate="2015-08-13T15:48:05.083" UserId="836" />
  <row Id="7247" PostId="6788" Score="0" Text="I think that's what makes the question difficult to answer, there are things behind deep dream that are widely applicable depending on how far you go, but is it still considered &quot;an application of deep dream&quot;? To me, it seems like deep dream is using an application of *those* techniques - which is what is applicable elsewhere. But I can't view that link at the moment so maybe I am incorrect." CreationDate="2015-08-13T16:02:10.147" UserId="12222" />
  <row Id="7248" PostId="6760" Score="0" Text="@David,@NBratley I updated my question please see above" CreationDate="2015-08-13T19:35:47.453" UserId="12240" />
  <row Id="7249" PostId="6786" Score="1" Text="Assuming you do have a set of weights for each component, why not use a metric like $d(x,y)=\sqrt{\sum_{i=1}^nw_i(x_i-y_i)^2}$ to figure out the closest neighbors?" CreationDate="2015-08-13T20:27:14.670" UserId="12241" />
  <row Id="7250" PostId="6786" Score="1" Text="Have you considered scaling your data before applying K-nerarest neighbours ?" CreationDate="2015-08-14T00:40:32.040" UserId="7980" />
  <row Id="7251" PostId="6786" Score="0" Text="@AlexR. Will using a custom metric as you suggested still work for knn search using kd-trees?" CreationDate="2015-08-14T06:57:35.167" UserId="226" />
  <row Id="7252" PostId="6789" Score="0" Text="11% accuracy is same as random guessing, or simply guessing same value each time. I cannot see any obvious bug. What is size of training set, and have you looked at your learning curve (the `J` values over time)? It is definitely worth checking your expansion of `y` into `Y` is correct - it looks over-complex, although could well be correct, I cannot tell. I might have instead just do something like `Y[y[i],i] =1;` for simplicity." CreationDate="2015-08-14T07:46:38.750" UserId="836" />
  <row Id="7253" PostId="6789" Score="0" Text="Have you verified that back propagation is working as you might expect on a small network and simple problem like XOR ?" CreationDate="2015-08-14T08:13:22.453" UserId="7980" />
  <row Id="7254" PostId="6787" Score="0" Text="Wonders in what context they meant that, regression, linearly separable data ?" CreationDate="2015-08-14T10:52:51.787" UserId="7980" />
  <row Id="7255" PostId="6766" Score="0" Text="Edmund, would you like me to upload a sample of the data set?  What's the best way to upload a file to this StackExchange?" CreationDate="2015-08-14T13:26:10.487" UserId="12202" />
  <row Id="7256" PostId="6760" Score="0" Text="How are you presenting the data to the model? Are you aggregating it at all? Traditional topic modeling on extremely short documents is difficult to get information out of." CreationDate="2015-08-14T14:40:09.230" UserId="11136" />
  <row Id="7257" PostId="6789" Score="0" Text="I've just ran your code with 10 iterations and got 70.58% of correct results on the training set. Can you repeat your experiment? Also, tracking error after each iteration (essentially, filling up your `J` array) may help to debug the issue if any." CreationDate="2015-08-14T14:55:18.163" UserId="1279" />
  <row Id="7258" PostId="6766" Score="0" Text="@Nick I suggest making a public link using Google Drive, or Drop Box, or some other cloud storage service and then share the link here." CreationDate="2015-08-14T16:57:33.930" UserId="10814" />
  <row Id="7259" PostId="6766" Score="0" Text="@Edmund, here is the link to a sample set of our data:&#xA;[Sample Claims Data](https://drive.google.com/file/d/0Bzg7bDcyZobbR0pfWkhWMGVuM0E/view?usp=sharing)" CreationDate="2015-08-14T19:38:13.143" UserId="12202" />
  <row Id="7261" PostId="6787" Score="0" Text="They probably meant the boundary between classes; is it composed of hyperplanes or not." CreationDate="2015-08-14T22:53:54.833" UserId="381" />
  <row Id="7262" PostId="6786" Score="0" Text="If you **want** to weight one dimension higher than others then I suggest you standardize all of your data so that the mean is zero and the standard deviation is one.  Then you can multiply the less important dimensions by a factor (2-10) so that they appear farther away to the KNN distance metric and leave the most important dimension un-scaled. Note that both standardizing and scaling are completely reversible processes, so there is very little reason not to use this simple solution." CreationDate="2015-08-14T23:29:10.197" UserId="9420" />
  <row Id="7263" PostId="6766" Score="0" Text="@Nick I could not figure this out in R. However, I did get it to work in Mathematica using its [Classify function](https://reference.wolfram.com/language/ref/Classify.html). But there is not enough data to meaningfully classify all the pairs. This is because there are 427 entries once the data is cleaned but there are 136 distinct {action, part} pairs. {replaced,console} repeats the most at 41 times. 3 other pairs are between 26 and 11. The rest, less than 10 with 65 pairs at 1 occurrence. I would need a larger sample than the 500 provided. I can post the code here if you like. But its not R." CreationDate="2015-08-14T23:57:09.173" UserId="10814" />
  <row Id="7264" PostId="6789" Score="0" Text="@friend You are right. It must be an after effects of late night coding that I mentioned 11% error. After running, 20 iterations the accuracy is 61.133333333333326 and the cost array looks like:&#xA;`20x1 Array{Float64,2}:&#xA; 4.14202&#xA; 4.65382&#xA; 4.02049&#xA; 4.57622&#xA; 4.55148&#xA; 5.61416&#xA; 5.51633&#xA; 4.70868&#xA; 4.68755&#xA; 5.10752&#xA; 4.79347&#xA; 5.51952&#xA; 5.05628&#xA; 5.04076&#xA; 5.07781&#xA; 4.9929 &#xA; 5.01385&#xA; 4.80254&#xA; 5.20314&#xA; 4.9887 `" CreationDate="2015-08-15T07:04:22.187" UserId="12250" />
  <row Id="7265" PostId="6789" Score="0" Text="@friend I am using cross entropy cost function:&#xA;`lambda = 1; &#xA;function costFunction(truth, prediction)&#xA;    cost = (-truth.*log(prediction)) - ((1-truth).*log(1-prediction));&#xA;    regularization = (lambda/(2*m))*(sum(sum(Theta1[2:end,:].^2)) + sum(sum(Theta2[2:end,:].^2)));&#xA;&#xA;    return (1/m)*sum(sum(cost)) + regularization; # regularized cost&#xA;end`" CreationDate="2015-08-15T07:16:45.673" UserId="12250" />
  <row Id="7266" PostId="6798" Score="0" Text="I don't know of a comprehensive list, but for now Kaggle has an open NLP competition. They tend to appear regularly on their website." CreationDate="2015-08-15T08:36:16.970" UserId="10517" />
  <row Id="7267" PostId="6789" Score="0" Text="Just to clarify, in every iteration I am using all the training inputs to modify the weight parameters. There is something I am missing which is not letting the cost to decrease with every iteration." CreationDate="2015-08-15T16:27:01.000" UserId="12250" />
</comments>