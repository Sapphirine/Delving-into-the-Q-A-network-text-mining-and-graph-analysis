kappa cohen kappa score inter rater agreement commonli use metric evalu perform machin learn algorithm human annotat particularli deal text linguist compar level agreement output human algorithm annotat ground truth label level agreement would occur random chanc good overview calcul kappa use evalu classifi stat stackexchang com answer depth explan kappa interpret paper entitl understand interobserv agreement kappa statist viera garrett 2005 benefit use kappa particularli unbalanc data set like 90 10 imbal class achiev 90 accuraci simpli label data point label commonli occur class kappa statist describ well classifi perform baselin level perform kappa rang 1 1 0 indic agreement rater 1 indic perfect agreement neg number indic systemat disagr interpret somewhat arbitrari task depend landi koch 1977 defin follow interpret system work gener rule thumb kappa agreement 0 less chanc agreement0 01 0 20 slight agreement0 21 0 40 fair agreement0 41 0 60 moder agreement0 61 0 80 substanti agreement0 81 0 99 almost perfect agreementwhich would indic algorithm perform moder well accuraci sd kappa sd respect standard deviat accuraci kappa score hope help !