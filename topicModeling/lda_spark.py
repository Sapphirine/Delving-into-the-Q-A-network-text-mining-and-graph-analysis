# -*- coding: utf-8 -*-
from pyspark import SparkContext
from pyspark.mllib.clustering import LDA, LDAModel
from pyspark.mllib.linalg import Vectors
from pyspark.sql import SQLContext, Row
sc = SparkContext()
# input file is a term-document matrix, which is generated by make_tdm.py
data = sc.textFile("/Users/Zhen/Desktop/Courses/BigData/stackexchange/topicModeling/result/matrix.csv") 
header = data.first() #extract header
data = data.filter(lambda x:x !=header)
data = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(',')]))


# Index documents with unique IDs
corpus = data.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()

# Cluster the documents into k topics using LDA
ldaModel = LDA.train(corpus, k=30)  

# Output topics. Each is a distribution over words (matching word count vectors)
print("Learned topics (as distributions over vocab of " + str(ldaModel.vocabSize()) + " words):")
topics = ldaModel.topicsMatrix()
# for topic in range(3):
#     print("Topic " + str(topic) + ":")
#     for word in range(0, ldaModel.vocabSize()):
#         print(" " + str(topics[word]))

import numpy
numpy.savetxt("/Users/Zhen/Desktop/Courses/BigData/stackexchange/topicModeling/result/lda_topicMatrix.csv", topics, delimiter=",")
